{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import h5py\n",
    "import random\n",
    "slim=tf.contrib.slim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[18.08,  2.49],\n",
       "       [18.14,  2.48]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f=h5py.File('D:/data/wind/fitt.h5','r')\n",
    "data_x=f['x']\n",
    "data_y=f['y']\n",
    "data_x[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#读取数据\n",
    "f=h5py.File('D:/data/wind/fitt.h5','r')\n",
    "data_x=f['x']\n",
    "data_y=f['y']\n",
    "\n",
    "\n",
    "#分配训练测试集\n",
    "num=list(range(data_x.shape[0]))\n",
    "num1=random.sample(num,1000)\n",
    "num2=set(num)-set(num1)\n",
    "num2=list(num2)\n",
    "num1.sort()\n",
    "\n",
    "\n",
    "x_train=data_x[num2]\n",
    "y_train=data_y[num2]\n",
    "\n",
    "x_test=data_x[num1]\n",
    "y_test=data_y[num1]\n",
    "#生成批次\n",
    "train_queue = tf.train.slice_input_producer([x_train,y_train],shuffle=None)\n",
    "val_queue = tf.train.slice_input_producer([x_test,y_test],shuffle=None)\n",
    "batch_xt,batch_yt=tf.train.shuffle_batch(train_queue,batch_size=16,capacity=500,min_after_dequeue=150)\n",
    "batch_xv,batch_yv=tf.train.shuffle_batch(val_queue,batch_size=16,capacity=500,min_after_dequeue=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 搭建模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 超参数设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optmizer: \n",
    "learning_rate = 0.007  # Small lr helps not to diverge during training. \n",
    "nb_iters = 10000  # How many times we perform a training step (therefore how many times we show a batch). \n",
    "lr_decay = 0.92  # default: 0.9 . Simulated annealing.\n",
    "momentum = 0.5  # default: 0.0 . Momentum technique in weights update\n",
    "lambda_l2_reg = 0.003  # L2 regularization of weights - avoids overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 搭建神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=tf.placeholder(tf.float32,[None,2])\n",
    "y=tf.placeholder(tf.float32,[None,1])\n",
    "with slim.arg_scope([slim.layers.fully_connected],\n",
    "                    weights_initializer=slim.xavier_initializer(),\n",
    "                    activation_fn=tf.nn.relu):\n",
    "    net=slim.fully_connected(x,4,scope='fc1')\n",
    "#     net=slim.fully_connected(net,8,scope='fc2')\n",
    "    net=slim.fully_connected(net,4,scope='fc3')\n",
    "    output=slim.fully_connected(net,1,activation_fn=None,scope='fc4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 搭建损失函数 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loss and optimizer\n",
    "\n",
    "with tf.variable_scope('Loss'):\n",
    "    # L2 loss\n",
    "   \n",
    "    output_loss = tf.reduce_mean(tf.nn.l2_loss(output - y))\n",
    "        \n",
    "    # L2 regularization (to avoid overfitting and to have a  better generalization capacity)\n",
    "    reg_loss = 0\n",
    "    for tf_var in tf.trainable_variables():\n",
    "        if not (\"Bias\" in tf_var.name or \"Output_\" in tf_var.name):\n",
    "            reg_loss += tf.reduce_mean(tf.nn.l2_loss(tf_var))\n",
    "            \n",
    "    loss = output_loss + lambda_l2_reg * reg_loss\n",
    "    tf.summary.scalar('loss',loss)\n",
    "\n",
    "with tf.variable_scope('acc'):\n",
    "    acc = tf.reduce_mean(tf.nn.l2_loss(y-output))/tf.reduce_mean(tf.nn.l2_loss(y-0))\n",
    "    \n",
    "    acc=1-acc\n",
    "    tf.summary.scalar('acc',acc)\n",
    "\n",
    "with tf.variable_scope('Optimizer'):\n",
    "    optimizer = tf.train.RMSPropOptimizer(learning_rate, decay=lr_decay, momentum=momentum)\n",
    "    train_op = optimizer.minimize(loss)\n",
    "    merged_summary = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0\n",
      "train: loss: 3380332.5 acc: 0.0  val: loss: 8976114.0 acc: 3.4749507904052734e-05\n",
      "step: 5\n",
      "train: loss: 7107475.5 acc: 0.0002345442771911621  val: loss: 5024643.0 acc: 0.0002886652946472168\n",
      "step: 10\n",
      "train: loss: 3517692.0 acc: 0.0005472898483276367  val: loss: 4183223.0 acc: 0.0005928277969360352\n",
      "step: 15\n",
      "train: loss: 3611249.5 acc: 0.0007811188697814941  val: loss: 3025991.5 acc: 0.0009829401969909668\n",
      "step: 20\n",
      "train: loss: 6083501.0 acc: 0.0008274316787719727  val: loss: 5969022.0 acc: 0.0006934404373168945\n",
      "step: 25\n",
      "train: loss: 6104327.0 acc: 0.0009404420852661133  val: loss: 7096881.0 acc: 0.0007951855659484863\n",
      "step: 30\n",
      "train: loss: 5970197.5 acc: 0.0009676814079284668  val: loss: 6694899.5 acc: 0.0008764863014221191\n",
      "step: 35\n",
      "train: loss: 5052377.5 acc: 0.0012707710266113281  val: loss: 8711819.0 acc: 0.0009052157402038574\n",
      "step: 40\n",
      "train: loss: 12240496.0 acc: 0.0008459687232971191  val: loss: 8622820.0 acc: 0.0009105801582336426\n",
      "step: 45\n",
      "train: loss: 13268662.0 acc: 0.001018524169921875  val: loss: 7417365.5 acc: 0.0014351606369018555\n",
      "step: 50\n",
      "train: loss: 13258836.0 acc: 0.0010704398155212402  val: loss: 3951366.5 acc: 0.002061605453491211\n",
      "step: 55\n",
      "train: loss: 4427985.0 acc: 0.001574873924255371  val: loss: 9401774.0 acc: 0.0012212395668029785\n",
      "step: 60\n",
      "train: loss: 22150156.0 acc: 0.0008718371391296387  val: loss: 8601506.0 acc: 0.0016472339630126953\n",
      "step: 65\n",
      "train: loss: 32361228.0 acc: 0.0009405016899108887  val: loss: 8299815.0 acc: 0.0015262365341186523\n",
      "step: 70\n",
      "train: loss: 28394148.0 acc: 0.001174628734588623  val: loss: 5284682.0 acc: 0.0019414424896240234\n",
      "step: 75\n",
      "train: loss: 19462788.0 acc: 0.001497030258178711  val: loss: 7073613.0 acc: 0.0022696852684020996\n",
      "step: 80\n",
      "train: loss: 14496952.0 acc: 0.001626133918762207  val: loss: 5141750.5 acc: 0.002420365810394287\n",
      "step: 85\n",
      "train: loss: 8022837.0 acc: 0.0023540258407592773  val: loss: 7320539.0 acc: 0.0021349191665649414\n",
      "step: 90\n",
      "train: loss: 4281728.5 acc: 0.003428041934967041  val: loss: 3455880.25 acc: 0.003661215305328369\n",
      "step: 95\n",
      "train: loss: 3997321.0 acc: 0.003708183765411377  val: loss: 5469915.5 acc: 0.002453625202178955\n",
      "step: 100\n",
      "train: loss: 2273372.5 acc: 0.005130946636199951  val: loss: 12210940.0 acc: 0.001738131046295166\n",
      "step: 105\n",
      "train: loss: 4629582.0 acc: 0.0032345056533813477  val: loss: 4680183.0 acc: 0.0031660795211791992\n",
      "step: 110\n",
      "train: loss: 1695938.375 acc: 0.005766928195953369  val: loss: 8182466.0 acc: 0.0023305416107177734\n",
      "step: 115\n",
      "train: loss: 1403723.75 acc: 0.006827175617218018  val: loss: 1937111.5 acc: 0.004842162132263184\n",
      "step: 120\n",
      "train: loss: 3193701.75 acc: 0.004538416862487793  val: loss: 6322254.0 acc: 0.002895534038543701\n",
      "step: 125\n",
      "train: loss: 1826918.25 acc: 0.006703436374664307  val: loss: 2567166.75 acc: 0.005005180835723877\n",
      "step: 130\n",
      "train: loss: 2623769.5 acc: 0.005739092826843262  val: loss: 4546177.0 acc: 0.0037429332733154297\n",
      "step: 135\n",
      "train: loss: 1659201.5 acc: 0.007811129093170166  val: loss: 3261455.5 acc: 0.004453063011169434\n",
      "step: 140\n",
      "train: loss: 1713041.375 acc: 0.008034884929656982  val: loss: 1990488.125 acc: 0.005821704864501953\n",
      "step: 145\n",
      "train: loss: 1196818.375 acc: 0.010264098644256592  val: loss: 7897122.0 acc: 0.002882659435272217\n",
      "step: 150\n",
      "train: loss: 1410865.75 acc: 0.00980287790298462  val: loss: 6274588.0 acc: 0.003345668315887451\n",
      "step: 155\n",
      "train: loss: 1313238.25 acc: 0.009656012058258057  val: loss: 2486178.75 acc: 0.006072878837585449\n",
      "step: 160\n",
      "train: loss: 1261595.75 acc: 0.011179327964782715  val: loss: 8924378.0 acc: 0.0027159452438354492\n",
      "step: 165\n",
      "train: loss: 1250097.875 acc: 0.011153876781463623  val: loss: 6045446.5 acc: 0.003544032573699951\n",
      "step: 170\n",
      "train: loss: 889350.625 acc: 0.013487100601196289  val: loss: 3388401.5 acc: 0.005777299404144287\n",
      "step: 175\n",
      "train: loss: 866527.75 acc: 0.01287078857421875  val: loss: 2632164.5 acc: 0.006285548210144043\n",
      "step: 180\n",
      "train: loss: 936077.6875 acc: 0.013024330139160156  val: loss: 8350432.0 acc: 0.003174424171447754\n",
      "step: 185\n",
      "train: loss: 1901033.875 acc: 0.00886547565460205  val: loss: 5441734.5 acc: 0.00496363639831543\n",
      "step: 190\n",
      "train: loss: 930566.3125 acc: 0.014776825904846191  val: loss: 4631136.5 acc: 0.0055348873138427734\n",
      "step: 195\n",
      "train: loss: 1216207.75 acc: 0.011984765529632568  val: loss: 2253995.0 acc: 0.008104145526885986\n",
      "step: 200\n",
      "train: loss: 1763880.875 acc: 0.008477628231048584  val: loss: 1289993.5 acc: 0.01156151294708252\n",
      "step: 205\n",
      "train: loss: 2731452.0 acc: 0.00742185115814209  val: loss: 5355829.0 acc: 0.005853891372680664\n",
      "step: 210\n",
      "train: loss: 6228247.0 acc: 0.005643486976623535  val: loss: 4211332.5 acc: 0.006287574768066406\n",
      "step: 215\n",
      "train: loss: 6808561.0 acc: 0.005432724952697754  val: loss: 2966523.0 acc: 0.007860779762268066\n",
      "step: 220\n",
      "train: loss: 12546264.0 acc: 0.004835188388824463  val: loss: 9317674.0 acc: 0.004952788352966309\n",
      "step: 225\n",
      "train: loss: 11333164.0 acc: 0.007748961448669434  val: loss: 3841561.5 acc: 0.01387178897857666\n",
      "step: 230\n",
      "train: loss: 5537032.5 acc: 0.017270803451538086  val: loss: 9723270.0 acc: 0.015353083610534668\n",
      "step: 235\n",
      "train: loss: 2508621.5 acc: 0.043415188789367676  val: loss: 8915534.0 acc: 0.030791044235229492\n",
      "step: 240\n",
      "train: loss: 11097223.0 acc: 0.038421571254730225  val: loss: 3243161.5 acc: 0.0584140419960022\n",
      "step: 245\n",
      "train: loss: 14404646.0 acc: 0.05817955732345581  val: loss: 7228096.0 acc: 0.06626677513122559\n",
      "step: 250\n",
      "train: loss: 10817130.0 acc: 0.0856063961982727  val: loss: 8364047.0 acc: 0.09232985973358154\n",
      "step: 255\n",
      "train: loss: 6064878.0 acc: 0.1247292160987854  val: loss: 8239748.0 acc: 0.14423775672912598\n",
      "step: 260\n",
      "train: loss: 3796553.25 acc: 0.15894603729248047  val: loss: 2293637.0 acc: 0.2454403042793274\n",
      "step: 265\n",
      "train: loss: 259928.4375 acc: 0.4248431324958801  val: loss: 11405805.0 acc: 0.15537726879119873\n",
      "step: 270\n",
      "train: loss: 3231131.25 acc: 0.19123899936676025  val: loss: 7217105.0 acc: 0.1691911816596985\n",
      "step: 275\n",
      "train: loss: 91040.5234375 acc: 0.6591401100158691  val: loss: 9090111.0 acc: 0.1677456498146057\n",
      "step: 280\n",
      "train: loss: 194060.234375 acc: 0.6361393928527832  val: loss: 1176859.0 acc: 0.38341236114501953\n",
      "step: 285\n",
      "train: loss: 305334.9375 acc: 0.5466611385345459  val: loss: 1168702.625 acc: 0.3981871008872986\n",
      "step: 290\n",
      "train: loss: 41336.984375 acc: 0.8178679347038269  val: loss: 4379990.5 acc: 0.2764607071876526\n",
      "step: 295\n",
      "train: loss: 270599.1875 acc: 0.5797154903411865  val: loss: 8894020.0 acc: 0.24152785539627075\n",
      "step: 300\n",
      "train: loss: 102978.25 acc: 0.7364280819892883  val: loss: 1814301.75 acc: 0.36464136838912964\n",
      "step: 305\n",
      "train: loss: 299081.6875 acc: 0.6063390970230103  val: loss: 3467580.0 acc: 0.31541186571121216\n",
      "step: 310\n",
      "train: loss: 1667621.25 acc: 0.372738778591156  val: loss: 3239754.5 acc: 0.30101579427719116\n",
      "step: 315\n",
      "train: loss: 893822.3125 acc: 0.5541809797286987  val: loss: 7175683.0 acc: 0.3458004593849182\n",
      "step: 320\n",
      "train: loss: 954196.3125 acc: 0.5770422220230103  val: loss: 1002140.8125 acc: 0.6196110248565674\n",
      "step: 325\n",
      "train: loss: 426663.5625 acc: 0.7432386875152588  val: loss: 438093.90625 acc: 0.7124669551849365\n",
      "step: 330\n",
      "train: loss: 612085.9375 acc: 0.5896418690681458  val: loss: 222218.890625 acc: 0.8463734984397888\n",
      "step: 335\n",
      "train: loss: 407965.0625 acc: 0.4828460216522217  val: loss: 1184726.875 acc: 0.5436390042304993\n",
      "step: 340\n",
      "train: loss: 602625.375 acc: 0.5807541012763977  val: loss: 170241.140625 acc: 0.868973970413208\n",
      "step: 345\n",
      "train: loss: 1155793.5 acc: 0.6265394687652588  val: loss: 514032.71875 acc: 0.6781522631645203\n",
      "step: 350\n",
      "train: loss: 891669.3125 acc: 0.6770782470703125  val: loss: 1441200.5 acc: 0.5830148458480835\n",
      "step: 355\n",
      "train: loss: 632292.625 acc: 0.7243579626083374  val: loss: 1953008.875 acc: 0.5540942549705505\n",
      "step: 360\n",
      "train: loss: 1300184.125 acc: 0.6677032113075256  val: loss: 2197051.5 acc: 0.5830948948860168\n",
      "step: 365\n",
      "train: loss: 1232253.875 acc: 0.6955361366271973  val: loss: 4966956.5 acc: 0.6087873578071594\n",
      "step: 370\n",
      "train: loss: 850873.4375 acc: 0.764060914516449  val: loss: 1155069.0 acc: 0.6863209009170532\n",
      "step: 375\n",
      "train: loss: 1205952.625 acc: 0.6727454662322998  val: loss: 1785370.375 acc: 0.650421142578125\n",
      "step: 380\n",
      "train: loss: 784270.4375 acc: 0.6600674986839294  val: loss: 1294699.875 acc: 0.55652916431427\n",
      "step: 385\n",
      "train: loss: 2617243.0 acc: 0.581954836845398  val: loss: 2076219.125 acc: 0.6293255090713501\n",
      "step: 390\n",
      "train: loss: 3301112.0 acc: 0.6475121974945068  val: loss: 3107726.5 acc: 0.5711761713027954\n",
      "step: 395\n",
      "train: loss: 2741706.5 acc: 0.6991699934005737  val: loss: 2610529.5 acc: 0.538677990436554\n",
      "step: 400\n",
      "train: loss: 2031085.625 acc: 0.650271475315094  val: loss: 2601319.0 acc: 0.6398447751998901\n",
      "step: 405\n",
      "train: loss: 2331877.5 acc: 0.5447171926498413  val: loss: 2300852.5 acc: -0.01124870777130127\n",
      "step: 410\n",
      "train: loss: 3659297.25 acc: 0.758402943611145  val: loss: 1987409.5 acc: 0.6109939813613892\n",
      "step: 415\n",
      "train: loss: 4340795.0 acc: 0.6615521907806396  val: loss: 3584286.25 acc: -0.08252823352813721\n",
      "step: 420\n",
      "train: loss: 5028421.0 acc: 0.42755961418151855  val: loss: 2118057.0 acc: 0.5552078485488892\n",
      "step: 425\n",
      "train: loss: 4618662.5 acc: 0.6477886438369751  val: loss: 2135752.0 acc: 0.6596052646636963\n",
      "step: 430\n",
      "train: loss: 7168072.0 acc: 0.7863361835479736  val: loss: 2942590.5 acc: -3.002376079559326\n",
      "step: 435\n",
      "train: loss: 2406530.5 acc: 0.9120819568634033  val: loss: 3159557.75 acc: -1.6856343746185303\n",
      "step: 440\n",
      "train: loss: 3591123.25 acc: 0.79326331615448  val: loss: 3184337.0 acc: 0.057123005390167236\n",
      "step: 445\n",
      "train: loss: 2536113.75 acc: 0.7650306820869446  val: loss: 3160792.0 acc: 0.28320711851119995\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 450\n",
      "train: loss: 1079340.5 acc: 0.8640819787979126  val: loss: 3351078.0 acc: 0.5082187652587891\n",
      "step: 455\n",
      "train: loss: 1473493.75 acc: 0.5906012058258057  val: loss: 2305908.0 acc: 0.5285784602165222\n",
      "step: 460\n",
      "train: loss: 1292206.375 acc: 0.4157140254974365  val: loss: 2192139.25 acc: 0.29166334867477417\n",
      "step: 465\n",
      "train: loss: 1512702.875 acc: 0.30429065227508545  val: loss: 1811217.25 acc: 0.3320707082748413\n",
      "step: 470\n",
      "train: loss: 1640997.625 acc: 0.5882787108421326  val: loss: 2628220.0 acc: 0.6147364974021912\n",
      "step: 475\n",
      "train: loss: 591869.9375 acc: 0.6984331607818604  val: loss: 1540693.25 acc: 0.4983922839164734\n",
      "step: 480\n",
      "train: loss: 760876.0625 acc: 0.6615423560142517  val: loss: 1173828.625 acc: 0.6351436376571655\n",
      "step: 485\n",
      "train: loss: 1024309.5 acc: 0.5301979780197144  val: loss: 1782009.0 acc: 0.5956850051879883\n",
      "step: 490\n",
      "train: loss: 806536.0625 acc: 0.5923933982849121  val: loss: 847700.875 acc: 0.4709100127220154\n",
      "step: 495\n",
      "train: loss: 377595.65625 acc: 0.7093868851661682  val: loss: 560842.125 acc: 0.7488278150558472\n",
      "step: 500\n",
      "train: loss: 178474.6875 acc: 0.8562275171279907  val: loss: 2186052.75 acc: 0.6789102554321289\n",
      "step: 505\n",
      "train: loss: 154649.015625 acc: 0.8662260174751282  val: loss: 2901243.5 acc: 0.5959254503250122\n",
      "step: 510\n",
      "train: loss: 116422.0234375 acc: 0.9259371757507324  val: loss: 1623364.625 acc: 0.7164838314056396\n",
      "step: 515\n",
      "train: loss: 75071.9375 acc: 0.9379358291625977  val: loss: 1461939.375 acc: 0.6722681522369385\n",
      "step: 520\n",
      "train: loss: 171395.46875 acc: 0.9028767347335815  val: loss: 2511445.25 acc: 0.5945465564727783\n",
      "step: 525\n",
      "train: loss: 311620.65625 acc: 0.8160510063171387  val: loss: 1116574.125 acc: 0.5809732675552368\n",
      "step: 530\n",
      "train: loss: 378023.8125 acc: 0.7754806876182556  val: loss: 5281180.0 acc: 0.5038034915924072\n",
      "step: 535\n",
      "train: loss: 145242.59375 acc: 0.8622432351112366  val: loss: 3388036.5 acc: 0.5661258697509766\n",
      "step: 540\n",
      "train: loss: 206893.75 acc: 0.8069137930870056  val: loss: 4189096.5 acc: 0.5294350385665894\n",
      "step: 545\n",
      "train: loss: 297830.125 acc: 0.8430546522140503  val: loss: 2323175.0 acc: 0.5941452980041504\n",
      "step: 550\n",
      "train: loss: 518004.8125 acc: 0.773257315158844  val: loss: 1142450.25 acc: 0.6431349515914917\n",
      "step: 555\n",
      "train: loss: 967360.6875 acc: 0.681057333946228  val: loss: 1522154.375 acc: 0.6406197547912598\n",
      "step: 560\n",
      "train: loss: 435534.28125 acc: 0.6273206472396851  val: loss: 241591.96875 acc: 0.686141848564148\n",
      "step: 565\n",
      "train: loss: 316292.625 acc: 0.8008070588111877  val: loss: 5181187.5 acc: 0.48261481523513794\n",
      "step: 570\n",
      "train: loss: 240092.015625 acc: 0.8662088513374329  val: loss: 3123900.0 acc: 0.5781617760658264\n",
      "step: 575\n",
      "train: loss: 2913479.75 acc: 0.5922074913978577  val: loss: 3252079.5 acc: 0.5863168239593506\n",
      "step: 580\n",
      "train: loss: 1172605.25 acc: 0.5391516089439392  val: loss: 969211.1875 acc: 0.7444993257522583\n",
      "step: 585\n",
      "train: loss: 3296122.0 acc: 0.7457976937294006  val: loss: 1462422.125 acc: 0.5603817105293274\n",
      "step: 590\n",
      "train: loss: 2167169.5 acc: 0.7851409912109375  val: loss: 2393193.5 acc: 0.7438857555389404\n",
      "step: 595\n",
      "train: loss: 1199187.625 acc: 0.6887401938438416  val: loss: 1034944.3125 acc: 0.7813325524330139\n",
      "step: 600\n",
      "train: loss: 1800029.625 acc: 0.7619722485542297  val: loss: 3262163.25 acc: 0.7182461023330688\n",
      "step: 605\n",
      "train: loss: 1419971.125 acc: 0.8327465057373047  val: loss: 2755770.5 acc: 0.7332226037979126\n",
      "step: 610\n",
      "train: loss: 1633400.625 acc: 0.8977159261703491  val: loss: 2367197.75 acc: 0.6377784013748169\n",
      "step: 615\n",
      "train: loss: 882608.125 acc: 0.9406017661094666  val: loss: 2047388.25 acc: 0.7000610828399658\n",
      "step: 620\n",
      "train: loss: 1447765.125 acc: 0.7628766298294067  val: loss: 2972935.0 acc: 0.69178307056427\n",
      "step: 625\n",
      "train: loss: 1772452.625 acc: 0.6159986853599548  val: loss: 2104460.5 acc: 0.3618740439414978\n",
      "step: 630\n",
      "train: loss: 2484952.5 acc: -0.09610986709594727  val: loss: 1566764.0 acc: 0.4833948612213135\n",
      "step: 635\n",
      "train: loss: 2143266.0 acc: -0.5522327423095703  val: loss: 1189652.375 acc: 0.5906519889831543\n",
      "step: 640\n",
      "train: loss: 1930358.5 acc: 0.2923622131347656  val: loss: 1709346.0 acc: 0.1333993673324585\n",
      "step: 645\n",
      "train: loss: 1129212.125 acc: -0.7697145938873291  val: loss: 766579.25 acc: 0.8138617277145386\n",
      "step: 650\n",
      "train: loss: 656136.5 acc: 0.16296786069869995  val: loss: 455383.59375 acc: 0.6652681827545166\n",
      "step: 655\n",
      "train: loss: 548171.3125 acc: -0.15841889381408691  val: loss: 639881.25 acc: 0.7002837657928467\n",
      "step: 660\n",
      "train: loss: 400443.40625 acc: 0.20318526029586792  val: loss: 849961.4375 acc: 0.6289474964141846\n",
      "step: 665\n",
      "train: loss: 290142.5625 acc: 0.30779409408569336  val: loss: 221658.421875 acc: 0.8258053064346313\n",
      "step: 670\n",
      "train: loss: 212469.671875 acc: 0.605798602104187  val: loss: 5777798.5 acc: 0.49062806367874146\n",
      "step: 675\n",
      "train: loss: 438221.40625 acc: 0.6873223781585693  val: loss: 4059707.75 acc: 0.5713579058647156\n",
      "step: 680\n",
      "train: loss: 250457.8125 acc: 0.7857359647750854  val: loss: 2004899.625 acc: 0.5970555543899536\n",
      "step: 685\n",
      "train: loss: 302026.90625 acc: 0.7866653203964233  val: loss: 201398.046875 acc: 0.8070096373558044\n",
      "step: 690\n",
      "train: loss: 265880.09375 acc: 0.8510773181915283  val: loss: 1583648.5 acc: 0.6472793817520142\n",
      "step: 695\n",
      "train: loss: 417735.65625 acc: 0.5144030451774597  val: loss: 4404733.5 acc: 0.5367854833602905\n",
      "step: 700\n",
      "train: loss: 418570.09375 acc: 0.7759792804718018  val: loss: 1021293.1875 acc: 0.6581734418869019\n",
      "step: 705\n",
      "train: loss: 248336.484375 acc: 0.6414991617202759  val: loss: 4221146.0 acc: 0.5228981971740723\n",
      "step: 710\n",
      "train: loss: 710348.375 acc: 0.6176576614379883  val: loss: 4494031.5 acc: 0.5092204809188843\n",
      "step: 715\n",
      "train: loss: 1038771.5625 acc: 0.7235303521156311  val: loss: 1518502.625 acc: 0.6865510940551758\n",
      "step: 720\n",
      "train: loss: 683012.1875 acc: 0.7578084468841553  val: loss: 1713455.875 acc: 0.6581345200538635\n",
      "step: 725\n",
      "train: loss: 944710.75 acc: 0.7958407402038574  val: loss: 1585753.625 acc: 0.6691572070121765\n",
      "step: 730\n",
      "train: loss: 529899.8125 acc: 0.7705050110816956  val: loss: 1196564.375 acc: 0.7156040668487549\n",
      "step: 735\n",
      "train: loss: 741669.625 acc: 0.8149420619010925  val: loss: 1328922.75 acc: 0.67859947681427\n",
      "step: 740\n",
      "train: loss: 280820.0 acc: 0.8248382210731506  val: loss: 2331868.0 acc: 0.6986857056617737\n",
      "step: 745\n",
      "train: loss: 920903.625 acc: 0.6618307828903198  val: loss: 741225.5 acc: 0.746511697769165\n",
      "step: 750\n",
      "train: loss: 2678219.25 acc: 0.6850470304489136  val: loss: 1660117.25 acc: 0.6216075420379639\n",
      "step: 755\n",
      "train: loss: 2342270.5 acc: 0.7386082410812378  val: loss: 1634947.5 acc: 0.5912706255912781\n",
      "step: 760\n",
      "train: loss: 1340383.375 acc: 0.8618156909942627  val: loss: 2374143.5 acc: 0.3580856919288635\n",
      "step: 765\n",
      "train: loss: 878859.5 acc: 0.8597993850708008  val: loss: 2044304.625 acc: 0.46795225143432617\n",
      "step: 770\n",
      "train: loss: 2854398.0 acc: -0.19141292572021484  val: loss: 1496118.75 acc: 0.8023561835289001\n",
      "step: 775\n",
      "train: loss: 2267463.0 acc: 0.8256599307060242  val: loss: 1490366.875 acc: 0.7557642459869385\n",
      "step: 780\n",
      "train: loss: 3620133.0 acc: 0.8156132698059082  val: loss: 2536385.5 acc: -1.5114221572875977\n",
      "step: 785\n",
      "train: loss: 3625581.75 acc: 0.6664938926696777  val: loss: 2173243.5 acc: 0.5677236318588257\n",
      "step: 790\n",
      "train: loss: 3898254.25 acc: 0.7655142545700073  val: loss: 1368414.0 acc: 0.6998602151870728\n",
      "step: 795\n",
      "train: loss: 3578549.5 acc: 0.8567265272140503  val: loss: 1415942.125 acc: 0.3729180693626404\n",
      "step: 800\n",
      "train: loss: 3742195.0 acc: 0.8969069719314575  val: loss: 2334998.25 acc: 0.2672421932220459\n",
      "step: 805\n",
      "train: loss: 2960653.25 acc: 0.85091233253479  val: loss: 1107086.0 acc: 0.8232696056365967\n",
      "step: 810\n",
      "train: loss: 1781412.75 acc: 0.9180489778518677  val: loss: 1406969.875 acc: 0.583394467830658\n",
      "step: 815\n",
      "train: loss: 1188635.5 acc: 0.8054077625274658  val: loss: 2616939.25 acc: 0.06785786151885986\n",
      "step: 820\n",
      "train: loss: 872599.125 acc: 0.9136601686477661  val: loss: 867899.25 acc: 0.7464873790740967\n",
      "step: 825\n",
      "train: loss: 619029.0 acc: 0.8788967728614807  val: loss: 1370007.625 acc: 0.7826647758483887\n",
      "step: 830\n",
      "train: loss: 709998.625 acc: 0.881251871585846  val: loss: 606603.75 acc: 0.7151122689247131\n",
      "step: 835\n",
      "train: loss: 949626.4375 acc: 0.4352940320968628  val: loss: 1649633.5 acc: 0.7463059425354004\n",
      "step: 840\n",
      "train: loss: 329342.5 acc: 0.7978512048721313  val: loss: 1874098.25 acc: 0.7286629676818848\n",
      "step: 845\n",
      "train: loss: 395835.4375 acc: 0.8588057160377502  val: loss: 1479593.875 acc: 0.7734118103981018\n",
      "step: 850\n",
      "train: loss: 221914.40625 acc: 0.9007011651992798  val: loss: 1361312.125 acc: 0.8451166152954102\n",
      "step: 855\n",
      "train: loss: 1311613.125 acc: 0.6167821884155273  val: loss: 4182005.75 acc: 0.6740331649780273\n",
      "step: 860\n",
      "train: loss: 564338.25 acc: 0.6486172676086426  val: loss: 795224.5625 acc: 0.6623538136482239\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 865\n",
      "train: loss: 731615.875 acc: 0.807420015335083  val: loss: 379060.71875 acc: 0.8010715842247009\n",
      "step: 870\n",
      "train: loss: 208550.5625 acc: 0.8559668064117432  val: loss: 781494.0 acc: 0.7622688412666321\n",
      "step: 875\n",
      "train: loss: 27859.34375 acc: 0.9757806658744812  val: loss: 2618828.5 acc: 0.6543935537338257\n",
      "step: 880\n",
      "train: loss: 16492.7421875 acc: 0.9869830012321472  val: loss: 1842820.625 acc: 0.7421854734420776\n",
      "step: 885\n",
      "train: loss: 121373.7734375 acc: 0.9113631248474121  val: loss: 940501.375 acc: 0.7492039203643799\n",
      "step: 890\n",
      "train: loss: 114505.421875 acc: 0.9274503588676453  val: loss: 601902.875 acc: 0.7746347188949585\n",
      "step: 895\n",
      "train: loss: 64437.71484375 acc: 0.9359728693962097  val: loss: 593616.625 acc: 0.776112973690033\n",
      "step: 900\n",
      "train: loss: 94868.6875 acc: 0.8979508280754089  val: loss: 4742634.5 acc: 0.6407012939453125\n",
      "step: 905\n",
      "train: loss: 59494.04296875 acc: 0.9455792307853699  val: loss: 1438736.0 acc: 0.7329958081245422\n",
      "step: 910\n",
      "train: loss: 236601.171875 acc: 0.7710732221603394  val: loss: 2069131.75 acc: 0.640788733959198\n",
      "step: 915\n",
      "train: loss: 274011.46875 acc: 0.835389256477356  val: loss: 1786919.875 acc: 0.7323083877563477\n",
      "step: 920\n",
      "train: loss: 680666.125 acc: 0.7497742176055908  val: loss: 593089.3125 acc: 0.7908132076263428\n",
      "step: 925\n",
      "train: loss: 229487.375 acc: 0.7774134278297424  val: loss: 2226618.5 acc: 0.670791506767273\n",
      "step: 930\n",
      "train: loss: 867508.125 acc: 0.718491792678833  val: loss: 2335755.0 acc: 0.6583462953567505\n",
      "step: 935\n",
      "train: loss: 256909.25 acc: 0.7859888076782227  val: loss: 3825230.5 acc: 0.6104095578193665\n",
      "step: 940\n",
      "train: loss: 1520512.75 acc: 0.6944347620010376  val: loss: 1186909.375 acc: 0.6873113512992859\n",
      "step: 945\n",
      "train: loss: 1385502.25 acc: 0.7533657550811768  val: loss: 608782.8125 acc: 0.8174415826797485\n",
      "step: 950\n",
      "train: loss: 1563957.875 acc: 0.8276408910751343  val: loss: 1568512.625 acc: 0.8096153736114502\n",
      "step: 955\n",
      "train: loss: 853389.75 acc: 0.9136168360710144  val: loss: 2265002.5 acc: 0.7832674980163574\n",
      "step: 960\n",
      "train: loss: 835104.5 acc: 0.8683567643165588  val: loss: 1314721.5 acc: 0.7190269231796265\n",
      "step: 965\n",
      "train: loss: 688629.5 acc: 0.8551222681999207  val: loss: 1476691.875 acc: 0.6589099764823914\n",
      "step: 970\n",
      "train: loss: 1160199.25 acc: 0.8829779028892517  val: loss: 1558995.125 acc: 0.36677050590515137\n",
      "step: 975\n",
      "train: loss: 770327.0625 acc: 0.9234180450439453  val: loss: 1183201.75 acc: 0.834404468536377\n",
      "step: 980\n",
      "train: loss: 292922.9375 acc: 0.9821689128875732  val: loss: 2475239.75 acc: 0.15059542655944824\n",
      "step: 985\n",
      "train: loss: 628172.875 acc: 0.9312592148780823  val: loss: 1987780.75 acc: 0.7574635744094849\n",
      "step: 990\n",
      "train: loss: 743667.8125 acc: 0.8340616226196289  val: loss: 1814204.625 acc: 0.55445396900177\n",
      "step: 995\n",
      "train: loss: 849024.75 acc: 0.8908527493476868  val: loss: 726728.375 acc: 0.6815630197525024\n",
      "step: 1000\n",
      "train: loss: 853003.625 acc: 0.7556008696556091  val: loss: 1416761.375 acc: 0.6081881523132324\n",
      "step: 1005\n",
      "train: loss: 163477.203125 acc: 0.3407135605812073  val: loss: 1001079.5625 acc: 0.847556471824646\n",
      "step: 1010\n",
      "train: loss: 135226.609375 acc: 0.8263806700706482  val: loss: 1036483.9375 acc: 0.8511145114898682\n",
      "step: 1015\n",
      "train: loss: 251715.484375 acc: 0.8520395159721375  val: loss: 1355340.375 acc: 0.8262898921966553\n",
      "step: 1020\n",
      "train: loss: 38508.484375 acc: 0.8968222141265869  val: loss: 551411.5625 acc: 0.7871964573860168\n",
      "step: 1025\n",
      "train: loss: 12308.2900390625 acc: 0.9398677349090576  val: loss: 525808.0 acc: 0.8373555541038513\n",
      "step: 1030\n",
      "train: loss: 122380.9375 acc: 0.8595101237297058  val: loss: 829229.3125 acc: 0.7861140966415405\n",
      "step: 1035\n",
      "train: loss: 125068.171875 acc: 0.8848715424537659  val: loss: 397302.46875 acc: 0.8573920130729675\n",
      "step: 1040\n",
      "train: loss: 71518.609375 acc: 0.9022364616394043  val: loss: 687819.375 acc: 0.6422967910766602\n",
      "step: 1045\n",
      "train: loss: 226031.515625 acc: 0.8737157583236694  val: loss: 1090310.125 acc: 0.8585763573646545\n",
      "step: 1050\n",
      "train: loss: 158263.296875 acc: 0.8718358278274536  val: loss: 599061.8125 acc: 0.86065673828125\n",
      "step: 1055\n",
      "train: loss: 301107.03125 acc: 0.8503515720367432  val: loss: 1013629.75 acc: 0.6250318288803101\n",
      "step: 1060\n",
      "train: loss: 300982.5625 acc: 0.8862977027893066  val: loss: 540374.625 acc: 0.8080387115478516\n",
      "step: 1065\n",
      "train: loss: 122520.3828125 acc: 0.8762086033821106  val: loss: 797433.75 acc: 0.8994868397712708\n",
      "step: 1070\n",
      "train: loss: 86027.0390625 acc: 0.9030117988586426  val: loss: 577589.0625 acc: 0.720098078250885\n",
      "step: 1075\n",
      "train: loss: 184837.65625 acc: 0.9265879988670349  val: loss: 358758.59375 acc: 0.9414496421813965\n",
      "step: 1080\n",
      "train: loss: 323317.09375 acc: 0.9360343217849731  val: loss: 416942.0625 acc: 0.9194022417068481\n",
      "step: 1085\n",
      "train: loss: 198753.34375 acc: 0.9276539087295532  val: loss: 837957.25 acc: 0.7579383850097656\n",
      "step: 1090\n",
      "train: loss: 245009.234375 acc: 0.9230514764785767  val: loss: 424743.90625 acc: 0.8863298892974854\n",
      "step: 1095\n",
      "train: loss: 152782.78125 acc: 0.9435800313949585  val: loss: 173446.921875 acc: 0.9434555768966675\n",
      "step: 1100\n",
      "train: loss: 549285.6875 acc: 0.8864627480506897  val: loss: 224818.734375 acc: 0.7256875038146973\n",
      "step: 1105\n",
      "train: loss: 206249.421875 acc: 0.9241049289703369  val: loss: 616599.6875 acc: 0.6808966994285583\n",
      "step: 1110\n",
      "train: loss: 296082.875 acc: 0.9207543134689331  val: loss: 358110.375 acc: 0.9173035025596619\n",
      "step: 1115\n",
      "train: loss: 701777.625 acc: 0.843804657459259  val: loss: 199638.609375 acc: 0.9490669965744019\n",
      "step: 1120\n",
      "train: loss: 695547.9375 acc: 0.91755211353302  val: loss: 431188.0 acc: 0.8348619937896729\n",
      "step: 1125\n",
      "train: loss: 171560.71875 acc: 0.9862149953842163  val: loss: 454293.40625 acc: 0.9469946622848511\n",
      "step: 1130\n",
      "train: loss: 328698.65625 acc: 0.9604752659797668  val: loss: 399070.25 acc: 0.9518216252326965\n",
      "step: 1135\n",
      "train: loss: 313014.5 acc: 0.9718443155288696  val: loss: 259866.875 acc: 0.9348042607307434\n",
      "step: 1140\n",
      "train: loss: 283166.09375 acc: 0.9613972902297974  val: loss: 674267.6875 acc: 0.9268816709518433\n",
      "step: 1145\n",
      "train: loss: 388014.28125 acc: 0.9738278388977051  val: loss: 1198221.875 acc: 0.8941664099693298\n",
      "step: 1150\n",
      "train: loss: 669315.1875 acc: 0.9124168157577515  val: loss: 309942.90625 acc: 0.9597902297973633\n",
      "step: 1155\n",
      "train: loss: 228115.796875 acc: 0.9659580588340759  val: loss: 610989.1875 acc: 0.877646803855896\n",
      "step: 1160\n",
      "train: loss: 914873.9375 acc: 0.9623522758483887  val: loss: 658436.8125 acc: 0.7430393695831299\n",
      "step: 1165\n",
      "train: loss: 1881948.25 acc: 0.9417309761047363  val: loss: 2149502.5 acc: 0.6949348449707031\n",
      "step: 1170\n",
      "train: loss: 2894023.75 acc: 0.8827314376831055  val: loss: 523576.8125 acc: 0.9472517371177673\n",
      "step: 1175\n",
      "train: loss: 272835.75 acc: 0.9793968200683594  val: loss: 165389.53125 acc: 0.9661977291107178\n",
      "step: 1180\n",
      "train: loss: 1301090.5 acc: 0.9052482843399048  val: loss: 786667.6875 acc: 0.7973177433013916\n",
      "step: 1185\n",
      "train: loss: 577790.4375 acc: 0.9444326162338257  val: loss: 1114894.625 acc: 0.7705169320106506\n",
      "step: 1190\n",
      "train: loss: 588019.8125 acc: 0.8734784722328186  val: loss: 760365.5625 acc: 0.8713148832321167\n",
      "step: 1195\n",
      "train: loss: 2457988.75 acc: -0.0861588716506958  val: loss: 411796.21875 acc: 0.8950064182281494\n",
      "step: 1200\n",
      "train: loss: 612144.0625 acc: 0.7107489109039307  val: loss: 2006048.875 acc: 0.8285852670669556\n",
      "step: 1205\n",
      "train: loss: 923357.125 acc: 0.4766587018966675  val: loss: 686050.875 acc: 0.8894436359405518\n",
      "step: 1210\n",
      "train: loss: 258596.984375 acc: 0.8927404284477234  val: loss: 966175.5625 acc: 0.8709655404090881\n",
      "step: 1215\n",
      "train: loss: 1114314.875 acc: 0.785568118095398  val: loss: 748129.8125 acc: 0.8390026688575745\n",
      "step: 1220\n",
      "train: loss: 2339110.75 acc: 0.3768501281738281  val: loss: 1956000.875 acc: 0.8138211369514465\n",
      "step: 1225\n",
      "train: loss: 667975.1875 acc: 0.7167927622795105  val: loss: 1276281.875 acc: 0.6082626581192017\n",
      "step: 1230\n",
      "train: loss: 873467.5625 acc: 0.6444263458251953  val: loss: 1851217.125 acc: 0.6140055060386658\n",
      "step: 1235\n",
      "train: loss: 375699.40625 acc: 0.6877671480178833  val: loss: 931593.9375 acc: 0.6887345314025879\n",
      "step: 1240\n",
      "train: loss: 468051.40625 acc: 0.6000252962112427  val: loss: 1222785.0 acc: 0.8252424597740173\n",
      "step: 1245\n",
      "train: loss: 237538.390625 acc: 0.8512541651725769  val: loss: 1622403.75 acc: 0.8471802473068237\n",
      "step: 1250\n",
      "train: loss: 168524.5 acc: 0.8860085606575012  val: loss: 1198507.5 acc: 0.48159104585647583\n",
      "step: 1255\n",
      "train: loss: 329031.84375 acc: 0.8509155511856079  val: loss: 835803.25 acc: 0.7502371072769165\n",
      "step: 1260\n",
      "train: loss: 641841.5625 acc: 0.5760321021080017  val: loss: 1778904.25 acc: 0.7686368227005005\n",
      "step: 1265\n",
      "train: loss: 134709.328125 acc: 0.8674095869064331  val: loss: 1457318.375 acc: 0.7974854707717896\n",
      "step: 1270\n",
      "train: loss: 223150.75 acc: 0.7419478893280029  val: loss: 983148.1875 acc: 0.87284916639328\n",
      "step: 1275\n",
      "train: loss: 211548.6875 acc: 0.8536847233772278  val: loss: 696867.1875 acc: 0.8275147676467896\n",
      "step: 1280\n",
      "train: loss: 316458.65625 acc: 0.7118982076644897  val: loss: 1255718.625 acc: 0.839382529258728\n",
      "step: 1285\n",
      "train: loss: 482572.71875 acc: 0.7462552189826965  val: loss: 1606970.5 acc: 0.6898287534713745\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 1290\n",
      "train: loss: 308512.5625 acc: 0.7043154835700989  val: loss: 313303.75 acc: 0.7467873096466064\n",
      "step: 1295\n",
      "train: loss: 286315.59375 acc: 0.8048158288002014  val: loss: 472865.84375 acc: 0.7379367351531982\n",
      "step: 1300\n",
      "train: loss: 740140.125 acc: 0.5879913568496704  val: loss: 517968.03125 acc: 0.7607003450393677\n",
      "step: 1305\n",
      "train: loss: 1545988.875 acc: 0.7584806680679321  val: loss: 658672.375 acc: 0.8132984042167664\n",
      "step: 1310\n",
      "train: loss: 688074.0 acc: 0.800528347492218  val: loss: 746221.625 acc: 0.7626786828041077\n",
      "step: 1315\n",
      "train: loss: 1318587.875 acc: 0.8849600553512573  val: loss: 1503696.125 acc: 0.7024616003036499\n",
      "step: 1320\n",
      "train: loss: 468838.46875 acc: 0.9440143704414368  val: loss: 720931.8125 acc: 0.8536074757575989\n",
      "step: 1325\n",
      "train: loss: 648637.75 acc: 0.8976967930793762  val: loss: 464276.28125 acc: 0.9285270571708679\n",
      "step: 1330\n",
      "train: loss: 328391.1875 acc: 0.928451418876648  val: loss: 2084926.375 acc: 0.7066618204116821\n",
      "step: 1335\n",
      "train: loss: 377682.28125 acc: 0.962276816368103  val: loss: 500301.84375 acc: 0.887993335723877\n",
      "step: 1340\n",
      "train: loss: 281019.9375 acc: 0.9779310822486877  val: loss: 809131.8125 acc: 0.8816738128662109\n",
      "step: 1345\n",
      "train: loss: 407644.71875 acc: 0.9693876504898071  val: loss: 1603152.25 acc: 0.6016762256622314\n",
      "step: 1350\n",
      "train: loss: 367288.625 acc: 0.9560429453849792  val: loss: 1003911.75 acc: 0.833515465259552\n",
      "step: 1355\n",
      "train: loss: 346721.84375 acc: 0.9317320585250854  val: loss: 686640.0625 acc: 0.7198090553283691\n",
      "step: 1360\n",
      "train: loss: 244889.265625 acc: 0.9336930513381958  val: loss: 1574283.25 acc: 0.8769850134849548\n",
      "step: 1365\n",
      "train: loss: 82054.671875 acc: 0.9746720790863037  val: loss: 533582.75 acc: 0.8976191282272339\n",
      "step: 1370\n",
      "train: loss: 177975.921875 acc: 0.9337195158004761  val: loss: 184045.703125 acc: 0.9475446939468384\n",
      "step: 1375\n",
      "train: loss: 27382.076171875 acc: 0.9710083603858948  val: loss: 570395.8125 acc: 0.805229663848877\n",
      "step: 1380\n",
      "train: loss: 27417.841796875 acc: 0.9684219360351562  val: loss: 248464.390625 acc: 0.9161791205406189\n",
      "step: 1385\n",
      "train: loss: 22601.990234375 acc: 0.9149541854858398  val: loss: 355604.8125 acc: 0.8764077425003052\n",
      "step: 1390\n",
      "train: loss: 35879.19921875 acc: 0.8782984614372253  val: loss: 387791.0625 acc: 0.8211243152618408\n",
      "step: 1395\n",
      "train: loss: 13931.2607421875 acc: 0.9796957969665527  val: loss: 349413.46875 acc: 0.9311845302581787\n",
      "step: 1400\n",
      "train: loss: 8937.62109375 acc: 0.9714136123657227  val: loss: 1158081.75 acc: 0.3754889965057373\n",
      "step: 1405\n",
      "train: loss: 64752.54296875 acc: 0.9444203972816467  val: loss: 331507.9375 acc: 0.8271209597587585\n",
      "step: 1410\n",
      "train: loss: 92431.1484375 acc: 0.9429088830947876  val: loss: 888501.8125 acc: 0.4946291446685791\n",
      "step: 1415\n",
      "train: loss: 104707.7890625 acc: 0.9488576054573059  val: loss: 866878.9375 acc: 0.6782100796699524\n",
      "step: 1420\n",
      "train: loss: 69110.578125 acc: 0.9395046830177307  val: loss: 728750.3125 acc: 0.9110900163650513\n",
      "step: 1425\n",
      "train: loss: 41065.18359375 acc: 0.963557243347168  val: loss: 980472.0625 acc: 0.7845607995986938\n",
      "step: 1430\n",
      "train: loss: 23863.01953125 acc: 0.9720675349235535  val: loss: 317659.625 acc: 0.9291994571685791\n",
      "step: 1435\n",
      "train: loss: 21501.2890625 acc: 0.9650173187255859  val: loss: 720035.0625 acc: 0.8989511728286743\n",
      "step: 1440\n",
      "train: loss: 91079.625 acc: 0.9442418813705444  val: loss: 535188.5625 acc: 0.9025503396987915\n",
      "step: 1445\n",
      "train: loss: 43114.52734375 acc: 0.9711402058601379  val: loss: 985154.0625 acc: 0.8024801015853882\n",
      "step: 1450\n",
      "train: loss: 153904.953125 acc: 0.9739176630973816  val: loss: 364921.25 acc: 0.9340948462486267\n",
      "step: 1455\n",
      "train: loss: 72811.515625 acc: 0.9737288355827332  val: loss: 834268.5625 acc: 0.7767772674560547\n",
      "step: 1460\n",
      "train: loss: 132839.8125 acc: 0.96249920129776  val: loss: 1710645.5 acc: 0.5608186721801758\n",
      "step: 1465\n",
      "train: loss: 246060.859375 acc: 0.8177245259284973  val: loss: 400724.375 acc: 0.9074262976646423\n",
      "step: 1470\n",
      "train: loss: 176626.90625 acc: 0.954929769039154  val: loss: 1661889.0 acc: 0.7861120700836182\n",
      "step: 1475\n",
      "train: loss: 57994.640625 acc: 0.8848429918289185  val: loss: 450816.3125 acc: 0.8711512088775635\n",
      "step: 1480\n",
      "train: loss: 588335.75 acc: 0.9026949405670166  val: loss: 372801.5 acc: 0.9314170479774475\n",
      "step: 1485\n",
      "train: loss: 345583.5 acc: 0.9667383432388306  val: loss: 575154.1875 acc: 0.9112573266029358\n",
      "step: 1490\n",
      "train: loss: 179823.734375 acc: 0.9810226559638977  val: loss: 1374021.125 acc: 0.6821681261062622\n",
      "step: 1495\n",
      "train: loss: 196929.796875 acc: 0.9735313057899475  val: loss: 2178930.25 acc: 0.6586847901344299\n",
      "step: 1500\n",
      "train: loss: 241799.796875 acc: 0.9631149172782898  val: loss: 246023.609375 acc: 0.7862732410430908\n",
      "step: 1505\n",
      "train: loss: 262859.25 acc: 0.979300320148468  val: loss: 278599.09375 acc: 0.9289002418518066\n",
      "step: 1510\n",
      "train: loss: 461348.84375 acc: 0.9748051762580872  val: loss: 298779.84375 acc: 0.9432946443557739\n",
      "step: 1515\n",
      "train: loss: 155099.0625 acc: 0.9802809953689575  val: loss: 854694.4375 acc: 0.9489232301712036\n",
      "step: 1520\n",
      "train: loss: 453453.09375 acc: 0.9631317853927612  val: loss: 1509531.25 acc: 0.6651215553283691\n",
      "step: 1525\n",
      "train: loss: 1319295.5 acc: 0.945356011390686  val: loss: 579226.1875 acc: 0.9281305074691772\n",
      "step: 1530\n",
      "train: loss: 1442047.625 acc: 0.9381332397460938  val: loss: 1383365.25 acc: 0.7480519413948059\n",
      "step: 1535\n",
      "train: loss: 1065126.5 acc: 0.9663567543029785  val: loss: 546820.8125 acc: 0.9356827735900879\n",
      "step: 1540\n",
      "train: loss: 1510876.25 acc: 0.9253947138786316  val: loss: 1573819.875 acc: 0.6669611930847168\n",
      "step: 1545\n",
      "train: loss: 1322769.875 acc: 0.9345765709877014  val: loss: 957707.3125 acc: 0.7516074180603027\n",
      "step: 1550\n",
      "train: loss: 713904.3125 acc: 0.8712132573127747  val: loss: 1668683.5 acc: 0.534313440322876\n",
      "step: 1555\n",
      "train: loss: 388150.21875 acc: 0.9469565153121948  val: loss: 1082760.5 acc: 0.5179819464683533\n",
      "step: 1560\n",
      "train: loss: 2185806.75 acc: 0.7035436034202576  val: loss: 302243.15625 acc: 0.8334404230117798\n",
      "step: 1565\n",
      "train: loss: 990703.0 acc: 0.8339952826499939  val: loss: 399632.75 acc: 0.9277447462081909\n",
      "step: 1570\n",
      "train: loss: 660718.8125 acc: 0.7619699835777283  val: loss: 357288.9375 acc: 0.826294481754303\n",
      "step: 1575\n",
      "train: loss: 469101.59375 acc: 0.7636727690696716  val: loss: 481217.4375 acc: 0.879217267036438\n",
      "step: 1580\n",
      "train: loss: 367869.40625 acc: 0.9025430679321289  val: loss: 809308.125 acc: 0.8454818725585938\n",
      "step: 1585\n",
      "train: loss: 1376455.625 acc: 0.4084169268608093  val: loss: 1397692.5 acc: 0.6184780597686768\n",
      "step: 1590\n",
      "train: loss: 1681858.0 acc: 0.21252590417861938  val: loss: 1175684.75 acc: 0.3648238182067871\n",
      "step: 1595\n",
      "train: loss: 1121866.25 acc: 0.4637632369995117  val: loss: 3564127.25 acc: 0.5866203308105469\n",
      "step: 1600\n",
      "train: loss: 640753.3125 acc: 0.3114602565765381  val: loss: 1471142.375 acc: 0.6885122656822205\n",
      "step: 1605\n",
      "train: loss: 187600.234375 acc: 0.8629366159439087  val: loss: 575975.6875 acc: 0.5657439231872559\n",
      "step: 1610\n",
      "train: loss: 285444.15625 acc: 0.8143210411071777  val: loss: 1612380.625 acc: 0.7265565395355225\n",
      "step: 1615\n",
      "train: loss: 445137.09375 acc: 0.8224604725837708  val: loss: 1324190.25 acc: 0.8704603910446167\n",
      "step: 1620\n",
      "train: loss: 255629.625 acc: 0.8020768761634827  val: loss: 599494.4375 acc: 0.7803082466125488\n",
      "step: 1625\n",
      "train: loss: 333991.40625 acc: 0.7660893201828003  val: loss: 928046.0 acc: 0.8023126721382141\n",
      "step: 1630\n",
      "train: loss: 344739.59375 acc: 0.8231514692306519  val: loss: 956980.3125 acc: 0.849731981754303\n",
      "step: 1635\n",
      "train: loss: 56723.4140625 acc: 0.9325383901596069  val: loss: 483443.09375 acc: 0.8283295631408691\n",
      "step: 1640\n",
      "train: loss: 94160.34375 acc: 0.8993589282035828  val: loss: 1258415.875 acc: 0.7690877914428711\n",
      "step: 1645\n",
      "train: loss: 651841.4375 acc: 0.31840604543685913  val: loss: 334711.6875 acc: 0.9304165244102478\n",
      "step: 1650\n",
      "train: loss: 805875.5625 acc: 0.5293976068496704  val: loss: 663833.75 acc: 0.7846238613128662\n",
      "step: 1655\n",
      "train: loss: 617898.75 acc: 0.7096283435821533  val: loss: 533747.25 acc: 0.6838476657867432\n",
      "step: 1660\n",
      "train: loss: 555670.75 acc: 0.6796483993530273  val: loss: 1241538.125 acc: 0.7080744504928589\n",
      "step: 1665\n",
      "train: loss: 193667.546875 acc: 0.8404586315155029  val: loss: 2446777.0 acc: 0.7517405152320862\n",
      "step: 1670\n",
      "train: loss: 1243704.25 acc: 0.7055673599243164  val: loss: 856681.5 acc: 0.9071370363235474\n",
      "step: 1675\n",
      "train: loss: 996379.875 acc: 0.7634937167167664  val: loss: 814233.75 acc: 0.858643651008606\n",
      "step: 1680\n",
      "train: loss: 1019372.75 acc: 0.7444247007369995  val: loss: 411060.28125 acc: 0.869695246219635\n",
      "step: 1685\n",
      "train: loss: 649945.8125 acc: 0.9562439322471619  val: loss: 543551.5625 acc: 0.864657998085022\n",
      "step: 1690\n",
      "train: loss: 458374.75 acc: 0.9267991185188293  val: loss: 1320951.125 acc: 0.5758399963378906\n",
      "step: 1695\n",
      "train: loss: 491837.40625 acc: 0.9242992401123047  val: loss: 1339236.875 acc: 0.567635178565979\n",
      "step: 1700\n",
      "train: loss: 407123.65625 acc: 0.9329847097396851  val: loss: 631640.4375 acc: 0.8378240466117859\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 1705\n",
      "train: loss: 282709.1875 acc: 0.9684848785400391  val: loss: 767539.75 acc: 0.5928479433059692\n",
      "step: 1710\n",
      "train: loss: 291189.25 acc: 0.9795644879341125  val: loss: 1251889.0 acc: 0.805086076259613\n",
      "step: 1715\n",
      "train: loss: 392970.53125 acc: 0.9569621086120605  val: loss: 386425.375 acc: 0.8833233118057251\n",
      "step: 1720\n",
      "train: loss: 408685.46875 acc: 0.9487995505332947  val: loss: 399605.625 acc: 0.9060736298561096\n",
      "step: 1725\n",
      "train: loss: 120304.2890625 acc: 0.9515981078147888  val: loss: 747040.9375 acc: 0.8304352760314941\n",
      "step: 1730\n",
      "train: loss: 154174.796875 acc: 0.9368341565132141  val: loss: 439512.78125 acc: 0.9099471569061279\n",
      "step: 1735\n",
      "train: loss: 11351.208984375 acc: 0.9916791915893555  val: loss: 1206115.625 acc: 0.8778471350669861\n",
      "step: 1740\n",
      "train: loss: 15741.7587890625 acc: 0.9596683979034424  val: loss: 761146.6875 acc: 0.7538918256759644\n",
      "step: 1745\n",
      "train: loss: 12062.318359375 acc: 0.9921257495880127  val: loss: 351334.875 acc: 0.9132819771766663\n",
      "step: 1750\n",
      "train: loss: 24837.994140625 acc: 0.9716780185699463  val: loss: 265130.75 acc: 0.9250129461288452\n",
      "step: 1755\n",
      "train: loss: 95836.9609375 acc: 0.8993548154830933  val: loss: 1366748.625 acc: 0.6838630437850952\n",
      "step: 1760\n",
      "train: loss: 16189.4326171875 acc: 0.9310417771339417  val: loss: 302007.78125 acc: 0.9225214719772339\n",
      "step: 1765\n",
      "train: loss: 18359.57421875 acc: 0.9618605375289917  val: loss: 710450.3125 acc: 0.9016909003257751\n",
      "step: 1770\n",
      "train: loss: 27603.638671875 acc: 0.9469578862190247  val: loss: 328076.34375 acc: 0.9465028643608093\n",
      "step: 1775\n",
      "train: loss: 203431.5625 acc: 0.9204042553901672  val: loss: 687886.25 acc: 0.8968285322189331\n",
      "step: 1780\n",
      "train: loss: 147639.515625 acc: 0.9451517462730408  val: loss: 161533.734375 acc: 0.9737560749053955\n",
      "step: 1785\n",
      "train: loss: 66231.5078125 acc: 0.9304640889167786  val: loss: 198057.515625 acc: 0.9472954869270325\n",
      "step: 1790\n",
      "train: loss: 134579.921875 acc: 0.9444933533668518  val: loss: 665382.4375 acc: 0.9381109476089478\n",
      "step: 1795\n",
      "train: loss: 41139.90234375 acc: 0.9607978463172913  val: loss: 416606.21875 acc: 0.8680413961410522\n",
      "step: 1800\n",
      "train: loss: 51207.60546875 acc: 0.938735842704773  val: loss: 1079600.5 acc: 0.9337419271469116\n",
      "step: 1805\n",
      "train: loss: 23424.150390625 acc: 0.9771825671195984  val: loss: 979010.4375 acc: 0.8649985194206238\n",
      "step: 1810\n",
      "train: loss: 50421.8046875 acc: 0.9789746403694153  val: loss: 275280.75 acc: 0.9713872075080872\n",
      "step: 1815\n",
      "train: loss: 83014.6953125 acc: 0.982696533203125  val: loss: 652921.1875 acc: 0.9245954751968384\n",
      "step: 1820\n",
      "train: loss: 178225.40625 acc: 0.9583203792572021  val: loss: 1694728.875 acc: 0.6590985059738159\n",
      "step: 1825\n",
      "train: loss: 47955.74609375 acc: 0.955452561378479  val: loss: 1632521.75 acc: 0.7225310802459717\n",
      "step: 1830\n",
      "train: loss: 194918.328125 acc: 0.946459949016571  val: loss: 782052.9375 acc: 0.8860519528388977\n",
      "step: 1835\n",
      "train: loss: 174210.203125 acc: 0.9360414743423462  val: loss: 4237097.0 acc: -1.8675975799560547\n",
      "step: 1840\n",
      "train: loss: 117601.1484375 acc: 0.8532517552375793  val: loss: 2184417.25 acc: 0.8394996523857117\n",
      "step: 1845\n",
      "train: loss: 361887.78125 acc: 0.9331327080726624  val: loss: 2591556.25 acc: 0.4452056288719177\n",
      "step: 1850\n",
      "train: loss: 196235.203125 acc: 0.9791332483291626  val: loss: 2233476.25 acc: 0.8185253143310547\n",
      "step: 1855\n",
      "train: loss: 831490.9375 acc: 0.9048225283622742  val: loss: 332733.375 acc: 0.8447954654693604\n",
      "step: 1860\n",
      "train: loss: 85501.078125 acc: 0.9807294011116028  val: loss: 1278096.875 acc: 0.695367693901062\n",
      "step: 1865\n",
      "train: loss: 166280.046875 acc: 0.970833957195282  val: loss: 783044.875 acc: 0.8849506378173828\n",
      "step: 1870\n",
      "train: loss: 203195.65625 acc: 0.9819746613502502  val: loss: 450321.71875 acc: 0.8348069787025452\n",
      "step: 1875\n",
      "train: loss: 126960.375 acc: 0.9909366369247437  val: loss: 2394109.25 acc: 0.6224124431610107\n",
      "step: 1880\n",
      "train: loss: 758564.9375 acc: 0.9648770093917847  val: loss: 963043.6875 acc: 0.9107337594032288\n",
      "step: 1885\n",
      "train: loss: 334556.53125 acc: 0.9593400359153748  val: loss: 502204.53125 acc: 0.8339802026748657\n",
      "step: 1890\n",
      "train: loss: 1129759.5 acc: 0.9482905864715576  val: loss: 1147076.25 acc: 0.7868578433990479\n",
      "step: 1895\n",
      "train: loss: 2322346.0 acc: 0.9321696162223816  val: loss: 405100.625 acc: 0.8391668796539307\n",
      "step: 1900\n",
      "train: loss: 1524835.25 acc: 0.9379927515983582  val: loss: 413113.78125 acc: 0.869786262512207\n",
      "step: 1905\n",
      "train: loss: 965734.9375 acc: 0.9367138743400574  val: loss: 572654.5625 acc: 0.798909068107605\n",
      "step: 1910\n",
      "train: loss: 785491.375 acc: 0.949151873588562  val: loss: 833685.1875 acc: 0.7815093994140625\n",
      "step: 1915\n",
      "train: loss: 482974.0625 acc: 0.9529630541801453  val: loss: 555140.375 acc: 0.8665266633033752\n",
      "step: 1920\n",
      "train: loss: 559085.3125 acc: 0.9327237606048584  val: loss: 1013071.5625 acc: 0.885032057762146\n",
      "step: 1925\n",
      "train: loss: 820607.5625 acc: 0.805295467376709  val: loss: 640459.1875 acc: 0.8983914256095886\n",
      "step: 1930\n",
      "train: loss: 964180.1875 acc: 0.6099987030029297  val: loss: 755266.1875 acc: 0.9136805534362793\n",
      "step: 1935\n",
      "train: loss: 1100438.875 acc: 0.5547682642936707  val: loss: 497233.75 acc: 0.8888358473777771\n",
      "step: 1940\n",
      "train: loss: 546428.1875 acc: 0.7852251529693604  val: loss: 491943.84375 acc: 0.9250346422195435\n",
      "step: 1945\n",
      "train: loss: 464542.40625 acc: 0.8506754040718079  val: loss: 356904.0 acc: 0.8517675995826721\n",
      "step: 1950\n",
      "train: loss: 1473619.875 acc: 0.4650586247444153  val: loss: 985235.8125 acc: 0.8545886278152466\n",
      "step: 1955\n",
      "train: loss: 967138.125 acc: 0.47427552938461304  val: loss: 2193043.25 acc: 0.7198463082313538\n",
      "step: 1960\n",
      "train: loss: 536881.1875 acc: 0.6076101064682007  val: loss: 1825624.125 acc: 0.6454564332962036\n",
      "step: 1965\n",
      "train: loss: 534542.0 acc: 0.5483561158180237  val: loss: 1971246.875 acc: 0.5778936147689819\n",
      "step: 1970\n",
      "train: loss: 370295.65625 acc: 0.6740544438362122  val: loss: 1943991.875 acc: 0.6004254817962646\n",
      "step: 1975\n",
      "train: loss: 72566.859375 acc: 0.9388120174407959  val: loss: 1726671.75 acc: 0.7959356904029846\n",
      "step: 1980\n",
      "train: loss: 116763.0703125 acc: 0.9150127172470093  val: loss: 1099570.375 acc: 0.8135116100311279\n",
      "step: 1985\n",
      "train: loss: 269275.71875 acc: 0.8439006805419922  val: loss: 768493.125 acc: 0.7650564908981323\n",
      "step: 1990\n",
      "train: loss: 662352.875 acc: 0.7877111434936523  val: loss: 614616.1875 acc: 0.7970818877220154\n",
      "step: 1995\n",
      "train: loss: 108657.140625 acc: 0.9024712443351746  val: loss: 318638.5 acc: 0.8302911520004272\n",
      "step: 2000\n",
      "train: loss: 276903.84375 acc: 0.7640981078147888  val: loss: 444876.5 acc: 0.8713223934173584\n",
      "step: 2005\n",
      "train: loss: 78314.625 acc: 0.9063212871551514  val: loss: 1645890.875 acc: 0.7826253175735474\n",
      "step: 2010\n",
      "train: loss: 461744.75 acc: 0.798466682434082  val: loss: 1023219.5 acc: 0.7789210081100464\n",
      "step: 2015\n",
      "train: loss: 730776.25 acc: 0.6909217834472656  val: loss: 1373202.625 acc: 0.6907355785369873\n",
      "step: 2020\n",
      "train: loss: 992864.25 acc: 0.6564887762069702  val: loss: 1307771.25 acc: 0.7872127294540405\n",
      "step: 2025\n",
      "train: loss: 451447.375 acc: 0.7731629610061646  val: loss: 997216.0 acc: 0.8125150203704834\n",
      "step: 2030\n",
      "train: loss: 1069552.375 acc: 0.6447350978851318  val: loss: 647105.5 acc: 0.7148448824882507\n",
      "step: 2035\n",
      "train: loss: 710080.875 acc: 0.7593295574188232  val: loss: 117192.1796875 acc: 0.8754032850265503\n",
      "step: 2040\n",
      "train: loss: 1040120.0 acc: 0.7640031576156616  val: loss: 558476.4375 acc: 0.7945801615715027\n",
      "step: 2045\n",
      "train: loss: 1585200.875 acc: 0.8607919812202454  val: loss: 374038.09375 acc: 0.8969475030899048\n",
      "step: 2050\n",
      "train: loss: 663222.625 acc: 0.9375864863395691  val: loss: 506793.5 acc: 0.7387288808822632\n",
      "step: 2055\n",
      "train: loss: 698093.5 acc: 0.9364492297172546  val: loss: 768952.125 acc: 0.18044447898864746\n",
      "step: 2060\n",
      "train: loss: 1066138.75 acc: 0.8367702960968018  val: loss: 515821.0 acc: 0.9096673727035522\n",
      "step: 2065\n",
      "train: loss: 456707.53125 acc: 0.9072806239128113  val: loss: 2224483.25 acc: 0.04585230350494385\n",
      "step: 2070\n",
      "train: loss: 286818.46875 acc: 0.9784507751464844  val: loss: 841912.75 acc: 0.8174021244049072\n",
      "step: 2075\n",
      "train: loss: 270479.21875 acc: 0.980477511882782  val: loss: 426173.03125 acc: 0.9241800308227539\n",
      "step: 2080\n",
      "train: loss: 332140.90625 acc: 0.9707969427108765  val: loss: 1000125.5625 acc: 0.7863612174987793\n",
      "step: 2085\n",
      "train: loss: 313602.75 acc: 0.9630885720252991  val: loss: 453285.78125 acc: 0.930133581161499\n",
      "step: 2090\n",
      "train: loss: 233185.828125 acc: 0.9279957413673401  val: loss: 814469.0625 acc: 0.7524495720863342\n",
      "step: 2095\n",
      "train: loss: 97047.7109375 acc: 0.9686508178710938  val: loss: 1298697.125 acc: 0.8203364014625549\n",
      "step: 2100\n",
      "train: loss: 65262.62890625 acc: 0.943371593952179  val: loss: 925148.8125 acc: 0.9220079183578491\n",
      "step: 2105\n",
      "train: loss: 19552.705078125 acc: 0.9278781414031982  val: loss: 542835.9375 acc: 0.943013608455658\n",
      "step: 2110\n",
      "train: loss: 70556.578125 acc: 0.970487117767334  val: loss: 1061788.625 acc: 0.6534426212310791\n",
      "step: 2115\n",
      "train: loss: 31180.044921875 acc: 0.9770443439483643  val: loss: 976910.75 acc: 0.895593523979187\n",
      "step: 2120\n",
      "train: loss: 36872.24609375 acc: 0.8482447862625122  val: loss: 458909.5625 acc: 0.9439677596092224\n",
      "step: 2125\n",
      "train: loss: 19584.375 acc: 0.969802737236023  val: loss: 362872.625 acc: 0.9097341895103455\n",
      "step: 2130\n",
      "train: loss: 5148.240234375 acc: 0.9963271021842957  val: loss: 189499.015625 acc: 0.9201391339302063\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 2135\n",
      "train: loss: 103237.1875 acc: 0.9434912204742432  val: loss: 630603.9375 acc: 0.9246083498001099\n",
      "step: 2140\n",
      "train: loss: 71102.8359375 acc: 0.9603707194328308  val: loss: 943539.0 acc: 0.8643877506256104\n",
      "step: 2145\n",
      "train: loss: 75443.6015625 acc: 0.959495484828949  val: loss: 1315472.0 acc: 0.5439881086349487\n",
      "step: 2150\n",
      "train: loss: 146068.734375 acc: 0.9161324501037598  val: loss: 2672192.75 acc: 0.20431864261627197\n",
      "step: 2155\n",
      "train: loss: 22648.06640625 acc: 0.9828956723213196  val: loss: 1844123.5 acc: 0.6615030765533447\n",
      "step: 2160\n",
      "train: loss: 55412.93359375 acc: 0.9615660309791565  val: loss: 705699.4375 acc: 0.7601008415222168\n",
      "step: 2165\n",
      "train: loss: 44226.8125 acc: 0.9540238380432129  val: loss: 970210.1875 acc: 0.9150146245956421\n",
      "step: 2170\n",
      "train: loss: 44548.94921875 acc: 0.9683147668838501  val: loss: 1156998.375 acc: 0.8400101661682129\n",
      "step: 2175\n",
      "train: loss: 109861.984375 acc: 0.9634997844696045  val: loss: 321145.53125 acc: 0.9542427062988281\n",
      "step: 2180\n",
      "train: loss: 111810.65625 acc: 0.9816790223121643  val: loss: 3466356.75 acc: 0.36485326290130615\n",
      "step: 2185\n",
      "train: loss: 118475.5078125 acc: 0.9651924967765808  val: loss: 872662.0625 acc: 0.8674044013023376\n",
      "step: 2190\n",
      "train: loss: 37835.1640625 acc: 0.9835238456726074  val: loss: 2071780.375 acc: 0.752575695514679\n",
      "step: 2195\n",
      "train: loss: 88979.6796875 acc: 0.9695792198181152  val: loss: 1617200.625 acc: 0.7038800120353699\n",
      "step: 2200\n",
      "train: loss: 161745.90625 acc: 0.9562602639198303  val: loss: 2267936.75 acc: -0.3898583650588989\n",
      "step: 2205\n",
      "train: loss: 212984.859375 acc: 0.8820679187774658  val: loss: 950500.8125 acc: 0.7561758160591125\n",
      "step: 2210\n",
      "train: loss: 161047.234375 acc: 0.9370627999305725  val: loss: 373545.59375 acc: 0.9118009805679321\n",
      "step: 2215\n",
      "train: loss: 64435.42578125 acc: 0.9850024580955505  val: loss: 2295649.75 acc: -0.09240567684173584\n",
      "step: 2220\n",
      "train: loss: 385796.96875 acc: 0.961397647857666  val: loss: 1380279.75 acc: -0.016553878784179688\n",
      "step: 2225\n",
      "train: loss: 163972.203125 acc: 0.9686102271080017  val: loss: 2587243.25 acc: 0.0954018235206604\n",
      "step: 2230\n",
      "train: loss: 80455.7265625 acc: 0.9909708499908447  val: loss: 561936.0625 acc: 0.7313430309295654\n",
      "step: 2235\n",
      "train: loss: 297643.53125 acc: 0.959524929523468  val: loss: 442805.84375 acc: 0.7317284941673279\n",
      "step: 2240\n",
      "train: loss: 296953.28125 acc: 0.9774665832519531  val: loss: 1337565.75 acc: -0.11278998851776123\n",
      "step: 2245\n",
      "train: loss: 423243.1875 acc: 0.9478830099105835  val: loss: 627225.9375 acc: 0.6620577573776245\n",
      "step: 2250\n",
      "train: loss: 310420.34375 acc: 0.8114949464797974  val: loss: 1016667.625 acc: 0.6848776936531067\n",
      "step: 2255\n",
      "train: loss: 3050448.75 acc: 0.9084362983703613  val: loss: 3031469.25 acc: 0.30590444803237915\n",
      "step: 2260\n",
      "train: loss: 2943046.25 acc: 0.9038093090057373  val: loss: 2848829.25 acc: 0.11176478862762451\n",
      "step: 2265\n",
      "train: loss: 732603.8125 acc: 0.9743956923484802  val: loss: 2388226.75 acc: 0.592939019203186\n",
      "step: 2270\n",
      "train: loss: 1253549.125 acc: 0.9395597577095032  val: loss: 667144.625 acc: 0.8936461210250854\n",
      "step: 2275\n",
      "train: loss: 750407.9375 acc: 0.9601268768310547  val: loss: 237870.6875 acc: 0.9631022810935974\n",
      "step: 2280\n",
      "train: loss: 1369835.125 acc: 0.9084991812705994  val: loss: 291565.3125 acc: 0.9157382845878601\n",
      "step: 2285\n",
      "train: loss: 258907.40625 acc: 0.9676014184951782  val: loss: 492804.71875 acc: 0.8532887101173401\n",
      "step: 2290\n",
      "train: loss: 565397.1875 acc: 0.9345395565032959  val: loss: 769735.0 acc: 0.7675541043281555\n",
      "step: 2295\n",
      "train: loss: 1854387.25 acc: -0.09885191917419434  val: loss: 684762.5625 acc: 0.608803391456604\n",
      "step: 2300\n",
      "train: loss: 745565.5625 acc: 0.5274658799171448  val: loss: 1524113.5 acc: 0.7013784646987915\n",
      "step: 2305\n",
      "train: loss: 324560.125 acc: 0.7693394422531128  val: loss: 657185.3125 acc: 0.8376601934432983\n",
      "step: 2310\n",
      "train: loss: 476680.1875 acc: 0.8515107035636902  val: loss: 1467131.125 acc: 0.7700303792953491\n",
      "step: 2315\n",
      "train: loss: 1250887.625 acc: 0.628472626209259  val: loss: 1681817.75 acc: 0.8496793508529663\n",
      "step: 2320\n",
      "train: loss: 1436131.125 acc: -0.0817408561706543  val: loss: 991788.5 acc: 0.6618319749832153\n",
      "step: 2325\n",
      "train: loss: 637150.5 acc: 0.479464054107666  val: loss: 2278555.0 acc: 0.501309871673584\n",
      "step: 2330\n",
      "train: loss: 784021.1875 acc: 0.37140846252441406  val: loss: 1769859.0 acc: 0.6177978515625\n",
      "step: 2335\n",
      "train: loss: 427538.53125 acc: 0.5888499021530151  val: loss: 881551.4375 acc: 0.7410948276519775\n",
      "step: 2340\n",
      "train: loss: 374485.375 acc: 0.6788969039916992  val: loss: 1630525.5 acc: 0.7663395404815674\n",
      "step: 2345\n",
      "train: loss: 133147.703125 acc: 0.9051859378814697  val: loss: 456345.65625 acc: 0.7442021369934082\n",
      "step: 2350\n",
      "train: loss: 581919.0625 acc: 0.6858785152435303  val: loss: 204526.25 acc: 0.9333715438842773\n",
      "step: 2355\n",
      "train: loss: 257815.734375 acc: 0.8464916348457336  val: loss: 502920.15625 acc: 0.6955302953720093\n",
      "step: 2360\n",
      "train: loss: 528278.0625 acc: 0.7580154538154602  val: loss: 1138194.625 acc: 0.8345250487327576\n",
      "step: 2365\n",
      "train: loss: 106015.4453125 acc: 0.8953702449798584  val: loss: 1294326.125 acc: 0.6167570352554321\n",
      "step: 2370\n",
      "train: loss: 185364.640625 acc: 0.8528648018836975  val: loss: 627159.3125 acc: 0.851051926612854\n",
      "step: 2375\n",
      "train: loss: 605333.0625 acc: 0.36826014518737793  val: loss: 329579.25 acc: 0.8588480949401855\n",
      "step: 2380\n",
      "train: loss: 772323.125 acc: 0.6663039922714233  val: loss: 702437.5 acc: 0.6893901228904724\n",
      "step: 2385\n",
      "train: loss: 1044452.5625 acc: 0.6891230940818787  val: loss: 925293.4375 acc: 0.81999272108078\n",
      "step: 2390\n",
      "train: loss: 196985.0625 acc: 0.8030992746353149  val: loss: 650217.625 acc: 0.884692907333374\n",
      "step: 2395\n",
      "train: loss: 310859.59375 acc: 0.70650315284729  val: loss: 2146058.0 acc: 0.6898573637008667\n",
      "step: 2400\n",
      "train: loss: 641672.0 acc: 0.6514439582824707  val: loss: 2612325.75 acc: 0.7420371174812317\n",
      "step: 2405\n",
      "train: loss: 2469910.0 acc: 0.7701457142829895  val: loss: 1887697.25 acc: 0.7998437285423279\n",
      "step: 2410\n",
      "train: loss: 872190.375 acc: 0.861011803150177  val: loss: 1195277.875 acc: 0.870858371257782\n",
      "step: 2415\n",
      "train: loss: 772423.5625 acc: 0.904768705368042  val: loss: 884013.25 acc: 0.9055861830711365\n",
      "step: 2420\n",
      "train: loss: 660869.0625 acc: 0.9365887641906738  val: loss: 587257.0625 acc: 0.9247398376464844\n",
      "step: 2425\n",
      "train: loss: 386739.21875 acc: 0.9410160779953003  val: loss: 1609308.25 acc: 0.8554321527481079\n",
      "step: 2430\n",
      "train: loss: 136620.140625 acc: 0.9741638898849487  val: loss: 882829.25 acc: 0.8858627676963806\n",
      "step: 2435\n",
      "train: loss: 287190.84375 acc: 0.9747217297554016  val: loss: 846147.375 acc: 0.8031043410301208\n",
      "step: 2440\n",
      "train: loss: 528423.8125 acc: 0.9360786080360413  val: loss: 845344.4375 acc: 0.8533246517181396\n",
      "step: 2445\n",
      "train: loss: 397101.15625 acc: 0.9665687680244446  val: loss: 744916.9375 acc: 0.8546510338783264\n",
      "step: 2450\n",
      "train: loss: 273041.25 acc: 0.9721790552139282  val: loss: 1339595.125 acc: 0.6916928887367249\n",
      "step: 2455\n",
      "train: loss: 70863.5703125 acc: 0.9773036241531372  val: loss: 861350.9375 acc: 0.8830574750900269\n",
      "step: 2460\n",
      "train: loss: 113805.421875 acc: 0.9187384843826294  val: loss: 479543.15625 acc: 0.9187980890274048\n",
      "step: 2465\n",
      "train: loss: 27526.791015625 acc: 0.9079827070236206  val: loss: 1033348.4375 acc: 0.7790019512176514\n",
      "step: 2470\n",
      "train: loss: 118999.0390625 acc: 0.9526872634887695  val: loss: 796070.3125 acc: 0.9304832220077515\n",
      "step: 2475\n",
      "train: loss: 15935.1337890625 acc: 0.9686242341995239  val: loss: 1390729.125 acc: 0.9118553996086121\n",
      "step: 2480\n",
      "train: loss: 20135.248046875 acc: 0.9589414596557617  val: loss: 542209.1875 acc: 0.9159362316131592\n",
      "step: 2485\n",
      "train: loss: 23010.98828125 acc: 0.951239287853241  val: loss: 458312.5625 acc: 0.9288782477378845\n",
      "step: 2490\n",
      "train: loss: 26243.359375 acc: 0.9385709166526794  val: loss: 680175.9375 acc: 0.8857969045639038\n",
      "step: 2495\n",
      "train: loss: 12567.509765625 acc: 0.9736295342445374  val: loss: 756850.5 acc: 0.6065548658370972\n",
      "step: 2500\n",
      "train: loss: 12459.7939453125 acc: 0.9776465892791748  val: loss: 764655.375 acc: 0.7636233568191528\n",
      "step: 2505\n",
      "train: loss: 63558.76171875 acc: 0.9422329068183899  val: loss: 714098.3125 acc: 0.9188709259033203\n",
      "step: 2510\n",
      "train: loss: 35660.5 acc: 0.9709638953208923  val: loss: 614073.9375 acc: 0.3324207067489624\n",
      "step: 2515\n",
      "train: loss: 70640.6953125 acc: 0.9559991359710693  val: loss: 228626.09375 acc: 0.9247949719429016\n",
      "step: 2520\n",
      "train: loss: 65745.546875 acc: 0.9607124328613281  val: loss: 4164634.0 acc: -0.6056908369064331\n",
      "step: 2525\n",
      "train: loss: 36523.3359375 acc: 0.9760275483131409  val: loss: 2585738.25 acc: 0.490908682346344\n",
      "step: 2530\n",
      "train: loss: 32463.154296875 acc: 0.9609474539756775  val: loss: 1369437.625 acc: 0.586877703666687\n",
      "step: 2535\n",
      "train: loss: 36830.7578125 acc: 0.9594601988792419  val: loss: 650053.4375 acc: 0.9389597773551941\n",
      "step: 2540\n",
      "train: loss: 23431.537109375 acc: 0.983092725276947  val: loss: 3466131.25 acc: 0.28891122341156006\n",
      "step: 2545\n",
      "train: loss: 67533.2109375 acc: 0.9827247262001038  val: loss: 955790.875 acc: 0.6596773862838745\n",
      "step: 2550\n",
      "train: loss: 77229.984375 acc: 0.9825839400291443  val: loss: 3430962.0 acc: 0.7575294971466064\n",
      "step: 2555\n",
      "train: loss: 91927.6796875 acc: 0.9846392273902893  val: loss: 1423658.5 acc: 0.7572396993637085\n",
      "step: 2560\n",
      "train: loss: 60331.30078125 acc: 0.9763166308403015  val: loss: 3589421.25 acc: -0.16188180446624756\n",
      "step: 2565\n",
      "train: loss: 124558.0390625 acc: 0.9554605484008789  val: loss: 853668.6875 acc: 0.8283414244651794\n",
      "step: 2570\n",
      "train: loss: 170292.078125 acc: 0.9339573979377747  val: loss: 502853.21875 acc: 0.9119107127189636\n",
      "step: 2575\n",
      "train: loss: 112396.9453125 acc: 0.9740891456604004  val: loss: 391394.1875 acc: 0.8914694786071777\n",
      "step: 2580\n",
      "train: loss: 178061.46875 acc: 0.9604401588439941  val: loss: 896376.125 acc: 0.8745890259742737\n",
      "step: 2585\n",
      "train: loss: 42124.58984375 acc: 0.9953404068946838  val: loss: 3483657.75 acc: 0.3640754222869873\n",
      "step: 2590\n",
      "train: loss: 439181.375 acc: 0.9436390399932861  val: loss: 673319.625 acc: 0.8262016773223877\n",
      "step: 2595\n",
      "train: loss: 281018.9375 acc: 0.9713099002838135  val: loss: 879717.4375 acc: 0.7474368810653687\n",
      "step: 2600\n",
      "train: loss: 199400.484375 acc: 0.969369113445282  val: loss: 1159601.5 acc: 0.8203935623168945\n",
      "step: 2605\n",
      "train: loss: 562719.5625 acc: 0.9612836837768555  val: loss: 512774.34375 acc: 0.8362794518470764\n",
      "step: 2610\n",
      "train: loss: 616779.8125 acc: 0.9698931574821472  val: loss: 3123390.0 acc: -0.19866669178009033\n",
      "step: 2615\n",
      "train: loss: 308614.15625 acc: 0.942472517490387  val: loss: 999992.875 acc: 0.8119761943817139\n",
      "step: 2620\n",
      "train: loss: 1118423.0 acc: 0.9443622827529907  val: loss: 767261.9375 acc: 0.859341025352478\n",
      "step: 2625\n",
      "train: loss: 4194695.5 acc: 0.8875125646591187  val: loss: 438690.59375 acc: 0.871241569519043\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 2630\n",
      "train: loss: 1161587.125 acc: 0.9511865973472595  val: loss: 1362430.25 acc: 0.6567150950431824\n",
      "step: 2635\n",
      "train: loss: 534293.125 acc: 0.9658249616622925  val: loss: 436100.5 acc: 0.9089838862419128\n",
      "step: 2640\n",
      "train: loss: 871888.0625 acc: 0.9376153349876404  val: loss: 618511.875 acc: 0.35066139698028564\n",
      "step: 2645\n",
      "train: loss: 743520.8125 acc: 0.9317439198493958  val: loss: 786620.3125 acc: 0.9141321182250977\n",
      "step: 2650\n",
      "train: loss: 253858.890625 acc: 0.9513377547264099  val: loss: 1024218.3125 acc: 0.8640711307525635\n",
      "step: 2655\n",
      "train: loss: 945494.0625 acc: 0.9021363258361816  val: loss: 254270.171875 acc: 0.9371963143348694\n",
      "step: 2660\n",
      "train: loss: 1981436.875 acc: 0.5626817941665649  val: loss: 563424.0625 acc: 0.828776478767395\n",
      "step: 2665\n",
      "train: loss: 1131106.5 acc: 0.7567832469940186  val: loss: 1115546.5 acc: 0.6954517960548401\n",
      "step: 2670\n",
      "train: loss: 836043.9375 acc: 0.6878587007522583  val: loss: 648940.0625 acc: 0.643953800201416\n",
      "step: 2675\n",
      "train: loss: 214441.15625 acc: 0.904435396194458  val: loss: 767832.75 acc: 0.7326775789260864\n",
      "step: 2680\n",
      "train: loss: 708654.4375 acc: 0.6454980969429016  val: loss: 406410.40625 acc: 0.8050339221954346\n",
      "step: 2685\n",
      "train: loss: 1482288.875 acc: 0.2824059724807739  val: loss: 1862328.625 acc: 0.598170280456543\n",
      "step: 2690\n",
      "train: loss: 1081405.875 acc: 0.602021336555481  val: loss: 2143760.25 acc: 0.6291748285293579\n",
      "step: 2695\n",
      "train: loss: 647756.8125 acc: 0.4781646132469177  val: loss: 1391332.125 acc: 0.6813815832138062\n",
      "step: 2700\n",
      "train: loss: 441301.125 acc: 0.6092410087585449  val: loss: 1491502.25 acc: 0.6580864191055298\n",
      "step: 2705\n",
      "train: loss: 331790.09375 acc: 0.8497851490974426  val: loss: 1040018.4375 acc: 0.7297409772872925\n",
      "step: 2710\n",
      "train: loss: 169701.671875 acc: 0.8773790597915649  val: loss: 816765.9375 acc: 0.7620730996131897\n",
      "step: 2715\n",
      "train: loss: 102343.7265625 acc: 0.9201824069023132  val: loss: 899802.625 acc: 0.7487945556640625\n",
      "step: 2720\n",
      "train: loss: 247144.90625 acc: 0.8301320672035217  val: loss: 2196035.25 acc: 0.8123322129249573\n",
      "step: 2725\n",
      "train: loss: 131532.0 acc: 0.8920082449913025  val: loss: 911033.3125 acc: 0.8292508125305176\n",
      "step: 2730\n",
      "train: loss: 148094.703125 acc: 0.8411610722541809  val: loss: 637128.5625 acc: 0.874748706817627\n",
      "step: 2735\n",
      "train: loss: 529721.9375 acc: 0.735249400138855  val: loss: 334101.0625 acc: 0.9225168228149414\n",
      "step: 2740\n",
      "train: loss: 796361.625 acc: 0.4854477643966675  val: loss: 1091385.875 acc: 0.7995490431785583\n",
      "step: 2745\n",
      "train: loss: 559023.0625 acc: 0.41184788942337036  val: loss: 516900.65625 acc: 0.7883049249649048\n",
      "step: 2750\n",
      "train: loss: 636992.375 acc: 0.564917266368866  val: loss: 1021088.125 acc: 0.7214323282241821\n",
      "step: 2755\n",
      "train: loss: 658104.8125 acc: 0.7776751518249512  val: loss: 1889226.625 acc: 0.703816294670105\n",
      "step: 2760\n",
      "train: loss: 578530.125 acc: 0.7150967717170715  val: loss: 899708.5625 acc: 0.8156622648239136\n",
      "step: 2765\n",
      "train: loss: 238381.75 acc: 0.7926656603813171  val: loss: 1460367.875 acc: 0.7517752051353455\n",
      "step: 2770\n",
      "train: loss: 1896950.25 acc: 0.7238741517066956  val: loss: 2876688.25 acc: 0.7728433609008789\n",
      "step: 2775\n",
      "train: loss: 668183.25 acc: 0.8254996538162231  val: loss: 768592.0 acc: 0.841812252998352\n",
      "step: 2780\n",
      "train: loss: 1344671.375 acc: 0.8336088061332703  val: loss: 756763.9375 acc: 0.8470677137374878\n",
      "step: 2785\n",
      "train: loss: 476424.0625 acc: 0.9596582055091858  val: loss: 1188575.625 acc: 0.8140052556991577\n",
      "step: 2790\n",
      "train: loss: 302061.5625 acc: 0.9272792339324951  val: loss: 861598.8125 acc: 0.6203000545501709\n",
      "step: 2795\n",
      "train: loss: 299070.78125 acc: 0.9156752228736877  val: loss: 1340616.875 acc: 0.7940192222595215\n",
      "step: 2800\n",
      "train: loss: 387637.5625 acc: 0.9657039046287537  val: loss: 1678409.125 acc: 0.7988095879554749\n",
      "step: 2805\n",
      "train: loss: 261933.0625 acc: 0.9847540259361267  val: loss: 1807216.625 acc: 0.7996593713760376\n",
      "step: 2810\n",
      "train: loss: 1028195.9375 acc: 0.9008619785308838  val: loss: 678679.0625 acc: 0.9088423848152161\n",
      "step: 2815\n",
      "train: loss: 276225.875 acc: 0.9736335277557373  val: loss: 436494.8125 acc: 0.9058855175971985\n",
      "step: 2820\n",
      "train: loss: 175881.6875 acc: 0.9339390397071838  val: loss: 633272.375 acc: 0.8907554745674133\n",
      "step: 2825\n",
      "train: loss: 178125.875 acc: 0.9349563121795654  val: loss: 930446.25 acc: 0.662266731262207\n",
      "step: 2830\n",
      "train: loss: 17428.61328125 acc: 0.9875167608261108  val: loss: 559042.1875 acc: 0.8955064415931702\n",
      "step: 2835\n",
      "train: loss: 20560.1484375 acc: 0.9844241738319397  val: loss: 1338232.75 acc: 0.7111138105392456\n",
      "step: 2840\n",
      "train: loss: 15271.1455078125 acc: 0.9655695557594299  val: loss: 1141295.625 acc: 0.8237593770027161\n",
      "step: 2845\n",
      "train: loss: 47607.47265625 acc: 0.9433398246765137  val: loss: 730555.1875 acc: 0.8618221879005432\n",
      "step: 2850\n",
      "train: loss: 13002.861328125 acc: 0.9578335285186768  val: loss: 781394.875 acc: 0.8332021236419678\n",
      "step: 2855\n",
      "train: loss: 31420.08984375 acc: 0.9800682067871094  val: loss: 465495.59375 acc: 0.9276250600814819\n",
      "step: 2860\n",
      "train: loss: 9731.37890625 acc: 0.9696449637413025  val: loss: 366728.4375 acc: 0.8137558102607727\n",
      "step: 2865\n",
      "train: loss: 100422.71875 acc: 0.7869434356689453  val: loss: 827578.3125 acc: 0.7734878063201904\n",
      "step: 2870\n",
      "train: loss: 33415.91015625 acc: 0.9674651026725769  val: loss: 754424.875 acc: 0.8408418297767639\n",
      "step: 2875\n",
      "train: loss: 52946.01171875 acc: 0.9500155448913574  val: loss: 1292034.0 acc: 0.8553844094276428\n",
      "step: 2880\n",
      "train: loss: 46054.296875 acc: 0.9600658416748047  val: loss: 511657.125 acc: 0.947149932384491\n",
      "step: 2885\n",
      "train: loss: 98407.75 acc: 0.96417236328125  val: loss: 192452.8125 acc: 0.964814305305481\n",
      "step: 2890\n",
      "train: loss: 40560.90234375 acc: 0.9763020873069763  val: loss: 895829.875 acc: 0.7885519862174988\n",
      "step: 2895\n",
      "train: loss: 33807.28515625 acc: 0.963235080242157  val: loss: 1117263.25 acc: 0.8439520001411438\n",
      "step: 2900\n",
      "train: loss: 44686.25 acc: 0.9590792059898376  val: loss: 2241477.75 acc: -0.3852694034576416\n",
      "step: 2905\n",
      "train: loss: 29969.66796875 acc: 0.9824671745300293  val: loss: 194075.59375 acc: 0.9715114831924438\n",
      "step: 2910\n",
      "train: loss: 75747.609375 acc: 0.9803704619407654  val: loss: 1112991.25 acc: 0.8137951493263245\n",
      "step: 2915\n",
      "train: loss: 79430.84375 acc: 0.9789692163467407  val: loss: 739526.8125 acc: 0.8777308464050293\n",
      "step: 2920\n",
      "train: loss: 79052.546875 acc: 0.9689648747444153  val: loss: 2163997.75 acc: 0.2712046504020691\n",
      "step: 2925\n",
      "train: loss: 89534.359375 acc: 0.9706304669380188  val: loss: 894107.8125 acc: 0.8088436722755432\n",
      "step: 2930\n",
      "train: loss: 139224.359375 acc: 0.9594459533691406  val: loss: 2342660.25 acc: 0.2378339171409607\n",
      "step: 2935\n",
      "train: loss: 253166.328125 acc: 0.9217391014099121  val: loss: 881332.875 acc: 0.4331943392753601\n",
      "step: 2940\n",
      "train: loss: 57623.87109375 acc: 0.964939296245575  val: loss: 884709.3125 acc: 0.7821633219718933\n",
      "step: 2945\n",
      "train: loss: 588130.1875 acc: 0.8994195461273193  val: loss: 707676.625 acc: 0.8938368558883667\n",
      "step: 2950\n",
      "train: loss: 163932.328125 acc: 0.9794402718544006  val: loss: 299092.09375 acc: 0.9021500945091248\n",
      "step: 2955\n",
      "train: loss: 117841.9140625 acc: 0.9869715571403503  val: loss: 2341900.25 acc: 0.24501484632492065\n",
      "step: 2960\n",
      "train: loss: 188947.75 acc: 0.9703868627548218  val: loss: 793351.875 acc: 0.6987784504890442\n",
      "step: 2965\n",
      "train: loss: 286023.09375 acc: 0.93877112865448  val: loss: 766250.5 acc: 0.8696923851966858\n",
      "step: 2970\n",
      "train: loss: 198656.65625 acc: 0.985705554485321  val: loss: 2024587.625 acc: 0.6941193342208862\n",
      "step: 2975\n",
      "train: loss: 333216.625 acc: 0.9665734767913818  val: loss: 1365272.75 acc: 0.7160791754722595\n",
      "step: 2980\n",
      "train: loss: 607719.9375 acc: 0.9733183979988098  val: loss: 900982.875 acc: 0.6455042362213135\n",
      "step: 2985\n",
      "train: loss: 1777872.75 acc: 0.9082944393157959  val: loss: 470901.46875 acc: 0.720453679561615\n",
      "step: 2990\n",
      "train: loss: 464154.09375 acc: 0.9838929176330566  val: loss: 948778.25 acc: 0.7183959484100342\n",
      "step: 2995\n",
      "train: loss: 2392399.0 acc: 0.9394761919975281  val: loss: 812220.1875 acc: 0.8868083953857422\n",
      "step: 3000\n",
      "train: loss: 4269793.5 acc: 0.7508518099784851  val: loss: 1051783.375 acc: 0.6457569599151611\n",
      "step: 3005\n",
      "train: loss: 542329.4375 acc: 0.9413868188858032  val: loss: 1387435.25 acc: 0.5715116262435913\n",
      "step: 3010\n",
      "train: loss: 392351.84375 acc: 0.9691681265830994  val: loss: 144861.140625 acc: 0.9559898376464844\n",
      "step: 3015\n",
      "train: loss: 397652.53125 acc: 0.9508280158042908  val: loss: 683386.625 acc: 0.6612975597381592\n",
      "step: 3020\n",
      "train: loss: 414805.875 acc: 0.8921704292297363  val: loss: 1194629.0 acc: 0.631450355052948\n",
      "step: 3025\n",
      "train: loss: 1318139.375 acc: 0.6992297172546387  val: loss: 723124.4375 acc: 0.7823789119720459\n",
      "step: 3030\n",
      "train: loss: 388149.90625 acc: 0.8340833187103271  val: loss: 922043.625 acc: 0.7506105303764343\n",
      "step: 3035\n",
      "train: loss: 539832.1875 acc: 0.7523981332778931  val: loss: 1025043.4375 acc: 0.7153677940368652\n",
      "step: 3040\n",
      "train: loss: 1590913.0 acc: 0.5578608512878418  val: loss: 1665127.375 acc: 0.8613287210464478\n",
      "step: 3045\n",
      "train: loss: 842789.5625 acc: 0.4944550395011902  val: loss: 1021675.0 acc: 0.8369984030723572\n",
      "step: 3050\n",
      "train: loss: 1849924.875 acc: 0.5421525239944458  val: loss: 1126954.625 acc: 0.7898972630500793\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 3055\n",
      "train: loss: 993719.9375 acc: 0.5372600555419922  val: loss: 1810486.125 acc: 0.5620640516281128\n",
      "step: 3060\n",
      "train: loss: 690473.125 acc: 0.4312628507614136  val: loss: 3486257.25 acc: 0.6615054607391357\n",
      "step: 3065\n",
      "train: loss: 316277.28125 acc: 0.7583807706832886  val: loss: 3222883.75 acc: 0.6243290901184082\n",
      "step: 3070\n",
      "train: loss: 329417.71875 acc: 0.8302538990974426  val: loss: 1596414.875 acc: 0.7828806042671204\n",
      "step: 3075\n",
      "train: loss: 322552.78125 acc: 0.8332533836364746  val: loss: 1421106.25 acc: 0.8221763372421265\n",
      "step: 3080\n",
      "train: loss: 180981.3125 acc: 0.8959352374076843  val: loss: 924883.6875 acc: 0.8119273781776428\n",
      "step: 3085\n",
      "train: loss: 227293.609375 acc: 0.8377748131752014  val: loss: 2359027.75 acc: 0.7567324042320251\n",
      "step: 3090\n",
      "train: loss: 296779.5 acc: 0.7437195777893066  val: loss: 998740.125 acc: 0.7228435277938843\n",
      "step: 3095\n",
      "train: loss: 114038.8203125 acc: 0.8888585567474365  val: loss: 1064770.0 acc: 0.800704836845398\n",
      "step: 3100\n",
      "train: loss: 212840.390625 acc: 0.8032965064048767  val: loss: 129020.296875 acc: 0.9160079956054688\n",
      "step: 3105\n",
      "train: loss: 239791.21875 acc: 0.8851779699325562  val: loss: 652198.9375 acc: 0.7799355387687683\n",
      "step: 3110\n",
      "train: loss: 638539.1875 acc: 0.7023531198501587  val: loss: 672380.3125 acc: 0.8260716795921326\n",
      "step: 3115\n",
      "train: loss: 206115.375 acc: 0.8108389973640442  val: loss: 516270.84375 acc: 0.7851072549819946\n",
      "step: 3120\n",
      "train: loss: 222314.5625 acc: 0.8659504652023315  val: loss: 2931538.75 acc: 0.6951673030853271\n",
      "step: 3125\n",
      "train: loss: 250127.0 acc: 0.7903794050216675  val: loss: 712320.75 acc: 0.7323808670043945\n",
      "step: 3130\n",
      "train: loss: 655580.75 acc: 0.7728597521781921  val: loss: 1542422.125 acc: 0.7028821110725403\n",
      "step: 3135\n",
      "train: loss: 1692391.125 acc: 0.7189581394195557  val: loss: 2119045.25 acc: 0.7216216325759888\n",
      "step: 3140\n",
      "train: loss: 1170681.125 acc: 0.8148819208145142  val: loss: 1000819.25 acc: 0.844820499420166\n",
      "step: 3145\n",
      "train: loss: 1138877.375 acc: 0.8891735672950745  val: loss: 691564.4375 acc: 0.8555827140808105\n",
      "step: 3150\n",
      "train: loss: 972765.125 acc: 0.9074470400810242  val: loss: 1034413.4375 acc: 0.10048872232437134\n",
      "step: 3155\n",
      "train: loss: 472934.625 acc: 0.9440520405769348  val: loss: 2079261.5 acc: 0.38515704870224\n",
      "step: 3160\n",
      "train: loss: 557900.25 acc: 0.894693911075592  val: loss: 1588246.125 acc: 0.777614414691925\n",
      "step: 3165\n",
      "train: loss: 327847.75 acc: 0.9691874980926514  val: loss: 984904.8125 acc: 0.5972784757614136\n",
      "step: 3170\n",
      "train: loss: 158893.875 acc: 0.9875856041908264  val: loss: 919175.4375 acc: 0.8555878400802612\n",
      "step: 3175\n",
      "train: loss: 304638.25 acc: 0.9737437963485718  val: loss: 843939.3125 acc: 0.5966949462890625\n",
      "step: 3180\n",
      "train: loss: 156167.78125 acc: 0.9722952246665955  val: loss: 2350063.0 acc: -0.11982035636901855\n",
      "step: 3185\n",
      "train: loss: 141248.625 acc: 0.9613728523254395  val: loss: 1153143.125 acc: 0.843625009059906\n",
      "step: 3190\n",
      "train: loss: 256543.984375 acc: 0.9427563548088074  val: loss: 1398328.25 acc: 0.5881844758987427\n",
      "step: 3195\n",
      "train: loss: 47285.33984375 acc: 0.9755933880805969  val: loss: 635195.8125 acc: 0.8242508769035339\n",
      "step: 3200\n",
      "train: loss: 17546.1953125 acc: 0.9642983675003052  val: loss: 451183.46875 acc: 0.8981506824493408\n",
      "step: 3205\n",
      "train: loss: 49680.8515625 acc: 0.9675415754318237  val: loss: 927432.25 acc: 0.8967451453208923\n",
      "step: 3210\n",
      "train: loss: 14661.626953125 acc: 0.9777209162712097  val: loss: 1650675.375 acc: 0.8334895372390747\n",
      "step: 3215\n",
      "train: loss: 21357.240234375 acc: 0.9580627083778381  val: loss: 989563.5625 acc: 0.4100547432899475\n",
      "step: 3220\n",
      "train: loss: 14229.544921875 acc: 0.950909435749054  val: loss: 96237.71875 acc: 0.9065576791763306\n",
      "step: 3225\n",
      "train: loss: 26699.265625 acc: 0.9067628383636475  val: loss: 1233017.0 acc: 0.09136384725570679\n",
      "step: 3230\n",
      "train: loss: 14952.2509765625 acc: 0.981537401676178  val: loss: 503011.4375 acc: 0.8937222361564636\n",
      "step: 3235\n",
      "train: loss: 31052.197265625 acc: 0.9836342334747314  val: loss: 1304926.125 acc: 0.38065433502197266\n",
      "step: 3240\n",
      "train: loss: 36320.21484375 acc: 0.9812431335449219  val: loss: 588675.625 acc: 0.8328248858451843\n",
      "step: 3245\n",
      "train: loss: 46510.89453125 acc: 0.9688267111778259  val: loss: 443086.53125 acc: 0.848005473613739\n",
      "step: 3250\n",
      "train: loss: 42287.04296875 acc: 0.9704177975654602  val: loss: 687354.0625 acc: 0.8716574907302856\n",
      "step: 3255\n",
      "train: loss: 31791.20703125 acc: 0.9723377823829651  val: loss: 336438.5 acc: 0.9610806107521057\n",
      "step: 3260\n",
      "train: loss: 44023.41796875 acc: 0.9738428592681885  val: loss: 701375.8125 acc: 0.8214152455329895\n",
      "step: 3265\n",
      "train: loss: 26148.08203125 acc: 0.9541746973991394  val: loss: 161456.234375 acc: 0.9611048102378845\n",
      "step: 3270\n",
      "train: loss: 63228.81640625 acc: 0.9667246341705322  val: loss: 482566.0 acc: 0.8223521113395691\n",
      "step: 3275\n",
      "train: loss: 62974.11328125 acc: 0.9780218601226807  val: loss: 457188.25 acc: 0.9228612184524536\n",
      "step: 3280\n",
      "train: loss: 125908.03125 acc: 0.9719582200050354  val: loss: 2391857.5 acc: -0.25524258613586426\n",
      "step: 3285\n",
      "train: loss: 84280.46875 acc: 0.9750726222991943  val: loss: 1004511.8125 acc: 0.6437200903892517\n",
      "step: 3290\n",
      "train: loss: 124120.84375 acc: 0.9634357690811157  val: loss: 606904.0625 acc: 0.7069995999336243\n",
      "step: 3295\n",
      "train: loss: 94823.875 acc: 0.9751976132392883  val: loss: 259774.03125 acc: 0.9387773275375366\n",
      "step: 3300\n",
      "train: loss: 99861.65625 acc: 0.9607669115066528  val: loss: 876485.9375 acc: 0.8623788356781006\n",
      "step: 3305\n",
      "train: loss: 109483.7890625 acc: 0.9555885195732117  val: loss: 161397.640625 acc: 0.822522759437561\n",
      "step: 3310\n",
      "train: loss: 127721.421875 acc: 0.9716411828994751  val: loss: 271120.6875 acc: 0.9662503004074097\n",
      "step: 3315\n",
      "train: loss: 176687.28125 acc: 0.9804175496101379  val: loss: 350747.9375 acc: 0.9026187658309937\n",
      "step: 3320\n",
      "train: loss: 96508.3203125 acc: 0.9889110922813416  val: loss: 1255124.5 acc: 0.8445674777030945\n",
      "step: 3325\n",
      "train: loss: 64075.88671875 acc: 0.9924705028533936  val: loss: 351118.21875 acc: 0.9523537755012512\n",
      "step: 3330\n",
      "train: loss: 232860.8125 acc: 0.9596492052078247  val: loss: 644728.6875 acc: 0.9027862548828125\n",
      "step: 3335\n",
      "train: loss: 500259.6875 acc: 0.9545918703079224  val: loss: 1229171.375 acc: 0.8175981640815735\n",
      "step: 3340\n",
      "train: loss: 731563.4375 acc: 0.9571982026100159  val: loss: 770978.8125 acc: 0.779540479183197\n",
      "step: 3345\n",
      "train: loss: 442480.78125 acc: 0.9064362049102783  val: loss: 978322.8125 acc: 0.9236675500869751\n",
      "step: 3350\n",
      "train: loss: 365637.53125 acc: 0.859889566898346  val: loss: 728895.25 acc: 0.8794218301773071\n",
      "step: 3355\n",
      "train: loss: 1031187.375 acc: 0.9706333875656128  val: loss: 863718.3125 acc: 0.9180129766464233\n",
      "step: 3360\n",
      "train: loss: 2003042.0 acc: 0.8951203227043152  val: loss: 442213.5625 acc: 0.9013131856918335\n",
      "step: 3365\n",
      "train: loss: 1124683.5 acc: 0.9285609722137451  val: loss: 592548.4375 acc: 0.9406765103340149\n",
      "step: 3370\n",
      "train: loss: 903551.5625 acc: 0.9516355991363525  val: loss: 787219.625 acc: 0.8649175763130188\n",
      "step: 3375\n",
      "train: loss: 79051.25 acc: 0.989172101020813  val: loss: 420603.5625 acc: 0.904643714427948\n",
      "step: 3380\n",
      "train: loss: 429398.375 acc: 0.9224030375480652  val: loss: 538449.625 acc: 0.8733134865760803\n",
      "step: 3385\n",
      "train: loss: 841830.5 acc: 0.9028486609458923  val: loss: 1060567.5 acc: 0.8703546524047852\n",
      "step: 3390\n",
      "train: loss: 1126935.625 acc: 0.7909295558929443  val: loss: 1187122.25 acc: 0.7663400173187256\n",
      "step: 3395\n",
      "train: loss: 1625428.375 acc: 0.4287375807762146  val: loss: 1950889.0 acc: 0.7012091875076294\n",
      "step: 3400\n",
      "train: loss: 1164076.125 acc: 0.5592050552368164  val: loss: 1453660.0 acc: 0.8494521379470825\n",
      "step: 3405\n",
      "train: loss: 399457.0625 acc: 0.8437798023223877  val: loss: 1583451.375 acc: 0.8203310966491699\n",
      "step: 3410\n",
      "train: loss: 467757.1875 acc: 0.8439860343933105  val: loss: 545545.3125 acc: 0.8223488330841064\n",
      "step: 3415\n",
      "train: loss: 2166839.0 acc: 0.15158241987228394  val: loss: 1170030.375 acc: 0.8784542083740234\n",
      "step: 3420\n",
      "train: loss: 1030260.5 acc: 0.48828911781311035  val: loss: 2774594.25 acc: 0.7288579940795898\n",
      "step: 3425\n",
      "train: loss: 562004.375 acc: 0.5799335241317749  val: loss: 2767218.25 acc: 0.6998357176780701\n",
      "step: 3430\n",
      "train: loss: 502517.5 acc: 0.7557545900344849  val: loss: 1003147.6875 acc: 0.7690994739532471\n",
      "step: 3435\n",
      "train: loss: 392081.1875 acc: 0.7280666828155518  val: loss: 2337106.5 acc: 0.7616349458694458\n",
      "step: 3440\n",
      "train: loss: 358793.28125 acc: 0.7197301387786865  val: loss: 751373.9375 acc: 0.7422977685928345\n",
      "step: 3445\n",
      "train: loss: 320563.0 acc: 0.7764030694961548  val: loss: 549165.3125 acc: 0.7809785604476929\n",
      "step: 3450\n",
      "train: loss: 209800.375 acc: 0.8561813831329346  val: loss: 485284.0625 acc: 0.8276048302650452\n",
      "step: 3455\n",
      "train: loss: 342963.96875 acc: 0.8125864267349243  val: loss: 895615.0625 acc: 0.6398460268974304\n",
      "step: 3460\n",
      "train: loss: 85735.8515625 acc: 0.9243518710136414  val: loss: 1453359.875 acc: 0.7610685229301453\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 3465\n",
      "train: loss: 345983.5 acc: 0.7917698621749878  val: loss: 442846.375 acc: 0.7697620987892151\n",
      "step: 3470\n",
      "train: loss: 156978.90625 acc: 0.8814921379089355  val: loss: 782173.6875 acc: 0.8497841358184814\n",
      "step: 3475\n",
      "train: loss: 466345.03125 acc: 0.6520681977272034  val: loss: 1289679.625 acc: 0.8204751014709473\n",
      "step: 3480\n",
      "train: loss: 648111.3125 acc: 0.6083217859268188  val: loss: 415975.15625 acc: 0.8511173725128174\n",
      "step: 3485\n",
      "train: loss: 908841.9375 acc: 0.655040979385376  val: loss: 1368151.375 acc: 0.7477153539657593\n",
      "step: 3490\n",
      "train: loss: 235308.53125 acc: 0.842893660068512  val: loss: 840336.625 acc: 0.7967283129692078\n",
      "step: 3495\n",
      "train: loss: 331697.96875 acc: 0.7070879936218262  val: loss: 3253043.75 acc: 0.6646561622619629\n",
      "step: 3500\n",
      "train: loss: 332029.28125 acc: 0.8356125354766846  val: loss: 1193393.375 acc: 0.7248597145080566\n",
      "step: 3505\n",
      "train: loss: 949576.0625 acc: 0.6633198857307434  val: loss: 1022972.8125 acc: 0.789576530456543\n",
      "step: 3510\n",
      "train: loss: 1412344.625 acc: 0.8425924777984619  val: loss: 418439.71875 acc: 0.8679462671279907\n",
      "step: 3515\n",
      "train: loss: 587230.3125 acc: 0.9592961668968201  val: loss: 1195153.875 acc: 0.848537802696228\n",
      "step: 3520\n",
      "train: loss: 290991.8125 acc: 0.954452633857727  val: loss: 804931.4375 acc: 0.7906122803688049\n",
      "step: 3525\n",
      "train: loss: 460839.59375 acc: 0.9265741109848022  val: loss: 1094522.125 acc: 0.903191328048706\n",
      "step: 3530\n",
      "train: loss: 485742.3125 acc: 0.9414262771606445  val: loss: 1338144.125 acc: 0.8364450335502625\n",
      "step: 3535\n",
      "train: loss: 189237.3125 acc: 0.9839504361152649  val: loss: 826736.625 acc: 0.7451268434524536\n",
      "step: 3540\n",
      "train: loss: 288568.21875 acc: 0.9778392314910889  val: loss: 1786325.625 acc: 0.5791940689086914\n",
      "step: 3545\n",
      "train: loss: 288772.9375 acc: 0.9772518277168274  val: loss: 924019.0 acc: 0.8497070074081421\n",
      "step: 3550\n",
      "train: loss: 307898.09375 acc: 0.9642922282218933  val: loss: 1444589.125 acc: 0.6587350964546204\n",
      "step: 3555\n",
      "train: loss: 111723.859375 acc: 0.9643114805221558  val: loss: 958577.8125 acc: 0.7156424522399902\n",
      "step: 3560\n",
      "train: loss: 124593.1796875 acc: 0.9624717831611633  val: loss: 460905.71875 acc: 0.927547812461853\n",
      "step: 3565\n",
      "train: loss: 32097.044921875 acc: 0.908962070941925  val: loss: 578929.5 acc: 0.8013901114463806\n",
      "step: 3570\n",
      "train: loss: 15513.6669921875 acc: 0.9821295142173767  val: loss: 540978.3125 acc: 0.833986222743988\n",
      "step: 3575\n",
      "train: loss: 75409.1328125 acc: 0.9653596878051758  val: loss: 1306586.625 acc: 0.7705360651016235\n",
      "step: 3580\n",
      "train: loss: 26387.56640625 acc: 0.8272117376327515  val: loss: 522857.96875 acc: 0.42202645540237427\n",
      "step: 3585\n",
      "train: loss: 22648.83984375 acc: 0.9017452001571655  val: loss: 138286.3125 acc: 0.9731721878051758\n",
      "step: 3590\n",
      "train: loss: 37093.28125 acc: 0.9267202019691467  val: loss: 800429.3125 acc: 0.6845775842666626\n",
      "step: 3595\n",
      "train: loss: 20412.35546875 acc: 0.9537734389305115  val: loss: 341646.9375 acc: 0.8967870473861694\n",
      "step: 3600\n",
      "train: loss: 22016.4296875 acc: 0.9655352234840393  val: loss: 667919.4375 acc: 0.6924528479576111\n",
      "step: 3605\n",
      "train: loss: 88656.75 acc: 0.9281615018844604  val: loss: 780815.6875 acc: 0.6844576001167297\n",
      "step: 3610\n",
      "train: loss: 123398.5625 acc: 0.9580610990524292  val: loss: 477722.9375 acc: 0.9252996444702148\n",
      "step: 3615\n",
      "train: loss: 81844.9375 acc: 0.9662319421768188  val: loss: 982975.8125 acc: 0.2908467650413513\n",
      "step: 3620\n",
      "train: loss: 46877.9375 acc: 0.9730450510978699  val: loss: 399153.1875 acc: 0.9137961864471436\n",
      "step: 3625\n",
      "train: loss: 92351.265625 acc: 0.8881577253341675  val: loss: 1061341.5 acc: 0.5875160694122314\n",
      "step: 3630\n",
      "train: loss: 11963.8662109375 acc: 0.9657843112945557  val: loss: 545262.6875 acc: 0.7084693312644958\n",
      "step: 3635\n",
      "train: loss: 44767.33203125 acc: 0.9653463959693909  val: loss: 225527.484375 acc: 0.9498701095581055\n",
      "step: 3640\n",
      "train: loss: 58303.71875 acc: 0.9846731424331665  val: loss: 359204.625 acc: 0.8543806076049805\n",
      "step: 3645\n",
      "train: loss: 29314.263671875 acc: 0.9808850288391113  val: loss: 428270.15625 acc: 0.905931293964386\n",
      "step: 3650\n",
      "train: loss: 63640.91796875 acc: 0.9785976409912109  val: loss: 2195242.5 acc: 0.4855307340621948\n",
      "step: 3655\n",
      "train: loss: 80928.390625 acc: 0.9737984538078308  val: loss: 460587.46875 acc: 0.9274894595146179\n",
      "step: 3660\n",
      "train: loss: 397408.21875 acc: 0.9138351082801819  val: loss: 448652.5 acc: 0.9511017203330994\n",
      "step: 3665\n",
      "train: loss: 170637.140625 acc: 0.9624080061912537  val: loss: 213133.359375 acc: 0.9731993079185486\n",
      "step: 3670\n",
      "train: loss: 58837.11328125 acc: 0.978911817073822  val: loss: 1090576.25 acc: 0.8960104584693909\n",
      "step: 3675\n",
      "train: loss: 196562.3125 acc: 0.9312837719917297  val: loss: 95681.28125 acc: 0.9919716715812683\n",
      "step: 3680\n",
      "train: loss: 236194.4375 acc: 0.9759862422943115  val: loss: 455554.09375 acc: 0.9025858640670776\n",
      "step: 3685\n",
      "train: loss: 45945.515625 acc: 0.9939855337142944  val: loss: 763074.5625 acc: 0.8240076899528503\n",
      "step: 3690\n",
      "train: loss: 65208.265625 acc: 0.9925428628921509  val: loss: 725424.75 acc: 0.8585203289985657\n",
      "step: 3695\n",
      "train: loss: 433784.9375 acc: 0.9651904702186584  val: loss: 1891160.25 acc: 0.6663406491279602\n",
      "step: 3700\n",
      "train: loss: 286361.65625 acc: 0.9727566838264465  val: loss: 2373968.5 acc: 0.5618549585342407\n",
      "step: 3705\n",
      "train: loss: 985668.5 acc: 0.9422662258148193  val: loss: 1028969.8125 acc: 0.8098786473274231\n",
      "step: 3710\n",
      "train: loss: 411952.53125 acc: 0.9659515619277954  val: loss: 1098485.0 acc: 0.8707143664360046\n",
      "step: 3715\n",
      "train: loss: 454243.71875 acc: 0.9563105702400208  val: loss: 1117549.5 acc: 0.8807690739631653\n",
      "step: 3720\n",
      "train: loss: 1110544.375 acc: 0.9433701038360596  val: loss: 2038428.75 acc: 0.6591284275054932\n",
      "step: 3725\n",
      "train: loss: 1706508.25 acc: 0.9465686082839966  val: loss: 1231789.375 acc: 0.9021212458610535\n",
      "step: 3730\n",
      "train: loss: 1993659.25 acc: 0.9295549988746643  val: loss: 1446336.875 acc: 0.5565006732940674\n",
      "step: 3735\n",
      "train: loss: 675614.8125 acc: 0.927653431892395  val: loss: 1894628.125 acc: 0.8441592454910278\n",
      "step: 3740\n",
      "train: loss: 450135.40625 acc: 0.9448467493057251  val: loss: 1992922.125 acc: 0.6016089916229248\n",
      "step: 3745\n",
      "train: loss: 575301.4375 acc: 0.9433256983757019  val: loss: 380499.75 acc: 0.777381420135498\n",
      "step: 3750\n",
      "train: loss: 261597.359375 acc: 0.9102272987365723  val: loss: 1873349.625 acc: 0.721823513507843\n",
      "step: 3755\n",
      "train: loss: 2058257.375 acc: 0.40926212072372437  val: loss: 388416.5 acc: 0.93595951795578\n",
      "step: 3760\n",
      "train: loss: 503566.3125 acc: 0.7954575419425964  val: loss: 1217994.625 acc: 0.43690216541290283\n",
      "step: 3765\n",
      "train: loss: 903465.5625 acc: 0.5182254910469055  val: loss: 900373.8125 acc: 0.7472026348114014\n",
      "step: 3770\n",
      "train: loss: 430719.6875 acc: 0.831668496131897  val: loss: 483901.875 acc: 0.6613810658454895\n",
      "step: 3775\n",
      "train: loss: 1226123.75 acc: 0.6390940546989441  val: loss: 400215.0 acc: 0.819038450717926\n",
      "step: 3780\n",
      "train: loss: 2078266.875 acc: 0.3962298631668091  val: loss: 1256600.875 acc: 0.5505641102790833\n",
      "step: 3785\n",
      "train: loss: 753521.9375 acc: 0.3301893472671509  val: loss: 1589541.75 acc: 0.553856372833252\n",
      "step: 3790\n",
      "train: loss: 641955.4375 acc: 0.4425960183143616  val: loss: 847270.6875 acc: 0.6423436403274536\n",
      "step: 3795\n",
      "train: loss: 946527.625 acc: 0.6172141432762146  val: loss: 2784105.5 acc: 0.733378529548645\n",
      "step: 3800\n",
      "train: loss: 159249.078125 acc: 0.8631615042686462  val: loss: 1064048.25 acc: 0.8127954006195068\n",
      "step: 3805\n",
      "train: loss: 324497.53125 acc: 0.7622629404067993  val: loss: 624436.5625 acc: 0.7283627986907959\n",
      "step: 3810\n",
      "train: loss: 437979.375 acc: 0.8131308555603027  val: loss: 582294.4375 acc: 0.8429211378097534\n",
      "step: 3815\n",
      "train: loss: 99468.734375 acc: 0.919574499130249  val: loss: 1776149.125 acc: 0.8110184073448181\n",
      "step: 3820\n",
      "train: loss: 167256.984375 acc: 0.8611555099487305  val: loss: 1606189.875 acc: 0.7324423789978027\n",
      "step: 3825\n",
      "train: loss: 181668.921875 acc: 0.8623780608177185  val: loss: 1210882.625 acc: 0.8065153360366821\n",
      "step: 3830\n",
      "train: loss: 255807.921875 acc: 0.6372964382171631  val: loss: 1008309.8125 acc: 0.843940794467926\n",
      "step: 3835\n",
      "train: loss: 73594.625 acc: 0.899130642414093  val: loss: 607997.875 acc: 0.8344459533691406\n",
      "step: 3840\n",
      "train: loss: 162402.125 acc: 0.8572044968605042  val: loss: 2758521.0 acc: 0.6744817495346069\n",
      "step: 3845\n",
      "train: loss: 225942.65625 acc: 0.8041484355926514  val: loss: 1228200.125 acc: 0.727030873298645\n",
      "step: 3850\n",
      "train: loss: 465547.90625 acc: 0.6962008476257324  val: loss: 997367.8125 acc: 0.793738603591919\n",
      "step: 3855\n",
      "train: loss: 336218.34375 acc: 0.5982861518859863  val: loss: 3558321.75 acc: 0.6357672214508057\n",
      "step: 3860\n",
      "train: loss: 242015.6875 acc: 0.8033972382545471  val: loss: 134741.390625 acc: 0.8583150506019592\n",
      "step: 3865\n",
      "train: loss: 774857.6875 acc: 0.6799605488777161  val: loss: 897098.5625 acc: 0.8244208097457886\n",
      "step: 3870\n",
      "train: loss: 777403.125 acc: 0.7506245374679565  val: loss: 286437.59375 acc: 0.9301190376281738\n",
      "step: 3875\n",
      "train: loss: 1174912.0 acc: 0.8659451007843018  val: loss: 1944458.125 acc: 0.3883046507835388\n",
      "step: 3880\n",
      "train: loss: 700432.4375 acc: 0.939746081829071  val: loss: 1356221.625 acc: 0.7427365779876709\n",
      "step: 3885\n",
      "train: loss: 428008.4375 acc: 0.9396378397941589  val: loss: 715256.8125 acc: 0.8640604019165039\n",
      "step: 3890\n",
      "train: loss: 482699.4375 acc: 0.9420682787895203  val: loss: 438718.28125 acc: 0.8954752087593079\n",
      "step: 3895\n",
      "train: loss: 335747.1875 acc: 0.9247384071350098  val: loss: 1120629.125 acc: 0.450702965259552\n",
      "step: 3900\n",
      "train: loss: 159819.34375 acc: 0.9874512553215027  val: loss: 648111.4375 acc: 0.5701085329055786\n",
      "step: 3905\n",
      "train: loss: 356703.15625 acc: 0.9720028042793274  val: loss: 1933005.875 acc: 0.3657788038253784\n",
      "step: 3910\n",
      "train: loss: 435330.8125 acc: 0.9627819657325745  val: loss: 556915.1875 acc: 0.37155890464782715\n",
      "step: 3915\n",
      "train: loss: 259353.125 acc: 0.9647265672683716  val: loss: 404624.0625 acc: 0.8579386472702026\n",
      "step: 3920\n",
      "train: loss: 91623.15625 acc: 0.9278866052627563  val: loss: 218907.71875 acc: 0.9625906348228455\n",
      "step: 3925\n",
      "train: loss: 31064.623046875 acc: 0.9704402089118958  val: loss: 736212.5 acc: 0.626530647277832\n",
      "step: 3930\n",
      "train: loss: 13298.484375 acc: 0.9905441403388977  val: loss: 1149079.125 acc: 0.7028610706329346\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 3935\n",
      "train: loss: 117431.515625 acc: 0.9470056891441345  val: loss: 230302.09375 acc: 0.9215602278709412\n",
      "step: 3940\n",
      "train: loss: 14068.8359375 acc: 0.9783856868743896  val: loss: 178030.828125 acc: 0.9460465312004089\n",
      "step: 3945\n",
      "train: loss: 108795.9921875 acc: 0.9248396158218384  val: loss: 685470.4375 acc: 0.9179730415344238\n",
      "step: 3950\n",
      "train: loss: 7816.93310546875 acc: 0.9768778681755066  val: loss: 459820.1875 acc: 0.9043770432472229\n",
      "step: 3955\n",
      "train: loss: 12483.9208984375 acc: 0.9631027579307556  val: loss: 451503.875 acc: 0.8969213366508484\n",
      "step: 3960\n",
      "train: loss: 9410.08984375 acc: 0.9747986197471619  val: loss: 480643.0 acc: 0.9332651495933533\n",
      "step: 3965\n",
      "train: loss: 20920.2890625 acc: 0.9688867926597595  val: loss: 410938.75 acc: 0.9666513800621033\n",
      "step: 3970\n",
      "train: loss: 112570.140625 acc: 0.959446907043457  val: loss: 692069.1875 acc: 0.7381951808929443\n",
      "step: 3975\n",
      "train: loss: 57544.18359375 acc: 0.9626198410987854  val: loss: 1899549.75 acc: 0.6489866375923157\n",
      "step: 3980\n",
      "train: loss: 43475.57421875 acc: 0.9756543636322021  val: loss: 262788.125 acc: 0.9541932344436646\n",
      "step: 3985\n",
      "train: loss: 34721.6171875 acc: 0.9686248302459717  val: loss: 221268.59375 acc: 0.96797776222229\n",
      "step: 3990\n",
      "train: loss: 20830.302734375 acc: 0.975394606590271  val: loss: 379267.9375 acc: 0.8916091918945312\n",
      "step: 3995\n",
      "train: loss: 20068.212890625 acc: 0.978023111820221  val: loss: 1086118.75 acc: 0.8909237384796143\n",
      "step: 4000\n",
      "train: loss: 37674.32421875 acc: 0.9828857183456421  val: loss: 442112.09375 acc: 0.9534158706665039\n",
      "step: 4005\n",
      "train: loss: 71005.8828125 acc: 0.9791510105133057  val: loss: 1395861.125 acc: 0.6903711557388306\n",
      "step: 4010\n",
      "train: loss: 42716.3984375 acc: 0.9818331003189087  val: loss: 1465478.125 acc: 0.8627265691757202\n",
      "step: 4015\n",
      "train: loss: 107566.984375 acc: 0.9641666412353516  val: loss: 1400139.0 acc: 0.7022172212600708\n",
      "step: 4020\n",
      "train: loss: 59422.125 acc: 0.9797441363334656  val: loss: 161608.078125 acc: 0.9808545112609863\n",
      "step: 4025\n",
      "train: loss: 61308.38671875 acc: 0.9795106053352356  val: loss: 3702749.75 acc: -0.6838372945785522\n",
      "step: 4030\n",
      "train: loss: 141344.234375 acc: 0.9522398114204407  val: loss: 964334.875 acc: 0.7549532055854797\n",
      "step: 4035\n",
      "train: loss: 50073.26171875 acc: 0.9589388966560364  val: loss: 686018.125 acc: 0.8578852415084839\n",
      "step: 4040\n",
      "train: loss: 190127.046875 acc: 0.9609853029251099  val: loss: 1584405.625 acc: 0.4417138695716858\n",
      "step: 4045\n",
      "train: loss: 144557.40625 acc: 0.9831748604774475  val: loss: 859094.4375 acc: 0.621842622756958\n",
      "step: 4050\n",
      "train: loss: 252946.1875 acc: 0.9746999740600586  val: loss: 944718.9375 acc: 0.8997305035591125\n",
      "step: 4055\n",
      "train: loss: 71892.4375 acc: 0.9912033677101135  val: loss: 1752151.5 acc: 0.8269802331924438\n",
      "step: 4060\n",
      "train: loss: 129734.234375 acc: 0.9790733456611633  val: loss: 730895.75 acc: 0.8392068147659302\n",
      "step: 4065\n",
      "train: loss: 150575.640625 acc: 0.9862819314002991  val: loss: 2770459.0 acc: 0.6159377098083496\n",
      "step: 4070\n",
      "train: loss: 456073.53125 acc: 0.9540683031082153  val: loss: 694372.9375 acc: 0.8057079315185547\n",
      "step: 4075\n",
      "train: loss: 560032.0625 acc: 0.9494324922561646  val: loss: 664007.9375 acc: 0.6498895883560181\n",
      "step: 4080\n",
      "train: loss: 1727182.0 acc: 0.7290210723876953  val: loss: 993035.5625 acc: 0.795061469078064\n",
      "step: 4085\n",
      "train: loss: 916782.9375 acc: 0.9590867757797241  val: loss: 1488386.75 acc: 0.2511671781539917\n",
      "step: 4090\n",
      "train: loss: 2871510.25 acc: 0.896856427192688  val: loss: 1169968.5 acc: 0.7597049474716187\n",
      "step: 4095\n",
      "train: loss: 1152626.5 acc: 0.9568420648574829  val: loss: 1511909.0 acc: 0.8586728572845459\n",
      "step: 4100\n",
      "train: loss: 443383.96875 acc: 0.977053165435791  val: loss: 984856.5625 acc: 0.38636672496795654\n",
      "step: 4105\n",
      "train: loss: 392810.375 acc: 0.9781771302223206  val: loss: 1532407.125 acc: 0.8553447127342224\n",
      "step: 4110\n",
      "train: loss: 138113.421875 acc: 0.9831765294075012  val: loss: 117297.2578125 acc: 0.971189022064209\n",
      "step: 4115\n",
      "train: loss: 676109.125 acc: 0.9161771535873413  val: loss: 530371.5625 acc: 0.9471691846847534\n",
      "step: 4120\n",
      "train: loss: 2731321.0 acc: 0.5288698673248291  val: loss: 368678.03125 acc: 0.8601424098014832\n",
      "step: 4125\n",
      "train: loss: 1819677.25 acc: 0.43314510583877563  val: loss: 997568.625 acc: 0.7322828769683838\n",
      "step: 4130\n",
      "train: loss: 536833.625 acc: 0.8103234171867371  val: loss: 2281380.25 acc: 0.7178314924240112\n",
      "step: 4135\n",
      "train: loss: 813684.0625 acc: 0.6129876375198364  val: loss: 1294791.25 acc: 0.8667306900024414\n",
      "step: 4140\n",
      "train: loss: 626002.9375 acc: 0.7909746170043945  val: loss: 637259.4375 acc: 0.822665810585022\n",
      "step: 4145\n",
      "train: loss: 1456021.875 acc: 0.35095465183258057  val: loss: 871425.0625 acc: 0.774048924446106\n",
      "step: 4150\n",
      "train: loss: 816530.1875 acc: 0.4672544598579407  val: loss: 2587734.75 acc: 0.739264726638794\n",
      "step: 4155\n",
      "train: loss: 912347.375 acc: 0.4205145835876465  val: loss: 2920601.75 acc: 0.5970174074172974\n",
      "step: 4160\n",
      "train: loss: 879458.6875 acc: 0.5370836853981018  val: loss: 1567395.375 acc: 0.7009991407394409\n",
      "step: 4165\n",
      "train: loss: 256899.5 acc: 0.8063856959342957  val: loss: 965922.1875 acc: 0.6744732856750488\n",
      "step: 4170\n",
      "train: loss: 293153.25 acc: 0.8384721279144287  val: loss: 729648.75 acc: 0.7767042517662048\n",
      "step: 4175\n",
      "train: loss: 153755.15625 acc: 0.9079040884971619  val: loss: 543867.9375 acc: 0.8696830868721008\n",
      "step: 4180\n",
      "train: loss: 279112.375 acc: 0.8341473340988159  val: loss: 456277.5625 acc: 0.8748867511749268\n",
      "step: 4185\n",
      "train: loss: 306480.6875 acc: 0.8376377820968628  val: loss: 733394.5625 acc: 0.7351551055908203\n",
      "step: 4190\n",
      "train: loss: 395496.71875 acc: 0.7653927206993103  val: loss: 434950.125 acc: 0.8579444289207458\n",
      "step: 4195\n",
      "train: loss: 82732.59375 acc: 0.9099698066711426  val: loss: 773434.6875 acc: 0.6418045163154602\n",
      "step: 4200\n",
      "train: loss: 302108.8125 acc: 0.8309361934661865  val: loss: 555888.375 acc: 0.8590823411941528\n",
      "step: 4205\n",
      "train: loss: 204288.625 acc: 0.8684662580490112  val: loss: 689810.0 acc: 0.8047147393226624\n",
      "step: 4210\n",
      "train: loss: 717556.0625 acc: 0.8051490187644958  val: loss: 726639.1875 acc: 0.7760511636734009\n",
      "step: 4215\n",
      "train: loss: 1105574.0 acc: 0.688235878944397  val: loss: 1953678.25 acc: 0.7248731851577759\n",
      "step: 4220\n",
      "train: loss: 544118.3125 acc: 0.7081823348999023  val: loss: 520824.46875 acc: 0.8067848086357117\n",
      "step: 4225\n",
      "train: loss: 363063.15625 acc: 0.7860815525054932  val: loss: 753593.3125 acc: 0.7541773319244385\n",
      "step: 4230\n",
      "train: loss: 847830.5625 acc: 0.7317240238189697  val: loss: 301916.59375 acc: 0.816484272480011\n",
      "step: 4235\n",
      "train: loss: 869452.875 acc: 0.8123927712440491  val: loss: 492449.96875 acc: 0.8897638916969299\n",
      "step: 4240\n",
      "train: loss: 1368685.25 acc: 0.8570746183395386  val: loss: 751000.8125 acc: 0.8853495717048645\n",
      "step: 4245\n",
      "train: loss: 616602.6875 acc: 0.9546627402305603  val: loss: 744412.1875 acc: 0.8758490681648254\n",
      "step: 4250\n",
      "train: loss: 386092.5 acc: 0.8914618492126465  val: loss: 1064669.75 acc: 0.829174280166626\n",
      "step: 4255\n",
      "train: loss: 365934.75 acc: 0.9421430826187134  val: loss: 426425.09375 acc: 0.8545602560043335\n",
      "step: 4260\n",
      "train: loss: 234380.484375 acc: 0.9676839113235474  val: loss: 689744.125 acc: 0.5472570657730103\n",
      "step: 4265\n",
      "train: loss: 298507.375 acc: 0.9786482453346252  val: loss: 213853.375 acc: 0.9259297251701355\n",
      "step: 4270\n",
      "train: loss: 261265.28125 acc: 0.9831773042678833  val: loss: 458730.1875 acc: 0.9167116284370422\n",
      "step: 4275\n",
      "train: loss: 209193.828125 acc: 0.9814476370811462  val: loss: 1356726.25 acc: 0.735226571559906\n",
      "step: 4280\n",
      "train: loss: 157258.0625 acc: 0.9626378417015076  val: loss: 896473.875 acc: 0.902518630027771\n",
      "step: 4285\n",
      "train: loss: 132236.265625 acc: 0.9717899560928345  val: loss: 180808.25 acc: 0.9709551930427551\n",
      "step: 4290\n",
      "train: loss: 6692.376953125 acc: 0.9783223867416382  val: loss: 1331826.875 acc: 0.9191967844963074\n",
      "step: 4295\n",
      "train: loss: 12550.935546875 acc: 0.9930570125579834  val: loss: 789913.9375 acc: 0.9274709224700928\n",
      "step: 4300\n",
      "train: loss: 20418.89453125 acc: 0.9851315021514893  val: loss: 211387.71875 acc: 0.9526481032371521\n",
      "step: 4305\n",
      "train: loss: 18890.736328125 acc: 0.9724341034889221  val: loss: 1260438.125 acc: 0.8313474059104919\n",
      "step: 4310\n",
      "train: loss: 46164.5703125 acc: 0.9441325664520264  val: loss: 957342.625 acc: 0.6802599430084229\n",
      "step: 4315\n",
      "train: loss: 103242.28125 acc: 0.8657149076461792  val: loss: 658663.125 acc: 0.8948334455490112\n",
      "step: 4320\n",
      "train: loss: 23187.890625 acc: 0.9601483345031738  val: loss: 642516.5 acc: 0.8779200315475464\n",
      "step: 4325\n",
      "train: loss: 24493.962890625 acc: 0.9251179695129395  val: loss: 724504.6875 acc: 0.9081382751464844\n",
      "step: 4330\n",
      "train: loss: 19078.064453125 acc: 0.9709532856941223  val: loss: 1646581.375 acc: 0.7457113265991211\n",
      "step: 4335\n",
      "train: loss: 50045.14453125 acc: 0.9649730324745178  val: loss: 1923738.75 acc: 0.3881072402000427\n",
      "step: 4340\n",
      "train: loss: 121384.7734375 acc: 0.9574618935585022  val: loss: 703279.6875 acc: 0.8921734690666199\n",
      "step: 4345\n",
      "train: loss: 86812.6953125 acc: 0.9393226504325867  val: loss: 1853581.75 acc: 0.7050665020942688\n",
      "step: 4350\n",
      "train: loss: 109007.46875 acc: 0.9457741975784302  val: loss: 774369.6875 acc: 0.8730989098548889\n",
      "step: 4355\n",
      "train: loss: 65316.25390625 acc: 0.9717493057250977  val: loss: 236991.65625 acc: 0.9780246615409851\n",
      "step: 4360\n",
      "train: loss: 26669.54296875 acc: 0.9714027643203735  val: loss: 753941.5625 acc: 0.7783697247505188\n",
      "step: 4365\n",
      "train: loss: 28161.041015625 acc: 0.9836297631263733  val: loss: 1533888.375 acc: 0.33002370595932007\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 4370\n",
      "train: loss: 67177.1484375 acc: 0.9782148599624634  val: loss: 497255.96875 acc: 0.9247199296951294\n",
      "step: 4375\n",
      "train: loss: 40084.234375 acc: 0.9869276881217957  val: loss: 3654792.5 acc: 0.5201709270477295\n",
      "step: 4380\n",
      "train: loss: 51910.55859375 acc: 0.9794526696205139  val: loss: 1257158.625 acc: 0.8148502111434937\n",
      "step: 4385\n",
      "train: loss: 52954.98046875 acc: 0.9755696058273315  val: loss: 1938788.25 acc: 0.019684016704559326\n",
      "step: 4390\n",
      "train: loss: 101991.3203125 acc: 0.876273512840271  val: loss: 321631.625 acc: 0.9358158707618713\n",
      "step: 4395\n",
      "train: loss: 123090.859375 acc: 0.9486911296844482  val: loss: 1780925.875 acc: 0.2818121314048767\n",
      "step: 4400\n",
      "train: loss: 58127.72265625 acc: 0.9651882648468018  val: loss: 1666792.75 acc: 0.6805698871612549\n",
      "step: 4405\n",
      "train: loss: 717886.3125 acc: 0.8798094987869263  val: loss: 2139386.25 acc: -0.08939003944396973\n",
      "step: 4410\n",
      "train: loss: 169270.03125 acc: 0.9808666706085205  val: loss: 358632.21875 acc: 0.7148997783660889\n",
      "step: 4415\n",
      "train: loss: 77323.984375 acc: 0.9913132786750793  val: loss: 692857.875 acc: 0.7316272258758545\n",
      "step: 4420\n",
      "train: loss: 104676.0625 acc: 0.989408016204834  val: loss: 1112671.5 acc: 0.9072440266609192\n",
      "step: 4425\n",
      "train: loss: 305615.8125 acc: 0.922578752040863  val: loss: 822047.5 acc: 0.821517288684845\n",
      "step: 4430\n",
      "train: loss: 266594.65625 acc: 0.9853188395500183  val: loss: 747280.375 acc: 0.5751832723617554\n",
      "step: 4435\n",
      "train: loss: 692755.125 acc: 0.9338692426681519  val: loss: 895840.375 acc: 0.7453503608703613\n",
      "step: 4440\n",
      "train: loss: 562464.75 acc: 0.9630124568939209  val: loss: 1818103.75 acc: 0.6287157535552979\n",
      "step: 4445\n",
      "train: loss: 344745.4375 acc: 0.8850095868110657  val: loss: 934058.9375 acc: 0.3049735426902771\n",
      "step: 4450\n",
      "train: loss: 1076492.625 acc: 0.9691243767738342  val: loss: 684075.375 acc: 0.9148190021514893\n",
      "step: 4455\n",
      "train: loss: 1727591.875 acc: 0.9570462107658386  val: loss: 243877.03125 acc: 0.9613855481147766\n",
      "step: 4460\n",
      "train: loss: 5308476.5 acc: 0.8308978080749512  val: loss: 1023818.0 acc: 0.5747718214988708\n",
      "step: 4465\n",
      "train: loss: 649610.6875 acc: 0.9645814895629883  val: loss: 431703.0 acc: 0.8130566477775574\n",
      "step: 4470\n",
      "train: loss: 283993.875 acc: 0.9797321557998657  val: loss: 1077709.25 acc: 0.7026746273040771\n",
      "step: 4475\n",
      "train: loss: 382135.71875 acc: 0.9302714467048645  val: loss: 220742.53125 acc: 0.9644818902015686\n",
      "step: 4480\n",
      "train: loss: 538293.0625 acc: 0.9270844459533691  val: loss: 1658798.75 acc: 0.6117737293243408\n",
      "step: 4485\n",
      "train: loss: 2246395.25 acc: 0.6944478154182434  val: loss: 411234.0 acc: 0.8970810174942017\n",
      "step: 4490\n",
      "train: loss: 1445211.375 acc: 0.7197490930557251  val: loss: 648804.8125 acc: 0.6748218536376953\n",
      "step: 4495\n",
      "train: loss: 761503.4375 acc: 0.5446362495422363  val: loss: 828016.1875 acc: 0.6871609687805176\n",
      "step: 4500\n",
      "train: loss: 946352.875 acc: 0.6562752723693848  val: loss: 568968.125 acc: 0.9032431244850159\n",
      "step: 4505\n",
      "train: loss: 626867.4375 acc: 0.8212756514549255  val: loss: 590001.4375 acc: 0.9008433818817139\n",
      "step: 4510\n",
      "train: loss: 1097472.0 acc: 0.7322909832000732  val: loss: 365532.0625 acc: 0.8263493180274963\n",
      "step: 4515\n",
      "train: loss: 1251179.875 acc: 0.4344927668571472  val: loss: 632432.3125 acc: 0.09585309028625488\n",
      "step: 4520\n",
      "train: loss: 1012307.8125 acc: 0.531729519367218  val: loss: 904874.9375 acc: 0.570916473865509\n",
      "step: 4525\n",
      "train: loss: 499769.75 acc: 0.5641528367996216  val: loss: 2357111.75 acc: 0.5194605588912964\n",
      "step: 4530\n",
      "train: loss: 421489.375 acc: 0.6973459124565125  val: loss: 808557.125 acc: 0.8062509894371033\n",
      "step: 4535\n",
      "train: loss: 270343.96875 acc: 0.7774986028671265  val: loss: 1621887.625 acc: 0.75418621301651\n",
      "step: 4540\n",
      "train: loss: 161477.21875 acc: 0.9074406623840332  val: loss: 853072.5 acc: 0.7850669622421265\n",
      "step: 4545\n",
      "train: loss: 149660.609375 acc: 0.9038618803024292  val: loss: 424946.75 acc: 0.8078506588935852\n",
      "step: 4550\n",
      "train: loss: 69711.4296875 acc: 0.9451465606689453  val: loss: 758353.625 acc: 0.8152974247932434\n",
      "step: 4555\n",
      "train: loss: 160304.75 acc: 0.8391051888465881  val: loss: 1059059.625 acc: 0.7663521766662598\n",
      "step: 4560\n",
      "train: loss: 71780.40625 acc: 0.933761715888977  val: loss: 981663.125 acc: 0.7577146291732788\n",
      "step: 4565\n",
      "train: loss: 55720.609375 acc: 0.9354960918426514  val: loss: 1855249.625 acc: 0.7065092325210571\n",
      "step: 4570\n",
      "train: loss: 223548.125 acc: 0.8237998485565186  val: loss: 1100155.125 acc: 0.8052077293395996\n",
      "step: 4575\n",
      "train: loss: 313037.96875 acc: 0.8015321493148804  val: loss: 170561.828125 acc: 0.809944212436676\n",
      "step: 4580\n",
      "train: loss: 283335.125 acc: 0.8019180297851562  val: loss: 1618658.375 acc: 0.6744238138198853\n",
      "step: 4585\n",
      "train: loss: 335091.0 acc: 0.7713258266448975  val: loss: 667108.4375 acc: 0.7862029671669006\n",
      "step: 4590\n",
      "train: loss: 856300.5625 acc: 0.7562495470046997  val: loss: 1150201.0 acc: 0.7089152336120605\n",
      "step: 4595\n",
      "train: loss: 684371.5625 acc: 0.7506074905395508  val: loss: 1216654.25 acc: 0.7381297945976257\n",
      "step: 4600\n",
      "train: loss: 1499568.5 acc: 0.7624987363815308  val: loss: 975391.25 acc: 0.8617258071899414\n",
      "step: 4605\n",
      "train: loss: 1207495.125 acc: 0.8280499577522278  val: loss: 1112943.125 acc: 0.8494884967803955\n",
      "step: 4610\n",
      "train: loss: 846925.0625 acc: 0.932260274887085  val: loss: 924743.1875 acc: 0.3714779019355774\n",
      "step: 4615\n",
      "train: loss: 490415.0625 acc: 0.9337286353111267  val: loss: 1073198.0 acc: 0.8875040411949158\n",
      "step: 4620\n",
      "train: loss: 341537.6875 acc: 0.9591385722160339  val: loss: 1084303.5 acc: 0.8613453507423401\n",
      "step: 4625\n",
      "train: loss: 405968.9375 acc: 0.9443858861923218  val: loss: 1029530.9375 acc: 0.6792049407958984\n",
      "step: 4630\n",
      "train: loss: 304393.53125 acc: 0.9725894331932068  val: loss: 712332.9375 acc: 0.8873605132102966\n",
      "step: 4635\n",
      "train: loss: 309354.875 acc: 0.9713560342788696  val: loss: 2426735.0 acc: 0.45617955923080444\n",
      "step: 4640\n",
      "train: loss: 232240.625 acc: 0.9809261560440063  val: loss: 864225.6875 acc: 0.8218678832054138\n",
      "step: 4645\n",
      "train: loss: 258784.140625 acc: 0.9629532098770142  val: loss: 970198.0 acc: 0.6532489061355591\n",
      "step: 4650\n",
      "train: loss: 190529.046875 acc: 0.9725061655044556  val: loss: 1086738.125 acc: 0.5988081693649292\n",
      "step: 4655\n",
      "train: loss: 90204.0625 acc: 0.9416816234588623  val: loss: 1674715.125 acc: 0.7335247993469238\n",
      "step: 4660\n",
      "train: loss: 45096.47265625 acc: 0.9801811575889587  val: loss: 1101731.0 acc: 0.8975027799606323\n",
      "step: 4665\n",
      "train: loss: 9663.6396484375 acc: 0.975209653377533  val: loss: 549569.25 acc: 0.9447894096374512\n",
      "step: 4670\n",
      "train: loss: 71567.8671875 acc: 0.9672448635101318  val: loss: 2514780.75 acc: 0.5034810304641724\n",
      "step: 4675\n",
      "train: loss: 24573.9609375 acc: 0.8796396851539612  val: loss: 1160321.75 acc: 0.7181470990180969\n",
      "step: 4680\n",
      "train: loss: 34003.50390625 acc: 0.9306125640869141  val: loss: 679855.3125 acc: 0.7568235993385315\n",
      "step: 4685\n",
      "train: loss: 10461.634765625 acc: 0.9740213751792908  val: loss: 997474.875 acc: 0.885801374912262\n",
      "step: 4690\n",
      "train: loss: 13647.353515625 acc: 0.9540895223617554  val: loss: 1047947.1875 acc: 0.8291987180709839\n",
      "step: 4695\n",
      "train: loss: 25393.12109375 acc: 0.9523522853851318  val: loss: 1127182.0 acc: 0.7677221298217773\n",
      "step: 4700\n",
      "train: loss: 33445.0859375 acc: 0.9754304885864258  val: loss: 1334508.625 acc: 0.8766061067581177\n",
      "step: 4705\n",
      "train: loss: 82101.3828125 acc: 0.960801362991333  val: loss: 1647565.25 acc: 0.2840302586555481\n",
      "step: 4710\n",
      "train: loss: 76111.6484375 acc: 0.9715595245361328  val: loss: 1561682.5 acc: 0.5152773857116699\n",
      "step: 4715\n",
      "train: loss: 28234.505859375 acc: 0.9775575399398804  val: loss: 1054524.375 acc: 0.8844678401947021\n",
      "step: 4720\n",
      "train: loss: 12586.5341796875 acc: 0.9678025841712952  val: loss: 1652005.5 acc: 0.688548743724823\n",
      "step: 4725\n",
      "train: loss: 47123.91015625 acc: 0.9527736306190491  val: loss: 368307.84375 acc: 0.914617121219635\n",
      "step: 4730\n",
      "train: loss: 43187.70703125 acc: 0.9688349366188049  val: loss: 2740275.75 acc: 0.3317068815231323\n",
      "step: 4735\n",
      "train: loss: 63584.41015625 acc: 0.9762744903564453  val: loss: 840017.6875 acc: 0.7592621445655823\n",
      "step: 4740\n",
      "train: loss: 76688.3125 acc: 0.9789731502532959  val: loss: 1973091.25 acc: 0.6297346353530884\n",
      "step: 4745\n",
      "train: loss: 29864.03515625 acc: 0.992780864238739  val: loss: 2948373.25 acc: -0.8960312604904175\n",
      "step: 4750\n",
      "train: loss: 69369.71875 acc: 0.9795317649841309  val: loss: 2307197.5 acc: 0.7032519578933716\n",
      "step: 4755\n",
      "train: loss: 81593.6015625 acc: 0.9778985381126404  val: loss: 898101.75 acc: 0.8060837984085083\n",
      "step: 4760\n",
      "train: loss: 290544.625 acc: 0.880502462387085  val: loss: 387392.625 acc: 0.9457498788833618\n",
      "step: 4765\n",
      "train: loss: 159663.3125 acc: 0.9436513185501099  val: loss: 379889.15625 acc: 0.891732931137085\n",
      "step: 4770\n",
      "train: loss: 120602.7734375 acc: 0.9658992886543274  val: loss: 1830534.625 acc: 0.826780378818512\n",
      "step: 4775\n",
      "train: loss: 156644.984375 acc: 0.9790903329849243  val: loss: 1576232.75 acc: 0.7266031503677368\n",
      "step: 4780\n",
      "train: loss: 218436.65625 acc: 0.9754260182380676  val: loss: 516372.96875 acc: 0.933793306350708\n",
      "step: 4785\n",
      "train: loss: 70990.5390625 acc: 0.9877371191978455  val: loss: 1793920.75 acc: 0.6532948017120361\n",
      "step: 4790\n",
      "train: loss: 492103.84375 acc: 0.9510523676872253  val: loss: 402695.09375 acc: 0.9297837018966675\n",
      "step: 4795\n",
      "train: loss: 237918.6875 acc: 0.9752786159515381  val: loss: 1767735.125 acc: 0.591299295425415\n",
      "step: 4800\n",
      "train: loss: 1634527.625 acc: 0.8220177888870239  val: loss: 710959.375 acc: 0.8428357839584351\n",
      "step: 4805\n",
      "train: loss: 702914.0 acc: 0.9501782655715942  val: loss: 742824.8125 acc: 0.6722543239593506\n",
      "step: 4810\n",
      "train: loss: 466807.21875 acc: 0.9356262683868408  val: loss: 908467.125 acc: 0.7054615020751953\n",
      "step: 4815\n",
      "train: loss: 1245678.125 acc: 0.9435384273529053  val: loss: 1709132.0 acc: 0.7671813368797302\n",
      "step: 4820\n",
      "train: loss: 690762.5 acc: 0.9740776419639587  val: loss: 697455.75 acc: 0.4018339514732361\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 4825\n",
      "train: loss: 2804199.0 acc: 0.8678300380706787  val: loss: 467675.03125 acc: 0.912415623664856\n",
      "step: 4830\n",
      "train: loss: 1089277.875 acc: 0.942593514919281  val: loss: 1176350.5 acc: 0.8459938168525696\n",
      "step: 4835\n",
      "train: loss: 900663.3125 acc: 0.9383701682090759  val: loss: 1054312.0 acc: 0.7107865810394287\n",
      "step: 4840\n",
      "train: loss: 3829977.0 acc: 0.7058075666427612  val: loss: 415572.15625 acc: 0.9422297477722168\n",
      "step: 4845\n",
      "train: loss: 471186.53125 acc: 0.9524414539337158  val: loss: 1134773.625 acc: 0.6563960313796997\n",
      "step: 4850\n",
      "train: loss: 794703.5 acc: 0.8467868566513062  val: loss: 677430.4375 acc: 0.45493263006210327\n",
      "step: 4855\n",
      "train: loss: 1699485.125 acc: 0.5168842673301697  val: loss: 536777.625 acc: 0.8590128421783447\n",
      "step: 4860\n",
      "train: loss: 912907.1875 acc: 0.6904992461204529  val: loss: 1092521.625 acc: 0.8118507862091064\n",
      "step: 4865\n",
      "train: loss: 634606.9375 acc: 0.7929297685623169  val: loss: 1004451.5625 acc: 0.8188203573226929\n",
      "step: 4870\n",
      "train: loss: 935335.1875 acc: 0.6486748456954956  val: loss: 1120296.0 acc: 0.8758252859115601\n",
      "step: 4875\n",
      "train: loss: 1495336.0 acc: 0.564698338508606  val: loss: 486905.40625 acc: 0.8777907490730286\n",
      "step: 4880\n",
      "train: loss: 878734.8125 acc: 0.3990055322647095  val: loss: 1871669.5 acc: 0.7222765684127808\n",
      "step: 4885\n",
      "train: loss: 634147.1875 acc: 0.49673599004745483  val: loss: 1108926.0 acc: 0.6382630467414856\n",
      "step: 4890\n",
      "train: loss: 494099.71875 acc: 0.7103095650672913  val: loss: 1314756.375 acc: 0.591741681098938\n",
      "step: 4895\n",
      "train: loss: 282233.875 acc: 0.7485824823379517  val: loss: 1262708.75 acc: 0.7220813035964966\n",
      "step: 4900\n",
      "train: loss: 588405.9375 acc: 0.7882716059684753  val: loss: 1307466.75 acc: 0.7856318950653076\n",
      "step: 4905\n",
      "train: loss: 49166.23046875 acc: 0.9542959928512573  val: loss: 1983364.125 acc: 0.7769638299942017\n",
      "step: 4910\n",
      "train: loss: 72176.859375 acc: 0.9431962370872498  val: loss: 2142944.75 acc: 0.689382791519165\n",
      "step: 4915\n",
      "train: loss: 82837.484375 acc: 0.9378808736801147  val: loss: 602843.3125 acc: 0.7915380001068115\n",
      "step: 4920\n",
      "train: loss: 274260.15625 acc: 0.7291306257247925  val: loss: 563515.8125 acc: 0.8187588453292847\n",
      "step: 4925\n",
      "train: loss: 353200.9375 acc: 0.7636473774909973  val: loss: 1562695.625 acc: 0.7551307082176208\n",
      "step: 4930\n",
      "train: loss: 59758.8671875 acc: 0.9217081069946289  val: loss: 435882.0625 acc: 0.8673874139785767\n",
      "step: 4935\n",
      "train: loss: 400952.09375 acc: 0.7978159189224243  val: loss: 1942095.75 acc: 0.7759214043617249\n",
      "step: 4940\n",
      "train: loss: 572712.9375 acc: 0.7932766079902649  val: loss: 2338054.5 acc: 0.7414467334747314\n",
      "step: 4945\n",
      "train: loss: 351271.40625 acc: 0.7219651937484741  val: loss: 2926152.25 acc: 0.6841768026351929\n",
      "step: 4950\n",
      "train: loss: 402739.8125 acc: 0.5939854383468628  val: loss: 4587791.0 acc: 0.616989016532898\n",
      "step: 4955\n",
      "train: loss: 242356.71875 acc: 0.8469204902648926  val: loss: 2862966.25 acc: 0.6988236904144287\n",
      "step: 4960\n",
      "train: loss: 651128.6875 acc: 0.7974338531494141  val: loss: 400671.09375 acc: 0.7607598304748535\n",
      "step: 4965\n",
      "train: loss: 1355672.125 acc: 0.7795637845993042  val: loss: 440659.75 acc: 0.6551481485366821\n",
      "step: 4970\n",
      "train: loss: 1247363.375 acc: 0.7035707235336304  val: loss: 1114583.5 acc: 0.7240111231803894\n",
      "step: 4975\n",
      "train: loss: 1366706.875 acc: 0.8802219033241272  val: loss: 507743.125 acc: 0.842275083065033\n",
      "step: 4980\n",
      "train: loss: 347346.9375 acc: 0.9591591358184814  val: loss: 1196616.125 acc: 0.7432553768157959\n",
      "step: 4985\n",
      "train: loss: 447283.875 acc: 0.8926874399185181  val: loss: 1668438.5 acc: 0.8694787621498108\n",
      "step: 4990\n",
      "train: loss: 248861.453125 acc: 0.9568937420845032  val: loss: 2333712.0 acc: 0.7698279023170471\n",
      "step: 4995\n",
      "train: loss: 219505.328125 acc: 0.9729472994804382  val: loss: 2741305.75 acc: -0.1595224142074585\n",
      "step: 5000\n",
      "train: loss: 232664.546875 acc: 0.9834323525428772  val: loss: 862648.125 acc: 0.4710867404937744\n",
      "step: 5005\n",
      "train: loss: 311105.34375 acc: 0.9683740735054016  val: loss: 1182394.5 acc: 0.5701892375946045\n",
      "step: 5010\n",
      "train: loss: 127041.5859375 acc: 0.9853849411010742  val: loss: 453022.15625 acc: 0.8673240542411804\n",
      "step: 5015\n",
      "train: loss: 112873.3046875 acc: 0.9696119427680969  val: loss: 365037.53125 acc: 0.9500570893287659\n",
      "step: 5020\n",
      "train: loss: 143054.4375 acc: 0.9607604146003723  val: loss: 1671903.125 acc: 0.6675337553024292\n",
      "step: 5025\n",
      "train: loss: 18236.5546875 acc: 0.9309841394424438  val: loss: 3009470.25 acc: 0.6184265613555908\n",
      "step: 5030\n",
      "train: loss: 54425.60546875 acc: 0.97104811668396  val: loss: 1032209.0625 acc: 0.8866031169891357\n",
      "step: 5035\n",
      "train: loss: 19590.0625 acc: 0.9198763370513916  val: loss: 1740247.5 acc: 0.4052698016166687\n",
      "step: 5040\n",
      "train: loss: 23411.23828125 acc: 0.9561556577682495  val: loss: 1388744.25 acc: -0.4029107093811035\n",
      "step: 5045\n",
      "train: loss: 13149.6083984375 acc: 0.9519585967063904  val: loss: 965043.9375 acc: 0.8526186943054199\n",
      "step: 5050\n",
      "train: loss: 13157.21484375 acc: 0.9586553573608398  val: loss: 777159.9375 acc: 0.5476014018058777\n",
      "step: 5055\n",
      "train: loss: 100388.4140625 acc: 0.9285430312156677  val: loss: 1024308.3125 acc: 0.8339472413063049\n",
      "step: 5060\n",
      "train: loss: 34049.546875 acc: 0.9565133452415466  val: loss: 877812.8125 acc: 0.598654568195343\n",
      "step: 5065\n",
      "train: loss: 72162.640625 acc: 0.9508793950080872  val: loss: 273330.90625 acc: 0.9618232250213623\n",
      "step: 5070\n",
      "train: loss: 66107.453125 acc: 0.9656187295913696  val: loss: 363684.875 acc: 0.8656816482543945\n",
      "step: 5075\n",
      "train: loss: 31497.375 acc: 0.9825648069381714  val: loss: 655476.875 acc: 0.8205263018608093\n",
      "step: 5080\n",
      "train: loss: 22297.845703125 acc: 0.9856433868408203  val: loss: 185780.75 acc: 0.9592843055725098\n",
      "step: 5085\n",
      "train: loss: 74613.3203125 acc: 0.9640514850616455  val: loss: 413558.40625 acc: 0.9498226642608643\n",
      "step: 5090\n",
      "train: loss: 36028.21484375 acc: 0.9777985215187073  val: loss: 749004.0625 acc: 0.5994837284088135\n",
      "step: 5095\n",
      "train: loss: 40914.296875 acc: 0.9768474698066711  val: loss: 681485.75 acc: 0.7556593418121338\n",
      "step: 5100\n",
      "train: loss: 35960.62109375 acc: 0.978823184967041  val: loss: 1623393.25 acc: 0.6727186441421509\n",
      "step: 5105\n",
      "train: loss: 66205.34375 acc: 0.9853556156158447  val: loss: 1957213.625 acc: 0.786069393157959\n",
      "step: 5110\n",
      "train: loss: 37745.28515625 acc: 0.9854565262794495  val: loss: 1027816.75 acc: 0.7872225046157837\n",
      "step: 5115\n",
      "train: loss: 31707.708984375 acc: 0.9880872964859009  val: loss: 938720.4375 acc: 0.7610195875167847\n",
      "step: 5120\n",
      "train: loss: 73654.0234375 acc: 0.9843400120735168  val: loss: 491534.96875 acc: 0.9125046730041504\n",
      "step: 5125\n",
      "train: loss: 87563.171875 acc: 0.9711191058158875  val: loss: 364508.5625 acc: 0.827491283416748\n",
      "step: 5130\n",
      "train: loss: 120906.5078125 acc: 0.9386018514633179  val: loss: 1312577.25 acc: 0.7347385883331299\n",
      "step: 5135\n",
      "train: loss: 55643.1640625 acc: 0.9755375981330872  val: loss: 978469.1875 acc: 0.8292984962463379\n",
      "step: 5140\n",
      "train: loss: 592803.0625 acc: 0.9201584458351135  val: loss: 844602.0625 acc: 0.5102916955947876\n",
      "step: 5145\n",
      "train: loss: 115851.65625 acc: 0.988967776298523  val: loss: 486146.03125 acc: 0.8868926167488098\n",
      "step: 5150\n",
      "train: loss: 147218.359375 acc: 0.9853911995887756  val: loss: 1360337.5 acc: 0.6229726672172546\n",
      "step: 5155\n",
      "train: loss: 280475.46875 acc: 0.9665400385856628  val: loss: 180242.609375 acc: 0.9644565582275391\n",
      "step: 5160\n",
      "train: loss: 157415.4375 acc: 0.9778975248336792  val: loss: 686677.8125 acc: 0.8517987728118896\n",
      "step: 5165\n",
      "train: loss: 494447.4375 acc: 0.9750695824623108  val: loss: 668793.875 acc: 0.7829605340957642\n",
      "step: 5170\n",
      "train: loss: 364775.9375 acc: 0.9727614521980286  val: loss: 825841.875 acc: 0.7887171506881714\n",
      "step: 5175\n",
      "train: loss: 456893.0 acc: 0.966144859790802  val: loss: 1121790.125 acc: 0.745732307434082\n",
      "step: 5180\n",
      "train: loss: 1895670.625 acc: 0.8764176964759827  val: loss: 785524.0 acc: 0.7762570381164551\n",
      "step: 5185\n",
      "train: loss: 1243534.5 acc: 0.9698984622955322  val: loss: 525183.5 acc: 0.8793762922286987\n",
      "step: 5190\n",
      "train: loss: 471042.96875 acc: 0.9732847213745117  val: loss: 1802278.375 acc: 0.6187483072280884\n",
      "step: 5195\n",
      "train: loss: 744377.3125 acc: 0.9606544375419617  val: loss: 276520.15625 acc: 0.945288896560669\n",
      "step: 5200\n",
      "train: loss: 1233605.75 acc: 0.9092440009117126  val: loss: 569122.4375 acc: 0.8612955212593079\n",
      "step: 5205\n",
      "train: loss: 447664.3125 acc: 0.9568443894386292  val: loss: 308316.09375 acc: 0.9494403600692749\n",
      "step: 5210\n",
      "train: loss: 461368.21875 acc: 0.9477135539054871  val: loss: 224636.328125 acc: 0.9745620489120483\n",
      "step: 5215\n",
      "train: loss: 479329.3125 acc: 0.9415774345397949  val: loss: 787041.875 acc: 0.8632235527038574\n",
      "step: 5220\n",
      "train: loss: 1984295.25 acc: 0.19423794746398926  val: loss: 801117.5625 acc: 0.9312537908554077\n",
      "step: 5225\n",
      "train: loss: 827560.9375 acc: 0.7462239265441895  val: loss: 717174.3125 acc: 0.88939368724823\n",
      "step: 5230\n",
      "train: loss: 605852.5625 acc: 0.8563259840011597  val: loss: 554840.6875 acc: 0.884190022945404\n",
      "step: 5235\n",
      "train: loss: 506660.03125 acc: 0.6695876717567444  val: loss: 251382.65625 acc: 0.8792569637298584\n",
      "step: 5240\n",
      "train: loss: 1020898.9375 acc: 0.5070011615753174  val: loss: 1620901.25 acc: 0.8094416856765747\n",
      "step: 5245\n",
      "train: loss: 1023978.625 acc: 0.34852367639541626  val: loss: 1792967.0 acc: 0.8417097330093384\n",
      "step: 5250\n",
      "train: loss: 862094.5625 acc: 0.5886090397834778  val: loss: 1257498.125 acc: 0.6984812021255493\n",
      "step: 5255\n",
      "train: loss: 620083.0625 acc: 0.5964084267616272  val: loss: 1704741.625 acc: 0.6089907288551331\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 5260\n",
      "train: loss: 481914.3125 acc: 0.6029580235481262  val: loss: 3387445.0 acc: 0.6596765518188477\n",
      "step: 5265\n",
      "train: loss: 381669.9375 acc: 0.7304229736328125  val: loss: 2373894.75 acc: 0.7088019847869873\n",
      "step: 5270\n",
      "train: loss: 205781.8125 acc: 0.826508641242981  val: loss: 2702391.0 acc: 0.7575608491897583\n",
      "step: 5275\n",
      "train: loss: 153593.421875 acc: 0.8904082179069519  val: loss: 1308016.25 acc: 0.7365256547927856\n",
      "step: 5280\n",
      "train: loss: 661571.5625 acc: 0.7503025531768799  val: loss: 287099.15625 acc: 0.863062858581543\n",
      "step: 5285\n",
      "train: loss: 58839.05859375 acc: 0.954247772693634  val: loss: 780044.1875 acc: 0.7552946209907532\n",
      "step: 5290\n",
      "train: loss: 83216.1171875 acc: 0.9155524373054504  val: loss: 850363.6875 acc: 0.7347012758255005\n",
      "step: 5295\n",
      "train: loss: 117632.0 acc: 0.8828575611114502  val: loss: 869349.6875 acc: 0.5810184478759766\n",
      "step: 5300\n",
      "train: loss: 69039.453125 acc: 0.9314032196998596  val: loss: 1265562.375 acc: 0.7767009139060974\n",
      "step: 5305\n",
      "train: loss: 512911.78125 acc: 0.7501987218856812  val: loss: 1861121.75 acc: 0.733356773853302\n",
      "step: 5310\n",
      "train: loss: 478295.625 acc: 0.7504856586456299  val: loss: 4689386.0 acc: 0.6219631433486938\n",
      "step: 5315\n",
      "train: loss: 678835.0 acc: 0.7428037524223328  val: loss: 557311.125 acc: 0.7443031668663025\n",
      "step: 5320\n",
      "train: loss: 400798.71875 acc: 0.7679588794708252  val: loss: 1332625.5 acc: 0.6572309732437134\n",
      "step: 5325\n",
      "train: loss: 476722.25 acc: 0.8066514730453491  val: loss: 606571.4375 acc: 0.7621721029281616\n",
      "step: 5330\n",
      "train: loss: 2652514.75 acc: 0.7019859552383423  val: loss: 1216572.25 acc: 0.7744680643081665\n",
      "step: 5335\n",
      "train: loss: 923118.375 acc: 0.7863472700119019  val: loss: 1705360.0 acc: 0.7445410490036011\n",
      "step: 5340\n",
      "train: loss: 1145088.375 acc: 0.9215648174285889  val: loss: 1099207.0 acc: 0.5335934162139893\n",
      "step: 5345\n",
      "train: loss: 567381.375 acc: 0.9206852912902832  val: loss: 1886301.25 acc: -0.3463921546936035\n",
      "step: 5350\n",
      "train: loss: 306591.78125 acc: 0.8970264196395874  val: loss: 680698.6875 acc: 0.8999878168106079\n",
      "step: 5355\n",
      "train: loss: 278753.53125 acc: 0.9705414772033691  val: loss: 640215.3125 acc: 0.8140190839767456\n",
      "step: 5360\n",
      "train: loss: 132256.984375 acc: 0.9876028299331665  val: loss: 828588.375 acc: 0.88780677318573\n",
      "step: 5365\n",
      "train: loss: 232826.765625 acc: 0.9822417497634888  val: loss: 515652.84375 acc: 0.8307008743286133\n",
      "step: 5370\n",
      "train: loss: 263285.65625 acc: 0.9745831489562988  val: loss: 1352564.25 acc: 0.7498081922531128\n",
      "step: 5375\n",
      "train: loss: 249728.984375 acc: 0.9538419842720032  val: loss: 1660908.375 acc: 0.5548210144042969\n",
      "step: 5380\n",
      "train: loss: 435838.09375 acc: 0.9303943514823914  val: loss: 1501254.375 acc: 0.8114224076271057\n",
      "step: 5385\n",
      "train: loss: 132651.859375 acc: 0.9349868297576904  val: loss: 375250.96875 acc: 0.9246945977210999\n",
      "step: 5390\n",
      "train: loss: 123568.875 acc: 0.97145015001297  val: loss: 511450.09375 acc: 0.900831937789917\n",
      "step: 5395\n",
      "train: loss: 41428.00390625 acc: 0.9615823030471802  val: loss: 1362365.0 acc: 0.3589949607849121\n",
      "step: 5400\n",
      "train: loss: 29424.712890625 acc: 0.9741353988647461  val: loss: 529371.875 acc: 0.916323184967041\n",
      "step: 5405\n",
      "train: loss: 6374.46484375 acc: 0.9960511326789856  val: loss: 330486.21875 acc: 0.8960005640983582\n",
      "step: 5410\n",
      "train: loss: 25110.111328125 acc: 0.9605710506439209  val: loss: 846478.5625 acc: 0.7483697533607483\n",
      "step: 5415\n",
      "train: loss: 15421.7294921875 acc: 0.9728664755821228  val: loss: 613806.5 acc: 0.8749629259109497\n",
      "step: 5420\n",
      "train: loss: 15964.8408203125 acc: 0.9727468490600586  val: loss: 651240.25 acc: 0.8231054544448853\n",
      "step: 5425\n",
      "train: loss: 14978.916015625 acc: 0.9850379824638367  val: loss: 400315.78125 acc: 0.9272796511650085\n",
      "step: 5430\n",
      "train: loss: 36676.87109375 acc: 0.9661037921905518  val: loss: 2665778.75 acc: 0.5078990459442139\n",
      "step: 5435\n",
      "train: loss: 64640.59375 acc: 0.9703835844993591  val: loss: 283588.09375 acc: 0.9256956577301025\n",
      "step: 5440\n",
      "train: loss: 64100.2578125 acc: 0.9665322303771973  val: loss: 580968.9375 acc: 0.8974490761756897\n",
      "step: 5445\n",
      "train: loss: 16195.314453125 acc: 0.9891787171363831  val: loss: 271942.0625 acc: 0.9663382172584534\n",
      "step: 5450\n",
      "train: loss: 19111.728515625 acc: 0.9737161993980408  val: loss: 791537.75 acc: 0.7908254861831665\n",
      "step: 5455\n",
      "train: loss: 24922.267578125 acc: 0.9702764749526978  val: loss: 933070.375 acc: 0.6788431406021118\n",
      "step: 5460\n",
      "train: loss: 9556.861328125 acc: 0.9837491512298584  val: loss: 570265.5 acc: 0.8610459566116333\n",
      "step: 5465\n",
      "train: loss: 16179.8818359375 acc: 0.9922420382499695  val: loss: 549388.6875 acc: 0.7788097262382507\n",
      "step: 5470\n",
      "train: loss: 56928.4453125 acc: 0.9829750657081604  val: loss: 496232.28125 acc: 0.9107120037078857\n",
      "step: 5475\n",
      "train: loss: 56919.66796875 acc: 0.979007363319397  val: loss: 2391748.75 acc: -0.04345595836639404\n",
      "step: 5480\n",
      "train: loss: 33917.66015625 acc: 0.9819575548171997  val: loss: 1125061.5 acc: 0.7751637697219849\n",
      "step: 5485\n",
      "train: loss: 50764.92578125 acc: 0.9744316935539246  val: loss: 681180.5 acc: 0.6525861024856567\n",
      "step: 5490\n",
      "train: loss: 157581.390625 acc: 0.965690016746521  val: loss: 1384168.125 acc: 0.4016813039779663\n",
      "step: 5495\n",
      "train: loss: 115017.75 acc: 0.970284640789032  val: loss: 370329.53125 acc: 0.9605017304420471\n",
      "step: 5500\n",
      "train: loss: 48740.484375 acc: 0.9835173487663269  val: loss: 561465.8125 acc: 0.8591657876968384\n",
      "step: 5505\n",
      "train: loss: 150679.546875 acc: 0.9686267375946045  val: loss: 437252.28125 acc: 0.8833894729614258\n",
      "step: 5510\n",
      "train: loss: 182575.078125 acc: 0.9797991514205933  val: loss: 2411255.25 acc: 0.3397599458694458\n",
      "step: 5515\n",
      "train: loss: 56336.44140625 acc: 0.9934994578361511  val: loss: 882953.875 acc: 0.8010704517364502\n",
      "step: 5520\n",
      "train: loss: 65229.23046875 acc: 0.9859950542449951  val: loss: 498437.125 acc: 0.9004316926002502\n",
      "step: 5525\n",
      "train: loss: 93398.875 acc: 0.9894644021987915  val: loss: 1258580.75 acc: 0.26551133394241333\n",
      "step: 5530\n",
      "train: loss: 341196.4375 acc: 0.9710260629653931  val: loss: 828155.75 acc: 0.8144779205322266\n",
      "step: 5535\n",
      "train: loss: 563336.625 acc: 0.9396545886993408  val: loss: 850918.125 acc: 0.7875046133995056\n",
      "step: 5540\n",
      "train: loss: 295586.1875 acc: 0.9810113310813904  val: loss: 686160.5 acc: 0.9054899215698242\n",
      "step: 5545\n",
      "train: loss: 921743.875 acc: 0.9130234718322754  val: loss: 1217872.5 acc: 0.8172922134399414\n",
      "step: 5550\n",
      "train: loss: 598521.125 acc: 0.9846811294555664  val: loss: 601229.3125 acc: 0.9333067536354065\n",
      "step: 5555\n",
      "train: loss: 1328798.875 acc: 0.9578356146812439  val: loss: 997402.0 acc: 0.8943901658058167\n",
      "step: 5560\n",
      "train: loss: 1415555.625 acc: 0.9285821914672852  val: loss: 575665.25 acc: 0.8478043675422668\n",
      "step: 5565\n",
      "train: loss: 572012.5625 acc: 0.9460839033126831  val: loss: 202306.09375 acc: 0.9616981744766235\n",
      "step: 5570\n",
      "train: loss: 314075.34375 acc: 0.9670881628990173  val: loss: 668121.375 acc: 0.8906583786010742\n",
      "step: 5575\n",
      "train: loss: 439742.34375 acc: 0.9429375529289246  val: loss: 345221.5 acc: 0.9774428009986877\n",
      "step: 5580\n",
      "train: loss: 1700430.625 acc: 0.8084739446640015  val: loss: 1028014.9375 acc: 0.8910371661186218\n",
      "step: 5585\n",
      "train: loss: 1299733.5 acc: 0.4282434582710266  val: loss: 2075378.75 acc: 0.8074541091918945\n",
      "step: 5590\n",
      "train: loss: 617841.375 acc: 0.7707252502441406  val: loss: 1127948.5 acc: 0.6298953294754028\n",
      "step: 5595\n",
      "train: loss: 315368.25 acc: 0.8717821836471558  val: loss: 1874604.875 acc: 0.5707926750183105\n",
      "step: 5600\n",
      "train: loss: 343477.6875 acc: 0.8781269192695618  val: loss: 1029655.875 acc: 0.8390384316444397\n",
      "step: 5605\n",
      "train: loss: 375952.28125 acc: 0.8585754632949829  val: loss: 953602.5 acc: 0.9133589267730713\n",
      "step: 5610\n",
      "train: loss: 1759033.125 acc: 0.21793347597122192  val: loss: 574498.75 acc: 0.4239538311958313\n",
      "step: 5615\n",
      "train: loss: 834234.875 acc: 0.5999091863632202  val: loss: 1511010.5 acc: 0.6517544984817505\n",
      "step: 5620\n",
      "train: loss: 560245.8125 acc: 0.5982208251953125  val: loss: 1110870.375 acc: 0.6146146059036255\n",
      "step: 5625\n",
      "train: loss: 365849.28125 acc: 0.723724365234375  val: loss: 1565380.375 acc: 0.7788283824920654\n",
      "step: 5630\n",
      "train: loss: 292775.09375 acc: 0.7326483726501465  val: loss: 2593107.5 acc: 0.7119699120521545\n",
      "step: 5635\n",
      "train: loss: 55809.63671875 acc: 0.9564239382743835  val: loss: 150426.8125 acc: 0.8457609415054321\n",
      "step: 5640\n",
      "train: loss: 149744.5625 acc: 0.8857290744781494  val: loss: 1640103.75 acc: 0.7552865743637085\n",
      "step: 5645\n",
      "train: loss: 170879.9375 acc: 0.8590499758720398  val: loss: 2388333.25 acc: 0.7432405352592468\n",
      "step: 5650\n",
      "train: loss: 165046.8125 acc: 0.8650489449501038  val: loss: 1294980.0 acc: 0.7847649455070496\n",
      "step: 5655\n",
      "train: loss: 176039.5625 acc: 0.8377794027328491  val: loss: 419226.71875 acc: 0.7967677712440491\n",
      "step: 5660\n",
      "train: loss: 227781.109375 acc: 0.8757581114768982  val: loss: 739723.25 acc: 0.7644028663635254\n",
      "step: 5665\n",
      "train: loss: 206208.84375 acc: 0.8663747310638428  val: loss: 712241.0 acc: 0.7783589363098145\n",
      "step: 5670\n",
      "train: loss: 442342.71875 acc: 0.7813255786895752  val: loss: 1015680.4375 acc: 0.7295585870742798\n",
      "step: 5675\n",
      "train: loss: 409670.40625 acc: 0.7071768045425415  val: loss: 3843478.5 acc: 0.6353845596313477\n",
      "step: 5680\n",
      "train: loss: 203194.0 acc: 0.8350236415863037  val: loss: 1431792.75 acc: 0.6623743176460266\n",
      "step: 5685\n",
      "train: loss: 309801.84375 acc: 0.7725505232810974  val: loss: 999535.4375 acc: 0.7563754320144653\n",
      "step: 5690\n",
      "train: loss: 715934.4375 acc: 0.6915467977523804  val: loss: 784996.9375 acc: 0.7753229737281799\n",
      "step: 5695\n",
      "train: loss: 2163408.0 acc: 0.6694282293319702  val: loss: 555549.125 acc: 0.8191525340080261\n",
      "step: 5700\n",
      "train: loss: 1152523.625 acc: 0.7417808771133423  val: loss: 860186.125 acc: 0.764290988445282\n",
      "step: 5705\n",
      "train: loss: 1117967.0 acc: 0.9133069515228271  val: loss: 1023448.25 acc: 0.6348134279251099\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 5710\n",
      "train: loss: 958182.8125 acc: 0.8619517087936401  val: loss: 1463860.0 acc: 0.709841251373291\n",
      "step: 5715\n",
      "train: loss: 704538.3125 acc: 0.8949943780899048  val: loss: 880188.875 acc: 0.8017192482948303\n",
      "step: 5720\n",
      "train: loss: 282801.53125 acc: 0.9463772773742676  val: loss: 1797127.25 acc: 0.23030400276184082\n",
      "step: 5725\n",
      "train: loss: 329234.40625 acc: 0.9620856046676636  val: loss: 518085.625 acc: 0.8624862432479858\n",
      "step: 5730\n",
      "train: loss: 149336.171875 acc: 0.9883171916007996  val: loss: 1077454.0 acc: 0.5535537004470825\n",
      "step: 5735\n",
      "train: loss: 351677.875 acc: 0.9703874588012695  val: loss: 347206.09375 acc: 0.9368407726287842\n",
      "step: 5740\n",
      "train: loss: 193000.90625 acc: 0.9766109585762024  val: loss: 574042.5 acc: 0.8384685516357422\n",
      "step: 5745\n",
      "train: loss: 282508.59375 acc: 0.9390276074409485  val: loss: 1021197.3125 acc: 0.7009005546569824\n",
      "step: 5750\n",
      "train: loss: 94144.8828125 acc: 0.969715416431427  val: loss: 991330.8125 acc: 0.7846693992614746\n",
      "step: 5755\n",
      "train: loss: 14429.513671875 acc: 0.9900181889533997  val: loss: 289382.21875 acc: 0.7590251564979553\n",
      "step: 5760\n",
      "train: loss: 7415.60986328125 acc: 0.9806327819824219  val: loss: 645660.375 acc: 0.9238778352737427\n",
      "step: 5765\n",
      "train: loss: 71988.375 acc: 0.9481282234191895  val: loss: 434563.28125 acc: 0.84914630651474\n",
      "step: 5770\n",
      "train: loss: 34906.54296875 acc: 0.9621627926826477  val: loss: 1956612.375 acc: 0.10367166996002197\n",
      "step: 5775\n",
      "train: loss: 127923.8359375 acc: 0.6949007511138916  val: loss: 795598.6875 acc: 0.8323478698730469\n",
      "step: 5780\n",
      "train: loss: 13840.384765625 acc: 0.9722923040390015  val: loss: 362729.0625 acc: 0.9435397386550903\n",
      "step: 5785\n",
      "train: loss: 5599.3310546875 acc: 0.980912446975708  val: loss: 1556427.75 acc: 0.7303798198699951\n",
      "step: 5790\n",
      "train: loss: 12259.181640625 acc: 0.9539069533348083  val: loss: 1618215.875 acc: 0.2712210416793823\n",
      "step: 5795\n",
      "train: loss: 27231.234375 acc: 0.9650972485542297  val: loss: 825530.5 acc: 0.6510359644889832\n",
      "step: 5800\n",
      "train: loss: 55376.41796875 acc: 0.9666453003883362  val: loss: 297222.625 acc: 0.8398962616920471\n",
      "step: 5805\n",
      "train: loss: 35166.7578125 acc: 0.976642906665802  val: loss: 2863407.0 acc: -2.174020767211914\n",
      "step: 5810\n",
      "train: loss: 28276.935546875 acc: 0.976806104183197  val: loss: 276985.28125 acc: 0.9325687885284424\n",
      "step: 5815\n",
      "train: loss: 26953.896484375 acc: 0.9779173731803894  val: loss: 694836.1875 acc: 0.7654001712799072\n",
      "step: 5820\n",
      "train: loss: 16985.5 acc: 0.9838575124740601  val: loss: 240100.3125 acc: 0.9057561755180359\n",
      "step: 5825\n",
      "train: loss: 15355.7314453125 acc: 0.9869024753570557  val: loss: 898892.5625 acc: 0.7752230167388916\n",
      "step: 5830\n",
      "train: loss: 36548.1484375 acc: 0.9742671251296997  val: loss: 953440.9375 acc: 0.7800037860870361\n",
      "step: 5835\n",
      "train: loss: 41442.39453125 acc: 0.991075873374939  val: loss: 440927.84375 acc: 0.939558207988739\n",
      "step: 5840\n",
      "train: loss: 24947.498046875 acc: 0.9887269139289856  val: loss: 593215.25 acc: 0.8622068166732788\n",
      "step: 5845\n",
      "train: loss: 48881.96875 acc: 0.9752824902534485  val: loss: 968689.375 acc: 0.34453684091567993\n",
      "step: 5850\n",
      "train: loss: 50157.0703125 acc: 0.9762754440307617  val: loss: 637674.0 acc: 0.9271343946456909\n",
      "step: 5855\n",
      "train: loss: 48620.87890625 acc: 0.9882475137710571  val: loss: 868153.75 acc: 0.8823645710945129\n",
      "step: 5860\n",
      "train: loss: 167937.6875 acc: 0.9331195950508118  val: loss: 1523975.0 acc: 0.8513166904449463\n",
      "step: 5865\n",
      "train: loss: 146893.734375 acc: 0.9028007984161377  val: loss: 202838.9375 acc: 0.9683371782302856\n",
      "step: 5870\n",
      "train: loss: 173433.859375 acc: 0.973757803440094  val: loss: 1034205.1875 acc: 0.7731773853302002\n",
      "step: 5875\n",
      "train: loss: 258076.34375 acc: 0.9683211445808411  val: loss: 683131.625 acc: 0.783361554145813\n",
      "step: 5880\n",
      "train: loss: 63798.4140625 acc: 0.9914655089378357  val: loss: 936916.5625 acc: 0.9231802821159363\n",
      "step: 5885\n",
      "train: loss: 142692.015625 acc: 0.9839563369750977  val: loss: 1061860.125 acc: 0.7248384952545166\n",
      "step: 5890\n",
      "train: loss: 452776.0 acc: 0.9570572972297668  val: loss: 562716.0 acc: 0.9535263776779175\n",
      "step: 5895\n",
      "train: loss: 173464.171875 acc: 0.9601008296012878  val: loss: 3205771.5 acc: 0.5797027945518494\n",
      "step: 5900\n",
      "train: loss: 481921.65625 acc: 0.9646696448326111  val: loss: 1519437.75 acc: 0.7845557332038879\n",
      "step: 5905\n",
      "train: loss: 249808.453125 acc: 0.8938590288162231  val: loss: 1575013.625 acc: 0.6402945518493652\n",
      "step: 5910\n",
      "train: loss: 467433.0 acc: 0.9801703691482544  val: loss: 933817.75 acc: 0.8453730940818787\n",
      "step: 5915\n",
      "train: loss: 352359.125 acc: 0.9832984209060669  val: loss: 1261136.125 acc: 0.5142048597335815\n",
      "step: 5920\n",
      "train: loss: 1710538.5 acc: 0.9573233723640442  val: loss: 282851.5625 acc: 0.918259859085083\n",
      "step: 5925\n",
      "train: loss: 1139134.375 acc: 0.935215950012207  val: loss: 1625302.25 acc: 0.8037722706794739\n",
      "step: 5930\n",
      "train: loss: 1913071.375 acc: 0.8760273456573486  val: loss: 1076643.125 acc: 0.49310779571533203\n",
      "step: 5935\n",
      "train: loss: 553561.9375 acc: 0.9548548460006714  val: loss: 1218945.125 acc: 0.7089492082595825\n",
      "step: 5940\n",
      "train: loss: 243413.15625 acc: 0.9154697060585022  val: loss: 1495979.5 acc: 0.7443581223487854\n",
      "step: 5945\n",
      "train: loss: 299274.5625 acc: 0.9511114358901978  val: loss: 1167988.5 acc: 0.7379231452941895\n",
      "step: 5950\n",
      "train: loss: 1597835.875 acc: 0.7883472442626953  val: loss: 485067.78125 acc: 0.9300202131271362\n",
      "step: 5955\n",
      "train: loss: 726882.125 acc: 0.5599230527877808  val: loss: 393665.25 acc: 0.7107913494110107\n",
      "step: 5960\n",
      "train: loss: 1345559.0 acc: 0.3002338409423828  val: loss: 1475801.625 acc: 0.8401145339012146\n",
      "step: 5965\n",
      "train: loss: 593866.25 acc: 0.778155505657196  val: loss: 761512.0 acc: 0.8122787475585938\n",
      "step: 5970\n",
      "train: loss: 1138173.5 acc: 0.6774253845214844  val: loss: 1212856.75 acc: 0.8205103278160095\n",
      "step: 5975\n",
      "train: loss: 2539394.5 acc: -1.205169916152954  val: loss: 1621344.125 acc: 0.8380353450775146\n",
      "step: 5980\n",
      "train: loss: 801342.375 acc: 0.6557406187057495  val: loss: 1104376.875 acc: 0.637367844581604\n",
      "step: 5985\n",
      "train: loss: 552515.75 acc: 0.5574730634689331  val: loss: 1831133.0 acc: 0.6038731932640076\n",
      "step: 5990\n",
      "train: loss: 545008.3125 acc: 0.7272775173187256  val: loss: 1761681.25 acc: 0.6742489337921143\n",
      "step: 5995\n",
      "train: loss: 181810.765625 acc: 0.8490000367164612  val: loss: 922364.0 acc: 0.7635797262191772\n",
      "step: 6000\n",
      "train: loss: 408780.78125 acc: 0.8013275861740112  val: loss: 886196.375 acc: 0.6999969482421875\n",
      "step: 6005\n",
      "train: loss: 565371.75 acc: 0.7771192193031311  val: loss: 2679529.25 acc: 0.6802845001220703\n",
      "step: 6010\n",
      "train: loss: 175069.65625 acc: 0.8925672173500061  val: loss: 1423867.75 acc: 0.7172116041183472\n",
      "step: 6015\n",
      "train: loss: 134198.203125 acc: 0.8942300081253052  val: loss: 5374801.5 acc: 0.6699222922325134\n",
      "step: 6020\n",
      "train: loss: 202351.453125 acc: 0.846207320690155  val: loss: 1174167.75 acc: 0.7702219486236572\n",
      "step: 6025\n",
      "train: loss: 178182.84375 acc: 0.8554574251174927  val: loss: 2532123.25 acc: 0.7361149191856384\n",
      "step: 6030\n",
      "train: loss: 35599.984375 acc: 0.954616904258728  val: loss: 773311.625 acc: 0.8176714181900024\n",
      "step: 6035\n",
      "train: loss: 132115.125 acc: 0.8695647120475769  val: loss: 1271487.0 acc: 0.7159942388534546\n",
      "step: 6040\n",
      "train: loss: 752649.125 acc: 0.6993176341056824  val: loss: 1084275.875 acc: 0.7445067167282104\n",
      "step: 6045\n",
      "train: loss: 307636.5 acc: 0.8077720403671265  val: loss: 1772142.5 acc: 0.6762319207191467\n",
      "step: 6050\n",
      "train: loss: 637764.875 acc: 0.7466479539871216  val: loss: 362652.21875 acc: 0.8013747930526733\n",
      "step: 6055\n",
      "train: loss: 507274.15625 acc: 0.7077646255493164  val: loss: 459499.0 acc: 0.8107466697692871\n",
      "step: 6060\n",
      "train: loss: 2798647.25 acc: 0.6606332063674927  val: loss: 1332697.25 acc: 0.658663272857666\n",
      "step: 6065\n",
      "train: loss: 1132835.125 acc: 0.7942436337471008  val: loss: 299623.5 acc: 0.8440682888031006\n",
      "step: 6070\n",
      "train: loss: 1530943.75 acc: 0.8549225926399231  val: loss: 1544933.625 acc: 0.7641463875770569\n",
      "step: 6075\n",
      "train: loss: 769998.25 acc: 0.9231363534927368  val: loss: 580108.9375 acc: 0.7319287061691284\n",
      "step: 6080\n",
      "train: loss: 168074.484375 acc: 0.952628493309021  val: loss: 931859.625 acc: 0.8888486623764038\n",
      "step: 6085\n",
      "train: loss: 398977.875 acc: 0.9542478919029236  val: loss: 418982.8125 acc: 0.8834586143493652\n",
      "step: 6090\n",
      "train: loss: 303257.75 acc: 0.9588037729263306  val: loss: 815382.25 acc: 0.8264937996864319\n",
      "step: 6095\n",
      "train: loss: 174534.8125 acc: 0.9879778623580933  val: loss: 1698591.75 acc: 0.5742738246917725\n",
      "step: 6100\n",
      "train: loss: 252041.3125 acc: 0.9827171564102173  val: loss: 275407.125 acc: 0.9605197906494141\n",
      "step: 6105\n",
      "train: loss: 216766.609375 acc: 0.9729018211364746  val: loss: 460460.46875 acc: 0.8454719185829163\n",
      "step: 6110\n",
      "train: loss: 138390.171875 acc: 0.9820663332939148  val: loss: 125980.7578125 acc: 0.9606974124908447\n",
      "step: 6115\n",
      "train: loss: 101029.640625 acc: 0.9760593771934509  val: loss: 446593.34375 acc: 0.9554470181465149\n",
      "step: 6120\n",
      "train: loss: 15588.7314453125 acc: 0.9882766604423523  val: loss: 239008.0625 acc: 0.9227062463760376\n",
      "step: 6125\n",
      "train: loss: 9200.74609375 acc: 0.9566223621368408  val: loss: 144769.0625 acc: 0.9325941205024719\n",
      "step: 6130\n",
      "train: loss: 24480.359375 acc: 0.9813082814216614  val: loss: 962989.3125 acc: 0.7847418785095215\n",
      "step: 6135\n",
      "train: loss: 15115.9189453125 acc: 0.9594643115997314  val: loss: 189393.015625 acc: 0.9743587970733643\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 6140\n",
      "train: loss: 40510.765625 acc: 0.9642332792282104  val: loss: 529757.625 acc: 0.9440546631813049\n",
      "step: 6145\n",
      "train: loss: 13742.4775390625 acc: 0.9387261867523193  val: loss: 401877.9375 acc: 0.9721072316169739\n",
      "step: 6150\n",
      "train: loss: 17840.587890625 acc: 0.9689137935638428  val: loss: 2272496.75 acc: 0.6682690382003784\n",
      "step: 6155\n",
      "train: loss: 23753.6796875 acc: 0.9588155150413513  val: loss: 1539913.75 acc: 0.795494794845581\n",
      "step: 6160\n",
      "train: loss: 24717.71484375 acc: 0.9738061428070068  val: loss: 249158.984375 acc: 0.8623095750808716\n",
      "step: 6165\n",
      "train: loss: 45195.72265625 acc: 0.9589052200317383  val: loss: 363128.875 acc: 0.9527133107185364\n",
      "step: 6170\n",
      "train: loss: 29697.380859375 acc: 0.9805759787559509  val: loss: 469711.78125 acc: 0.9615290760993958\n",
      "step: 6175\n",
      "train: loss: 48357.26953125 acc: 0.9741535186767578  val: loss: 898423.375 acc: 0.9003612399101257\n",
      "step: 6180\n",
      "train: loss: 54193.02734375 acc: 0.9840996861457825  val: loss: 866228.375 acc: 0.6284753084182739\n",
      "step: 6185\n",
      "train: loss: 20753.435546875 acc: 0.9871911406517029  val: loss: 868828.8125 acc: 0.8221274614334106\n",
      "step: 6190\n",
      "train: loss: 23771.822265625 acc: 0.9687737226486206  val: loss: 1311183.0 acc: 0.8259425163269043\n",
      "step: 6195\n",
      "train: loss: 5405.119140625 acc: 0.9970764517784119  val: loss: 1487098.875 acc: 0.6859986186027527\n",
      "step: 6200\n",
      "train: loss: 57853.8359375 acc: 0.9839468598365784  val: loss: 481362.59375 acc: 0.9361292123794556\n",
      "step: 6205\n",
      "train: loss: 15704.3076171875 acc: 0.9956094026565552  val: loss: 2186820.0 acc: 0.06688046455383301\n",
      "step: 6210\n",
      "train: loss: 49873.83203125 acc: 0.9864199161529541  val: loss: 2769923.25 acc: -0.19815540313720703\n",
      "step: 6215\n",
      "train: loss: 35893.47265625 acc: 0.9831767082214355  val: loss: 945774.5 acc: 0.8643227815628052\n",
      "step: 6220\n",
      "train: loss: 68064.9375 acc: 0.964426577091217  val: loss: 748297.125 acc: 0.8898351192474365\n",
      "step: 6225\n",
      "train: loss: 129500.6015625 acc: 0.9445122480392456  val: loss: 1400567.25 acc: 0.3403353691101074\n",
      "step: 6230\n",
      "train: loss: 82043.1328125 acc: 0.9452340006828308  val: loss: 361529.125 acc: 0.8436852693557739\n",
      "step: 6235\n",
      "train: loss: 134065.890625 acc: 0.9714973568916321  val: loss: 1716096.0 acc: 0.7594496011734009\n",
      "step: 6240\n",
      "train: loss: 136855.046875 acc: 0.9819414019584656  val: loss: 1371456.125 acc: 0.37583303451538086\n",
      "step: 6245\n",
      "train: loss: 711424.0625 acc: 0.9432874917984009  val: loss: 424763.3125 acc: 0.956244707107544\n",
      "step: 6250\n",
      "train: loss: 174823.8125 acc: 0.9824435710906982  val: loss: 376652.9375 acc: 0.9354307651519775\n",
      "step: 6255\n",
      "train: loss: 189225.421875 acc: 0.9789218902587891  val: loss: 1691500.125 acc: 0.7488539814949036\n",
      "step: 6260\n",
      "train: loss: 344611.65625 acc: 0.9372228384017944  val: loss: 260616.3125 acc: 0.9608904123306274\n",
      "step: 6265\n",
      "train: loss: 708049.625 acc: 0.9577203989028931  val: loss: 649817.375 acc: 0.6551715135574341\n",
      "step: 6270\n",
      "train: loss: 1729350.75 acc: 0.8762934803962708  val: loss: 697757.0625 acc: 0.9029117822647095\n",
      "step: 6275\n",
      "train: loss: 276514.53125 acc: 0.9521658420562744  val: loss: 1844721.0 acc: 0.5860697031021118\n",
      "step: 6280\n",
      "train: loss: 248419.875 acc: 0.9860128164291382  val: loss: 485741.28125 acc: 0.8847980499267578\n",
      "step: 6285\n",
      "train: loss: 759603.6875 acc: 0.9747450351715088  val: loss: 2214375.25 acc: 0.40112167596817017\n",
      "step: 6290\n",
      "train: loss: 2027139.875 acc: 0.9388306140899658  val: loss: 1778744.25 acc: 0.7214125990867615\n",
      "step: 6295\n",
      "train: loss: 259888.75 acc: 0.9768975973129272  val: loss: 706849.75 acc: 0.8783730864524841\n",
      "step: 6300\n",
      "train: loss: 647654.625 acc: 0.9487031102180481  val: loss: 977464.75 acc: 0.5832664966583252\n",
      "step: 6305\n",
      "train: loss: 486314.375 acc: 0.9622945189476013  val: loss: 2065522.0 acc: 0.5560939311981201\n",
      "step: 6310\n",
      "train: loss: 323352.4375 acc: 0.8973948955535889  val: loss: 962772.375 acc: 0.8722909688949585\n",
      "step: 6315\n",
      "train: loss: 1257472.75 acc: 0.757487416267395  val: loss: 593723.125 acc: 0.786405086517334\n",
      "step: 6320\n",
      "train: loss: 1677880.25 acc: 0.8185991644859314  val: loss: 690830.6875 acc: 0.6803209781646729\n",
      "step: 6325\n",
      "train: loss: 708682.375 acc: 0.7621954679489136  val: loss: 1954522.0 acc: 0.8120407462120056\n",
      "step: 6330\n",
      "train: loss: 463366.375 acc: 0.8555687665939331  val: loss: 712852.4375 acc: 0.83784019947052\n",
      "step: 6335\n",
      "train: loss: 856854.9375 acc: 0.7660552263259888  val: loss: 441244.78125 acc: 0.9016456604003906\n",
      "step: 6340\n",
      "train: loss: 1835895.375 acc: 0.15511786937713623  val: loss: 469620.875 acc: 0.8408488035202026\n",
      "step: 6345\n",
      "train: loss: 619342.8125 acc: 0.688428521156311  val: loss: 1308189.75 acc: 0.7326570153236389\n",
      "step: 6350\n",
      "train: loss: 624861.3125 acc: 0.6852712631225586  val: loss: 2308318.25 acc: 0.6456646919250488\n",
      "step: 6355\n",
      "train: loss: 381149.8125 acc: 0.7335495948791504  val: loss: 1184300.75 acc: 0.7213684320449829\n",
      "step: 6360\n",
      "train: loss: 249653.546875 acc: 0.7904654741287231  val: loss: 889669.25 acc: 0.6550285816192627\n",
      "step: 6365\n",
      "train: loss: 150889.40625 acc: 0.877579927444458  val: loss: 539103.375 acc: 0.7627646327018738\n",
      "step: 6370\n",
      "train: loss: 82998.765625 acc: 0.9310020804405212  val: loss: 1515397.25 acc: 0.7199652194976807\n",
      "step: 6375\n",
      "train: loss: 526274.6875 acc: 0.7939286231994629  val: loss: 756736.125 acc: 0.7354136109352112\n",
      "step: 6380\n",
      "train: loss: 211153.25 acc: 0.8160200119018555  val: loss: 794117.8125 acc: 0.7498709559440613\n",
      "step: 6385\n",
      "train: loss: 127748.2421875 acc: 0.9073171019554138  val: loss: 1594831.75 acc: 0.7872433662414551\n",
      "step: 6390\n",
      "train: loss: 63610.85546875 acc: 0.9339065551757812  val: loss: 907881.875 acc: 0.7409172058105469\n",
      "step: 6395\n",
      "train: loss: 81121.125 acc: 0.9210265278816223  val: loss: 591197.875 acc: 0.7223538160324097\n",
      "step: 6400\n",
      "train: loss: 568779.625 acc: 0.6151822805404663  val: loss: 1264281.875 acc: 0.6884540915489197\n",
      "step: 6405\n",
      "train: loss: 296637.6875 acc: 0.6608401536941528  val: loss: 1485934.75 acc: 0.6823660135269165\n",
      "step: 6410\n",
      "train: loss: 431495.3125 acc: 0.6701726913452148  val: loss: 1299237.75 acc: 0.6628448963165283\n",
      "step: 6415\n",
      "train: loss: 283974.53125 acc: 0.7942076325416565  val: loss: 1084334.375 acc: 0.6584333181381226\n",
      "step: 6420\n",
      "train: loss: 143417.71875 acc: 0.8390031456947327  val: loss: 462431.15625 acc: 0.7676757574081421\n",
      "step: 6425\n",
      "train: loss: 1263748.0 acc: 0.6696249842643738  val: loss: 1171162.0 acc: 0.7364094257354736\n",
      "step: 6430\n",
      "train: loss: 1331308.25 acc: 0.8006114959716797  val: loss: 583235.0 acc: 0.8867806196212769\n",
      "step: 6435\n",
      "train: loss: 772408.5 acc: 0.9155110120773315  val: loss: 513010.34375 acc: 0.8738980293273926\n",
      "step: 6440\n",
      "train: loss: 640442.4375 acc: 0.9208314418792725  val: loss: 1249087.5 acc: 0.6931716203689575\n",
      "step: 6445\n",
      "train: loss: 200647.359375 acc: 0.9812276363372803  val: loss: 238070.015625 acc: 0.8968867063522339\n",
      "step: 6450\n",
      "train: loss: 145287.640625 acc: 0.9784120917320251  val: loss: 331660.03125 acc: 0.8751353025436401\n",
      "step: 6455\n",
      "train: loss: 430185.46875 acc: 0.940277636051178  val: loss: 517873.0 acc: 0.93278568983078\n",
      "step: 6460\n",
      "train: loss: 202156.5 acc: 0.9822709560394287  val: loss: 372596.96875 acc: 0.8295633792877197\n",
      "step: 6465\n",
      "train: loss: 160151.484375 acc: 0.986729621887207  val: loss: 666791.0625 acc: 0.913254976272583\n",
      "step: 6470\n",
      "train: loss: 185990.171875 acc: 0.9728631377220154  val: loss: 673783.6875 acc: 0.9438318610191345\n",
      "step: 6475\n",
      "train: loss: 47453.25 acc: 0.9905611872673035  val: loss: 793005.25 acc: 0.940003514289856\n",
      "step: 6480\n",
      "train: loss: 94999.0078125 acc: 0.982966423034668  val: loss: 324064.96875 acc: 0.8896408677101135\n",
      "step: 6485\n",
      "train: loss: 88968.8984375 acc: 0.9553077816963196  val: loss: 1198795.0 acc: 0.7929680347442627\n",
      "step: 6490\n",
      "train: loss: 63294.01953125 acc: 0.9747623801231384  val: loss: 469985.75 acc: 0.7858574390411377\n",
      "step: 6495\n",
      "train: loss: 7323.2216796875 acc: 0.9870542883872986  val: loss: 891423.75 acc: 0.8690581321716309\n",
      "step: 6500\n",
      "train: loss: 14884.275390625 acc: 0.985521137714386  val: loss: 359001.40625 acc: 0.896185576915741\n",
      "step: 6505\n",
      "train: loss: 13219.4013671875 acc: 0.9770061373710632  val: loss: 489803.1875 acc: 0.8637806177139282\n",
      "step: 6510\n",
      "train: loss: 23083.771484375 acc: 0.9275046586990356  val: loss: 1177380.75 acc: 0.8757326602935791\n",
      "step: 6515\n",
      "train: loss: 10160.0888671875 acc: 0.9809534549713135  val: loss: 2112949.25 acc: 0.4178863763809204\n",
      "step: 6520\n",
      "train: loss: 5110.6064453125 acc: 0.9723061919212341  val: loss: 2063104.625 acc: 0.8316465020179749\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 6525\n",
      "train: loss: 36098.140625 acc: 0.9721251130104065  val: loss: 481986.03125 acc: 0.9258420467376709\n",
      "step: 6530\n",
      "train: loss: 46022.796875 acc: 0.9793813824653625  val: loss: 947117.125 acc: 0.8794411420822144\n",
      "step: 6535\n",
      "train: loss: 30452.259765625 acc: 0.9844817519187927  val: loss: 2086624.75 acc: 0.7683205604553223\n",
      "step: 6540\n",
      "train: loss: 31185.650390625 acc: 0.9677267074584961  val: loss: 2935385.25 acc: -0.35301637649536133\n",
      "step: 6545\n",
      "train: loss: 1763.7803955078125 acc: 0.9978436827659607  val: loss: 850217.5625 acc: 0.8089958429336548\n",
      "step: 6550\n",
      "train: loss: 41858.44140625 acc: 0.9838811159133911  val: loss: 1222463.5 acc: 0.9017504453659058\n",
      "step: 6555\n",
      "train: loss: 30071.173828125 acc: 0.9910134077072144  val: loss: 939994.5625 acc: 0.8500485420227051\n",
      "step: 6560\n",
      "train: loss: 18027.787109375 acc: 0.9950911998748779  val: loss: 1221297.5 acc: 0.8812932372093201\n",
      "step: 6565\n",
      "train: loss: 19515.015625 acc: 0.988883376121521  val: loss: 1954950.0 acc: 0.6161450147628784\n",
      "step: 6570\n",
      "train: loss: 64412.8984375 acc: 0.9821823835372925  val: loss: 2592877.25 acc: 0.3764239549636841\n",
      "step: 6575\n",
      "train: loss: 18710.5546875 acc: 0.9915770292282104  val: loss: 3492438.5 acc: -0.337887167930603\n",
      "step: 6580\n",
      "train: loss: 34505.97265625 acc: 0.9802216291427612  val: loss: 1379943.75 acc: -0.556369423866272\n",
      "step: 6585\n",
      "train: loss: 524278.1875 acc: 0.6903113126754761  val: loss: 1148108.5 acc: 0.48223602771759033\n",
      "step: 6590\n",
      "train: loss: 151373.96875 acc: 0.9216940999031067  val: loss: 2066918.0 acc: 0.5110635757446289\n",
      "step: 6595\n",
      "train: loss: 70936.8203125 acc: 0.9702755808830261  val: loss: 1634910.625 acc: 0.6435272097587585\n",
      "step: 6600\n",
      "train: loss: 441331.5 acc: 0.8349038362503052  val: loss: 2180523.75 acc: 0.6160150766372681\n",
      "step: 6605\n",
      "train: loss: 442568.84375 acc: 0.9502456188201904  val: loss: 726683.9375 acc: 0.8646320104598999\n",
      "step: 6610\n",
      "train: loss: 89253.671875 acc: 0.9928571581840515  val: loss: 1756108.5 acc: 0.8588805794715881\n",
      "step: 6615\n",
      "train: loss: 537877.8125 acc: 0.9342952370643616  val: loss: 287091.65625 acc: 0.9480457305908203\n",
      "step: 6620\n",
      "train: loss: 162495.5 acc: 0.9729628562927246  val: loss: 1215459.25 acc: 0.7610472440719604\n",
      "step: 6625\n",
      "train: loss: 299569.375 acc: 0.9738591909408569  val: loss: 1168327.0 acc: 0.8011556267738342\n",
      "step: 6630\n",
      "train: loss: 547583.3125 acc: 0.9732237458229065  val: loss: 4945375.0 acc: -0.15847432613372803\n",
      "step: 6635\n",
      "train: loss: 347342.03125 acc: 0.9565678238868713  val: loss: 726592.5625 acc: 0.8801793456077576\n",
      "step: 6640\n",
      "train: loss: 277280.65625 acc: 0.9737762212753296  val: loss: 1456914.0 acc: 0.7624387741088867\n",
      "step: 6645\n",
      "train: loss: 988687.0 acc: 0.9674065709114075  val: loss: 858774.4375 acc: 0.8397625684738159\n",
      "step: 6650\n",
      "train: loss: 1901747.25 acc: 0.9400411248207092  val: loss: 473258.875 acc: 0.9495289325714111\n",
      "step: 6655\n",
      "train: loss: 1651038.5 acc: 0.9082889556884766  val: loss: 2298636.0 acc: 0.20416194200515747\n",
      "step: 6660\n",
      "train: loss: 650232.0625 acc: 0.9596126675605774  val: loss: 319839.0625 acc: 0.9566431045532227\n",
      "step: 6665\n",
      "train: loss: 589828.4375 acc: 0.95746248960495  val: loss: 770227.125 acc: 0.1997736096382141\n",
      "step: 6670\n",
      "train: loss: 637613.625 acc: 0.9527716636657715  val: loss: 351471.84375 acc: 0.9165191054344177\n",
      "step: 6675\n",
      "train: loss: 432274.15625 acc: 0.9586981534957886  val: loss: 503669.59375 acc: 0.920951247215271\n",
      "step: 6680\n",
      "train: loss: 2146367.75 acc: 0.3777114748954773  val: loss: 214632.8125 acc: 0.9676156044006348\n",
      "step: 6685\n",
      "train: loss: 906753.25 acc: 0.6418665051460266  val: loss: 581685.3125 acc: 0.89089035987854\n",
      "step: 6690\n",
      "train: loss: 214853.421875 acc: 0.8859606385231018  val: loss: 548514.6875 acc: 0.8734369277954102\n",
      "step: 6695\n",
      "train: loss: 437262.59375 acc: 0.8741162419319153  val: loss: 656247.375 acc: 0.8235604763031006\n",
      "step: 6700\n",
      "train: loss: 564053.4375 acc: 0.8675357103347778  val: loss: 557232.125 acc: 0.7332866787910461\n",
      "step: 6705\n",
      "train: loss: 2596141.75 acc: -0.26813340187072754  val: loss: 1190590.875 acc: 0.5610636472702026\n",
      "step: 6710\n",
      "train: loss: 1151806.75 acc: 0.3889872431755066  val: loss: 1309280.0 acc: 0.6965039968490601\n",
      "step: 6715\n",
      "train: loss: 768188.1875 acc: 0.600005567073822  val: loss: 1166938.0 acc: 0.6535902619361877\n",
      "step: 6720\n",
      "train: loss: 524142.875 acc: 0.6764653921127319  val: loss: 2445394.25 acc: 0.6023164391517639\n",
      "step: 6725\n",
      "train: loss: 546497.125 acc: 0.7106331586837769  val: loss: 2041353.875 acc: 0.5672218799591064\n",
      "step: 6730\n",
      "train: loss: 131700.765625 acc: 0.8832710385322571  val: loss: 1329789.0 acc: 0.6179873943328857\n",
      "step: 6735\n",
      "train: loss: 303932.0625 acc: 0.7983582019805908  val: loss: 2194030.5 acc: 0.6863636374473572\n",
      "step: 6740\n",
      "train: loss: 212186.515625 acc: 0.829461932182312  val: loss: 1579975.875 acc: 0.7037854790687561\n",
      "step: 6745\n",
      "train: loss: 128924.0390625 acc: 0.913716197013855  val: loss: 476403.9375 acc: 0.6881422996520996\n",
      "step: 6750\n",
      "train: loss: 255624.859375 acc: 0.8514177799224854  val: loss: 793362.0 acc: 0.7504805326461792\n",
      "step: 6755\n",
      "train: loss: 77953.9609375 acc: 0.926006019115448  val: loss: 1070562.0 acc: 0.7179415225982666\n",
      "step: 6760\n",
      "train: loss: 277916.875 acc: 0.8175857067108154  val: loss: 495062.34375 acc: 0.7824177145957947\n",
      "step: 6765\n",
      "train: loss: 420610.375 acc: 0.7786703109741211  val: loss: 999958.375 acc: 0.719392716884613\n",
      "step: 6770\n",
      "train: loss: 272682.34375 acc: 0.8076554536819458  val: loss: 409277.6875 acc: 0.7651615142822266\n",
      "step: 6775\n",
      "train: loss: 441582.1875 acc: 0.7451040744781494  val: loss: 2260621.75 acc: 0.6521477103233337\n",
      "step: 6780\n",
      "train: loss: 967837.625 acc: 0.6146316528320312  val: loss: 7289617.5 acc: 0.5498150587081909\n",
      "step: 6785\n",
      "train: loss: 114683.140625 acc: 0.8879809975624084  val: loss: 1055252.0 acc: 0.6890722513198853\n",
      "step: 6790\n",
      "train: loss: 1230644.375 acc: 0.6688382029533386  val: loss: 3851984.75 acc: 0.6277234554290771\n",
      "step: 6795\n",
      "train: loss: 1475564.75 acc: 0.806810200214386  val: loss: 745463.5625 acc: 0.858371913433075\n",
      "step: 6800\n",
      "train: loss: 1120507.25 acc: 0.8552944660186768  val: loss: 443470.34375 acc: 0.8831949234008789\n",
      "step: 6805\n",
      "train: loss: 545411.125 acc: 0.9688137769699097  val: loss: 1397210.625 acc: 0.8746072053909302\n",
      "step: 6810\n",
      "train: loss: 421023.25 acc: 0.949671745300293  val: loss: 809890.9375 acc: 0.8403874635696411\n",
      "step: 6815\n",
      "train: loss: 634607.4375 acc: 0.9302204847335815  val: loss: 301029.21875 acc: 0.8225025534629822\n",
      "step: 6820\n",
      "train: loss: 204286.40625 acc: 0.9511662125587463  val: loss: 1456493.75 acc: 0.7745544910430908\n",
      "step: 6825\n",
      "train: loss: 180314.09375 acc: 0.9791542291641235  val: loss: 1193539.125 acc: 0.9076725840568542\n",
      "step: 6830\n",
      "train: loss: 180183.640625 acc: 0.9859803318977356  val: loss: 274805.6875 acc: 0.9590414762496948\n",
      "step: 6835\n",
      "train: loss: 204600.75 acc: 0.9726553559303284  val: loss: 1273058.5 acc: 0.4204902648925781\n",
      "step: 6840\n",
      "train: loss: 105631.6328125 acc: 0.9827566742897034  val: loss: 952670.5625 acc: 0.7921027541160583\n",
      "step: 6845\n",
      "train: loss: 258292.546875 acc: 0.9198756217956543  val: loss: 1504752.25 acc: 0.6098123788833618\n",
      "step: 6850\n",
      "train: loss: 100480.7578125 acc: 0.9387359023094177  val: loss: 1540060.25 acc: 0.4083983898162842\n",
      "step: 6855\n",
      "train: loss: 5700.85107421875 acc: 0.996166467666626  val: loss: 754094.9375 acc: 0.8324059844017029\n",
      "step: 6860\n",
      "train: loss: 15091.787109375 acc: 0.9920136332511902  val: loss: 1488984.75 acc: 0.7492370009422302\n",
      "step: 6865\n",
      "train: loss: 5711.9921875 acc: 0.992239773273468  val: loss: 783498.9375 acc: 0.849461555480957\n",
      "step: 6870\n",
      "train: loss: 20841.025390625 acc: 0.9879795908927917  val: loss: 1589845.125 acc: 0.6993472576141357\n",
      "step: 6875\n",
      "train: loss: 30542.259765625 acc: 0.9441165924072266  val: loss: 431735.84375 acc: 0.7457295656204224\n",
      "step: 6880\n",
      "train: loss: 18971.611328125 acc: 0.9864591360092163  val: loss: 687018.25 acc: 0.8017196655273438\n",
      "step: 6885\n",
      "train: loss: 33661.94921875 acc: 0.9790233373641968  val: loss: 893135.4375 acc: 0.8831914663314819\n",
      "step: 6890\n",
      "train: loss: 16497.0078125 acc: 0.9698031544685364  val: loss: 1052630.375 acc: 0.5842796564102173\n",
      "step: 6895\n",
      "train: loss: 101804.0 acc: 0.9706704616546631  val: loss: 1701975.0 acc: -0.49426090717315674\n",
      "step: 6900\n",
      "train: loss: 36975.328125 acc: 0.9732744693756104  val: loss: 2392160.25 acc: 0.15882229804992676\n",
      "step: 6905\n",
      "train: loss: 31182.38671875 acc: 0.9816185832023621  val: loss: 450581.75 acc: 0.8782140016555786\n",
      "step: 6910\n",
      "train: loss: 39692.51171875 acc: 0.9852477312088013  val: loss: 932123.1875 acc: 0.863093912601471\n",
      "step: 6915\n",
      "train: loss: 16227.4951171875 acc: 0.9869158864021301  val: loss: 1559181.875 acc: 0.6338329315185547\n",
      "step: 6920\n",
      "train: loss: 16347.2158203125 acc: 0.9759997129440308  val: loss: 2715033.75 acc: 0.2372789978981018\n",
      "step: 6925\n",
      "train: loss: 12899.3955078125 acc: 0.9764584898948669  val: loss: 1596909.0 acc: 0.7318853139877319\n",
      "step: 6930\n",
      "train: loss: 43560.17578125 acc: 0.984914243221283  val: loss: 1838794.0 acc: 0.5430395007133484\n",
      "step: 6935\n",
      "train: loss: 42057.58984375 acc: 0.9899281859397888  val: loss: 2090508.125 acc: 0.5170494318008423\n",
      "step: 6940\n",
      "train: loss: 42189.08203125 acc: 0.9837578535079956  val: loss: 1559849.625 acc: 0.7878034114837646\n",
      "step: 6945\n",
      "train: loss: 25858.6875 acc: 0.9874868392944336  val: loss: 1738844.375 acc: 0.6163641214370728\n",
      "step: 6950\n",
      "train: loss: 115982.6875 acc: 0.9741723537445068  val: loss: 1808104.875 acc: 0.766099750995636\n",
      "step: 6955\n",
      "train: loss: 110984.609375 acc: 0.9660913348197937  val: loss: 3090627.0 acc: 0.5693637132644653\n",
      "step: 6960\n",
      "train: loss: 251461.78125 acc: 0.8692560195922852  val: loss: 2851698.5 acc: 0.6455986499786377\n",
      "step: 6965\n",
      "train: loss: 410288.40625 acc: 0.9150779247283936  val: loss: 1114398.25 acc: 0.4542103409767151\n",
      "step: 6970\n",
      "train: loss: 132721.4375 acc: 0.9773823022842407  val: loss: 854780.3125 acc: 0.8912343978881836\n",
      "step: 6975\n",
      "train: loss: 330123.1875 acc: 0.9551966190338135  val: loss: 429081.125 acc: 0.935157060623169\n",
      "step: 6980\n",
      "train: loss: 507774.25 acc: 0.9454421997070312  val: loss: 198275.78125 acc: 0.9148950576782227\n",
      "step: 6985\n",
      "train: loss: 148280.59375 acc: 0.980571985244751  val: loss: 2246070.5 acc: 0.3067270517349243\n",
      "step: 6990\n",
      "train: loss: 333386.375 acc: 0.9641169905662537  val: loss: 2118227.5 acc: 0.6497283577919006\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 6995\n",
      "train: loss: 470965.25 acc: 0.9681078195571899  val: loss: 525786.3125 acc: 0.8754730224609375\n",
      "step: 7000\n",
      "train: loss: 294555.8125 acc: 0.9310990571975708  val: loss: 837423.0625 acc: 0.8451976776123047\n",
      "step: 7005\n",
      "train: loss: 213154.421875 acc: 0.9707037806510925  val: loss: 292317.34375 acc: 0.9172475934028625\n",
      "step: 7010\n",
      "train: loss: 647759.0625 acc: 0.9718832969665527  val: loss: 398360.28125 acc: 0.9098318219184875\n",
      "step: 7015\n",
      "train: loss: 1557797.875 acc: 0.9524348974227905  val: loss: 817020.0625 acc: 0.5947059392929077\n",
      "step: 7020\n",
      "train: loss: 2299102.5 acc: 0.9228401780128479  val: loss: 938248.4375 acc: 0.7380161881446838\n",
      "step: 7025\n",
      "train: loss: 3561192.25 acc: 0.7982479333877563  val: loss: 186807.59375 acc: 0.9690239429473877\n",
      "step: 7030\n",
      "train: loss: 952237.1875 acc: 0.9403683543205261  val: loss: 754937.875 acc: 0.6625518798828125\n",
      "step: 7035\n",
      "train: loss: 1702658.625 acc: 0.8180570602416992  val: loss: 85025.7734375 acc: 0.9793481230735779\n",
      "step: 7040\n",
      "train: loss: 918522.875 acc: 0.920428454875946  val: loss: 262063.015625 acc: 0.9441086053848267\n",
      "step: 7045\n",
      "train: loss: 756482.5625 acc: 0.9014175534248352  val: loss: 319312.9375 acc: 0.8769782781600952\n",
      "step: 7050\n",
      "train: loss: 835549.3125 acc: 0.46173614263534546  val: loss: 1123060.125 acc: 0.6290029287338257\n",
      "step: 7055\n",
      "train: loss: 1076027.0 acc: 0.6742139458656311  val: loss: 1225899.625 acc: 0.8741675615310669\n",
      "step: 7060\n",
      "train: loss: 957241.375 acc: 0.7171356081962585  val: loss: 1176991.0 acc: 0.7920677661895752\n",
      "step: 7065\n",
      "train: loss: 457339.15625 acc: 0.8188782334327698  val: loss: 1286969.5 acc: 0.6633884310722351\n",
      "step: 7070\n",
      "train: loss: 851784.25 acc: 0.6459027528762817  val: loss: 984175.5625 acc: 0.7000466585159302\n",
      "step: 7075\n",
      "train: loss: 1281365.5 acc: 0.3201809525489807  val: loss: 1782412.125 acc: 0.6758790016174316\n",
      "step: 7080\n",
      "train: loss: 861697.6875 acc: 0.6785181760787964  val: loss: 975875.1875 acc: 0.5920153260231018\n",
      "step: 7085\n",
      "train: loss: 484362.6875 acc: 0.7087783813476562  val: loss: 1613231.875 acc: 0.7426601648330688\n",
      "step: 7090\n",
      "train: loss: 294301.8125 acc: 0.7629005908966064  val: loss: 579598.75 acc: 0.7112032175064087\n",
      "step: 7095\n",
      "train: loss: 251093.75 acc: 0.8372567892074585  val: loss: 784272.5 acc: 0.6475793123245239\n",
      "step: 7100\n",
      "train: loss: 153921.125 acc: 0.8818485140800476  val: loss: 3668908.25 acc: 0.7353414297103882\n",
      "step: 7105\n",
      "train: loss: 148980.71875 acc: 0.8889611959457397  val: loss: 1582308.5 acc: 0.7014678716659546\n",
      "step: 7110\n",
      "train: loss: 112809.171875 acc: 0.9069005846977234  val: loss: 1266354.0 acc: 0.7603816390037537\n",
      "step: 7115\n",
      "train: loss: 415489.65625 acc: 0.7442766427993774  val: loss: 897372.6875 acc: 0.6892597675323486\n",
      "step: 7120\n",
      "train: loss: 73169.1015625 acc: 0.9247198104858398  val: loss: 760835.25 acc: 0.7788572311401367\n",
      "step: 7125\n",
      "train: loss: 68621.5 acc: 0.9116796255111694  val: loss: 1541275.125 acc: 0.6811738014221191\n",
      "step: 7130\n",
      "train: loss: 235560.578125 acc: 0.8568044900894165  val: loss: 971107.0625 acc: 0.6904551386833191\n",
      "step: 7135\n",
      "train: loss: 130715.953125 acc: 0.9179829955101013  val: loss: 1689779.0 acc: 0.7819550633430481\n",
      "step: 7140\n",
      "train: loss: 462582.6875 acc: 0.7706037163734436  val: loss: 5568814.0 acc: 0.5798386335372925\n",
      "step: 7145\n",
      "train: loss: 322836.78125 acc: 0.7268153429031372  val: loss: 1900827.125 acc: 0.6379293203353882\n",
      "step: 7150\n",
      "train: loss: 212449.6875 acc: 0.8757465481758118  val: loss: 1480183.5 acc: 0.6939109563827515\n",
      "step: 7155\n",
      "train: loss: 580761.0625 acc: 0.7924015522003174  val: loss: 1316957.875 acc: 0.7102837562561035\n",
      "step: 7160\n",
      "train: loss: 617737.6875 acc: 0.737636923789978  val: loss: 845174.9375 acc: 0.8239774703979492\n",
      "step: 7165\n",
      "train: loss: 526788.25 acc: 0.8747072219848633  val: loss: 945687.6875 acc: 0.8609504103660583\n",
      "step: 7170\n",
      "train: loss: 466333.875 acc: 0.9470159411430359  val: loss: 1495513.375 acc: 0.854092001914978\n",
      "step: 7175\n",
      "train: loss: 511929.625 acc: 0.951229453086853  val: loss: 1341914.5 acc: 0.861331045627594\n",
      "step: 7180\n",
      "train: loss: 96239.2421875 acc: 0.978681743144989  val: loss: 931573.875 acc: 0.7488550543785095\n",
      "step: 7185\n",
      "train: loss: 141579.90625 acc: 0.9786197543144226  val: loss: 720151.3125 acc: 0.9171476364135742\n",
      "step: 7190\n",
      "train: loss: 398824.46875 acc: 0.9579702019691467  val: loss: 2233623.75 acc: 0.6785134077072144\n",
      "step: 7195\n",
      "train: loss: 163599.3125 acc: 0.98678058385849  val: loss: 1556753.5 acc: 0.5865066051483154\n",
      "step: 7200\n",
      "train: loss: 183827.796875 acc: 0.9841028451919556  val: loss: 1712518.375 acc: 0.493724524974823\n",
      "step: 7205\n",
      "train: loss: 146283.953125 acc: 0.9747744202613831  val: loss: 1523513.5 acc: 0.7839756608009338\n",
      "step: 7210\n",
      "train: loss: 94427.953125 acc: 0.983254611492157  val: loss: 1186164.625 acc: 0.5833524465560913\n",
      "step: 7215\n",
      "train: loss: 23445.076171875 acc: 0.9720534682273865  val: loss: 1218791.0 acc: 0.623456597328186\n",
      "step: 7220\n",
      "train: loss: 28615.25390625 acc: 0.9687730073928833  val: loss: 845993.25 acc: 0.819118082523346\n",
      "step: 7225\n",
      "train: loss: 9675.8701171875 acc: 0.9595057368278503  val: loss: 1394922.75 acc: 0.8550779819488525\n",
      "step: 7230\n",
      "train: loss: 53680.6328125 acc: 0.9753047227859497  val: loss: 1115846.125 acc: 0.7622007131576538\n",
      "step: 7235\n",
      "train: loss: 27278.560546875 acc: 0.9574823975563049  val: loss: 1181446.0 acc: 0.8495627641677856\n",
      "step: 7240\n",
      "train: loss: 15052.52734375 acc: 0.955768346786499  val: loss: 1748922.375 acc: 0.07000893354415894\n",
      "step: 7245\n",
      "train: loss: 15470.6044921875 acc: 0.9740455746650696  val: loss: 1859705.375 acc: 0.6670229434967041\n",
      "step: 7250\n",
      "train: loss: 3202.91162109375 acc: 0.9911785125732422  val: loss: 479638.40625 acc: 0.9274493455886841\n",
      "step: 7255\n",
      "train: loss: 13780.318359375 acc: 0.9787866473197937  val: loss: 1692491.875 acc: 0.6646401286125183\n",
      "step: 7260\n",
      "train: loss: 38445.26171875 acc: 0.9786810278892517  val: loss: 433993.5625 acc: 0.8282629251480103\n",
      "step: 7265\n",
      "train: loss: 15010.8203125 acc: 0.9863656163215637  val: loss: 245260.9375 acc: 0.9624104499816895\n",
      "step: 7270\n",
      "train: loss: 40853.99609375 acc: 0.9831242561340332  val: loss: 2347816.75 acc: 0.7461793422698975\n",
      "step: 7275\n",
      "train: loss: 27654.50390625 acc: 0.9855540990829468  val: loss: 1205259.75 acc: 0.8825752139091492\n",
      "step: 7280\n",
      "train: loss: 14338.767578125 acc: 0.9893680810928345  val: loss: 1431884.875 acc: 0.3703632950782776\n",
      "step: 7285\n",
      "train: loss: 8122.10498046875 acc: 0.9633386731147766  val: loss: 683514.4375 acc: 0.6763862371444702\n",
      "step: 7290\n",
      "train: loss: 11106.8203125 acc: 0.988096296787262  val: loss: 715860.9375 acc: 0.7030954360961914\n",
      "step: 7295\n",
      "train: loss: 24361.73828125 acc: 0.9782974123954773  val: loss: 1554545.875 acc: 0.7929084300994873\n",
      "step: 7300\n",
      "train: loss: 48472.22265625 acc: 0.9886777400970459  val: loss: 709707.3125 acc: 0.8161740899085999\n",
      "step: 7305\n",
      "train: loss: 18217.201171875 acc: 0.994875431060791  val: loss: 1699397.625 acc: 0.589382529258728\n",
      "step: 7310\n",
      "train: loss: 57303.70703125 acc: 0.9645439386367798  val: loss: 972980.9375 acc: 0.802641749382019\n",
      "step: 7315\n",
      "train: loss: 66605.359375 acc: 0.9735588431358337  val: loss: 1465043.625 acc: 0.5416455268859863\n",
      "step: 7320\n",
      "train: loss: 72954.796875 acc: 0.9828590154647827  val: loss: 778797.3125 acc: 0.7663977742195129\n",
      "step: 7325\n",
      "train: loss: 136300.3125 acc: 0.9473811388015747  val: loss: 1002063.8125 acc: 0.8144986033439636\n",
      "step: 7330\n",
      "train: loss: 233833.265625 acc: 0.9321476221084595  val: loss: 722138.875 acc: 0.31096649169921875\n",
      "step: 7335\n",
      "train: loss: 98766.1015625 acc: 0.985152542591095  val: loss: 4971012.5 acc: -0.09776484966278076\n",
      "step: 7340\n",
      "train: loss: 81611.3203125 acc: 0.9917078614234924  val: loss: 1366474.75 acc: 0.2221207618713379\n",
      "step: 7345\n",
      "train: loss: 50087.83984375 acc: 0.9914125204086304  val: loss: 1393117.125 acc: 0.38983529806137085\n",
      "step: 7350\n",
      "train: loss: 586315.8125 acc: 0.8978738188743591  val: loss: 1163710.0 acc: 0.6511480808258057\n",
      "step: 7355\n",
      "train: loss: 223706.390625 acc: 0.9865709543228149  val: loss: 400764.71875 acc: 0.8913999795913696\n",
      "step: 7360\n",
      "train: loss: 243751.25 acc: 0.9860878586769104  val: loss: 325853.8125 acc: 0.8929624557495117\n",
      "step: 7365\n",
      "train: loss: 526330.0 acc: 0.9308949708938599  val: loss: 935311.125 acc: 0.8058478832244873\n",
      "step: 7370\n",
      "train: loss: 353792.34375 acc: 0.9618979692459106  val: loss: 1038036.0 acc: 0.7801318168640137\n",
      "step: 7375\n",
      "train: loss: 1090118.0 acc: 0.9557024836540222  val: loss: 547085.5 acc: 0.8771816492080688\n",
      "step: 7380\n",
      "train: loss: 1484664.25 acc: 0.9153975248336792  val: loss: 420316.75 acc: 0.9142073392868042\n",
      "step: 7385\n",
      "train: loss: 1448636.5 acc: 0.946992814540863  val: loss: 680161.75 acc: 0.9425954818725586\n",
      "step: 7390\n",
      "train: loss: 940031.4375 acc: 0.9329007863998413  val: loss: 421002.75 acc: 0.9016358852386475\n",
      "step: 7395\n",
      "train: loss: 462604.375 acc: 0.9683553576469421  val: loss: 226676.546875 acc: 0.9032276272773743\n",
      "step: 7400\n",
      "train: loss: 345248.65625 acc: 0.9734212756156921  val: loss: 392620.0 acc: 0.44718337059020996\n",
      "step: 7405\n",
      "train: loss: 791257.0625 acc: 0.9453710913658142  val: loss: 483680.21875 acc: 0.9346820712089539\n",
      "step: 7410\n",
      "train: loss: 2049872.375 acc: 0.6558793187141418  val: loss: 1011180.6875 acc: 0.8701200485229492\n",
      "step: 7415\n",
      "train: loss: 996088.1875 acc: 0.5950661897659302  val: loss: 632586.0625 acc: 0.8448572158813477\n",
      "step: 7420\n",
      "train: loss: 940218.75 acc: 0.5970627069473267  val: loss: 252850.21875 acc: 0.890004575252533\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 7425\n",
      "train: loss: 468862.875 acc: 0.8356429934501648  val: loss: 356097.1875 acc: 0.9279680848121643\n",
      "step: 7430\n",
      "train: loss: 572112.0625 acc: 0.8341183066368103  val: loss: 1142825.875 acc: 0.8651853799819946\n",
      "step: 7435\n",
      "train: loss: 1142682.875 acc: 0.19688308238983154  val: loss: 600872.875 acc: 0.7978410124778748\n",
      "step: 7440\n",
      "train: loss: 1417364.125 acc: 0.5338273048400879  val: loss: 2193513.25 acc: 0.7958495616912842\n",
      "step: 7445\n",
      "train: loss: 691866.1875 acc: 0.7407935261726379  val: loss: 2615363.75 acc: 0.78711998462677\n",
      "step: 7450\n",
      "train: loss: 507745.375 acc: 0.5860407948493958  val: loss: 1815314.75 acc: 0.5307937264442444\n",
      "step: 7455\n",
      "train: loss: 243253.359375 acc: 0.7637131214141846  val: loss: 1840268.125 acc: 0.7581982612609863\n",
      "step: 7460\n",
      "train: loss: 493728.1875 acc: 0.7290360927581787  val: loss: 685201.125 acc: 0.6903728246688843\n",
      "step: 7465\n",
      "train: loss: 198336.484375 acc: 0.8578459024429321  val: loss: 3285923.5 acc: 0.6820456981658936\n",
      "step: 7470\n",
      "train: loss: 235546.53125 acc: 0.8253552317619324  val: loss: 2951784.75 acc: 0.6617354154586792\n",
      "step: 7475\n",
      "train: loss: 588146.4375 acc: 0.7565045952796936  val: loss: 2411389.25 acc: 0.7056618928909302\n",
      "step: 7480\n",
      "train: loss: 179063.484375 acc: 0.8658193945884705  val: loss: 1585928.875 acc: 0.7138794660568237\n",
      "step: 7485\n",
      "train: loss: 207150.171875 acc: 0.8153318166732788  val: loss: 1607049.5 acc: 0.6866166591644287\n",
      "step: 7490\n",
      "train: loss: 39540.65234375 acc: 0.9483587741851807  val: loss: 1259953.875 acc: 0.7455052137374878\n",
      "step: 7495\n",
      "train: loss: 459054.84375 acc: 0.8039501905441284  val: loss: 736244.4375 acc: 0.8309460282325745\n",
      "step: 7500\n",
      "train: loss: 194435.8125 acc: 0.8200014233589172  val: loss: 1018913.1875 acc: 0.7084599733352661\n",
      "step: 7505\n",
      "train: loss: 375927.65625 acc: 0.7228406071662903  val: loss: 1656563.0 acc: 0.6440924406051636\n",
      "step: 7510\n",
      "train: loss: 697817.9375 acc: 0.6432240605354309  val: loss: 781715.875 acc: 0.702546238899231\n",
      "step: 7515\n",
      "train: loss: 185700.4375 acc: 0.8533974885940552  val: loss: 705728.3125 acc: 0.7578779458999634\n",
      "step: 7520\n",
      "train: loss: 402840.4375 acc: 0.7647483944892883  val: loss: 675417.8125 acc: 0.7061898708343506\n",
      "step: 7525\n",
      "train: loss: 2434794.25 acc: 0.7130354642868042  val: loss: 1775065.375 acc: 0.7955424785614014\n",
      "step: 7530\n",
      "train: loss: 437404.78125 acc: 0.8861722350120544  val: loss: 202440.984375 acc: 0.7946963906288147\n",
      "step: 7535\n",
      "train: loss: 744808.25 acc: 0.9039866328239441  val: loss: 752605.3125 acc: 0.8759607672691345\n",
      "step: 7540\n",
      "train: loss: 305302.625 acc: 0.9592221975326538  val: loss: 589447.0625 acc: 0.9447243213653564\n",
      "step: 7545\n",
      "train: loss: 652789.8125 acc: 0.9122035503387451  val: loss: 1048836.375 acc: 0.16088157892227173\n",
      "step: 7550\n",
      "train: loss: 278915.375 acc: 0.9518638253211975  val: loss: 1033379.5625 acc: 0.5735399723052979\n",
      "step: 7555\n",
      "train: loss: 185371.03125 acc: 0.977790355682373  val: loss: 1401683.25 acc: 0.5978891253471375\n",
      "step: 7560\n",
      "train: loss: 123118.2265625 acc: 0.989086389541626  val: loss: 1418224.5 acc: 0.6595032215118408\n",
      "step: 7565\n",
      "train: loss: 145279.453125 acc: 0.9870349764823914  val: loss: 567391.9375 acc: 0.9114351272583008\n",
      "step: 7570\n",
      "train: loss: 119808.5234375 acc: 0.9831416606903076  val: loss: 677416.5 acc: 0.8989723920822144\n",
      "step: 7575\n",
      "train: loss: 148297.21875 acc: 0.9653696417808533  val: loss: 504440.15625 acc: 0.8987069129943848\n",
      "step: 7580\n",
      "train: loss: 41954.5 acc: 0.9911764860153198  val: loss: 1239411.875 acc: 0.7133466601371765\n",
      "step: 7585\n",
      "train: loss: 8215.5615234375 acc: 0.9849688410758972  val: loss: 930688.25 acc: 0.5326645374298096\n",
      "step: 7590\n",
      "train: loss: 34910.98046875 acc: 0.9531582593917847  val: loss: 512663.40625 acc: 0.9243484139442444\n",
      "step: 7595\n",
      "train: loss: 33199.6328125 acc: 0.9811884164810181  val: loss: 1512740.25 acc: -0.03183174133300781\n",
      "step: 7600\n",
      "train: loss: 41777.5859375 acc: 0.9679394960403442  val: loss: 1792627.5 acc: 0.567755937576294\n",
      "step: 7605\n",
      "train: loss: 9326.1279296875 acc: 0.9932101368904114  val: loss: 512347.90625 acc: 0.9124011397361755\n",
      "step: 7610\n",
      "train: loss: 17763.048828125 acc: 0.9796339273452759  val: loss: 358611.96875 acc: 0.9224119186401367\n",
      "step: 7615\n",
      "train: loss: 3363.71142578125 acc: 0.986479640007019  val: loss: 1473018.625 acc: 0.5249248147010803\n",
      "step: 7620\n",
      "train: loss: 27331.927734375 acc: 0.9679003357887268  val: loss: 212886.859375 acc: 0.8302286267280579\n",
      "step: 7625\n",
      "train: loss: 65483.99609375 acc: 0.9734712839126587  val: loss: 1068990.875 acc: 0.7811176776885986\n",
      "step: 7630\n",
      "train: loss: 17351.650390625 acc: 0.9738296270370483  val: loss: 964654.75 acc: 0.7715067863464355\n",
      "step: 7635\n",
      "train: loss: 28642.7421875 acc: 0.9834473133087158  val: loss: 155469.09375 acc: 0.9759717583656311\n",
      "step: 7640\n",
      "train: loss: 7632.72705078125 acc: 0.9953227639198303  val: loss: 2056536.375 acc: 0.6723335981369019\n",
      "step: 7645\n",
      "train: loss: 26327.55859375 acc: 0.9803999066352844  val: loss: 134691.359375 acc: 0.8862699270248413\n",
      "step: 7650\n",
      "train: loss: 29907.080078125 acc: 0.9726526141166687  val: loss: 1487311.625 acc: 0.6268777847290039\n",
      "step: 7655\n",
      "train: loss: 7403.03759765625 acc: 0.9935591220855713  val: loss: 504595.03125 acc: 0.7917366027832031\n",
      "step: 7660\n",
      "train: loss: 12828.0205078125 acc: 0.9900839328765869  val: loss: 600944.5 acc: 0.9454624056816101\n",
      "step: 7665\n",
      "train: loss: 39045.4296875 acc: 0.9893484115600586  val: loss: 330496.46875 acc: 0.8920188546180725\n",
      "step: 7670\n",
      "train: loss: 43528.81640625 acc: 0.9876616597175598  val: loss: 1811551.375 acc: 0.7623125910758972\n",
      "step: 7675\n",
      "train: loss: 55832.62109375 acc: 0.9860491752624512  val: loss: 154185.453125 acc: 0.9392088055610657\n",
      "step: 7680\n",
      "train: loss: 38724.109375 acc: 0.9813454151153564  val: loss: 1724591.875 acc: 0.6808879971504211\n",
      "step: 7685\n",
      "train: loss: 179039.5625 acc: 0.937926709651947  val: loss: 85367.4453125 acc: 0.9724712371826172\n",
      "step: 7690\n",
      "train: loss: 58573.05859375 acc: 0.9714242815971375  val: loss: 157572.8125 acc: 0.9823640584945679\n",
      "step: 7695\n",
      "train: loss: 134730.078125 acc: 0.9574277997016907  val: loss: 217656.546875 acc: 0.9385907649993896\n",
      "step: 7700\n",
      "train: loss: 122143.5625 acc: 0.9730263352394104  val: loss: 520805.53125 acc: 0.8899191617965698\n",
      "step: 7705\n",
      "train: loss: 66395.75 acc: 0.9888395667076111  val: loss: 585588.1875 acc: 0.8619604110717773\n",
      "step: 7710\n",
      "train: loss: 111032.3125 acc: 0.9894376993179321  val: loss: 335688.5 acc: 0.9214766621589661\n",
      "step: 7715\n",
      "train: loss: 50226.0 acc: 0.9924191832542419  val: loss: 531372.375 acc: 0.9401327967643738\n",
      "step: 7720\n",
      "train: loss: 315726.65625 acc: 0.9660000801086426  val: loss: 321645.84375 acc: 0.9556492567062378\n",
      "step: 7725\n",
      "train: loss: 293860.09375 acc: 0.979468822479248  val: loss: 318414.3125 acc: 0.9471004009246826\n",
      "step: 7730\n",
      "train: loss: 500627.4375 acc: 0.9663532972335815  val: loss: 286686.71875 acc: 0.940209150314331\n",
      "step: 7735\n",
      "train: loss: 759329.875 acc: 0.8276333808898926  val: loss: 820565.8125 acc: 0.9289920926094055\n",
      "step: 7740\n",
      "train: loss: 336536.1875 acc: 0.958633303642273  val: loss: 2403903.75 acc: 0.7163625359535217\n",
      "step: 7745\n",
      "train: loss: 499375.125 acc: 0.9837821125984192  val: loss: 565647.875 acc: 0.9184934496879578\n",
      "step: 7750\n",
      "train: loss: 1542006.25 acc: 0.9274417161941528  val: loss: 795918.125 acc: 0.8634917736053467\n",
      "step: 7755\n",
      "train: loss: 2087282.75 acc: 0.8998050093650818  val: loss: 1612315.625 acc: 0.8678442239761353\n",
      "step: 7760\n",
      "train: loss: 564172.5625 acc: 0.9679198861122131  val: loss: 508980.75 acc: 0.9398712515830994\n",
      "step: 7765\n",
      "train: loss: 371039.15625 acc: 0.9568591713905334  val: loss: 1171354.625 acc: 0.8431909084320068\n",
      "step: 7770\n",
      "train: loss: 158375.734375 acc: 0.9703437685966492  val: loss: 2328527.0 acc: 0.7214205265045166\n",
      "step: 7775\n",
      "train: loss: 559278.625 acc: 0.90472412109375  val: loss: 991534.0 acc: 0.8431947231292725\n",
      "step: 7780\n",
      "train: loss: 1551390.25 acc: 0.2215598225593567  val: loss: 318142.03125 acc: 0.8524024486541748\n",
      "step: 7785\n",
      "train: loss: 688003.3125 acc: 0.672692060470581  val: loss: 1375122.625 acc: 0.7449527978897095\n",
      "step: 7790\n",
      "train: loss: 362919.59375 acc: 0.8727142810821533  val: loss: 302146.40625 acc: 0.938971221446991\n",
      "step: 7795\n",
      "train: loss: 435193.84375 acc: 0.7715722322463989  val: loss: 714252.5 acc: 0.8734408617019653\n",
      "step: 7800\n",
      "train: loss: 943151.9375 acc: 0.4317436218261719  val: loss: 991249.25 acc: 0.674475908279419\n",
      "step: 7805\n",
      "train: loss: 2291459.75 acc: 0.030385851860046387  val: loss: 891085.9375 acc: 0.7989296317100525\n",
      "step: 7810\n",
      "train: loss: 640793.125 acc: 0.4987620711326599  val: loss: 1004114.5 acc: 0.6841879487037659\n",
      "step: 7815\n",
      "train: loss: 375273.1875 acc: 0.8026960492134094  val: loss: 1097744.625 acc: 0.7552402019500732\n",
      "step: 7820\n",
      "train: loss: 168722.109375 acc: 0.8412259817123413  val: loss: 939861.125 acc: 0.6321990489959717\n",
      "step: 7825\n",
      "train: loss: 214897.015625 acc: 0.8341423273086548  val: loss: 546796.1875 acc: 0.790590226650238\n",
      "step: 7830\n",
      "train: loss: 197248.453125 acc: 0.8575129508972168  val: loss: 529625.0625 acc: 0.789525032043457\n",
      "step: 7835\n",
      "train: loss: 248784.015625 acc: 0.848299503326416  val: loss: 3080936.25 acc: 0.6723483800888062\n",
      "step: 7840\n",
      "train: loss: 93110.484375 acc: 0.9204282164573669  val: loss: 1812052.875 acc: 0.6762299537658691\n",
      "step: 7845\n",
      "train: loss: 528590.5 acc: 0.8025468587875366  val: loss: 151711.96875 acc: 0.8658703565597534\n",
      "step: 7850\n",
      "train: loss: 445538.15625 acc: 0.7879106998443604  val: loss: 1476281.625 acc: 0.7176632881164551\n",
      "step: 7855\n",
      "train: loss: 61562.31640625 acc: 0.9432355761528015  val: loss: 3966129.75 acc: 0.6406779885292053\n",
      "step: 7860\n",
      "train: loss: 155832.21875 acc: 0.8561205863952637  val: loss: 2612046.75 acc: 0.6760854125022888\n",
      "step: 7865\n",
      "train: loss: 161427.484375 acc: 0.8533536195755005  val: loss: 1570165.875 acc: 0.7484240531921387\n",
      "step: 7870\n",
      "train: loss: 186537.078125 acc: 0.8554304838180542  val: loss: 692629.1875 acc: 0.7426334619522095\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 7875\n",
      "train: loss: 1015678.1875 acc: 0.7110960483551025  val: loss: 508241.75 acc: 0.6877932548522949\n",
      "step: 7880\n",
      "train: loss: 112318.90625 acc: 0.8932414650917053  val: loss: 2229708.75 acc: 0.6806380748748779\n",
      "step: 7885\n",
      "train: loss: 227914.59375 acc: 0.8600145578384399  val: loss: 1387799.5 acc: 0.6571359634399414\n",
      "step: 7890\n",
      "train: loss: 1585839.125 acc: 0.6004990339279175  val: loss: 968736.6875 acc: 0.7917803525924683\n",
      "step: 7895\n",
      "train: loss: 1920941.25 acc: 0.7363739013671875  val: loss: 718246.9375 acc: 0.7958657741546631\n",
      "step: 7900\n",
      "train: loss: 1206871.625 acc: 0.8777958750724792  val: loss: 563547.875 acc: 0.8124532699584961\n",
      "step: 7905\n",
      "train: loss: 370427.21875 acc: 0.9614437222480774  val: loss: 525017.0625 acc: 0.9317412376403809\n",
      "step: 7910\n",
      "train: loss: 282689.0 acc: 0.9596614241600037  val: loss: 1812871.5 acc: -0.005582332611083984\n",
      "step: 7915\n",
      "train: loss: 171495.90625 acc: 0.9734870791435242  val: loss: 1303126.625 acc: 0.8505899906158447\n",
      "step: 7920\n",
      "train: loss: 130426.5703125 acc: 0.9852316379547119  val: loss: 471702.15625 acc: 0.9080777168273926\n",
      "step: 7925\n",
      "train: loss: 136945.234375 acc: 0.9875575304031372  val: loss: 1626378.875 acc: 0.5211549997329712\n",
      "step: 7930\n",
      "train: loss: 171407.171875 acc: 0.9867203235626221  val: loss: 1368173.875 acc: -0.13094079494476318\n",
      "step: 7935\n",
      "train: loss: 142389.171875 acc: 0.9823823571205139  val: loss: 510549.09375 acc: 0.939312219619751\n",
      "step: 7940\n",
      "train: loss: 65347.4921875 acc: 0.9873852133750916  val: loss: 1007682.3125 acc: 0.9021676778793335\n",
      "step: 7945\n",
      "train: loss: 80966.6875 acc: 0.9597575664520264  val: loss: 666998.6875 acc: 0.8825787901878357\n",
      "step: 7950\n",
      "train: loss: 35662.33984375 acc: 0.9908745288848877  val: loss: 1135133.0 acc: 0.6829572319984436\n",
      "step: 7955\n",
      "train: loss: 12406.85546875 acc: 0.9771803021430969  val: loss: 430656.1875 acc: 0.8262298107147217\n",
      "step: 7960\n",
      "train: loss: 9922.841796875 acc: 0.9766423106193542  val: loss: 431786.125 acc: 0.9580388069152832\n",
      "step: 7965\n",
      "train: loss: 30584.8828125 acc: 0.9776756763458252  val: loss: 615620.5625 acc: 0.8499761819839478\n",
      "step: 7970\n",
      "train: loss: 156301.96875 acc: 0.849177896976471  val: loss: 655155.375 acc: 0.8327089548110962\n",
      "step: 7975\n",
      "train: loss: 9857.080078125 acc: 0.9798692464828491  val: loss: 1058328.125 acc: 0.7131811380386353\n",
      "step: 7980\n",
      "train: loss: 5159.00341796875 acc: 0.9869778752326965  val: loss: 142384.109375 acc: 0.9674666523933411\n",
      "step: 7985\n",
      "train: loss: 2615.486572265625 acc: 0.9937167167663574  val: loss: 368811.625 acc: 0.756887674331665\n",
      "step: 7990\n",
      "train: loss: 20249.119140625 acc: 0.9781126976013184  val: loss: 489434.34375 acc: 0.8833128213882446\n",
      "step: 7995\n",
      "train: loss: 22589.294921875 acc: 0.9802052974700928  val: loss: 1377076.0 acc: 0.8251968026161194\n",
      "step: 8000\n",
      "train: loss: 51363.140625 acc: 0.9771702885627747  val: loss: 112633.5703125 acc: 0.9674651622772217\n",
      "step: 8005\n",
      "train: loss: 106300.53125 acc: 0.945231020450592  val: loss: 257290.6875 acc: 0.917618989944458\n",
      "step: 8010\n",
      "train: loss: 35040.55859375 acc: 0.9800992608070374  val: loss: 2343707.75 acc: 0.06814241409301758\n",
      "step: 8015\n",
      "train: loss: 14902.09375 acc: 0.9898675084114075  val: loss: 325477.09375 acc: 0.9269047975540161\n",
      "step: 8020\n",
      "train: loss: 22149.765625 acc: 0.9833082556724548  val: loss: 332740.09375 acc: 0.9616468548774719\n",
      "step: 8025\n",
      "train: loss: 17475.791015625 acc: 0.9911283254623413  val: loss: 350834.0625 acc: 0.9758868217468262\n",
      "step: 8030\n",
      "train: loss: 27161.294921875 acc: 0.9906197786331177  val: loss: 957137.4375 acc: 0.912735104560852\n",
      "step: 8035\n",
      "train: loss: 25472.41015625 acc: 0.9912787675857544  val: loss: 2025816.375 acc: 0.6262156963348389\n",
      "step: 8040\n",
      "train: loss: 44594.3203125 acc: 0.9855419993400574  val: loss: 1508603.5 acc: 0.6925445795059204\n",
      "step: 8045\n",
      "train: loss: 28859.4375 acc: 0.9906876087188721  val: loss: 459195.75 acc: 0.8228550553321838\n",
      "step: 8050\n",
      "train: loss: 60017.77734375 acc: 0.9855673909187317  val: loss: 347254.0 acc: 0.9496155977249146\n",
      "step: 8055\n",
      "train: loss: 62330.87109375 acc: 0.9709103107452393  val: loss: 574440.5 acc: 0.9307336211204529\n",
      "step: 8060\n",
      "train: loss: 78590.375 acc: 0.9721691608428955  val: loss: 611691.3125 acc: 0.9272935390472412\n",
      "step: 8065\n",
      "train: loss: 636645.125 acc: 0.8864260315895081  val: loss: 194537.84375 acc: 0.9374288320541382\n",
      "step: 8070\n",
      "train: loss: 132541.171875 acc: 0.9891602993011475  val: loss: 240148.859375 acc: 0.9560455679893494\n",
      "step: 8075\n",
      "train: loss: 61265.90234375 acc: 0.9916410446166992  val: loss: 1016005.625 acc: 0.8069273829460144\n",
      "step: 8080\n",
      "train: loss: 94369.5390625 acc: 0.9878954887390137  val: loss: 2661459.25 acc: 0.5756956934928894\n",
      "step: 8085\n",
      "train: loss: 214693.25 acc: 0.9441231489181519  val: loss: 1694043.875 acc: 0.4831879138946533\n",
      "step: 8090\n",
      "train: loss: 1680179.25 acc: 0.7721729278564453  val: loss: 1054885.125 acc: 0.701273500919342\n",
      "step: 8095\n",
      "train: loss: 494876.25 acc: 0.9682996869087219  val: loss: 2448472.5 acc: 0.6988497972488403\n",
      "step: 8100\n",
      "train: loss: 384880.25 acc: 0.9647396206855774  val: loss: 1048802.875 acc: 0.7651879787445068\n",
      "step: 8105\n",
      "train: loss: 643081.0 acc: 0.9590792655944824  val: loss: 1829856.75 acc: 0.8006836771965027\n",
      "step: 8110\n",
      "train: loss: 362213.0 acc: 0.9806534051895142  val: loss: 929233.0 acc: 0.8538601398468018\n",
      "step: 8115\n",
      "train: loss: 2738233.5 acc: 0.8958269953727722  val: loss: 499180.4375 acc: 0.9260983467102051\n",
      "step: 8120\n",
      "train: loss: 1397084.875 acc: 0.9440843462944031  val: loss: 542654.0 acc: 0.762026309967041\n",
      "step: 8125\n",
      "train: loss: 4163231.0 acc: 0.7683355808258057  val: loss: 448841.125 acc: 0.8632010221481323\n",
      "step: 8130\n",
      "train: loss: 524389.8125 acc: 0.9580072164535522  val: loss: 544323.5 acc: 0.8417528867721558\n",
      "step: 8135\n",
      "train: loss: 374848.3125 acc: 0.9564252495765686  val: loss: 662381.25 acc: 0.832023024559021\n",
      "step: 8140\n",
      "train: loss: 587249.25 acc: 0.9457442760467529  val: loss: 545083.0 acc: 0.8896716237068176\n",
      "step: 8145\n",
      "train: loss: 1278660.375 acc: 0.7326449155807495  val: loss: 1816619.625 acc: 0.39357006549835205\n",
      "step: 8150\n",
      "train: loss: 316364.71875 acc: 0.8715287446975708  val: loss: 722662.4375 acc: 0.8454413414001465\n",
      "step: 8155\n",
      "train: loss: 1409080.375 acc: 0.75575852394104  val: loss: 980767.4375 acc: 0.6600973606109619\n",
      "step: 8160\n",
      "train: loss: 1528597.125 acc: 0.35079413652420044  val: loss: 876377.75 acc: 0.7101126909255981\n",
      "step: 8165\n",
      "train: loss: 339102.90625 acc: 0.7796472311019897  val: loss: 284344.34375 acc: 0.890814483165741\n",
      "step: 8170\n",
      "train: loss: 1714585.5 acc: 0.4831780195236206  val: loss: 1161768.625 acc: 0.6334089040756226\n",
      "step: 8175\n",
      "train: loss: 627655.8125 acc: 0.5642305016517639  val: loss: 909581.25 acc: 0.7577178478240967\n",
      "step: 8180\n",
      "train: loss: 747072.5 acc: 0.5977344512939453  val: loss: 1762446.625 acc: 0.7522324323654175\n",
      "step: 8185\n",
      "train: loss: 263346.09375 acc: 0.7999675869941711  val: loss: 1127049.0 acc: 0.6337385773658752\n",
      "step: 8190\n",
      "train: loss: 424236.15625 acc: 0.7869656085968018  val: loss: 959926.9375 acc: 0.7656652331352234\n",
      "step: 8195\n",
      "train: loss: 369715.625 acc: 0.7857106924057007  val: loss: 1208461.5 acc: 0.7429746389389038\n",
      "step: 8200\n",
      "train: loss: 428921.09375 acc: 0.7511495351791382  val: loss: 629423.5 acc: 0.7955237627029419\n",
      "step: 8205\n",
      "train: loss: 272768.96875 acc: 0.8400497436523438  val: loss: 1843792.625 acc: 0.8067525029182434\n",
      "step: 8210\n",
      "train: loss: 244329.4375 acc: 0.765961766242981  val: loss: 479182.0 acc: 0.7514547109603882\n",
      "step: 8215\n",
      "train: loss: 120679.2890625 acc: 0.9130354523658752  val: loss: 1120525.125 acc: 0.8291129469871521\n",
      "step: 8220\n",
      "train: loss: 126622.609375 acc: 0.8620244860649109  val: loss: 1015882.25 acc: 0.8252499103546143\n",
      "step: 8225\n",
      "train: loss: 233054.125 acc: 0.8637821674346924  val: loss: 577009.5 acc: 0.8331754207611084\n",
      "step: 8230\n",
      "train: loss: 508549.0625 acc: 0.6499180793762207  val: loss: 1503202.625 acc: 0.7668315768241882\n",
      "step: 8235\n",
      "train: loss: 658826.9375 acc: 0.7668817043304443  val: loss: 860871.875 acc: 0.7865889072418213\n",
      "step: 8240\n",
      "train: loss: 660123.25 acc: 0.6525582671165466  val: loss: 1331087.375 acc: 0.7075939178466797\n",
      "step: 8245\n",
      "train: loss: 176849.390625 acc: 0.7502726316452026  val: loss: 2059155.625 acc: 0.6597751379013062\n",
      "step: 8250\n",
      "train: loss: 473455.96875 acc: 0.7147909998893738  val: loss: 1521820.375 acc: 0.7235058546066284\n",
      "step: 8255\n",
      "train: loss: 1154257.875 acc: 0.7205796837806702  val: loss: 1994189.375 acc: 0.7473001480102539\n",
      "step: 8260\n",
      "train: loss: 746299.8125 acc: 0.7929053902626038  val: loss: 189210.46875 acc: 0.8557907342910767\n",
      "step: 8265\n",
      "train: loss: 1381890.875 acc: 0.8475469350814819  val: loss: 790869.625 acc: 0.6754148006439209\n",
      "step: 8270\n",
      "train: loss: 474230.34375 acc: 0.9639526605606079  val: loss: 1441251.75 acc: 0.1122404932975769\n",
      "step: 8275\n",
      "train: loss: 334860.65625 acc: 0.959781289100647  val: loss: 416147.65625 acc: 0.797349750995636\n",
      "step: 8280\n",
      "train: loss: 646595.4375 acc: 0.9098576307296753  val: loss: 406480.625 acc: 0.9323245286941528\n",
      "step: 8285\n",
      "train: loss: 539326.75 acc: 0.9544739723205566  val: loss: 435991.5625 acc: 0.9444055557250977\n",
      "step: 8290\n",
      "train: loss: 258906.5 acc: 0.9767322540283203  val: loss: 916887.75 acc: 0.7345742583274841\n",
      "step: 8295\n",
      "train: loss: 260124.609375 acc: 0.9810481667518616  val: loss: 480256.0625 acc: 0.8841709494590759\n",
      "step: 8300\n",
      "train: loss: 315600.375 acc: 0.9633080959320068  val: loss: 1173239.75 acc: 0.461561918258667\n",
      "step: 8305\n",
      "train: loss: 174036.203125 acc: 0.9699949622154236  val: loss: 610951.75 acc: 0.8898643255233765\n",
      "step: 8310\n",
      "train: loss: 152409.765625 acc: 0.9670976400375366  val: loss: 381299.875 acc: 0.9269821047782898\n",
      "step: 8315\n",
      "train: loss: 181155.546875 acc: 0.963478684425354  val: loss: 944724.625 acc: 0.6798038482666016\n",
      "step: 8320\n",
      "train: loss: 21032.771484375 acc: 0.985556423664093  val: loss: 401041.5 acc: 0.9389309883117676\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 8325\n",
      "train: loss: 22968.6171875 acc: 0.965144157409668  val: loss: 627269.0625 acc: 0.7741022109985352\n",
      "step: 8330\n",
      "train: loss: 19556.224609375 acc: 0.9659504294395447  val: loss: 310726.8125 acc: 0.9541802406311035\n",
      "step: 8335\n",
      "train: loss: 15657.87109375 acc: 0.9443637132644653  val: loss: 210248.328125 acc: 0.9610965251922607\n",
      "step: 8340\n",
      "train: loss: 81699.7890625 acc: 0.943321168422699  val: loss: 792950.875 acc: 0.9084121584892273\n",
      "step: 8345\n",
      "train: loss: 11743.7626953125 acc: 0.9815866351127625  val: loss: 428744.21875 acc: 0.8938375115394592\n",
      "step: 8350\n",
      "train: loss: 16434.234375 acc: 0.9782655835151672  val: loss: 837589.625 acc: 0.9407770037651062\n",
      "step: 8355\n",
      "train: loss: 5551.76611328125 acc: 0.9917849898338318  val: loss: 244192.203125 acc: 0.9468778967857361\n",
      "step: 8360\n",
      "train: loss: 30477.298828125 acc: 0.9344374537467957  val: loss: 557718.625 acc: 0.8642392754554749\n",
      "step: 8365\n",
      "train: loss: 71285.6171875 acc: 0.9710655808448792  val: loss: 710254.75 acc: 0.9300934672355652\n",
      "step: 8370\n",
      "train: loss: 80598.2265625 acc: 0.9247246384620667  val: loss: 686256.75 acc: 0.8420072793960571\n",
      "step: 8375\n",
      "train: loss: 29042.642578125 acc: 0.9788451194763184  val: loss: 387872.5 acc: 0.9504571557044983\n",
      "step: 8380\n",
      "train: loss: 36202.16015625 acc: 0.9777989387512207  val: loss: 403559.625 acc: 0.9466922283172607\n",
      "step: 8385\n",
      "train: loss: 5616.41259765625 acc: 0.9918228387832642  val: loss: 1140717.625 acc: 0.681224524974823\n",
      "step: 8390\n",
      "train: loss: 26865.888671875 acc: 0.9870258569717407  val: loss: 733816.3125 acc: 0.9282047152519226\n",
      "step: 8395\n",
      "train: loss: 38103.21875 acc: 0.9863715171813965  val: loss: 1842324.0 acc: 0.825803279876709\n",
      "step: 8400\n",
      "train: loss: 46698.890625 acc: 0.9770646691322327  val: loss: 1202375.125 acc: 0.8962447047233582\n",
      "step: 8405\n",
      "train: loss: 22098.93359375 acc: 0.9935025572776794  val: loss: 2847752.25 acc: 0.4969936013221741\n",
      "step: 8410\n",
      "train: loss: 33701.01953125 acc: 0.9862708449363708  val: loss: 1585681.875 acc: 0.8564342856407166\n",
      "step: 8415\n",
      "train: loss: 224894.546875 acc: 0.9309899806976318  val: loss: 938981.875 acc: 0.8532819151878357\n",
      "step: 8420\n",
      "train: loss: 83579.046875 acc: 0.9742926955223083  val: loss: 2437250.5 acc: 0.24316376447677612\n",
      "step: 8425\n",
      "train: loss: 54437.30078125 acc: 0.9678080677986145  val: loss: 1106615.375 acc: 0.6905922889709473\n",
      "step: 8430\n",
      "train: loss: 678015.375 acc: 0.8963648080825806  val: loss: 681822.8125 acc: 0.9110062122344971\n",
      "step: 8435\n",
      "train: loss: 521508.875 acc: 0.9278677105903625  val: loss: 600447.0625 acc: 0.7993245124816895\n",
      "step: 8440\n",
      "train: loss: 113461.8671875 acc: 0.9879261255264282  val: loss: 694755.1875 acc: 0.7148641347885132\n",
      "step: 8445\n",
      "train: loss: 93224.6328125 acc: 0.9889784455299377  val: loss: 1189432.125 acc: 0.8628692626953125\n",
      "step: 8450\n",
      "train: loss: 172022.671875 acc: 0.9808583855628967  val: loss: 1841438.125 acc: 0.7249588966369629\n",
      "step: 8455\n",
      "train: loss: 128462.3515625 acc: 0.9823386669158936  val: loss: 1049303.375 acc: 0.8527186512947083\n",
      "step: 8460\n",
      "train: loss: 1340630.375 acc: 0.9102174043655396  val: loss: 1093393.75 acc: 0.6337401866912842\n",
      "step: 8465\n",
      "train: loss: 352496.71875 acc: 0.9715415835380554  val: loss: 2158457.5 acc: 0.28841739892959595\n",
      "step: 8470\n",
      "train: loss: 384111.5625 acc: 0.9416500329971313  val: loss: 2049074.0 acc: 0.3091534972190857\n",
      "step: 8475\n",
      "train: loss: 1115257.375 acc: 0.9620216488838196  val: loss: 557551.6875 acc: 0.8971086740493774\n",
      "step: 8480\n",
      "train: loss: 2721955.0 acc: 0.9270807504653931  val: loss: 406028.4375 acc: 0.8175169229507446\n",
      "step: 8485\n",
      "train: loss: 1464157.5 acc: 0.9348157644271851  val: loss: 616041.3125 acc: 0.8837994933128357\n",
      "step: 8490\n",
      "train: loss: 1234849.125 acc: 0.9367269277572632  val: loss: 1834070.875 acc: 0.3847684860229492\n",
      "step: 8495\n",
      "train: loss: 128381.0859375 acc: 0.9710208177566528  val: loss: 501074.96875 acc: 0.8849917054176331\n",
      "step: 8500\n",
      "train: loss: 183420.953125 acc: 0.9537991285324097  val: loss: 864398.8125 acc: 0.8387688994407654\n",
      "step: 8505\n",
      "train: loss: 852373.875 acc: 0.9034330248832703  val: loss: 707214.3125 acc: 0.71905118227005\n",
      "step: 8510\n",
      "train: loss: 2220866.5 acc: 0.7053208351135254  val: loss: 833734.75 acc: 0.6537577509880066\n",
      "step: 8515\n",
      "train: loss: 1481938.625 acc: 0.4300827980041504  val: loss: 281158.59375 acc: 0.9568110108375549\n",
      "step: 8520\n",
      "train: loss: 1269330.25 acc: 0.7841509580612183  val: loss: 654850.1875 acc: 0.6542729139328003\n",
      "step: 8525\n",
      "train: loss: 722053.125 acc: 0.8262373805046082  val: loss: 1902459.25 acc: 0.5023314952850342\n",
      "step: 8530\n",
      "train: loss: 770802.625 acc: 0.5581302046775818  val: loss: 753814.75 acc: 0.856723427772522\n",
      "step: 8535\n",
      "train: loss: 1095088.375 acc: 0.5553989410400391  val: loss: 766222.0 acc: 0.7636362314224243\n",
      "step: 8540\n",
      "train: loss: 1005380.5 acc: 0.6489317417144775  val: loss: 2902220.0 acc: 0.5945912003517151\n",
      "step: 8545\n",
      "train: loss: 834743.3125 acc: 0.7098776698112488  val: loss: 2280894.75 acc: 0.6288573741912842\n",
      "step: 8550\n",
      "train: loss: 120333.2421875 acc: 0.8922551870346069  val: loss: 550050.0 acc: 0.7293808460235596\n",
      "step: 8555\n",
      "train: loss: 57314.65625 acc: 0.9502739906311035  val: loss: 221776.203125 acc: 0.8639801144599915\n",
      "step: 8560\n",
      "train: loss: 103669.515625 acc: 0.9290181398391724  val: loss: 2606036.75 acc: 0.6168562173843384\n",
      "step: 8565\n",
      "train: loss: 212673.265625 acc: 0.8799977898597717  val: loss: 2149378.75 acc: 0.6694024801254272\n",
      "step: 8570\n",
      "train: loss: 142863.328125 acc: 0.9064593315124512  val: loss: 978664.625 acc: 0.6718592643737793\n",
      "step: 8575\n",
      "train: loss: 105838.34375 acc: 0.9261554479598999  val: loss: 1436762.625 acc: 0.644376277923584\n",
      "step: 8580\n",
      "train: loss: 106954.3828125 acc: 0.8997185230255127  val: loss: 1558252.75 acc: 0.6629023551940918\n",
      "step: 8585\n",
      "train: loss: 63419.6640625 acc: 0.9476926922798157  val: loss: 1485697.125 acc: 0.6864511370658875\n",
      "step: 8590\n",
      "train: loss: 123981.7578125 acc: 0.9078124165534973  val: loss: 435791.625 acc: 0.7904537916183472\n",
      "step: 8595\n",
      "train: loss: 30135.263671875 acc: 0.9651710987091064  val: loss: 1094472.375 acc: 0.6335669755935669\n",
      "step: 8600\n",
      "train: loss: 621234.125 acc: 0.7784445881843567  val: loss: 1675890.5 acc: 0.7144684791564941\n",
      "step: 8605\n",
      "train: loss: 92194.4296875 acc: 0.897463858127594  val: loss: 698881.9375 acc: 0.6782206296920776\n",
      "step: 8610\n",
      "train: loss: 258091.828125 acc: 0.7959691286087036  val: loss: 819100.25 acc: 0.6517421007156372\n",
      "step: 8615\n",
      "train: loss: 193905.703125 acc: 0.8765373826026917  val: loss: 1245624.125 acc: 0.6634317636489868\n",
      "step: 8620\n",
      "train: loss: 884035.375 acc: 0.7246950268745422  val: loss: 1404258.375 acc: 0.7111482620239258\n",
      "step: 8625\n",
      "train: loss: 883756.125 acc: 0.7860233187675476  val: loss: 600232.5 acc: 0.32231569290161133\n",
      "step: 8630\n",
      "train: loss: 1079419.875 acc: 0.9034346342086792  val: loss: 279291.34375 acc: 0.7989161610603333\n",
      "step: 8635\n",
      "train: loss: 1139355.125 acc: 0.8912599086761475  val: loss: 124441.8203125 acc: 0.9703054428100586\n",
      "step: 8640\n",
      "train: loss: 171697.40625 acc: 0.9809690117835999  val: loss: 328253.875 acc: 0.9628469944000244\n",
      "step: 8645\n",
      "train: loss: 156969.921875 acc: 0.9775552749633789  val: loss: 930194.0 acc: 0.8388580083847046\n",
      "step: 8650\n",
      "train: loss: 209535.671875 acc: 0.9778441786766052  val: loss: 137503.71875 acc: 0.9436954855918884\n",
      "step: 8655\n",
      "train: loss: 116478.9609375 acc: 0.989128053188324  val: loss: 471706.34375 acc: 0.7551777362823486\n",
      "step: 8660\n",
      "train: loss: 192369.828125 acc: 0.9853144884109497  val: loss: 673288.3125 acc: 0.8892732262611389\n",
      "step: 8665\n",
      "train: loss: 100981.46875 acc: 0.990571141242981  val: loss: 317724.40625 acc: 0.9323411583900452\n",
      "step: 8670\n",
      "train: loss: 99454.0390625 acc: 0.9880679845809937  val: loss: 680794.375 acc: 0.9302799701690674\n",
      "step: 8675\n",
      "train: loss: 16646.580078125 acc: 0.9906615018844604  val: loss: 831592.6875 acc: 0.9180624485015869\n",
      "step: 8680\n",
      "train: loss: 49659.71484375 acc: 0.9852944016456604  val: loss: 234060.8125 acc: 0.9492498636245728\n",
      "step: 8685\n",
      "train: loss: 11515.7314453125 acc: 0.9925372004508972  val: loss: 387169.90625 acc: 0.9573582410812378\n",
      "step: 8690\n",
      "train: loss: 13931.9052734375 acc: 0.9919600486755371  val: loss: 1251291.125 acc: 0.3619857430458069\n",
      "step: 8695\n",
      "train: loss: 18273.42578125 acc: 0.9795700907707214  val: loss: 748324.0625 acc: 0.8367083668708801\n",
      "step: 8700\n",
      "train: loss: 8535.7265625 acc: 0.9785253405570984  val: loss: 1637966.625 acc: 0.535502016544342\n",
      "step: 8705\n",
      "train: loss: 5867.36083984375 acc: 0.9860100150108337  val: loss: 214426.53125 acc: 0.9328035116195679\n",
      "step: 8710\n",
      "train: loss: 11789.8408203125 acc: 0.9625186324119568  val: loss: 1630776.125 acc: 0.42598652839660645\n",
      "step: 8715\n",
      "train: loss: 10860.671875 acc: 0.9813548922538757  val: loss: 573357.5 acc: 0.8886606693267822\n",
      "step: 8720\n",
      "train: loss: 23528.0625 acc: 0.9510441422462463  val: loss: 1736389.25 acc: 0.8235557079315186\n",
      "step: 8725\n",
      "train: loss: 17514.029296875 acc: 0.980333149433136  val: loss: 1245202.875 acc: 0.8302338123321533\n",
      "step: 8730\n",
      "train: loss: 37502.4609375 acc: 0.9853931069374084  val: loss: 1374883.375 acc: 0.5574414730072021\n",
      "step: 8735\n",
      "train: loss: 38261.92578125 acc: 0.9820513725280762  val: loss: 2388119.0 acc: 0.638386607170105\n",
      "step: 8740\n",
      "train: loss: 32389.134765625 acc: 0.9871904253959656  val: loss: 1000951.3125 acc: 0.8848961591720581\n",
      "step: 8745\n",
      "train: loss: 17779.337890625 acc: 0.9852226376533508  val: loss: 2175878.0 acc: 0.7242530584335327\n",
      "step: 8750\n",
      "train: loss: 14387.2880859375 acc: 0.9600993990898132  val: loss: 1360055.375 acc: 0.898535966873169\n",
      "step: 8755\n",
      "train: loss: 6695.0888671875 acc: 0.9961985945701599  val: loss: 730080.4375 acc: 0.9181740283966064\n",
      "step: 8760\n",
      "train: loss: 22838.6796875 acc: 0.9896144270896912  val: loss: 528768.0625 acc: 0.8957787156105042\n",
      "step: 8765\n",
      "train: loss: 35700.57421875 acc: 0.9910584092140198  val: loss: 2349823.0 acc: 0.6835951805114746\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 8770\n",
      "train: loss: 30775.150390625 acc: 0.9918065667152405  val: loss: 1615260.625 acc: 0.499312162399292\n",
      "step: 8775\n",
      "train: loss: 32090.61328125 acc: 0.9854297637939453  val: loss: 1182896.625 acc: 0.7178955078125\n",
      "step: 8780\n",
      "train: loss: 185976.765625 acc: 0.9143615961074829  val: loss: 695055.375 acc: 0.8698328137397766\n",
      "step: 8785\n",
      "train: loss: 60053.37890625 acc: 0.9808563590049744  val: loss: 2591648.5 acc: -0.9898349046707153\n",
      "step: 8790\n",
      "train: loss: 103673.7421875 acc: 0.9597060084342957  val: loss: 2496123.0 acc: -0.4827888011932373\n",
      "step: 8795\n",
      "train: loss: 115093.796875 acc: 0.9731147885322571  val: loss: 1826687.625 acc: 0.7069759368896484\n",
      "step: 8800\n",
      "train: loss: 155252.546875 acc: 0.9830076694488525  val: loss: 667210.5625 acc: 0.6798161268234253\n",
      "step: 8805\n",
      "train: loss: 153520.203125 acc: 0.9850464463233948  val: loss: 998095.25 acc: 0.5701212882995605\n",
      "step: 8810\n",
      "train: loss: 202240.703125 acc: 0.9771544337272644  val: loss: 1493146.625 acc: 0.6777270436286926\n",
      "step: 8815\n",
      "train: loss: 192306.734375 acc: 0.9604618549346924  val: loss: 292909.59375 acc: 0.9493823051452637\n",
      "step: 8820\n",
      "train: loss: 158407.640625 acc: 0.9868977069854736  val: loss: 1367252.125 acc: 0.7738953828811646\n",
      "step: 8825\n",
      "train: loss: 511539.0625 acc: 0.964479923248291  val: loss: 3306034.25 acc: 0.4825162887573242\n",
      "step: 8830\n",
      "train: loss: 521966.625 acc: 0.9386653900146484  val: loss: 1822201.375 acc: 0.7997592687606812\n",
      "step: 8835\n",
      "train: loss: 204400.5625 acc: 0.9688030481338501  val: loss: 1323089.25 acc: 0.7998220324516296\n",
      "step: 8840\n",
      "train: loss: 1364113.875 acc: 0.9340777397155762  val: loss: 1226578.5 acc: 0.8201963305473328\n",
      "step: 8845\n",
      "train: loss: 971243.0625 acc: 0.9740821123123169  val: loss: 555275.9375 acc: 0.8694800138473511\n",
      "step: 8850\n",
      "train: loss: 1165295.5 acc: 0.9088127613067627  val: loss: 716406.1875 acc: 0.9408405423164368\n",
      "step: 8855\n",
      "train: loss: 796864.6875 acc: 0.9491272568702698  val: loss: 702252.25 acc: 0.8833627700805664\n",
      "step: 8860\n",
      "train: loss: 212167.140625 acc: 0.9693885445594788  val: loss: 1073922.875 acc: 0.7877910733222961\n",
      "step: 8865\n",
      "train: loss: 526013.9375 acc: 0.9671425223350525  val: loss: 209533.890625 acc: 0.9266061782836914\n",
      "step: 8870\n",
      "train: loss: 569128.4375 acc: 0.9120362401008606  val: loss: 509852.375 acc: 0.7146092057228088\n",
      "step: 8875\n",
      "train: loss: 1054994.75 acc: 0.8419947028160095  val: loss: 1584786.5 acc: 0.5503896474838257\n",
      "step: 8880\n",
      "train: loss: 1144161.75 acc: 0.44225525856018066  val: loss: 350120.0625 acc: 0.9147994518280029\n",
      "step: 8885\n",
      "train: loss: 1498366.125 acc: 0.49584394693374634  val: loss: 1296988.375 acc: 0.7329620122909546\n",
      "step: 8890\n",
      "train: loss: 315221.65625 acc: 0.7262767553329468  val: loss: 509275.65625 acc: 0.7534905076026917\n",
      "step: 8895\n",
      "train: loss: 506971.40625 acc: 0.8370728492736816  val: loss: 1327847.625 acc: 0.7089841365814209\n",
      "step: 8900\n",
      "train: loss: 1097310.125 acc: 0.7326724529266357  val: loss: 559215.1875 acc: 0.814566969871521\n",
      "step: 8905\n",
      "train: loss: 1521893.375 acc: 0.25047290325164795  val: loss: 945185.0 acc: 0.4987083077430725\n",
      "step: 8910\n",
      "train: loss: 357571.21875 acc: 0.8048353791236877  val: loss: 2517564.5 acc: 0.6286799907684326\n",
      "step: 8915\n",
      "train: loss: 428555.90625 acc: 0.720533549785614  val: loss: 3014309.5 acc: 0.6534972190856934\n",
      "step: 8920\n",
      "train: loss: 169782.65625 acc: 0.8704699873924255  val: loss: 940962.4375 acc: 0.6818318367004395\n",
      "step: 8925\n",
      "train: loss: 124195.1484375 acc: 0.8982894420623779  val: loss: 1629440.125 acc: 0.6329432725906372\n",
      "step: 8930\n",
      "train: loss: 55171.59765625 acc: 0.9550621509552002  val: loss: 1994956.375 acc: 0.6182268857955933\n",
      "step: 8935\n",
      "train: loss: 276778.875 acc: 0.8343517184257507  val: loss: 446371.59375 acc: 0.7259392738342285\n",
      "step: 8940\n",
      "train: loss: 188433.296875 acc: 0.8532339930534363  val: loss: 1713012.875 acc: 0.6630980968475342\n",
      "step: 8945\n",
      "train: loss: 85687.46875 acc: 0.923747181892395  val: loss: 2187208.75 acc: 0.6112226247787476\n",
      "step: 8950\n",
      "train: loss: 220745.296875 acc: 0.8555096983909607  val: loss: 3353948.0 acc: 0.5971757173538208\n",
      "step: 8955\n",
      "train: loss: 77417.6015625 acc: 0.8697158694267273  val: loss: 1673494.625 acc: 0.639114499092102\n",
      "step: 8960\n",
      "train: loss: 270833.53125 acc: 0.834865152835846  val: loss: 1569282.375 acc: 0.6346020102500916\n",
      "step: 8965\n",
      "train: loss: 115521.8984375 acc: 0.8823544979095459  val: loss: 3213911.75 acc: 0.5140484571456909\n",
      "step: 8970\n",
      "train: loss: 106062.5859375 acc: 0.9009304046630859  val: loss: 2562330.75 acc: 0.6143895387649536\n",
      "step: 8975\n",
      "train: loss: 744100.1875 acc: 0.7330070734024048  val: loss: 2028511.5 acc: 0.626305341720581\n",
      "step: 8980\n",
      "train: loss: 341459.0 acc: 0.7558649778366089  val: loss: 3162136.75 acc: 0.5032289028167725\n",
      "step: 8985\n",
      "train: loss: 2002400.625 acc: 0.6389397382736206  val: loss: 874294.75 acc: 0.7130728363990784\n",
      "step: 8990\n",
      "train: loss: 988080.875 acc: 0.8197336196899414  val: loss: 437454.375 acc: 0.8423020839691162\n",
      "step: 8995\n",
      "train: loss: 963825.875 acc: 0.9034472107887268  val: loss: 866337.8125 acc: 0.7686874866485596\n",
      "step: 9000\n",
      "train: loss: 281393.625 acc: 0.9765327572822571  val: loss: 696729.0625 acc: 0.9300692081451416\n",
      "step: 9005\n",
      "train: loss: 250950.5 acc: 0.9621677398681641  val: loss: 267266.625 acc: 0.9033721089363098\n",
      "step: 9010\n",
      "train: loss: 596661.25 acc: 0.9117529392242432  val: loss: 772470.625 acc: 0.9414981603622437\n",
      "step: 9015\n",
      "train: loss: 302309.65625 acc: 0.9666542410850525  val: loss: 2021229.5 acc: 0.4268289804458618\n",
      "step: 9020\n",
      "train: loss: 179452.3125 acc: 0.9839636087417603  val: loss: 960053.4375 acc: 0.8438277244567871\n",
      "step: 9025\n",
      "train: loss: 224447.609375 acc: 0.9843586683273315  val: loss: 747625.5625 acc: 0.8447331786155701\n",
      "step: 9030\n",
      "train: loss: 132167.0625 acc: 0.9869338274002075  val: loss: 1747343.875 acc: 0.3809093236923218\n",
      "step: 9035\n",
      "train: loss: 86596.7890625 acc: 0.9848358035087585  val: loss: 919183.8125 acc: 0.8812161087989807\n",
      "step: 9040\n",
      "train: loss: 87896.5546875 acc: 0.9827666282653809  val: loss: 2530196.0 acc: 0.6575912237167358\n",
      "step: 9045\n",
      "train: loss: 34252.48828125 acc: 0.9777722358703613  val: loss: 1446865.125 acc: 0.4441758990287781\n",
      "step: 9050\n",
      "train: loss: 22408.791015625 acc: 0.9864226579666138  val: loss: 986410.1875 acc: 0.3484649658203125\n",
      "step: 9055\n",
      "train: loss: 20402.767578125 acc: 0.970988392829895  val: loss: 1323996.0 acc: 0.07651811838150024\n",
      "step: 9060\n",
      "train: loss: 191641.046875 acc: 0.8926059603691101  val: loss: 1524832.0 acc: 0.7331032156944275\n",
      "step: 9065\n",
      "train: loss: 22758.3359375 acc: 0.8931185007095337  val: loss: 738451.3125 acc: 0.9241184592247009\n",
      "step: 9070\n",
      "train: loss: 13197.857421875 acc: 0.9773499369621277  val: loss: 943251.4375 acc: 0.893444299697876\n",
      "step: 9075\n",
      "train: loss: 8084.81396484375 acc: 0.976864218711853  val: loss: 999763.3125 acc: 0.6242730617523193\n",
      "step: 9080\n",
      "train: loss: 14686.3203125 acc: 0.9656139016151428  val: loss: 738711.8125 acc: 0.8710290789604187\n",
      "step: 9085\n",
      "train: loss: 5162.60302734375 acc: 0.9837814569473267  val: loss: 192778.546875 acc: 0.8954347372055054\n",
      "step: 9090\n",
      "train: loss: 33873.45703125 acc: 0.972538411617279  val: loss: 466829.84375 acc: 0.9367809295654297\n",
      "step: 9095\n",
      "train: loss: 28776.87109375 acc: 0.9868347644805908  val: loss: 3091521.0 acc: 0.3784070611000061\n",
      "step: 9100\n",
      "train: loss: 7831.35595703125 acc: 0.9942838549613953  val: loss: 2454747.5 acc: 0.3162004351615906\n",
      "step: 9105\n",
      "train: loss: 11961.6806640625 acc: 0.991157054901123  val: loss: 3265755.0 acc: -0.08647465705871582\n",
      "step: 9110\n",
      "train: loss: 14790.14453125 acc: 0.9904559850692749  val: loss: 2232469.25 acc: 0.6875478029251099\n",
      "step: 9115\n",
      "train: loss: 11459.115234375 acc: 0.989071786403656  val: loss: 633698.875 acc: 0.7377904653549194\n",
      "step: 9120\n",
      "train: loss: 10045.837890625 acc: 0.9961559176445007  val: loss: 463166.09375 acc: 0.9493648409843445\n",
      "step: 9125\n",
      "train: loss: 50449.44921875 acc: 0.9666875004768372  val: loss: 2964993.75 acc: 0.7118320465087891\n",
      "step: 9130\n",
      "train: loss: 23095.01953125 acc: 0.9939016699790955  val: loss: 1862360.75 acc: 0.7148662805557251\n",
      "step: 9135\n",
      "train: loss: 10284.013671875 acc: 0.9974563121795654  val: loss: 1557875.125 acc: 0.7654159069061279\n",
      "step: 9140\n",
      "train: loss: 44172.70703125 acc: 0.9684182405471802  val: loss: 2323493.5 acc: 0.19482266902923584\n",
      "step: 9145\n",
      "train: loss: 56684.046875 acc: 0.9830097556114197  val: loss: 1085687.625 acc: 0.755294144153595\n",
      "step: 9150\n",
      "train: loss: 65810.125 acc: 0.9869751334190369  val: loss: 609943.5625 acc: 0.8302516341209412\n",
      "step: 9155\n",
      "train: loss: 187456.484375 acc: 0.949414849281311  val: loss: 378704.1875 acc: 0.8435707092285156\n",
      "step: 9160\n",
      "train: loss: 493749.90625 acc: 0.8211582899093628  val: loss: 665086.8125 acc: 0.8148981332778931\n",
      "step: 9165\n",
      "train: loss: 785988.375 acc: 0.8727628588676453  val: loss: 911802.375 acc: 0.9270693063735962\n",
      "step: 9170\n",
      "train: loss: 239836.15625 acc: 0.9814438819885254  val: loss: 2309911.0 acc: 0.8191972374916077\n",
      "step: 9175\n",
      "train: loss: 122017.9453125 acc: 0.9889768958091736  val: loss: 2031336.875 acc: 0.717259407043457\n",
      "step: 9180\n",
      "train: loss: 135785.171875 acc: 0.9805912375450134  val: loss: 1154295.0 acc: 0.6420345902442932\n",
      "step: 9185\n",
      "train: loss: 236001.578125 acc: 0.9613440036773682  val: loss: 2872134.0 acc: -0.004496216773986816\n",
      "step: 9190\n",
      "train: loss: 888617.625 acc: 0.9531245231628418  val: loss: 237706.171875 acc: 0.8785949349403381\n",
      "step: 9195\n",
      "train: loss: 401231.78125 acc: 0.9706028699874878  val: loss: 303819.65625 acc: 0.8992279767990112\n",
      "step: 9200\n",
      "train: loss: 223016.65625 acc: 0.9152345061302185  val: loss: 1903441.5 acc: 0.059808969497680664\n",
      "step: 9205\n",
      "train: loss: 1203326.25 acc: 0.9536370038986206  val: loss: 552796.0625 acc: 0.8313642144203186\n",
      "step: 9210\n",
      "train: loss: 4645872.5 acc: 0.8241621255874634  val: loss: 633911.0 acc: 0.7832216620445251\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 9215\n",
      "train: loss: 2079575.5 acc: 0.9303745627403259  val: loss: 317886.1875 acc: 0.9109153151512146\n",
      "step: 9220\n",
      "train: loss: 2180539.25 acc: 0.9055238962173462  val: loss: 717079.6875 acc: 0.8575753569602966\n",
      "step: 9225\n",
      "train: loss: 763099.5625 acc: 0.943010687828064  val: loss: 808893.1875 acc: 0.7140625715255737\n",
      "step: 9230\n",
      "train: loss: 262455.53125 acc: 0.9671087861061096  val: loss: 650685.6875 acc: 0.824217677116394\n",
      "step: 9235\n",
      "train: loss: 784178.125 acc: 0.9283664226531982  val: loss: 243197.421875 acc: 0.9309078454971313\n",
      "step: 9240\n",
      "train: loss: 369056.875 acc: 0.9171598553657532  val: loss: 249955.921875 acc: 0.9101082682609558\n",
      "step: 9245\n",
      "train: loss: 1997920.75 acc: 0.21726948022842407  val: loss: 447114.09375 acc: 0.48004162311553955\n",
      "step: 9250\n",
      "train: loss: 1222853.5 acc: 0.8012203574180603  val: loss: 1162561.0 acc: 0.8679770827293396\n",
      "step: 9255\n",
      "train: loss: 1256054.125 acc: 0.32446861267089844  val: loss: 735459.75 acc: 0.7552943229675293\n",
      "step: 9260\n",
      "train: loss: 636002.375 acc: 0.7013978958129883  val: loss: 712270.125 acc: 0.698965311050415\n",
      "step: 9265\n",
      "train: loss: 1221882.25 acc: 0.6013703346252441  val: loss: 1755267.375 acc: 0.853029191493988\n",
      "step: 9270\n",
      "train: loss: 1114990.625 acc: 0.36878514289855957  val: loss: 725099.6875 acc: 0.8501414060592651\n",
      "step: 9275\n",
      "train: loss: 501152.625 acc: 0.5798335075378418  val: loss: 1416989.875 acc: 0.6571078896522522\n",
      "step: 9280\n",
      "train: loss: 240343.03125 acc: 0.7775249481201172  val: loss: 911892.8125 acc: 0.7405245304107666\n",
      "step: 9285\n",
      "train: loss: 217126.453125 acc: 0.8143879771232605  val: loss: 3397719.25 acc: 0.7267496585845947\n",
      "step: 9290\n",
      "train: loss: 106236.84375 acc: 0.9214583039283752  val: loss: 2432585.5 acc: 0.7610306739807129\n",
      "step: 9295\n",
      "train: loss: 92889.1171875 acc: 0.9181004762649536  val: loss: 1355586.875 acc: 0.7009034156799316\n",
      "step: 9300\n",
      "train: loss: 26055.787109375 acc: 0.978886604309082  val: loss: 1093061.875 acc: 0.7641015648841858\n",
      "step: 9305\n",
      "train: loss: 331079.34375 acc: 0.8258984684944153  val: loss: 3917756.5 acc: 0.6989230513572693\n",
      "step: 9310\n",
      "train: loss: 365901.125 acc: 0.7797856330871582  val: loss: 1757589.875 acc: 0.6962152719497681\n",
      "step: 9315\n",
      "train: loss: 156152.328125 acc: 0.8506777286529541  val: loss: 1127140.875 acc: 0.8895159959793091\n",
      "step: 9320\n",
      "train: loss: 78543.4296875 acc: 0.9087792634963989  val: loss: 1470944.0 acc: 0.7631427049636841\n",
      "step: 9325\n",
      "train: loss: 472063.1875 acc: 0.7399129867553711  val: loss: 2579629.75 acc: 0.7595350742340088\n",
      "step: 9330\n",
      "train: loss: 401968.84375 acc: 0.804624080657959  val: loss: 2374634.0 acc: 0.7121669054031372\n",
      "step: 9335\n",
      "train: loss: 700260.5 acc: 0.7324994802474976  val: loss: 2816419.5 acc: 0.6000418663024902\n",
      "step: 9340\n",
      "train: loss: 793692.6875 acc: 0.7016007900238037  val: loss: 618904.6875 acc: 0.7170344591140747\n",
      "step: 9345\n",
      "train: loss: 601688.9375 acc: 0.7676969766616821  val: loss: 730555.1875 acc: 0.7259635925292969\n",
      "step: 9350\n",
      "train: loss: 89892.7890625 acc: 0.9158647060394287  val: loss: 1406850.625 acc: 0.6696399450302124\n",
      "step: 9355\n",
      "train: loss: 1385329.25 acc: 0.7103084921836853  val: loss: 331914.1875 acc: 0.8220068216323853\n",
      "step: 9360\n",
      "train: loss: 2301911.5 acc: 0.7827364206314087  val: loss: 2114340.5 acc: 0.7907159924507141\n",
      "step: 9365\n",
      "train: loss: 1347946.75 acc: 0.8619735240936279  val: loss: 736827.4375 acc: 0.766639769077301\n",
      "step: 9370\n",
      "train: loss: 1059661.25 acc: 0.8566812872886658  val: loss: 1180037.375 acc: 0.9014201164245605\n",
      "step: 9375\n",
      "train: loss: 546074.5 acc: 0.876749575138092  val: loss: 2011860.375 acc: 0.8097456097602844\n",
      "step: 9380\n",
      "train: loss: 637858.5625 acc: 0.9173435568809509  val: loss: 1525835.0 acc: 0.6418202519416809\n",
      "step: 9385\n",
      "train: loss: 628687.1875 acc: 0.9555668234825134  val: loss: 1109310.375 acc: 0.5217673778533936\n",
      "step: 9390\n",
      "train: loss: 598218.9375 acc: 0.9534186720848083  val: loss: 1058218.625 acc: 0.840449333190918\n",
      "step: 9395\n",
      "train: loss: 407533.875 acc: 0.9621241688728333  val: loss: 1613265.5 acc: 0.355593740940094\n",
      "step: 9400\n",
      "train: loss: 649358.875 acc: 0.7754040360450745  val: loss: 1222096.75 acc: 0.7636034488677979\n",
      "step: 9405\n",
      "train: loss: 435395.84375 acc: 0.9063669443130493  val: loss: 708726.625 acc: 0.7128749489784241\n",
      "step: 9410\n",
      "train: loss: 280712.03125 acc: 0.7148573398590088  val: loss: 805714.75 acc: 0.828487753868103\n",
      "step: 9415\n",
      "train: loss: 62593.09375 acc: 0.8012638092041016  val: loss: 326585.78125 acc: 0.8548312187194824\n",
      "step: 9420\n",
      "train: loss: 117540.8359375 acc: 0.8916593194007874  val: loss: 387755.625 acc: 0.8549774885177612\n",
      "step: 9425\n",
      "train: loss: 142935.609375 acc: 0.9103186130523682  val: loss: 1062567.375 acc: 0.8836053609848022\n",
      "step: 9430\n",
      "train: loss: 31076.609375 acc: 0.9254741072654724  val: loss: 560151.8125 acc: 0.8107057809829712\n",
      "step: 9435\n",
      "train: loss: 8437.15625 acc: 0.976584255695343  val: loss: 1174084.75 acc: 0.8190589547157288\n",
      "step: 9440\n",
      "train: loss: 21779.18359375 acc: 0.9629159569740295  val: loss: 1329441.75 acc: 0.8459383845329285\n",
      "step: 9445\n",
      "train: loss: 21846.98046875 acc: 0.9616480469703674  val: loss: 965114.25 acc: 0.8232551217079163\n",
      "step: 9450\n",
      "train: loss: 31509.421875 acc: 0.9477806091308594  val: loss: 318572.03125 acc: 0.8126721382141113\n",
      "step: 9455\n",
      "train: loss: 163893.46875 acc: 0.9003250002861023  val: loss: 120479.8515625 acc: 0.9676747918128967\n",
      "step: 9460\n",
      "train: loss: 90608.40625 acc: 0.9485206604003906  val: loss: 398926.59375 acc: 0.9574224352836609\n",
      "step: 9465\n",
      "train: loss: 55242.70703125 acc: 0.9373852610588074  val: loss: 1168297.75 acc: 0.833297848701477\n",
      "step: 9470\n",
      "train: loss: 283560.71875 acc: 0.9198470711708069  val: loss: 143128.265625 acc: 0.9600857496261597\n",
      "step: 9475\n",
      "train: loss: 56440.48046875 acc: 0.9095318913459778  val: loss: 287549.125 acc: 0.9259514212608337\n",
      "step: 9480\n",
      "train: loss: 92005.21875 acc: 0.9279754161834717  val: loss: 545986.25 acc: 0.8153602480888367\n",
      "step: 9485\n",
      "train: loss: 43236.26953125 acc: 0.9609939455986023  val: loss: 882267.625 acc: 0.8062984347343445\n",
      "step: 9490\n",
      "train: loss: 66521.1171875 acc: 0.9706557989120483  val: loss: 1271757.0 acc: -0.21761643886566162\n",
      "step: 9495\n",
      "train: loss: 59005.2890625 acc: 0.9870203733444214  val: loss: 345883.59375 acc: 0.8906617164611816\n",
      "step: 9500\n",
      "train: loss: 80767.90625 acc: 0.9804617166519165  val: loss: 853225.875 acc: 0.8093717098236084\n",
      "step: 9505\n",
      "train: loss: 95085.71875 acc: 0.9744360446929932  val: loss: 186668.296875 acc: 0.885718822479248\n",
      "step: 9510\n",
      "train: loss: 80945.3125 acc: 0.9648818373680115  val: loss: 1100863.0 acc: 0.02715909481048584\n",
      "step: 9515\n",
      "train: loss: 159398.015625 acc: 0.9594394564628601  val: loss: 2383387.75 acc: 0.5893269181251526\n",
      "step: 9520\n",
      "train: loss: 286721.90625 acc: 0.9533305168151855  val: loss: 1231084.25 acc: 0.7090707421302795\n",
      "step: 9525\n",
      "train: loss: 32114.21484375 acc: 0.9926307201385498  val: loss: 333625.09375 acc: 0.9204319715499878\n",
      "step: 9530\n",
      "train: loss: 190042.9375 acc: 0.9664980173110962  val: loss: 220269.953125 acc: 0.9045831561088562\n",
      "step: 9535\n",
      "train: loss: 110395.5546875 acc: 0.9908379316329956  val: loss: 496160.96875 acc: 0.8293588757514954\n",
      "step: 9540\n",
      "train: loss: 317624.875 acc: 0.9545747637748718  val: loss: 606396.875 acc: 0.8628818392753601\n",
      "step: 9545\n",
      "train: loss: 244303.890625 acc: 0.9168285131454468  val: loss: 540239.125 acc: 0.8780061602592468\n",
      "step: 9550\n",
      "train: loss: 413161.96875 acc: 0.9700398445129395  val: loss: 1017359.875 acc: 0.6303179264068604\n",
      "step: 9555\n",
      "train: loss: 373772.59375 acc: 0.9816545248031616  val: loss: 637481.125 acc: 0.8250361680984497\n",
      "step: 9560\n",
      "train: loss: 282741.75 acc: 0.9755860567092896  val: loss: 1285064.5 acc: 0.41989052295684814\n",
      "step: 9565\n",
      "train: loss: 196422.953125 acc: 0.9696736931800842  val: loss: 1979826.0 acc: 0.134088397026062\n",
      "step: 9570\n",
      "train: loss: 633001.3125 acc: 0.978812575340271  val: loss: 622319.75 acc: 0.8336713314056396\n",
      "step: 9575\n",
      "train: loss: 6958434.5 acc: 0.7486848831176758  val: loss: 410781.4375 acc: 0.8993916511535645\n",
      "step: 9580\n",
      "train: loss: 2046058.875 acc: 0.9196842312812805  val: loss: 555903.25 acc: 0.9051026701927185\n",
      "step: 9585\n",
      "train: loss: 663328.9375 acc: 0.9331830739974976  val: loss: 241267.90625 acc: 0.9374037384986877\n",
      "step: 9590\n",
      "train: loss: 439348.375 acc: 0.9699289202690125  val: loss: 1011305.625 acc: 0.9381000399589539\n",
      "step: 9595\n",
      "train: loss: 1040484.4375 acc: 0.8670619130134583  val: loss: 361102.5 acc: 0.9483165740966797\n",
      "step: 9600\n",
      "train: loss: 333721.53125 acc: 0.9641363024711609  val: loss: 1123037.25 acc: 0.659332275390625\n",
      "step: 9605\n",
      "train: loss: 364219.9375 acc: 0.9378889799118042  val: loss: 370232.4375 acc: 0.9559719562530518\n",
      "step: 9610\n",
      "train: loss: 1286134.125 acc: 0.3159932494163513  val: loss: 1444799.25 acc: 0.7543973922729492\n",
      "step: 9615\n",
      "train: loss: 1200274.75 acc: 0.811672568321228  val: loss: 552396.6875 acc: 0.753229558467865\n",
      "step: 9620\n",
      "train: loss: 574957.125 acc: 0.7216655015945435  val: loss: 2221772.0 acc: 0.839766263961792\n",
      "step: 9625\n",
      "train: loss: 623599.0625 acc: 0.8261509537696838  val: loss: 551179.25 acc: 0.9209050536155701\n",
      "step: 9630\n",
      "train: loss: 1295465.75 acc: 0.7163652181625366  val: loss: 1410566.625 acc: 0.8800991773605347\n",
      "step: 9635\n",
      "train: loss: 1620442.125 acc: 0.22365647554397583  val: loss: 814784.625 acc: 0.7052775025367737\n",
      "step: 9640\n",
      "train: loss: 950972.6875 acc: 0.31995922327041626  val: loss: 2410512.75 acc: 0.7294431924819946\n",
      "step: 9645\n",
      "train: loss: 879098.5 acc: 0.5566550493240356  val: loss: 2295668.5 acc: 0.6785244941711426\n",
      "step: 9650\n",
      "train: loss: 421402.625 acc: 0.6579631567001343  val: loss: 1522737.625 acc: 0.820696234703064\n",
      "step: 9655\n",
      "train: loss: 428375.46875 acc: 0.6316701173782349  val: loss: 376356.375 acc: 0.7372990250587463\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 9660\n",
      "train: loss: 158453.578125 acc: 0.8655397295951843  val: loss: 499877.84375 acc: 0.8044096231460571\n",
      "step: 9665\n",
      "train: loss: 286009.90625 acc: 0.8290684819221497  val: loss: 1557682.625 acc: 0.6844767332077026\n",
      "step: 9670\n",
      "train: loss: 94620.453125 acc: 0.9204103946685791  val: loss: 1335935.375 acc: 0.6971570253372192\n",
      "step: 9675\n",
      "train: loss: 583944.25 acc: 0.7325242161750793  val: loss: 347055.90625 acc: 0.8898059129714966\n",
      "step: 9680\n",
      "train: loss: 307389.90625 acc: 0.8127285242080688  val: loss: 641100.3125 acc: 0.7252858281135559\n",
      "step: 9685\n",
      "train: loss: 30577.45703125 acc: 0.9682365655899048  val: loss: 664111.875 acc: 0.8101942539215088\n",
      "step: 9690\n",
      "train: loss: 274044.9375 acc: 0.8340615034103394  val: loss: 1194258.5 acc: 0.7664440870285034\n",
      "step: 9695\n",
      "train: loss: 457180.0 acc: 0.7541033029556274  val: loss: 1798418.375 acc: 0.7445158958435059\n",
      "step: 9700\n",
      "train: loss: 1436019.875 acc: 0.3558931350708008  val: loss: 3814825.25 acc: 0.7721801400184631\n",
      "step: 9705\n",
      "train: loss: 537746.75 acc: 0.7403300404548645  val: loss: 1052876.5 acc: 0.7467241287231445\n",
      "step: 9710\n",
      "train: loss: 166739.375 acc: 0.8417272567749023  val: loss: 1107705.375 acc: 0.7404485940933228\n",
      "step: 9715\n",
      "train: loss: 1029278.4375 acc: 0.6739596128463745  val: loss: 1828636.5 acc: 0.7546869516372681\n",
      "step: 9720\n",
      "train: loss: 1380424.625 acc: 0.768281102180481  val: loss: 437984.65625 acc: 0.8480539321899414\n",
      "step: 9725\n",
      "train: loss: 1363060.125 acc: 0.797764003276825  val: loss: 439133.375 acc: 0.76882004737854\n",
      "step: 9730\n",
      "train: loss: 1170124.375 acc: 0.9238607883453369  val: loss: 1251914.875 acc: 0.29586875438690186\n",
      "step: 9735\n",
      "train: loss: 350831.65625 acc: 0.9575048089027405  val: loss: 344779.875 acc: 0.9150872826576233\n",
      "step: 9740\n",
      "train: loss: 263770.59375 acc: 0.9363816976547241  val: loss: 2360478.0 acc: 0.554322361946106\n",
      "step: 9745\n",
      "train: loss: 278573.0625 acc: 0.9553046226501465  val: loss: 697978.3125 acc: 0.8699554204940796\n",
      "step: 9750\n",
      "train: loss: 119221.0078125 acc: 0.9878474473953247  val: loss: 2412325.25 acc: 0.3787726163864136\n",
      "step: 9755\n",
      "train: loss: 351116.03125 acc: 0.9706352949142456  val: loss: 667060.9375 acc: 0.8419665098190308\n",
      "step: 9760\n",
      "train: loss: 456647.125 acc: 0.9626688957214355  val: loss: 580799.125 acc: 0.9123620986938477\n",
      "step: 9765\n",
      "train: loss: 155357.6875 acc: 0.9308536052703857  val: loss: 492951.1875 acc: 0.8167271614074707\n",
      "step: 9770\n",
      "train: loss: 189407.15625 acc: 0.9237050414085388  val: loss: 602465.875 acc: 0.8809410333633423\n",
      "step: 9775\n",
      "train: loss: 124213.890625 acc: 0.9552580714225769  val: loss: 778740.6875 acc: 0.6622642874717712\n",
      "step: 9780\n",
      "train: loss: 84959.015625 acc: 0.9585367441177368  val: loss: 2291182.75 acc: 0.06515288352966309\n",
      "step: 9785\n",
      "train: loss: 13477.375 acc: 0.9742795825004578  val: loss: 1338302.25 acc: 0.7351061105728149\n",
      "step: 9790\n",
      "train: loss: 19851.8828125 acc: 0.9670724272727966  val: loss: 689155.6875 acc: 0.7432904243469238\n",
      "step: 9795\n",
      "train: loss: 12597.8046875 acc: 0.9836852550506592  val: loss: 579404.8125 acc: 0.9426912665367126\n",
      "step: 9800\n",
      "train: loss: 29377.92578125 acc: 0.9204795360565186  val: loss: 1081813.25 acc: 0.7544030547142029\n",
      "step: 9805\n",
      "train: loss: 4531.16015625 acc: 0.9894838333129883  val: loss: 1700399.0 acc: 0.22940093278884888\n",
      "step: 9810\n",
      "train: loss: 25974.896484375 acc: 0.9454243183135986  val: loss: 1443915.25 acc: 0.5612678527832031\n",
      "step: 9815\n",
      "train: loss: 22421.421875 acc: 0.9669519066810608  val: loss: 666216.25 acc: 0.8707758784294128\n",
      "step: 9820\n",
      "train: loss: 36955.5390625 acc: 0.9689251184463501  val: loss: 737428.0625 acc: 0.7360689640045166\n",
      "step: 9825\n",
      "train: loss: 73122.09375 acc: 0.9655715823173523  val: loss: 925441.375 acc: 0.854676365852356\n",
      "step: 9830\n",
      "train: loss: 33656.81640625 acc: 0.977488100528717  val: loss: 1459697.375 acc: 0.810423731803894\n",
      "step: 9835\n",
      "train: loss: 25692.681640625 acc: 0.9832897186279297  val: loss: 291001.875 acc: 0.9639672040939331\n",
      "step: 9840\n",
      "train: loss: 55951.80078125 acc: 0.9781671762466431  val: loss: 131790.40625 acc: 0.9807499647140503\n",
      "step: 9845\n",
      "train: loss: 26097.67578125 acc: 0.976588785648346  val: loss: 715794.375 acc: 0.8994548320770264\n",
      "step: 9850\n",
      "train: loss: 16493.01953125 acc: 0.9837453365325928  val: loss: 402036.21875 acc: 0.874431848526001\n",
      "step: 9855\n",
      "train: loss: 50875.59375 acc: 0.9805873036384583  val: loss: 452871.875 acc: 0.9108915328979492\n",
      "step: 9860\n",
      "train: loss: 55158.33984375 acc: 0.9892671704292297  val: loss: 219154.53125 acc: 0.8682861924171448\n",
      "step: 9865\n",
      "train: loss: 32600.9921875 acc: 0.9912028908729553  val: loss: 141698.03125 acc: 0.9424527883529663\n",
      "step: 9870\n",
      "train: loss: 54786.78125 acc: 0.9833128452301025  val: loss: 399832.5 acc: 0.9103754758834839\n",
      "step: 9875\n",
      "train: loss: 21545.18359375 acc: 0.9918434619903564  val: loss: 125130.1015625 acc: 0.9776105284690857\n",
      "step: 9880\n",
      "train: loss: 156146.0 acc: 0.9521878361701965  val: loss: 856327.4375 acc: 0.644689679145813\n",
      "step: 9885\n",
      "train: loss: 75092.5 acc: 0.9598924517631531  val: loss: 221815.90625 acc: 0.9444200992584229\n",
      "step: 9890\n",
      "train: loss: 778277.625 acc: 0.8815265893936157  val: loss: 408588.5 acc: 0.9422090649604797\n",
      "step: 9895\n",
      "train: loss: 155578.390625 acc: 0.9768245220184326  val: loss: 648326.5625 acc: 0.9301442503929138\n",
      "step: 9900\n",
      "train: loss: 101777.8984375 acc: 0.9896316528320312  val: loss: 543723.5 acc: 0.9405417442321777\n",
      "step: 9905\n",
      "train: loss: 70005.8984375 acc: 0.9912797212600708  val: loss: 281174.15625 acc: 0.9522625207901001\n",
      "step: 9910\n",
      "train: loss: 81906.046875 acc: 0.9881455898284912  val: loss: 373171.1875 acc: 0.9506818652153015\n",
      "step: 9915\n",
      "train: loss: 440189.0 acc: 0.9577367305755615  val: loss: 454920.0625 acc: 0.6710072755813599\n",
      "step: 9920\n",
      "train: loss: 343498.125 acc: 0.983127236366272  val: loss: 236055.125 acc: 0.9450404644012451\n",
      "step: 9925\n",
      "train: loss: 1932920.375 acc: 0.7814874649047852  val: loss: 2767148.5 acc: 0.644229531288147\n",
      "step: 9930\n",
      "train: loss: 222807.59375 acc: 0.9784055948257446  val: loss: 545615.6875 acc: 0.9171240925788879\n",
      "step: 9935\n",
      "train: loss: 734984.4375 acc: 0.9563732147216797  val: loss: 824432.6875 acc: 0.6314015984535217\n",
      "step: 9940\n",
      "train: loss: 2784501.75 acc: 0.9291102886199951  val: loss: 486128.375 acc: 0.9313859343528748\n",
      "step: 9945\n",
      "train: loss: 2211214.75 acc: 0.9326061010360718  val: loss: 328541.5 acc: 0.9365177154541016\n",
      "step: 9950\n",
      "train: loss: 807257.625 acc: 0.9594424366950989  val: loss: 696796.875 acc: 0.8328475952148438\n",
      "step: 9955\n",
      "train: loss: 368867.375 acc: 0.9693969488143921  val: loss: 405565.75 acc: 0.8693203926086426\n",
      "step: 9960\n",
      "train: loss: 2298720.25 acc: 0.8190007209777832  val: loss: 1332192.875 acc: 0.41817373037338257\n",
      "step: 9965\n",
      "train: loss: 307334.4375 acc: 0.9243112206459045  val: loss: 298594.0 acc: 0.9348893165588379\n",
      "step: 9970\n",
      "train: loss: 801914.4375 acc: 0.8813933730125427  val: loss: 720925.1875 acc: 0.8227733373641968\n",
      "step: 9975\n",
      "train: loss: 1741881.5 acc: 0.2340027093887329  val: loss: 611283.25 acc: 0.8434973955154419\n",
      "step: 9980\n",
      "train: loss: 435800.625 acc: 0.8799225687980652  val: loss: 171551.4375 acc: 0.9116341471672058\n",
      "step: 9985\n",
      "train: loss: 1018706.8125 acc: 0.5247822999954224  val: loss: 1430516.0 acc: 0.8208461999893188\n",
      "step: 9990\n",
      "train: loss: 686334.875 acc: 0.8537735939025879  val: loss: 563360.8125 acc: 0.873002290725708\n",
      "step: 9995\n",
      "train: loss: 937330.0625 acc: 0.7105897665023804  val: loss: 359680.5625 acc: 0.8233366012573242\n",
      "step: 10000\n",
      "train: loss: 1429112.0 acc: 0.44615185260772705  val: loss: 1368956.625 acc: 0.7474879026412964\n",
      "step: 10005\n",
      "train: loss: 814074.8125 acc: 0.6105637550354004  val: loss: 978727.375 acc: 0.6767640113830566\n",
      "step: 10010\n",
      "train: loss: 443532.59375 acc: 0.6714329123497009  val: loss: 848436.9375 acc: 0.659917950630188\n",
      "step: 10015\n",
      "train: loss: 131760.703125 acc: 0.8670585751533508  val: loss: 1524599.375 acc: 0.7781286239624023\n",
      "step: 10020\n",
      "train: loss: 90393.8203125 acc: 0.9031379818916321  val: loss: 1090702.0 acc: 0.7903398275375366\n",
      "step: 10025\n",
      "train: loss: 168300.96875 acc: 0.8658770322799683  val: loss: 854833.0625 acc: 0.7686813473701477\n",
      "step: 10030\n",
      "train: loss: 288949.8125 acc: 0.86024010181427  val: loss: 522245.875 acc: 0.6978296041488647\n",
      "step: 10035\n",
      "train: loss: 153717.109375 acc: 0.893174409866333  val: loss: 1615816.875 acc: 0.7753922939300537\n",
      "step: 10040\n",
      "train: loss: 180692.015625 acc: 0.8817718029022217  val: loss: 624310.5625 acc: 0.7541078329086304\n",
      "step: 10045\n",
      "train: loss: 200112.625 acc: 0.8800170421600342  val: loss: 506383.75 acc: 0.8436056971549988\n",
      "step: 10050\n",
      "train: loss: 156263.765625 acc: 0.7979378700256348  val: loss: 1968203.375 acc: 0.785870373249054\n",
      "step: 10055\n",
      "train: loss: 300110.625 acc: 0.8127366304397583  val: loss: 945704.25 acc: 0.7999702095985413\n",
      "step: 10060\n",
      "train: loss: 383852.15625 acc: 0.6442303657531738  val: loss: 954780.3125 acc: 0.7781891822814941\n",
      "step: 10065\n",
      "train: loss: 686378.25 acc: 0.7229900360107422  val: loss: 2850610.0 acc: 0.6560971736907959\n",
      "step: 10070\n",
      "train: loss: 412005.96875 acc: 0.7329810857772827  val: loss: 793947.0 acc: 0.8163464069366455\n",
      "step: 10075\n",
      "train: loss: 856912.5625 acc: 0.7270209789276123  val: loss: 509189.53125 acc: 0.757293164730072\n",
      "step: 10080\n",
      "train: loss: 345965.625 acc: 0.8027023673057556  val: loss: 620812.0 acc: 0.7364374399185181\n",
      "step: 10085\n",
      "train: loss: 791279.75 acc: 0.7456257343292236  val: loss: 1113680.125 acc: 0.683739185333252\n",
      "step: 10090\n",
      "train: loss: 1421909.75 acc: 0.7591502666473389  val: loss: 883898.6875 acc: 0.7940504550933838\n",
      "step: 10095\n",
      "train: loss: 978363.9375 acc: 0.9042692184448242  val: loss: 1112764.5 acc: 0.7289910316467285\n",
      "step: 10100\n",
      "train: loss: 890132.0 acc: 0.8665176630020142  val: loss: 724886.0 acc: 0.8271035552024841\n",
      "step: 10105\n",
      "train: loss: 558150.8125 acc: 0.937558650970459  val: loss: 1532947.5 acc: 0.6140158176422119\n",
      "step: 10110\n",
      "train: loss: 268659.90625 acc: 0.9393106698989868  val: loss: 406230.59375 acc: 0.794044554233551\n",
      "step: 10115\n",
      "train: loss: 377351.4375 acc: 0.9552658200263977  val: loss: 929028.25 acc: 0.8267778754234314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 10120\n",
      "train: loss: 460023.59375 acc: 0.9619994759559631  val: loss: 1227408.625 acc: 0.777712345123291\n",
      "step: 10125\n",
      "train: loss: 399181.34375 acc: 0.9655730724334717  val: loss: 1299177.625 acc: 0.799662709236145\n",
      "step: 10130\n",
      "train: loss: 292582.59375 acc: 0.9505665302276611  val: loss: 706737.375 acc: 0.8192173838615417\n",
      "step: 10135\n",
      "train: loss: 383373.15625 acc: 0.9348779916763306  val: loss: 350833.90625 acc: 0.8332931399345398\n",
      "step: 10140\n",
      "train: loss: 99904.125 acc: 0.9696148037910461  val: loss: 404099.46875 acc: 0.9057300686836243\n",
      "step: 10145\n",
      "train: loss: 65096.22265625 acc: 0.982024073600769  val: loss: 241399.328125 acc: 0.8838774561882019\n",
      "step: 10150\n",
      "train: loss: 5970.20263671875 acc: 0.99649578332901  val: loss: 1491477.625 acc: 0.8094189167022705\n",
      "step: 10155\n",
      "train: loss: 105412.5078125 acc: 0.9566954374313354  val: loss: 531920.5625 acc: 0.46912169456481934\n",
      "step: 10160\n",
      "train: loss: 12341.52734375 acc: 0.9656351208686829  val: loss: 943404.9375 acc: 0.885219156742096\n",
      "step: 10165\n",
      "train: loss: 82027.2109375 acc: 0.935382068157196  val: loss: 769089.0625 acc: 0.9042108654975891\n",
      "step: 10170\n",
      "train: loss: 14507.1953125 acc: 0.9340077638626099  val: loss: 156162.578125 acc: 0.9387498497962952\n",
      "step: 10175\n",
      "train: loss: 15772.7890625 acc: 0.9769248366355896  val: loss: 335196.0 acc: 0.8919807076454163\n",
      "step: 10180\n",
      "train: loss: 29741.013671875 acc: 0.9485904574394226  val: loss: 635329.875 acc: 0.6713970899581909\n",
      "step: 10185\n",
      "train: loss: 165901.21875 acc: 0.9264647960662842  val: loss: 761277.4375 acc: 0.8951255083084106\n",
      "step: 10190\n",
      "train: loss: 52047.8671875 acc: 0.9543738961219788  val: loss: 112288.8125 acc: 0.9726642966270447\n",
      "step: 10195\n",
      "train: loss: 66610.09375 acc: 0.9542935490608215  val: loss: 234736.921875 acc: 0.942389726638794\n",
      "step: 10200\n",
      "train: loss: 144156.03125 acc: 0.9145573377609253  val: loss: 272745.84375 acc: 0.9524232745170593\n",
      "step: 10205\n",
      "train: loss: 70287.3671875 acc: 0.9540998339653015  val: loss: 144438.09375 acc: 0.9434561729431152\n",
      "step: 10210\n",
      "train: loss: 37484.421875 acc: 0.9601191282272339  val: loss: 447283.0625 acc: 0.9137349128723145\n",
      "step: 10215\n",
      "train: loss: 42924.40625 acc: 0.96462082862854  val: loss: 728699.75 acc: 0.9151456952095032\n",
      "step: 10220\n",
      "train: loss: 78362.7734375 acc: 0.9722212553024292  val: loss: 1604680.375 acc: 0.8434317111968994\n",
      "step: 10225\n",
      "train: loss: 61715.81640625 acc: 0.9854525327682495  val: loss: 684598.625 acc: 0.902786910533905\n",
      "step: 10230\n",
      "train: loss: 58432.25 acc: 0.9867085814476013  val: loss: 1675130.125 acc: 0.8219234943389893\n",
      "step: 10235\n",
      "train: loss: 35464.83984375 acc: 0.9887458086013794  val: loss: 364839.34375 acc: 0.9686184525489807\n",
      "step: 10240\n",
      "train: loss: 149892.4375 acc: 0.9713014364242554  val: loss: 1672120.375 acc: 0.5356181859970093\n",
      "step: 10245\n",
      "train: loss: 208098.265625 acc: 0.9557721018791199  val: loss: 1053460.5 acc: 0.6657381653785706\n",
      "step: 10250\n",
      "train: loss: 122589.5546875 acc: 0.9557287693023682  val: loss: 340543.75 acc: 0.9324280023574829\n",
      "step: 10255\n",
      "train: loss: 95620.0234375 acc: 0.930018961429596  val: loss: 1117424.0 acc: 0.8138527870178223\n",
      "step: 10260\n",
      "train: loss: 538532.3125 acc: 0.9132877588272095  val: loss: 607531.9375 acc: 0.6916670799255371\n",
      "step: 10265\n",
      "train: loss: 127386.1328125 acc: 0.9876582622528076  val: loss: 1448673.0 acc: 0.7313224673271179\n",
      "step: 10270\n",
      "train: loss: 182681.65625 acc: 0.979075014591217  val: loss: 621823.875 acc: 0.903128445148468\n",
      "step: 10275\n",
      "train: loss: 50722.109375 acc: 0.9898062348365784  val: loss: 1670821.875 acc: 0.7292673587799072\n",
      "step: 10280\n",
      "train: loss: 258877.421875 acc: 0.9511734843254089  val: loss: 1211104.25 acc: 0.8950632810592651\n",
      "step: 10285\n",
      "train: loss: 375113.84375 acc: 0.9639170169830322  val: loss: 2756607.5 acc: 0.6137725114822388\n",
      "step: 10290\n",
      "train: loss: 2126462.0 acc: 0.7871619462966919  val: loss: 1223138.25 acc: 0.8686381578445435\n",
      "step: 10295\n",
      "train: loss: 215987.53125 acc: 0.9654953479766846  val: loss: 2260441.5 acc: 0.6897754669189453\n",
      "step: 10300\n",
      "train: loss: 1260353.125 acc: 0.9322889447212219  val: loss: 1123493.5 acc: 0.7561448812484741\n",
      "step: 10305\n",
      "train: loss: 429589.125 acc: 0.9870424270629883  val: loss: 501882.125 acc: 0.6899641752243042\n",
      "step: 10310\n",
      "train: loss: 1841909.875 acc: 0.9381467700004578  val: loss: 1378623.375 acc: 0.6359477043151855\n",
      "step: 10315\n",
      "train: loss: 797250.875 acc: 0.9488129615783691  val: loss: 501297.84375 acc: 0.9253434538841248\n",
      "step: 10320\n",
      "train: loss: 893887.75 acc: 0.9423680901527405  val: loss: 519506.21875 acc: 0.8616806864738464\n",
      "step: 10325\n",
      "train: loss: 275946.625 acc: 0.9695339798927307  val: loss: 2898940.25 acc: 0.5229586958885193\n",
      "step: 10330\n",
      "train: loss: 564318.625 acc: 0.94814532995224  val: loss: 1048409.25 acc: 0.6560898423194885\n",
      "step: 10335\n",
      "train: loss: 395333.21875 acc: 0.9624429941177368  val: loss: 675853.9375 acc: 0.8956112861633301\n",
      "step: 10340\n",
      "train: loss: 1512342.25 acc: 0.4812605381011963  val: loss: 1138977.625 acc: 0.2039610743522644\n",
      "step: 10345\n",
      "train: loss: 1771846.0 acc: 0.3712220788002014  val: loss: 1122333.0 acc: 0.8635108470916748\n",
      "step: 10350\n",
      "train: loss: 549655.625 acc: 0.7986337542533875  val: loss: 1221088.25 acc: 0.6848844289779663\n",
      "step: 10355\n",
      "train: loss: 469803.625 acc: 0.827025294303894  val: loss: 1946709.875 acc: 0.6495523452758789\n",
      "step: 10360\n",
      "train: loss: 1471579.125 acc: 0.08273833990097046  val: loss: 934110.75 acc: 0.8274743556976318\n",
      "step: 10365\n",
      "train: loss: 1928004.375 acc: -0.08316683769226074  val: loss: 579933.3125 acc: 0.8199081420898438\n",
      "step: 10370\n",
      "train: loss: 948114.4375 acc: 0.5401246547698975  val: loss: 1598069.75 acc: 0.7529014945030212\n",
      "step: 10375\n",
      "train: loss: 543973.1875 acc: 0.7194550037384033  val: loss: 1951197.0 acc: 0.8156057000160217\n",
      "step: 10380\n",
      "train: loss: 358045.03125 acc: 0.7699628472328186  val: loss: 1620594.25 acc: 0.7202584147453308\n",
      "step: 10385\n",
      "train: loss: 322530.21875 acc: 0.7246204018592834  val: loss: 1854975.125 acc: 0.7244852781295776\n",
      "step: 10390\n",
      "train: loss: 92148.484375 acc: 0.9193153381347656  val: loss: 2519409.25 acc: 0.7773427367210388\n",
      "step: 10395\n",
      "train: loss: 333358.71875 acc: 0.8570945262908936  val: loss: 933346.875 acc: 0.8703008890151978\n",
      "step: 10400\n",
      "train: loss: 333231.0625 acc: 0.756343424320221  val: loss: 691605.3125 acc: 0.7392538785934448\n",
      "step: 10405\n",
      "train: loss: 180222.46875 acc: 0.8638322353363037  val: loss: 831796.9375 acc: 0.8755543231964111\n",
      "step: 10410\n",
      "train: loss: 130528.21875 acc: 0.8972785472869873  val: loss: 1283485.875 acc: 0.8278924822807312\n",
      "step: 10415\n",
      "train: loss: 125553.90625 acc: 0.9045167565345764  val: loss: 626238.25 acc: 0.8559550046920776\n",
      "step: 10420\n",
      "train: loss: 481658.1875 acc: 0.6605892181396484  val: loss: 472246.5625 acc: 0.7863727807998657\n",
      "step: 10425\n",
      "train: loss: 708484.4375 acc: 0.3516639471054077  val: loss: 776473.6875 acc: 0.7874352335929871\n",
      "step: 10430\n",
      "train: loss: 621573.75 acc: 0.6983259320259094  val: loss: 938942.6875 acc: 0.794288158416748\n",
      "step: 10435\n",
      "train: loss: 1135830.125 acc: 0.27935993671417236  val: loss: 699929.1875 acc: 0.704871416091919\n",
      "step: 10440\n",
      "train: loss: 338788.8125 acc: 0.6333810687065125  val: loss: 1317597.375 acc: 0.7653661966323853\n",
      "step: 10445\n",
      "train: loss: 377514.875 acc: 0.4744824767112732  val: loss: 1859500.5 acc: 0.7516506910324097\n",
      "step: 10450\n",
      "train: loss: 1746939.25 acc: 0.7299106121063232  val: loss: 1728956.5 acc: 0.7611454725265503\n",
      "step: 10455\n",
      "train: loss: 1267232.625 acc: 0.7930944561958313  val: loss: 924488.0 acc: 0.7858768701553345\n",
      "step: 10460\n",
      "train: loss: 563286.5625 acc: 0.9280653595924377  val: loss: 241778.640625 acc: 0.7572215795516968\n",
      "step: 10465\n",
      "train: loss: 490831.03125 acc: 0.9330862760543823  val: loss: 420931.9375 acc: 0.8790420293807983\n",
      "step: 10470\n",
      "train: loss: 621806.125 acc: 0.8609777688980103  val: loss: 2129762.25 acc: 0.04836714267730713\n",
      "step: 10475\n",
      "train: loss: 300427.5625 acc: 0.9565926790237427  val: loss: 1761584.75 acc: 0.7334291934967041\n",
      "step: 10480\n",
      "train: loss: 802924.4375 acc: 0.8941282629966736  val: loss: 420947.84375 acc: 0.8965516090393066\n",
      "step: 10485\n",
      "train: loss: 242191.09375 acc: 0.9840408563613892  val: loss: 1387794.625 acc: 0.7485445141792297\n",
      "step: 10490\n",
      "train: loss: 383238.1875 acc: 0.9771615862846375  val: loss: 1677537.75 acc: -0.7194589376449585\n",
      "step: 10495\n",
      "train: loss: 298679.78125 acc: 0.9654338359832764  val: loss: 79044.1953125 acc: 0.9750397205352783\n",
      "step: 10500\n",
      "train: loss: 183311.140625 acc: 0.9412803053855896  val: loss: 678639.9375 acc: 0.7895117402076721\n",
      "step: 10505\n",
      "train: loss: 202474.734375 acc: 0.9550116658210754  val: loss: 497500.15625 acc: 0.8720497488975525\n",
      "step: 10510\n",
      "train: loss: 74138.53125 acc: 0.9167112112045288  val: loss: 607560.5625 acc: 0.9051703214645386\n",
      "step: 10515\n",
      "train: loss: 16302.388671875 acc: 0.9883204102516174  val: loss: 214635.453125 acc: 0.9125106930732727\n",
      "step: 10520\n",
      "train: loss: 11284.5498046875 acc: 0.9819318056106567  val: loss: 461450.34375 acc: 0.9592502117156982\n",
      "step: 10525\n",
      "train: loss: 23294.7578125 acc: 0.9671310782432556  val: loss: 107179.15625 acc: 0.9707993268966675\n",
      "step: 10530\n",
      "train: loss: 13963.923828125 acc: 0.9815322160720825  val: loss: 296178.71875 acc: 0.9144498705863953\n",
      "step: 10535\n",
      "train: loss: 6350.982421875 acc: 0.9747815132141113  val: loss: 992674.6875 acc: 0.22428089380264282\n",
      "step: 10540\n",
      "train: loss: 9661.08984375 acc: 0.9833593964576721  val: loss: 794366.1875 acc: 0.8631471395492554\n",
      "step: 10545\n",
      "train: loss: 14986.4931640625 acc: 0.9736290574073792  val: loss: 194847.671875 acc: 0.9651058912277222\n",
      "step: 10550\n",
      "train: loss: 14799.5966796875 acc: 0.9763159155845642  val: loss: 496470.34375 acc: 0.9465229511260986\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 10555\n",
      "train: loss: 62766.16796875 acc: 0.9666175246238708  val: loss: 891120.1875 acc: 0.8025181293487549\n",
      "step: 10560\n",
      "train: loss: 20019.462890625 acc: 0.9734757542610168  val: loss: 719632.0625 acc: 0.9028422832489014\n",
      "step: 10565\n",
      "train: loss: 82290.6953125 acc: 0.9141073822975159  val: loss: 2173986.0 acc: 0.74433434009552\n",
      "step: 10570\n",
      "train: loss: 35109.828125 acc: 0.9769891500473022  val: loss: 1011812.0 acc: 0.8448014855384827\n",
      "step: 10575\n",
      "train: loss: 16486.94140625 acc: 0.9836503863334656  val: loss: 2118974.25 acc: 0.600608229637146\n",
      "step: 10580\n",
      "train: loss: 25560.32421875 acc: 0.9628422260284424  val: loss: 1336147.0 acc: 0.7186822891235352\n",
      "step: 10585\n",
      "train: loss: 26103.353515625 acc: 0.9909284114837646  val: loss: 1648622.125 acc: 0.17224901914596558\n",
      "step: 10590\n",
      "train: loss: 28959.99609375 acc: 0.9936370849609375  val: loss: 2208303.25 acc: 0.7081933617591858\n",
      "step: 10595\n",
      "train: loss: 43173.40625 acc: 0.9829007387161255  val: loss: 3161887.5 acc: 0.6322599649429321\n",
      "step: 10600\n",
      "train: loss: 53375.86328125 acc: 0.9778046607971191  val: loss: 1016841.0 acc: 0.7717385292053223\n",
      "step: 10605\n",
      "train: loss: 70008.78125 acc: 0.9656934142112732  val: loss: 1760470.0 acc: 0.5561631321907043\n",
      "step: 10610\n",
      "train: loss: 64610.9296875 acc: 0.9707707166671753  val: loss: 2805022.75 acc: 0.36028462648391724\n",
      "step: 10615\n",
      "train: loss: 96163.375 acc: 0.9708308577537537  val: loss: 313262.375 acc: 0.9467054009437561\n",
      "step: 10620\n",
      "train: loss: 85159.828125 acc: 0.9504619836807251  val: loss: 1152083.375 acc: 0.7970966100692749\n",
      "step: 10625\n",
      "train: loss: 138760.421875 acc: 0.9782275557518005  val: loss: 1935121.125 acc: 0.6363214254379272\n",
      "step: 10630\n",
      "train: loss: 135497.140625 acc: 0.9866311550140381  val: loss: 1010927.4375 acc: 0.6344958543777466\n",
      "step: 10635\n",
      "train: loss: 50735.03125 acc: 0.9932965040206909  val: loss: 852747.625 acc: 0.8776267170906067\n",
      "step: 10640\n",
      "train: loss: 263928.625 acc: 0.9644602537155151  val: loss: 658116.3125 acc: 0.8629302978515625\n",
      "step: 10645\n",
      "train: loss: 438899.875 acc: 0.970918595790863  val: loss: 863232.25 acc: 0.7314385175704956\n",
      "step: 10650\n",
      "train: loss: 197388.5 acc: 0.9860979318618774  val: loss: 995325.4375 acc: 0.7791911363601685\n",
      "step: 10655\n",
      "train: loss: 2573309.0 acc: 0.7993154525756836  val: loss: 1062726.125 acc: 0.7897345423698425\n",
      "step: 10660\n",
      "train: loss: 723564.625 acc: 0.859591007232666  val: loss: 950525.0625 acc: 0.7618858218193054\n",
      "step: 10665\n",
      "train: loss: 353391.34375 acc: 0.9767922759056091  val: loss: 304807.53125 acc: 0.9359596371650696\n",
      "step: 10670\n",
      "train: loss: 572529.1875 acc: 0.9815725088119507  val: loss: 2131460.0 acc: -1.0656094551086426\n",
      "step: 10675\n",
      "train: loss: 1525058.75 acc: 0.9598910808563232  val: loss: 1052989.75 acc: 0.7669205665588379\n",
      "step: 10680\n",
      "train: loss: 1048931.625 acc: 0.9509173631668091  val: loss: 804751.625 acc: 0.882533073425293\n",
      "step: 10685\n",
      "train: loss: 541119.5625 acc: 0.9515700340270996  val: loss: 480804.59375 acc: 0.9298518896102905\n",
      "step: 10690\n",
      "train: loss: 346496.59375 acc: 0.9644870162010193  val: loss: 610885.625 acc: 0.9423014521598816\n",
      "step: 10695\n",
      "train: loss: 244897.25 acc: 0.8881984353065491  val: loss: 816683.125 acc: 0.9204018115997314\n",
      "step: 10700\n",
      "train: loss: 284477.71875 acc: 0.9670975804328918  val: loss: 469107.21875 acc: 0.9622883796691895\n",
      "step: 10705\n",
      "train: loss: 2505277.75 acc: -0.2218918800354004  val: loss: 720095.8125 acc: 0.904518723487854\n",
      "step: 10710\n",
      "train: loss: 568215.1875 acc: 0.7337907552719116  val: loss: 489261.46875 acc: 0.8172417879104614\n",
      "step: 10715\n",
      "train: loss: 1818030.125 acc: 0.7597073912620544  val: loss: 1065968.625 acc: 0.8823444843292236\n",
      "step: 10720\n",
      "train: loss: 618972.875 acc: 0.8590933680534363  val: loss: 657438.5625 acc: 0.9043915867805481\n",
      "step: 10725\n",
      "train: loss: 533529.8125 acc: 0.8176879286766052  val: loss: 423570.1875 acc: 0.8790905475616455\n",
      "step: 10730\n",
      "train: loss: 1791880.625 acc: -0.7720848321914673  val: loss: 784130.75 acc: 0.7412604093551636\n",
      "step: 10735\n",
      "train: loss: 1142361.125 acc: 0.6190325021743774  val: loss: 3252336.25 acc: 0.7046874761581421\n",
      "step: 10740\n",
      "train: loss: 330102.21875 acc: 0.8373887538909912  val: loss: 1920136.125 acc: 0.7065138220787048\n",
      "step: 10745\n",
      "train: loss: 761180.4375 acc: 0.7406579256057739  val: loss: 2408400.25 acc: 0.710932731628418\n",
      "step: 10750\n",
      "train: loss: 322010.21875 acc: 0.7406143546104431  val: loss: 1062934.875 acc: 0.7501934766769409\n",
      "step: 10755\n",
      "train: loss: 44780.64453125 acc: 0.9667007923126221  val: loss: 1329736.5 acc: 0.7047765254974365\n",
      "step: 10760\n",
      "train: loss: 378395.625 acc: 0.814266562461853  val: loss: 424818.84375 acc: 0.8145562410354614\n",
      "step: 10765\n",
      "train: loss: 512099.71875 acc: 0.7988071441650391  val: loss: 2680275.75 acc: 0.6766353249549866\n",
      "step: 10770\n",
      "train: loss: 77371.6328125 acc: 0.9394698143005371  val: loss: 408497.84375 acc: 0.7212154865264893\n",
      "step: 10775\n",
      "train: loss: 134116.921875 acc: 0.8966478109359741  val: loss: 539033.0625 acc: 0.8454078435897827\n",
      "step: 10780\n",
      "train: loss: 115523.3671875 acc: 0.9177629351615906  val: loss: 714599.0625 acc: 0.7416852712631226\n",
      "step: 10785\n",
      "train: loss: 418428.125 acc: 0.7996242046356201  val: loss: 1212366.375 acc: 0.7312807440757751\n",
      "step: 10790\n",
      "train: loss: 343904.90625 acc: 0.8098492622375488  val: loss: 2517864.5 acc: 0.7047504782676697\n",
      "step: 10795\n",
      "train: loss: 189587.640625 acc: 0.8298575282096863  val: loss: 1990148.375 acc: 0.6920191049575806\n",
      "step: 10800\n",
      "train: loss: 578900.0 acc: 0.6829298734664917  val: loss: 1303078.75 acc: 0.6681442856788635\n",
      "step: 10805\n",
      "train: loss: 242211.09375 acc: 0.8527730107307434  val: loss: 1467204.875 acc: 0.6304630041122437\n",
      "step: 10810\n",
      "train: loss: 392974.78125 acc: 0.6432857513427734  val: loss: 2195428.75 acc: 0.6808893084526062\n",
      "step: 10815\n",
      "train: loss: 2116746.5 acc: 0.6680711507797241  val: loss: 532847.4375 acc: 0.7708072066307068\n",
      "step: 10820\n",
      "train: loss: 706445.5625 acc: 0.8375160098075867  val: loss: 612597.25 acc: 0.8287529349327087\n",
      "step: 10825\n",
      "train: loss: 986649.3125 acc: 0.8651818037033081  val: loss: 475067.25 acc: 0.7657827138900757\n",
      "step: 10830\n",
      "train: loss: 496389.65625 acc: 0.9410672187805176  val: loss: 1176329.125 acc: 0.7462468147277832\n",
      "step: 10835\n",
      "train: loss: 331171.875 acc: 0.9577711820602417  val: loss: 356277.625 acc: 0.8829604983329773\n",
      "step: 10840\n",
      "train: loss: 618077.625 acc: 0.9327818155288696  val: loss: 563288.25 acc: 0.9206295609474182\n",
      "step: 10845\n",
      "train: loss: 225100.3125 acc: 0.9698836803436279  val: loss: 683471.875 acc: 0.8292398452758789\n",
      "step: 10850\n",
      "train: loss: 291588.15625 acc: 0.9794620871543884  val: loss: 919322.75 acc: 0.9159058928489685\n",
      "step: 10855\n",
      "train: loss: 360195.125 acc: 0.9757810831069946  val: loss: 467774.78125 acc: 0.9505921602249146\n",
      "step: 10860\n",
      "train: loss: 257570.578125 acc: 0.9650024175643921  val: loss: 715722.625 acc: 0.8051955103874207\n",
      "step: 10865\n",
      "train: loss: 116296.15625 acc: 0.9831353425979614  val: loss: 2109917.0 acc: 0.557149350643158\n",
      "step: 10870\n",
      "train: loss: 104577.9140625 acc: 0.9767531752586365  val: loss: 575397.875 acc: 0.9158856868743896\n",
      "step: 10875\n",
      "train: loss: 114667.4140625 acc: 0.9732747673988342  val: loss: 807049.5625 acc: 0.8741976022720337\n",
      "step: 10880\n",
      "train: loss: 168651.453125 acc: 0.9634863138198853  val: loss: 483509.875 acc: 0.8687601685523987\n",
      "step: 10885\n",
      "train: loss: 22992.0625 acc: 0.9578059315681458  val: loss: 708065.3125 acc: 0.8860618472099304\n",
      "step: 10890\n",
      "train: loss: 13349.2197265625 acc: 0.982947826385498  val: loss: 1678967.5 acc: 0.7383222579956055\n",
      "step: 10895\n",
      "train: loss: 39121.7421875 acc: 0.8811505436897278  val: loss: 965039.5625 acc: 0.8245784044265747\n",
      "step: 10900\n",
      "train: loss: 6276.16650390625 acc: 0.9767575263977051  val: loss: 1898097.375 acc: 0.7361327409744263\n",
      "step: 10905\n",
      "train: loss: 24517.205078125 acc: 0.9783599972724915  val: loss: 1627463.875 acc: 0.6185060739517212\n",
      "step: 10910\n",
      "train: loss: 14093.111328125 acc: 0.9663019180297852  val: loss: 517945.03125 acc: 0.9119060039520264\n",
      "step: 10915\n",
      "train: loss: 30622.0859375 acc: 0.9569324254989624  val: loss: 639829.3125 acc: 0.8809601068496704\n",
      "step: 10920\n",
      "train: loss: 27443.662109375 acc: 0.9774194359779358  val: loss: 2791606.25 acc: 0.6439531445503235\n",
      "step: 10925\n",
      "train: loss: 37273.38671875 acc: 0.978144645690918  val: loss: 1183477.125 acc: 0.49463850259780884\n",
      "step: 10930\n",
      "train: loss: 98374.4296875 acc: 0.9522488713264465  val: loss: 1377949.375 acc: 0.1647350788116455\n",
      "step: 10935\n",
      "train: loss: 45277.40234375 acc: 0.9726495742797852  val: loss: 1490236.25 acc: 0.8937361240386963\n",
      "step: 10940\n",
      "train: loss: 38176.48828125 acc: 0.9799970984458923  val: loss: 3556609.25 acc: 0.03996020555496216\n",
      "step: 10945\n",
      "train: loss: 8813.169921875 acc: 0.9853257536888123  val: loss: 1341314.375 acc: 0.736146092414856\n",
      "step: 10950\n",
      "train: loss: 16033.9150390625 acc: 0.9934126138687134  val: loss: 3341548.75 acc: -0.06676721572875977\n",
      "step: 10955\n",
      "train: loss: 45461.4921875 acc: 0.989582359790802  val: loss: 1494354.875 acc: -1.4713804721832275\n",
      "step: 10960\n",
      "train: loss: 31069.2109375 acc: 0.9924026131629944  val: loss: 1826475.75 acc: 0.16347098350524902\n",
      "step: 10965\n",
      "train: loss: 44673.359375 acc: 0.9784888625144958  val: loss: 1641022.625 acc: 0.689063310623169\n",
      "step: 10970\n",
      "train: loss: 53867.296875 acc: 0.9748904705047607  val: loss: 2733763.0 acc: 0.4751679301261902\n",
      "step: 10975\n",
      "train: loss: 206396.875 acc: 0.9316122531890869  val: loss: 986593.1875 acc: 0.8354447484016418\n",
      "step: 10980\n",
      "train: loss: 131150.859375 acc: 0.9295820593833923  val: loss: 476666.65625 acc: 0.8572628498077393\n",
      "step: 10985\n",
      "train: loss: 87269.1484375 acc: 0.9298223257064819  val: loss: 3204430.25 acc: 0.5347598791122437\n",
      "step: 10990\n",
      "train: loss: 211245.515625 acc: 0.9573882818222046  val: loss: 317061.96875 acc: 0.9098499417304993\n",
      "step: 10995\n",
      "train: loss: 574741.6875 acc: 0.9382620453834534  val: loss: 1105486.75 acc: 0.8640190958976746\n",
      "step: 11000\n",
      "train: loss: 53592.078125 acc: 0.9946991801261902  val: loss: 2228200.75 acc: 0.5326635241508484\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 11005\n",
      "train: loss: 74903.0703125 acc: 0.9915449023246765  val: loss: 1131040.875 acc: 0.9082461595535278\n",
      "step: 11010\n",
      "train: loss: 181189.203125 acc: 0.9863505959510803  val: loss: 529194.5 acc: 0.8877038359642029\n",
      "step: 11015\n",
      "train: loss: 204803.15625 acc: 0.9724278450012207  val: loss: 1394558.375 acc: 0.7102903127670288\n",
      "step: 11020\n",
      "train: loss: 659739.1875 acc: 0.9500333666801453  val: loss: 693552.375 acc: 0.9031892418861389\n",
      "step: 11025\n",
      "train: loss: 181240.96875 acc: 0.976858913898468  val: loss: 1905808.625 acc: 0.3385358452796936\n",
      "step: 11030\n",
      "train: loss: 192695.78125 acc: 0.9668009877204895  val: loss: 911227.1875 acc: 0.5837392807006836\n",
      "step: 11035\n",
      "train: loss: 837081.0625 acc: 0.9555779695510864  val: loss: 616287.875 acc: 0.7004661560058594\n",
      "step: 11040\n",
      "train: loss: 1860893.875 acc: 0.9460216164588928  val: loss: 425863.5 acc: 0.8175143003463745\n",
      "step: 11045\n",
      "train: loss: 2162979.25 acc: 0.9382339715957642  val: loss: 2455761.25 acc: 0.1410422921180725\n",
      "step: 11050\n",
      "train: loss: 1408638.125 acc: 0.9165621399879456  val: loss: 553530.375 acc: 0.8819724321365356\n",
      "step: 11055\n",
      "train: loss: 326434.5 acc: 0.9752351641654968  val: loss: 2378306.0 acc: 0.22416311502456665\n",
      "step: 11060\n",
      "train: loss: 392449.25 acc: 0.955892026424408  val: loss: 810649.625 acc: 0.781604528427124\n",
      "step: 11065\n",
      "train: loss: 123815.4609375 acc: 0.9871401786804199  val: loss: 576836.1875 acc: 0.8639483451843262\n",
      "step: 11070\n",
      "train: loss: 2114739.25 acc: 0.06873303651809692  val: loss: 119189.3515625 acc: 0.9501926898956299\n",
      "step: 11075\n",
      "train: loss: 618089.4375 acc: 0.6893209218978882  val: loss: 368360.0625 acc: 0.8334078192710876\n",
      "step: 11080\n",
      "train: loss: 561263.5625 acc: 0.8022853136062622  val: loss: 1113643.375 acc: 0.8698310852050781\n",
      "step: 11085\n",
      "train: loss: 445088.25 acc: 0.8120272755622864  val: loss: 657566.125 acc: 0.8773707747459412\n",
      "step: 11090\n",
      "train: loss: 457691.625 acc: 0.8305756449699402  val: loss: 1021882.125 acc: 0.8200687170028687\n",
      "step: 11095\n",
      "train: loss: 814303.9375 acc: 0.634089469909668  val: loss: 312586.15625 acc: 0.7843765020370483\n",
      "step: 11100\n",
      "train: loss: 610751.375 acc: 0.6768901348114014  val: loss: 1235347.125 acc: 0.728328287601471\n",
      "step: 11105\n",
      "train: loss: 469603.65625 acc: 0.7313201427459717  val: loss: 1453257.625 acc: 0.6564218401908875\n",
      "step: 11110\n",
      "train: loss: 140264.015625 acc: 0.8849250078201294  val: loss: 548406.625 acc: 0.7318482995033264\n",
      "step: 11115\n",
      "train: loss: 87863.953125 acc: 0.9312127828598022  val: loss: 1735929.875 acc: 0.7254951000213623\n",
      "step: 11120\n",
      "train: loss: 145076.890625 acc: 0.8903703093528748  val: loss: 1462563.125 acc: 0.6863505840301514\n",
      "step: 11125\n",
      "train: loss: 151455.25 acc: 0.9019144177436829  val: loss: 2525042.75 acc: 0.6874924898147583\n",
      "step: 11130\n",
      "train: loss: 256497.765625 acc: 0.8242599964141846  val: loss: 1221105.5 acc: 0.6969864368438721\n",
      "step: 11135\n",
      "train: loss: 278484.65625 acc: 0.8253000974655151  val: loss: 680145.75 acc: 0.7166389226913452\n",
      "step: 11140\n",
      "train: loss: 401457.5625 acc: 0.7951955795288086  val: loss: 1526852.25 acc: 0.7520914673805237\n",
      "step: 11145\n",
      "train: loss: 25127.59375 acc: 0.9655888080596924  val: loss: 3457972.5 acc: 0.6862307190895081\n",
      "step: 11150\n",
      "train: loss: 165187.234375 acc: 0.8602399230003357  val: loss: 5003862.5 acc: 0.6758638620376587\n",
      "step: 11155\n",
      "train: loss: 343205.15625 acc: 0.8366473913192749  val: loss: 3754650.0 acc: 0.6382465362548828\n",
      "step: 11160\n",
      "train: loss: 701204.0 acc: 0.721110463142395  val: loss: 1310787.75 acc: 0.6665219664573669\n",
      "step: 11165\n",
      "train: loss: 769294.9375 acc: 0.6929992437362671  val: loss: 1068832.75 acc: 0.7094157934188843\n",
      "step: 11170\n",
      "train: loss: 192796.484375 acc: 0.8193198442459106  val: loss: 4044937.5 acc: 0.5967472791671753\n",
      "step: 11175\n",
      "train: loss: 582398.5625 acc: 0.7575642466545105  val: loss: 5145663.0 acc: 0.6145393252372742\n",
      "step: 11180\n",
      "train: loss: 906996.3125 acc: 0.7400330305099487  val: loss: 1336396.875 acc: 0.8000076413154602\n",
      "step: 11185\n",
      "train: loss: 788206.25 acc: 0.41463565826416016  val: loss: 746079.375 acc: 0.8708973526954651\n",
      "step: 11190\n",
      "train: loss: 1026436.5 acc: 0.8543223738670349  val: loss: 248708.40625 acc: 0.9061044454574585\n",
      "step: 11195\n",
      "train: loss: 606539.875 acc: 0.9502537846565247  val: loss: 690273.0 acc: 0.8709650039672852\n",
      "step: 11200\n",
      "train: loss: 207454.5625 acc: 0.9677542448043823  val: loss: 1600767.25 acc: 0.5147140622138977\n",
      "step: 11205\n",
      "train: loss: 404160.6875 acc: 0.9099617600440979  val: loss: 559290.0 acc: 0.8730429410934448\n",
      "step: 11210\n",
      "train: loss: 188319.75 acc: 0.9759296178817749  val: loss: 1128698.5 acc: 0.865935742855072\n",
      "step: 11215\n",
      "train: loss: 213550.671875 acc: 0.975503146648407  val: loss: 1720455.375 acc: 0.897263765335083\n",
      "step: 11220\n",
      "train: loss: 607312.25 acc: 0.9563923478126526  val: loss: 1178416.625 acc: 0.5318120718002319\n",
      "step: 11225\n",
      "train: loss: 118214.7890625 acc: 0.9845192432403564  val: loss: 2000481.375 acc: 0.0380629301071167\n",
      "step: 11230\n",
      "train: loss: 105575.3203125 acc: 0.9789448380470276  val: loss: 969011.125 acc: 0.9264118075370789\n",
      "step: 11235\n",
      "train: loss: 109551.28125 acc: 0.9786165952682495  val: loss: 849688.25 acc: 0.7100079655647278\n",
      "step: 11240\n",
      "train: loss: 46030.51953125 acc: 0.9794129729270935  val: loss: 717422.5625 acc: 0.9090569615364075\n",
      "step: 11245\n",
      "train: loss: 41537.4375 acc: 0.9677634239196777  val: loss: 721174.625 acc: 0.9103067517280579\n",
      "step: 11250\n",
      "train: loss: 29522.47265625 acc: 0.9838874340057373  val: loss: 1280458.125 acc: 0.8284100890159607\n",
      "step: 11255\n",
      "train: loss: 11453.548828125 acc: 0.9823845624923706  val: loss: 2153317.0 acc: 0.7466158866882324\n",
      "step: 11260\n",
      "train: loss: 28085.078125 acc: 0.974209189414978  val: loss: 789694.75 acc: 0.8010327219963074\n",
      "step: 11265\n",
      "train: loss: 31531.259765625 acc: 0.953709602355957  val: loss: 926916.0 acc: 0.7787988185882568\n",
      "step: 11270\n",
      "train: loss: 8749.3046875 acc: 0.9841843247413635  val: loss: 2028741.125 acc: 0.03493708372116089\n",
      "step: 11275\n",
      "train: loss: 3323.64990234375 acc: 0.9830397963523865  val: loss: 2724480.0 acc: -0.41655170917510986\n",
      "step: 11280\n",
      "train: loss: 17495.830078125 acc: 0.9805730581283569  val: loss: 2058854.125 acc: 0.5354156494140625\n",
      "step: 11285\n",
      "train: loss: 34336.1953125 acc: 0.9064072370529175  val: loss: 1970870.125 acc: 0.5557416081428528\n",
      "step: 11290\n",
      "train: loss: 55905.6640625 acc: 0.9531570076942444  val: loss: 2211800.5 acc: 0.4294103980064392\n",
      "step: 11295\n",
      "train: loss: 4947.78857421875 acc: 0.9953978061676025  val: loss: 905953.25 acc: 0.8508475422859192\n",
      "step: 11300\n",
      "train: loss: 6382.24609375 acc: 0.9958696961402893  val: loss: 1565335.0 acc: 0.4771086573600769\n",
      "step: 11305\n",
      "train: loss: 23980.310546875 acc: 0.9882769584655762  val: loss: 4244108.0 acc: 0.3742144703865051\n",
      "step: 11310\n",
      "train: loss: 11739.1220703125 acc: 0.9597947001457214  val: loss: 1094790.375 acc: 0.7925964593887329\n",
      "step: 11315\n",
      "train: loss: 14959.701171875 acc: 0.9918818473815918  val: loss: 2784613.5 acc: 0.42670512199401855\n",
      "step: 11320\n",
      "train: loss: 39095.92578125 acc: 0.9859350323677063  val: loss: 2357180.5 acc: 0.6502020359039307\n",
      "step: 11325\n",
      "train: loss: 24824.072265625 acc: 0.9951167702674866  val: loss: 694996.625 acc: 0.9234321117401123\n",
      "step: 11330\n",
      "train: loss: 32137.51171875 acc: 0.9928539395332336  val: loss: 1606005.25 acc: 0.46584200859069824\n",
      "step: 11335\n",
      "train: loss: 25565.8671875 acc: 0.9817869067192078  val: loss: 2780211.5 acc: 0.09630554914474487\n",
      "step: 11340\n",
      "train: loss: 94778.265625 acc: 0.967417299747467  val: loss: 1946675.125 acc: 0.6904449462890625\n",
      "step: 11345\n",
      "train: loss: 138901.484375 acc: 0.921066164970398  val: loss: 507488.96875 acc: 0.9033954739570618\n",
      "step: 11350\n",
      "train: loss: 266014.0 acc: 0.88725346326828  val: loss: 376872.03125 acc: 0.7921810746192932\n",
      "step: 11355\n",
      "train: loss: 568512.5 acc: 0.8971489071846008  val: loss: 883911.25 acc: 0.3264804482460022\n",
      "step: 11360\n",
      "train: loss: 293411.4375 acc: 0.955975353717804  val: loss: 989898.5 acc: 0.548723578453064\n",
      "step: 11365\n",
      "train: loss: 451286.875 acc: 0.9539415836334229  val: loss: 625294.8125 acc: 0.8971681594848633\n",
      "step: 11370\n",
      "train: loss: 117411.6171875 acc: 0.9871324300765991  val: loss: 1493220.25 acc: 0.21502238512039185\n",
      "step: 11375\n",
      "train: loss: 86788.3359375 acc: 0.9864853620529175  val: loss: 1567221.5 acc: 0.6247994899749756\n",
      "step: 11380\n",
      "train: loss: 396551.125 acc: 0.9729557037353516  val: loss: 458816.53125 acc: 0.7585917711257935\n",
      "step: 11385\n",
      "train: loss: 337453.59375 acc: 0.980720579624176  val: loss: 604371.125 acc: 0.6986904144287109\n",
      "step: 11390\n",
      "train: loss: 555647.1875 acc: 0.9665370583534241  val: loss: 741302.5 acc: 0.7517068982124329\n",
      "step: 11395\n",
      "train: loss: 245824.484375 acc: 0.9014871120452881  val: loss: 492299.1875 acc: 0.8062474727630615\n",
      "step: 11400\n",
      "train: loss: 999253.9375 acc: 0.9596460461616516  val: loss: 340544.15625 acc: 0.945379376411438\n",
      "step: 11405\n",
      "train: loss: 1220693.75 acc: 0.9654774069786072  val: loss: 474242.28125 acc: 0.7114220857620239\n",
      "step: 11410\n",
      "train: loss: 2245504.5 acc: 0.9141048192977905  val: loss: 375448.46875 acc: 0.9126554131507874\n",
      "step: 11415\n",
      "train: loss: 948287.125 acc: 0.9474892020225525  val: loss: 434103.34375 acc: 0.8614721894264221\n",
      "step: 11420\n",
      "train: loss: 643615.125 acc: 0.9702836871147156  val: loss: 1078656.125 acc: 0.7966867685317993\n",
      "step: 11425\n",
      "train: loss: 599600.125 acc: 0.9569271206855774  val: loss: 421727.78125 acc: 0.8801285028457642\n",
      "step: 11430\n",
      "train: loss: 522289.625 acc: 0.9452351927757263  val: loss: 269761.125 acc: 0.9189176559448242\n",
      "step: 11435\n",
      "train: loss: 3020838.5 acc: 0.6712104082107544  val: loss: 509534.3125 acc: 0.7781875729560852\n",
      "step: 11440\n",
      "train: loss: 992144.25 acc: 0.6246567964553833  val: loss: 517917.0625 acc: 0.8818026185035706\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 11445\n",
      "train: loss: 884299.1875 acc: 0.8738710880279541  val: loss: 1530833.125 acc: 0.5220818519592285\n",
      "step: 11450\n",
      "train: loss: 1299314.375 acc: 0.12543070316314697  val: loss: 828309.875 acc: 0.828123927116394\n",
      "step: 11455\n",
      "train: loss: 415144.0625 acc: 0.8953503370285034  val: loss: 314384.75 acc: 0.8842355012893677\n",
      "step: 11460\n",
      "train: loss: 1640436.375 acc: 0.5522089004516602  val: loss: 1210869.625 acc: 0.7818759083747864\n",
      "step: 11465\n",
      "train: loss: 457020.75 acc: 0.7392243146896362  val: loss: 3422581.0 acc: 0.6703181266784668\n",
      "step: 11470\n",
      "train: loss: 212528.6875 acc: 0.8265128135681152  val: loss: 3388000.0 acc: 0.6273010969161987\n",
      "step: 11475\n",
      "train: loss: 555983.75 acc: 0.7187240123748779  val: loss: 4936002.5 acc: 0.6209418773651123\n",
      "step: 11480\n",
      "train: loss: 115902.6796875 acc: 0.915556013584137  val: loss: 1738585.125 acc: 0.6884015202522278\n",
      "step: 11485\n",
      "train: loss: 67849.0390625 acc: 0.950156033039093  val: loss: 2830030.0 acc: 0.7592506408691406\n",
      "step: 11490\n",
      "train: loss: 160748.203125 acc: 0.8847155570983887  val: loss: 1164861.25 acc: 0.688434362411499\n",
      "step: 11495\n",
      "train: loss: 120104.515625 acc: 0.921600878238678  val: loss: 629051.6875 acc: 0.7465424537658691\n",
      "step: 11500\n",
      "train: loss: 115868.7265625 acc: 0.8965985774993896  val: loss: 3462131.0 acc: 0.6137191653251648\n",
      "step: 11505\n",
      "train: loss: 89502.8671875 acc: 0.9141708016395569  val: loss: 2223709.5 acc: 0.670332133769989\n",
      "step: 11510\n",
      "train: loss: 294103.5625 acc: 0.8287014961242676  val: loss: 1105165.125 acc: 0.7444878816604614\n",
      "step: 11515\n",
      "train: loss: 133867.421875 acc: 0.9002155065536499  val: loss: 1217360.125 acc: 0.6982735395431519\n",
      "step: 11520\n",
      "train: loss: 448070.375 acc: 0.7693067193031311  val: loss: 1146870.625 acc: 0.7098163366317749\n",
      "step: 11525\n",
      "train: loss: 250461.265625 acc: 0.8171687722206116  val: loss: 1391523.125 acc: 0.6697109937667847\n",
      "step: 11530\n",
      "train: loss: 238663.328125 acc: 0.8406946659088135  val: loss: 3600588.75 acc: 0.6212983131408691\n",
      "step: 11535\n",
      "train: loss: 231902.203125 acc: 0.8514022827148438  val: loss: 1441901.0 acc: 0.6155303716659546\n",
      "step: 11540\n",
      "train: loss: 115375.4609375 acc: 0.9068460464477539  val: loss: 71017.5625 acc: 0.9250209331512451\n",
      "step: 11545\n",
      "train: loss: 285907.15625 acc: 0.860161542892456  val: loss: 2355833.5 acc: 0.6288884282112122\n",
      "step: 11550\n",
      "train: loss: 1565608.375 acc: 0.7112717628479004  val: loss: 633763.0 acc: 0.8067953586578369\n",
      "step: 11555\n",
      "train: loss: 1303601.625 acc: 0.8587899804115295  val: loss: 1781469.75 acc: 0.8469867706298828\n",
      "step: 11560\n",
      "train: loss: 311250.78125 acc: 0.9742022752761841  val: loss: 483782.96875 acc: 0.9106684327125549\n",
      "step: 11565\n",
      "train: loss: 276495.84375 acc: 0.9601094126701355  val: loss: 397671.84375 acc: 0.8671835660934448\n",
      "step: 11570\n",
      "train: loss: 560817.0625 acc: 0.8853998780250549  val: loss: 447456.09375 acc: 0.9432365298271179\n",
      "step: 11575\n",
      "train: loss: 216074.78125 acc: 0.9691656827926636  val: loss: 1247237.625 acc: 0.8547255396842957\n",
      "step: 11580\n",
      "train: loss: 108280.078125 acc: 0.9893590211868286  val: loss: 2551212.5 acc: 0.4621930718421936\n",
      "step: 11585\n",
      "train: loss: 123336.28125 acc: 0.9889995455741882  val: loss: 2257116.5 acc: 0.43962663412094116\n",
      "step: 11590\n",
      "train: loss: 151594.3125 acc: 0.9883520603179932  val: loss: 482622.15625 acc: 0.8796795606613159\n",
      "step: 11595\n",
      "train: loss: 92456.6171875 acc: 0.9830329418182373  val: loss: 1619149.625 acc: 0.3506156802177429\n",
      "step: 11600\n",
      "train: loss: 67073.6484375 acc: 0.9748641848564148  val: loss: 410938.28125 acc: 0.88924241065979\n",
      "step: 11605\n",
      "train: loss: 49600.48046875 acc: 0.9808351993560791  val: loss: 433371.25 acc: 0.7412619590759277\n",
      "step: 11610\n",
      "train: loss: 16488.1484375 acc: 0.925369143486023  val: loss: 1102680.125 acc: 0.7438814640045166\n",
      "step: 11615\n",
      "train: loss: 19843.9921875 acc: 0.9705381393432617  val: loss: 2280360.5 acc: 0.5163099765777588\n",
      "step: 11620\n",
      "train: loss: 10733.6357421875 acc: 0.9930212497711182  val: loss: 2710222.25 acc: 0.39847153425216675\n",
      "step: 11625\n",
      "train: loss: 11907.611328125 acc: 0.9674432277679443  val: loss: 1921247.625 acc: 0.40281879901885986\n",
      "step: 11630\n",
      "train: loss: 6247.20703125 acc: 0.9836235642433167  val: loss: 780871.125 acc: 0.8173550963401794\n",
      "step: 11635\n",
      "train: loss: 13876.6904296875 acc: 0.9569259285926819  val: loss: 3667954.75 acc: -0.800929069519043\n",
      "step: 11640\n",
      "train: loss: 6229.82421875 acc: 0.9856811165809631  val: loss: 922298.125 acc: 0.876610279083252\n",
      "step: 11645\n",
      "train: loss: 8008.01953125 acc: 0.9715336561203003  val: loss: 897391.0 acc: 0.8953040242195129\n",
      "step: 11650\n",
      "train: loss: 14503.525390625 acc: 0.9878470301628113  val: loss: 535282.0 acc: 0.8802180886268616\n",
      "step: 11655\n",
      "train: loss: 37959.4296875 acc: 0.9842614531517029  val: loss: 1431766.625 acc: 0.4907212257385254\n",
      "step: 11660\n",
      "train: loss: 15087.078125 acc: 0.9948212504386902  val: loss: 1026350.5 acc: 0.8643320202827454\n",
      "step: 11665\n",
      "train: loss: 44434.87890625 acc: 0.9806166887283325  val: loss: 980661.4375 acc: 0.39374667406082153\n",
      "step: 11670\n",
      "train: loss: 91949.453125 acc: 0.9285381436347961  val: loss: 640922.75 acc: 0.8427413105964661\n",
      "step: 11675\n",
      "train: loss: 10399.3466796875 acc: 0.9779964089393616  val: loss: 2676532.0 acc: 0.6788380146026611\n",
      "step: 11680\n",
      "train: loss: 12396.8583984375 acc: 0.9936070442199707  val: loss: 1391575.625 acc: 0.8055243492126465\n",
      "step: 11685\n",
      "train: loss: 11069.146484375 acc: 0.9952733516693115  val: loss: 630295.0 acc: 0.7157721519470215\n",
      "step: 11690\n",
      "train: loss: 31128.263671875 acc: 0.9882271885871887  val: loss: 830836.3125 acc: 0.8942568898200989\n",
      "step: 11695\n",
      "train: loss: 28225.201171875 acc: 0.9923487305641174  val: loss: 839079.6875 acc: 0.7342082262039185\n",
      "step: 11700\n",
      "train: loss: 15839.921875 acc: 0.9947051405906677  val: loss: 219780.1875 acc: 0.9596556425094604\n",
      "step: 11705\n",
      "train: loss: 110906.53125 acc: 0.9767473936080933  val: loss: 121506.875 acc: 0.9704979062080383\n",
      "step: 11710\n",
      "train: loss: 170839.515625 acc: 0.9442107677459717  val: loss: 1357133.875 acc: 0.8066524863243103\n",
      "step: 11715\n",
      "train: loss: 103974.0 acc: 0.9558852910995483  val: loss: 267503.71875 acc: 0.9704715609550476\n",
      "step: 11720\n",
      "train: loss: 94080.21875 acc: 0.9782493710517883  val: loss: 172892.578125 acc: 0.9225321412086487\n",
      "step: 11725\n",
      "train: loss: 135455.84375 acc: 0.9735516905784607  val: loss: 1041330.875 acc: 0.7984971404075623\n",
      "step: 11730\n",
      "train: loss: 50865.71875 acc: 0.9938213229179382  val: loss: 436225.15625 acc: 0.8600703477859497\n",
      "step: 11735\n",
      "train: loss: 33028.44921875 acc: 0.9946428537368774  val: loss: 517625.71875 acc: 0.8969720602035522\n",
      "step: 11740\n",
      "train: loss: 107454.7421875 acc: 0.9841530323028564  val: loss: 228523.765625 acc: 0.9555228352546692\n",
      "step: 11745\n",
      "train: loss: 204834.390625 acc: 0.9791433215141296  val: loss: 481249.84375 acc: 0.909125804901123\n",
      "step: 11750\n",
      "train: loss: 550109.5625 acc: 0.9751705527305603  val: loss: 457879.84375 acc: 0.8643175959587097\n",
      "step: 11755\n",
      "train: loss: 294833.125 acc: 0.9684970378875732  val: loss: 1002860.4375 acc: 0.6937614679336548\n",
      "step: 11760\n",
      "train: loss: 277430.0 acc: 0.9647349119186401  val: loss: 974126.375 acc: 0.7823910713195801\n",
      "step: 11765\n",
      "train: loss: 1661632.625 acc: 0.9176533818244934  val: loss: 341625.71875 acc: 0.9063275456428528\n",
      "step: 11770\n",
      "train: loss: 1288438.875 acc: 0.9533958435058594  val: loss: 767645.3125 acc: 0.8278861045837402\n",
      "step: 11775\n",
      "train: loss: 1316417.5 acc: 0.955273449420929  val: loss: 300387.28125 acc: 0.8283637762069702\n",
      "step: 11780\n",
      "train: loss: 687711.125 acc: 0.9341098070144653  val: loss: 847615.0625 acc: 0.9308693408966064\n",
      "step: 11785\n",
      "train: loss: 448328.59375 acc: 0.9702935218811035  val: loss: 1821491.625 acc: 0.7491979598999023\n",
      "step: 11790\n",
      "train: loss: 374903.15625 acc: 0.9631644487380981  val: loss: 280565.09375 acc: 0.9584565758705139\n",
      "step: 11795\n",
      "train: loss: 1210526.375 acc: 0.7913658618927002  val: loss: 521246.25 acc: 0.9187407493591309\n",
      "step: 11800\n",
      "train: loss: 283617.5625 acc: 0.9595661759376526  val: loss: 1397608.625 acc: 0.8867698311805725\n",
      "step: 11805\n",
      "train: loss: 1311780.625 acc: 0.2877834439277649  val: loss: 1101747.75 acc: 0.698208212852478\n",
      "step: 11810\n",
      "train: loss: 262860.90625 acc: 0.7919421195983887  val: loss: 1308647.375 acc: 0.7491816282272339\n",
      "step: 11815\n",
      "train: loss: 195690.28125 acc: 0.8678338527679443  val: loss: 522519.84375 acc: 0.8901558518409729\n",
      "step: 11820\n",
      "train: loss: 502517.96875 acc: 0.8476098775863647  val: loss: 1318048.5 acc: 0.8419857025146484\n",
      "step: 11825\n",
      "train: loss: 1115711.75 acc: 0.5732688903808594  val: loss: 1394501.125 acc: 0.7720048427581787\n",
      "step: 11830\n",
      "train: loss: 364039.125 acc: 0.7680282592773438  val: loss: 2734678.75 acc: 0.6645335555076599\n",
      "step: 11835\n",
      "train: loss: 472566.21875 acc: 0.7762073278427124  val: loss: 3119793.0 acc: 0.6140754222869873\n",
      "step: 11840\n",
      "train: loss: 126708.2578125 acc: 0.8942832946777344  val: loss: 4300633.0 acc: 0.6176770329475403\n",
      "step: 11845\n",
      "train: loss: 57451.5703125 acc: 0.9499660730361938  val: loss: 1560480.625 acc: 0.6531440615653992\n",
      "step: 11850\n",
      "train: loss: 78589.328125 acc: 0.943320631980896  val: loss: 757926.3125 acc: 0.7145015597343445\n",
      "step: 11855\n",
      "train: loss: 216826.09375 acc: 0.8757980465888977  val: loss: 2008928.0 acc: 0.6711500287055969\n",
      "step: 11860\n",
      "train: loss: 152845.671875 acc: 0.8949340581893921  val: loss: 597746.625 acc: 0.7450805902481079\n",
      "step: 11865\n",
      "train: loss: 57122.9296875 acc: 0.9491826891899109  val: loss: 771549.375 acc: 0.7520492076873779\n",
      "step: 11870\n",
      "train: loss: 75769.140625 acc: 0.9357269406318665  val: loss: 1630771.0 acc: 0.7024236917495728\n",
      "step: 11875\n",
      "train: loss: 486413.46875 acc: 0.748694896697998  val: loss: 2755941.0 acc: 0.6365175247192383\n",
      "step: 11880\n",
      "train: loss: 88690.7421875 acc: 0.8967177271842957  val: loss: 2304301.5 acc: 0.6535731554031372\n",
      "step: 11885\n",
      "train: loss: 338447.53125 acc: 0.8272185325622559  val: loss: 2670076.5 acc: 0.6296191215515137\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 11890\n",
      "train: loss: 198436.078125 acc: 0.8508386015892029  val: loss: 3539300.75 acc: 0.5660836100578308\n",
      "step: 11895\n",
      "train: loss: 83984.0078125 acc: 0.9252811074256897  val: loss: 156209.40625 acc: 0.8984366655349731\n",
      "step: 11900\n",
      "train: loss: 299552.34375 acc: 0.8073992133140564  val: loss: 231162.953125 acc: 0.8410103917121887\n",
      "step: 11905\n",
      "train: loss: 360638.1875 acc: 0.7738648653030396  val: loss: 438262.9375 acc: 0.7963600158691406\n",
      "step: 11910\n",
      "train: loss: 903244.875 acc: 0.7042560577392578  val: loss: 2639639.0 acc: 0.6353393793106079\n",
      "step: 11915\n",
      "train: loss: 1919983.125 acc: 0.7646539807319641  val: loss: 929086.9375 acc: 0.8479294776916504\n",
      "step: 11920\n",
      "train: loss: 1090176.875 acc: 0.8639558553695679  val: loss: 911785.6875 acc: 0.7472445368766785\n",
      "step: 11925\n",
      "train: loss: 642093.875 acc: 0.9429736137390137  val: loss: 937725.0625 acc: 0.8832374811172485\n",
      "step: 11930\n",
      "train: loss: 269117.6875 acc: 0.9600173234939575  val: loss: 482036.125 acc: 0.8779604434967041\n",
      "step: 11935\n",
      "train: loss: 191542.609375 acc: 0.9728890061378479  val: loss: 772435.375 acc: 0.7342349886894226\n",
      "step: 11940\n",
      "train: loss: 160622.578125 acc: 0.9751194715499878  val: loss: 573624.4375 acc: 0.8761693239212036\n",
      "step: 11945\n",
      "train: loss: 140001.09375 acc: 0.9885163307189941  val: loss: 2173894.75 acc: 0.7038856148719788\n",
      "step: 11950\n",
      "train: loss: 78843.2734375 acc: 0.9935668110847473  val: loss: 973819.875 acc: 0.8457989692687988\n",
      "step: 11955\n",
      "train: loss: 97392.8203125 acc: 0.9851442575454712  val: loss: 830481.3125 acc: 0.7628898620605469\n",
      "step: 11960\n",
      "train: loss: 126118.59375 acc: 0.9718765020370483  val: loss: 1894754.25 acc: 0.4623163938522339\n",
      "step: 11965\n",
      "train: loss: 75792.640625 acc: 0.9876767992973328  val: loss: 866711.3125 acc: 0.8932753205299377\n",
      "step: 11970\n",
      "train: loss: 44245.77734375 acc: 0.9898357391357422  val: loss: 1351494.375 acc: 0.3713475465774536\n",
      "step: 11975\n",
      "train: loss: 20753.119140625 acc: 0.9854162931442261  val: loss: 440766.8125 acc: 0.8373425006866455\n",
      "step: 11980\n",
      "train: loss: 27452.59375 acc: 0.9677972793579102  val: loss: 1066790.25 acc: 0.6435844898223877\n",
      "step: 11985\n",
      "train: loss: 15931.013671875 acc: 0.9776433706283569  val: loss: 1004997.8125 acc: 0.8331538438796997\n",
      "step: 11990\n",
      "train: loss: 10219.2744140625 acc: 0.9614875316619873  val: loss: 1017773.5 acc: 0.7462973594665527\n",
      "step: 11995\n",
      "train: loss: 22722.12890625 acc: 0.9697186350822449  val: loss: 474108.875 acc: 0.8533241152763367\n",
      "step: 12000\n",
      "train: loss: 20290.77734375 acc: 0.9523850083351135  val: loss: 1785934.5 acc: 0.3416438698768616\n",
      "step: 12005\n",
      "train: loss: 7331.03955078125 acc: 0.9765088558197021  val: loss: 576921.8125 acc: 0.6819063425064087\n",
      "step: 12010\n",
      "train: loss: 17507.169921875 acc: 0.9221608638763428  val: loss: 1234373.125 acc: 0.6534358263015747\n",
      "step: 12015\n",
      "train: loss: 35190.5546875 acc: 0.9755999445915222  val: loss: 359084.15625 acc: 0.8793595433235168\n",
      "step: 12020\n",
      "train: loss: 44846.15234375 acc: 0.9770506024360657  val: loss: 213544.09375 acc: 0.9509545564651489\n",
      "step: 12025\n",
      "train: loss: 11755.8798828125 acc: 0.9936580657958984  val: loss: 1519470.5 acc: 0.6616219282150269\n",
      "step: 12030\n",
      "train: loss: 14539.4892578125 acc: 0.9887246489524841  val: loss: 791418.875 acc: 0.7338775396347046\n",
      "step: 12035\n",
      "train: loss: 21876.5859375 acc: 0.9374583959579468  val: loss: 356504.625 acc: 0.9197065830230713\n",
      "step: 12040\n",
      "train: loss: 18161.84375 acc: 0.985270082950592  val: loss: 1311478.125 acc: 0.8147032856941223\n",
      "step: 12045\n",
      "train: loss: 14090.8408203125 acc: 0.9892675876617432  val: loss: 265126.09375 acc: 0.9637628197669983\n",
      "step: 12050\n",
      "train: loss: 17283.02734375 acc: 0.9940266013145447  val: loss: 1059688.625 acc: 0.5667364597320557\n",
      "step: 12055\n",
      "train: loss: 35057.8828125 acc: 0.990860104560852  val: loss: 448257.9375 acc: 0.9373831748962402\n",
      "step: 12060\n",
      "train: loss: 26478.6328125 acc: 0.9921237230300903  val: loss: 514122.625 acc: 0.5838191509246826\n",
      "step: 12065\n",
      "train: loss: 37747.94921875 acc: 0.9761577844619751  val: loss: 573313.0625 acc: 0.760088324546814\n",
      "step: 12070\n",
      "train: loss: 28175.7265625 acc: 0.9944426417350769  val: loss: 296762.4375 acc: 0.9102281332015991\n",
      "step: 12075\n",
      "train: loss: 64815.578125 acc: 0.9840070605278015  val: loss: 808166.0625 acc: 0.7635865211486816\n",
      "step: 12080\n",
      "train: loss: 76776.5390625 acc: 0.9712281823158264  val: loss: 330259.75 acc: 0.9465436935424805\n",
      "step: 12085\n",
      "train: loss: 254032.53125 acc: 0.9372060894966125  val: loss: 223604.1875 acc: 0.980372428894043\n",
      "step: 12090\n",
      "train: loss: 498816.9375 acc: 0.9295371174812317  val: loss: 997600.8125 acc: 0.8041770458221436\n",
      "step: 12095\n",
      "train: loss: 73999.3046875 acc: 0.9924691915512085  val: loss: 481295.90625 acc: 0.768185019493103\n",
      "step: 12100\n",
      "train: loss: 140534.15625 acc: 0.9832436442375183  val: loss: 301299.3125 acc: 0.8950695395469666\n",
      "step: 12105\n",
      "train: loss: 281655.28125 acc: 0.932181715965271  val: loss: 1749357.25 acc: 0.813734233379364\n",
      "step: 12110\n",
      "train: loss: 220551.625 acc: 0.9726340770721436  val: loss: 1108587.75 acc: 0.8808950185775757\n",
      "step: 12115\n",
      "train: loss: 444666.1875 acc: 0.980664849281311  val: loss: 1578479.5 acc: 0.7953519821166992\n",
      "step: 12120\n",
      "train: loss: 1764802.75 acc: 0.8401622772216797  val: loss: 523726.65625 acc: 0.9518546462059021\n",
      "step: 12125\n",
      "train: loss: 229212.671875 acc: 0.9667567014694214  val: loss: 1372406.75 acc: 0.7976301908493042\n",
      "step: 12130\n",
      "train: loss: 1944755.375 acc: 0.9300457239151001  val: loss: 300451.84375 acc: 0.9160899519920349\n",
      "step: 12135\n",
      "train: loss: 4265748.5 acc: 0.865976095199585  val: loss: 503699.125 acc: 0.8693104386329651\n",
      "step: 12140\n",
      "train: loss: 3252392.0 acc: 0.8841966986656189  val: loss: 358282.03125 acc: 0.8771411180496216\n",
      "step: 12145\n",
      "train: loss: 1477853.125 acc: 0.9467624425888062  val: loss: 823460.125 acc: 0.9405401349067688\n",
      "step: 12150\n",
      "train: loss: 190145.46875 acc: 0.9751583933830261  val: loss: 1444605.875 acc: 0.7819502949714661\n",
      "step: 12155\n",
      "train: loss: 695323.3125 acc: 0.9585061073303223  val: loss: 2188251.0 acc: 0.4459182620048523\n",
      "step: 12160\n",
      "train: loss: 225279.03125 acc: 0.9718270301818848  val: loss: 150387.71875 acc: 0.9442712068557739\n",
      "step: 12165\n",
      "train: loss: 375028.625 acc: 0.9045127630233765  val: loss: 363474.03125 acc: 0.9605969786643982\n",
      "step: 12170\n",
      "train: loss: 2384471.0 acc: 0.5955605506896973  val: loss: 1098722.875 acc: 0.8431414365768433\n",
      "step: 12175\n",
      "train: loss: 1906289.875 acc: 0.45414477586746216  val: loss: 451260.9375 acc: 0.7364456653594971\n",
      "step: 12180\n",
      "train: loss: 426848.625 acc: 0.8371812105178833  val: loss: 1570402.875 acc: 0.786243736743927\n",
      "step: 12185\n",
      "train: loss: 323761.1875 acc: 0.8801689743995667  val: loss: 923452.3125 acc: 0.8108757138252258\n",
      "step: 12190\n",
      "train: loss: 1456140.875 acc: 0.4128495454788208  val: loss: 675929.0 acc: 0.8524540662765503\n",
      "step: 12195\n",
      "train: loss: 677900.4375 acc: 0.6087393760681152  val: loss: 449011.5 acc: 0.7754859328269958\n",
      "step: 12200\n",
      "train: loss: 357548.65625 acc: 0.8038628697395325  val: loss: 1812888.0 acc: 0.6905883550643921\n",
      "step: 12205\n",
      "train: loss: 328872.625 acc: 0.8220918774604797  val: loss: 1249497.375 acc: 0.7360047101974487\n",
      "step: 12210\n",
      "train: loss: 151194.265625 acc: 0.8831052184104919  val: loss: 354496.90625 acc: 0.781830906867981\n",
      "step: 12215\n",
      "train: loss: 184254.515625 acc: 0.8824988603591919  val: loss: 1353970.375 acc: 0.685185432434082\n",
      "step: 12220\n",
      "train: loss: 56965.44140625 acc: 0.9580526351928711  val: loss: 528192.1875 acc: 0.7531216740608215\n",
      "step: 12225\n",
      "train: loss: 142969.6875 acc: 0.8534548282623291  val: loss: 5186569.5 acc: 0.5810370445251465\n",
      "step: 12230\n",
      "train: loss: 134657.921875 acc: 0.9150195121765137  val: loss: 1961201.625 acc: 0.704491138458252\n",
      "step: 12235\n",
      "train: loss: 135202.3125 acc: 0.9012422561645508  val: loss: 2621671.75 acc: 0.6132094264030457\n",
      "step: 12240\n",
      "train: loss: 112610.421875 acc: 0.8905938267707825  val: loss: 1908148.0 acc: 0.6903025507926941\n",
      "step: 12245\n",
      "train: loss: 65367.51171875 acc: 0.9347783327102661  val: loss: 4613761.0 acc: 0.5698307752609253\n",
      "step: 12250\n",
      "train: loss: 98080.21875 acc: 0.8932821750640869  val: loss: 1885753.125 acc: 0.6474482417106628\n",
      "step: 12255\n",
      "train: loss: 355336.71875 acc: 0.7884981632232666  val: loss: 1490858.875 acc: 0.6336292028427124\n",
      "step: 12260\n",
      "train: loss: 634543.25 acc: 0.7437363266944885  val: loss: 2145462.5 acc: 0.6090250611305237\n",
      "step: 12265\n",
      "train: loss: 466467.6875 acc: 0.7176125645637512  val: loss: 2071709.0 acc: 0.6625282764434814\n",
      "step: 12270\n",
      "train: loss: 252812.171875 acc: 0.8406325578689575  val: loss: 1946696.75 acc: 0.6544740200042725\n",
      "step: 12275\n",
      "train: loss: 862943.875 acc: 0.7274892330169678  val: loss: 1702891.625 acc: 0.6217939853668213\n",
      "step: 12280\n",
      "train: loss: 2362690.75 acc: 0.6383458375930786  val: loss: 1291121.375 acc: 0.7019333839416504\n",
      "step: 12285\n",
      "train: loss: 1047835.9375 acc: 0.6585588455200195  val: loss: 1315142.75 acc: 0.7702661156654358\n",
      "step: 12290\n",
      "train: loss: 281684.625 acc: 0.9724382758140564  val: loss: 1103031.75 acc: 0.7211411595344543\n",
      "step: 12295\n",
      "train: loss: 674518.8125 acc: 0.9252194166183472  val: loss: 1116720.875 acc: 0.525536298751831\n",
      "step: 12300\n",
      "train: loss: 114075.3984375 acc: 0.9797551035881042  val: loss: 1318894.0 acc: 0.44513875246047974\n",
      "step: 12305\n",
      "train: loss: 118832.234375 acc: 0.9634931087493896  val: loss: 179954.953125 acc: 0.9710807204246521\n",
      "step: 12310\n",
      "train: loss: 146077.578125 acc: 0.9866843223571777  val: loss: 452363.46875 acc: 0.8089191317558289\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 12315\n",
      "train: loss: 145522.71875 acc: 0.9894709587097168  val: loss: 1079454.25 acc: 0.07350164651870728\n",
      "step: 12320\n",
      "train: loss: 121335.046875 acc: 0.9890600442886353  val: loss: 453557.84375 acc: 0.836808979511261\n",
      "step: 12325\n",
      "train: loss: 129459.25 acc: 0.976997435092926  val: loss: 264714.625 acc: 0.9354052543640137\n",
      "step: 12330\n",
      "train: loss: 112570.8515625 acc: 0.9719229936599731  val: loss: 1059976.25 acc: 0.5490925312042236\n",
      "step: 12335\n",
      "train: loss: 36932.47265625 acc: 0.9834468364715576  val: loss: 988776.0625 acc: 0.7219586968421936\n",
      "step: 12340\n",
      "train: loss: 15851.9052734375 acc: 0.9905676245689392  val: loss: 619894.1875 acc: 0.8142468333244324\n",
      "step: 12345\n",
      "train: loss: 16362.7919921875 acc: 0.9648690223693848  val: loss: 1747547.0 acc: 0.685521125793457\n",
      "step: 12350\n",
      "train: loss: 24212.412109375 acc: 0.9882404208183289  val: loss: 526698.75 acc: 0.926805853843689\n",
      "step: 12355\n",
      "train: loss: 19399.5 acc: 0.8764827847480774  val: loss: 882088.3125 acc: 0.43039751052856445\n",
      "step: 12360\n",
      "train: loss: 11949.1875 acc: 0.9904056787490845  val: loss: 228594.25 acc: 0.9207292795181274\n",
      "step: 12365\n",
      "train: loss: 8593.6982421875 acc: 0.9782888293266296  val: loss: 252533.984375 acc: 0.9638773202896118\n",
      "step: 12370\n",
      "train: loss: 7065.4267578125 acc: 0.9767212867736816  val: loss: 468534.46875 acc: 0.8413588404655457\n",
      "step: 12375\n",
      "train: loss: 18456.587890625 acc: 0.9783772826194763  val: loss: 251093.34375 acc: 0.872908890247345\n",
      "step: 12380\n",
      "train: loss: 38019.5546875 acc: 0.9787750244140625  val: loss: 1962861.125 acc: 0.3388395309448242\n",
      "step: 12385\n",
      "train: loss: 56910.82421875 acc: 0.9600896239280701  val: loss: 1147836.625 acc: 0.6869533061981201\n",
      "step: 12390\n",
      "train: loss: 96530.5078125 acc: 0.956394374370575  val: loss: 514713.1875 acc: 0.9185543060302734\n",
      "step: 12395\n",
      "train: loss: 31858.376953125 acc: 0.9741665720939636  val: loss: 701930.3125 acc: 0.8889222145080566\n",
      "step: 12400\n",
      "train: loss: 8891.8828125 acc: 0.9929299354553223  val: loss: 220746.03125 acc: 0.9756681323051453\n",
      "step: 12405\n",
      "train: loss: 17922.658203125 acc: 0.9896346926689148  val: loss: 596825.9375 acc: 0.9259212017059326\n",
      "step: 12410\n",
      "train: loss: 9033.7021484375 acc: 0.9858501553535461  val: loss: 1284393.75 acc: 0.5765020847320557\n",
      "step: 12415\n",
      "train: loss: 9805.861328125 acc: 0.9940650463104248  val: loss: 730820.0625 acc: 0.9496074914932251\n",
      "step: 12420\n",
      "train: loss: 27868.365234375 acc: 0.9922714233398438  val: loss: 655288.6875 acc: 0.949346125125885\n",
      "step: 12425\n",
      "train: loss: 37533.61328125 acc: 0.98951256275177  val: loss: 1530952.125 acc: 0.6784266233444214\n",
      "step: 12430\n",
      "train: loss: 33417.85546875 acc: 0.9906246066093445  val: loss: 672216.1875 acc: 0.9431788325309753\n",
      "step: 12435\n",
      "train: loss: 55861.12109375 acc: 0.9799813032150269  val: loss: 1834327.875 acc: 0.7428056001663208\n",
      "step: 12440\n",
      "train: loss: 113299.671875 acc: 0.980789303779602  val: loss: 376300.03125 acc: 0.9199204444885254\n",
      "step: 12445\n",
      "train: loss: 158449.046875 acc: 0.9653592705726624  val: loss: 2079246.5 acc: 0.5162663459777832\n",
      "step: 12450\n",
      "train: loss: 256226.53125 acc: 0.8858194947242737  val: loss: 2217909.75 acc: 0.541560173034668\n",
      "step: 12455\n",
      "train: loss: 43677.81640625 acc: 0.9878159761428833  val: loss: 2193698.0 acc: 0.43162643909454346\n",
      "step: 12460\n",
      "train: loss: 99175.09375 acc: 0.9913674592971802  val: loss: 1473708.5 acc: 0.5131001472473145\n",
      "step: 12465\n",
      "train: loss: 72110.046875 acc: 0.9925209879875183  val: loss: 527710.0625 acc: 0.8335139155387878\n",
      "step: 12470\n",
      "train: loss: 601856.5625 acc: 0.9053231477737427  val: loss: 1111993.125 acc: 0.7415715456008911\n",
      "step: 12475\n",
      "train: loss: 271905.65625 acc: 0.9689363241195679  val: loss: 2885807.75 acc: -0.4871014356613159\n",
      "step: 12480\n",
      "train: loss: 987644.5 acc: 0.9507730603218079  val: loss: 535943.8125 acc: 0.8931514620780945\n",
      "step: 12485\n",
      "train: loss: 269567.5 acc: 0.9782759547233582  val: loss: 1247553.0 acc: -0.5897061824798584\n",
      "step: 12490\n",
      "train: loss: 289466.65625 acc: 0.9228979349136353  val: loss: 343204.4375 acc: 0.918750524520874\n",
      "step: 12495\n",
      "train: loss: 2101293.5 acc: 0.7158181071281433  val: loss: 507555.28125 acc: 0.6211854219436646\n",
      "step: 12500\n",
      "train: loss: 1531564.75 acc: 0.9590501189231873  val: loss: 1303342.75 acc: 0.7881358861923218\n",
      "step: 12505\n",
      "train: loss: 1945336.25 acc: 0.9388815760612488  val: loss: 840964.25 acc: 0.7827680110931396\n",
      "step: 12510\n",
      "train: loss: 1518494.75 acc: 0.926062285900116  val: loss: 794948.3125 acc: 0.8586053848266602\n",
      "step: 12515\n",
      "train: loss: 537375.125 acc: 0.9283446669578552  val: loss: 339577.5 acc: 0.9629297256469727\n",
      "step: 12520\n",
      "train: loss: 509998.46875 acc: 0.9449812769889832  val: loss: 1078938.625 acc: 0.7465652823448181\n",
      "step: 12525\n",
      "train: loss: 3110798.75 acc: 0.6435607671737671  val: loss: 1484705.25 acc: 0.39746594429016113\n",
      "step: 12530\n",
      "train: loss: 597274.1875 acc: 0.8736788034439087  val: loss: 1550255.0 acc: 0.4680525064468384\n",
      "step: 12535\n",
      "train: loss: 1633461.125 acc: 0.7727418541908264  val: loss: 1627705.625 acc: 0.8155531883239746\n",
      "step: 12540\n",
      "train: loss: 824795.1875 acc: 0.7950678467750549  val: loss: 535601.4375 acc: 0.71369868516922\n",
      "step: 12545\n",
      "train: loss: 727362.9375 acc: 0.7356032133102417  val: loss: 1093352.25 acc: 0.811048686504364\n",
      "step: 12550\n",
      "train: loss: 168196.296875 acc: 0.9033223986625671  val: loss: 674073.875 acc: 0.8097513914108276\n",
      "step: 12555\n",
      "train: loss: 960766.0 acc: 0.718059241771698  val: loss: 2099676.25 acc: 0.7761260867118835\n",
      "step: 12560\n",
      "train: loss: 890461.3125 acc: 0.7304473519325256  val: loss: 3397411.0 acc: 0.5981236696243286\n",
      "step: 12565\n",
      "train: loss: 250926.09375 acc: 0.8463038802146912  val: loss: 1239044.75 acc: 0.5851010084152222\n",
      "step: 12570\n",
      "train: loss: 67399.75 acc: 0.9440218210220337  val: loss: 2942827.5 acc: 0.6626546382904053\n",
      "step: 12575\n",
      "train: loss: 131193.71875 acc: 0.9116529822349548  val: loss: 2089471.75 acc: 0.6057032942771912\n",
      "step: 12580\n",
      "train: loss: 231333.21875 acc: 0.8516849279403687  val: loss: 849803.5625 acc: 0.6827394962310791\n",
      "step: 12585\n",
      "train: loss: 101350.65625 acc: 0.9263061285018921  val: loss: 764272.25 acc: 0.7057936191558838\n",
      "step: 12590\n",
      "train: loss: 208206.0 acc: 0.8840497136116028  val: loss: 870357.75 acc: 0.6487424373626709\n",
      "step: 12595\n",
      "train: loss: 441910.625 acc: 0.794325590133667  val: loss: 2045937.875 acc: 0.6145215034484863\n",
      "step: 12600\n",
      "train: loss: 214103.140625 acc: 0.8363789319992065  val: loss: 1875868.875 acc: 0.6166622638702393\n",
      "step: 12605\n",
      "train: loss: 57956.453125 acc: 0.9533027410507202  val: loss: 1597085.75 acc: 0.5896897315979004\n",
      "step: 12610\n",
      "train: loss: 112873.953125 acc: 0.8729130625724792  val: loss: 1763734.0 acc: 0.667661190032959\n",
      "step: 12615\n",
      "train: loss: 237767.890625 acc: 0.8325670957565308  val: loss: 2967439.25 acc: 0.6148648262023926\n",
      "step: 12620\n",
      "train: loss: 477986.625 acc: 0.7484928369522095  val: loss: 1148014.375 acc: 0.6837416887283325\n",
      "step: 12625\n",
      "train: loss: 391019.46875 acc: 0.76719731092453  val: loss: 2193437.5 acc: 0.583441436290741\n",
      "step: 12630\n",
      "train: loss: 878280.125 acc: 0.6595314145088196  val: loss: 429062.90625 acc: 0.7804766893386841\n",
      "step: 12635\n",
      "train: loss: 282974.46875 acc: 0.7811028361320496  val: loss: 1069013.875 acc: 0.6530424952507019\n",
      "step: 12640\n",
      "train: loss: 975151.125 acc: 0.7162556648254395  val: loss: 1175409.25 acc: 0.7237201929092407\n",
      "step: 12645\n",
      "train: loss: 2041461.5 acc: 0.6728078126907349  val: loss: 161153.53125 acc: 0.7793519496917725\n",
      "step: 12650\n",
      "train: loss: 873585.3125 acc: 0.8706827163696289  val: loss: 575125.625 acc: 0.9113606214523315\n",
      "step: 12655\n",
      "train: loss: 756025.125 acc: 0.9156885147094727  val: loss: 376091.59375 acc: 0.8286055326461792\n",
      "step: 12660\n",
      "train: loss: 204847.0625 acc: 0.9850443005561829  val: loss: 553621.375 acc: 0.891727089881897\n",
      "step: 12665\n",
      "train: loss: 371130.09375 acc: 0.9473757147789001  val: loss: 988348.875 acc: -0.24514532089233398\n",
      "step: 12670\n",
      "train: loss: 121037.6796875 acc: 0.9727648496627808  val: loss: 354033.03125 acc: 0.8796510696411133\n",
      "step: 12675\n",
      "train: loss: 162413.328125 acc: 0.9813535809516907  val: loss: 271637.71875 acc: 0.9653168320655823\n",
      "step: 12680\n",
      "train: loss: 132844.984375 acc: 0.9898916482925415  val: loss: 828970.125 acc: 0.7629984617233276\n",
      "step: 12685\n",
      "train: loss: 128197.8671875 acc: 0.9911689162254333  val: loss: 981654.3125 acc: 0.7402960062026978\n",
      "step: 12690\n",
      "train: loss: 80309.6484375 acc: 0.988671600818634  val: loss: 1249511.0 acc: 0.6723043322563171\n",
      "step: 12695\n",
      "train: loss: 47180.6953125 acc: 0.9909153580665588  val: loss: 1036923.8125 acc: 0.8049014806747437\n",
      "step: 12700\n",
      "train: loss: 63815.39453125 acc: 0.9816747307777405  val: loss: 1190515.125 acc: 0.3580799698829651\n",
      "step: 12705\n",
      "train: loss: 9454.2080078125 acc: 0.9956787824630737  val: loss: 274211.15625 acc: 0.8891898393630981\n",
      "step: 12710\n",
      "train: loss: 16175.501953125 acc: 0.9457674622535706  val: loss: 416881.65625 acc: 0.9419960975646973\n",
      "step: 12715\n",
      "train: loss: 28390.373046875 acc: 0.9697743058204651  val: loss: 323077.1875 acc: 0.9558985233306885\n",
      "step: 12720\n",
      "train: loss: 9494.4091796875 acc: 0.9855435490608215  val: loss: 127931.2734375 acc: 0.9789556264877319\n",
      "step: 12725\n",
      "train: loss: 19077.626953125 acc: 0.9588034749031067  val: loss: 393717.90625 acc: 0.9642588496208191\n",
      "step: 12730\n",
      "train: loss: 7263.73828125 acc: 0.9806499481201172  val: loss: 2304145.25 acc: 0.7755143642425537\n",
      "step: 12735\n",
      "train: loss: 7040.43896484375 acc: 0.9654436111450195  val: loss: 366720.78125 acc: 0.9186616539955139\n",
      "step: 12740\n",
      "train: loss: 17638.2890625 acc: 0.9787126779556274  val: loss: 556171.25 acc: 0.9460203051567078\n",
      "step: 12745\n",
      "train: loss: 44828.21875 acc: 0.9728862047195435  val: loss: 1431964.875 acc: 0.8556815981864929\n",
      "step: 12750\n",
      "train: loss: 41615.1953125 acc: 0.981380820274353  val: loss: 956986.5 acc: 0.9231530427932739\n",
      "step: 12755\n",
      "train: loss: 20613.625 acc: 0.9837660193443298  val: loss: 1650095.625 acc: 0.6974090337753296\n",
      "step: 12760\n",
      "train: loss: 17242.572265625 acc: 0.988149881362915  val: loss: 1768286.0 acc: 0.8777951598167419\n",
      "step: 12765\n",
      "train: loss: 8141.41796875 acc: 0.9932432174682617  val: loss: 1050649.5 acc: 0.6502529382705688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 12770\n",
      "train: loss: 19022.27734375 acc: 0.9890245199203491  val: loss: 3442300.5 acc: 0.16441279649734497\n",
      "step: 12775\n",
      "train: loss: 10902.4375 acc: 0.9894877672195435  val: loss: 2252568.25 acc: 0.8506666421890259\n",
      "step: 12780\n",
      "train: loss: 16733.78515625 acc: 0.9929965734481812  val: loss: 1088565.875 acc: 0.8301823139190674\n",
      "step: 12785\n",
      "train: loss: 21955.728515625 acc: 0.9959428310394287  val: loss: 2303878.0 acc: 0.3633612394332886\n",
      "step: 12790\n",
      "train: loss: 30389.556640625 acc: 0.992129921913147  val: loss: 3942735.0 acc: 0.42006534337997437\n",
      "step: 12795\n",
      "train: loss: 15266.28125 acc: 0.9805370569229126  val: loss: 399183.40625 acc: 0.9473099708557129\n",
      "step: 12800\n",
      "train: loss: 54507.87890625 acc: 0.9842705130577087  val: loss: 2518702.75 acc: 0.7330942153930664\n",
      "step: 12805\n",
      "train: loss: 44967.921875 acc: 0.9893626570701599  val: loss: 2465129.75 acc: 0.4704905152320862\n",
      "step: 12810\n",
      "train: loss: 89947.25 acc: 0.9698604941368103  val: loss: 2188317.0 acc: 0.4287055730819702\n",
      "step: 12815\n",
      "train: loss: 101149.03125 acc: 0.9718645215034485  val: loss: 524146.90625 acc: 0.9135971665382385\n",
      "step: 12820\n",
      "train: loss: 83810.09375 acc: 0.9689458608627319  val: loss: 1697854.375 acc: 0.5833974480628967\n",
      "step: 12825\n",
      "train: loss: 129140.2734375 acc: 0.98567134141922  val: loss: 568646.25 acc: 0.9020055532455444\n",
      "step: 12830\n",
      "train: loss: 44348.65625 acc: 0.991738498210907  val: loss: 2562109.25 acc: 0.7162957191467285\n",
      "step: 12835\n",
      "train: loss: 94185.125 acc: 0.9829872846603394  val: loss: 1116456.5 acc: 0.23234516382217407\n",
      "step: 12840\n",
      "train: loss: 275854.78125 acc: 0.9554270505905151  val: loss: 2730313.5 acc: 0.12555038928985596\n",
      "step: 12845\n",
      "train: loss: 611356.0625 acc: 0.9631312489509583  val: loss: 1850603.125 acc: 0.5495864152908325\n",
      "step: 12850\n",
      "train: loss: 487583.875 acc: 0.9599671363830566  val: loss: 1078262.75 acc: 0.5316649675369263\n",
      "step: 12855\n",
      "train: loss: 328556.25 acc: 0.9003161191940308  val: loss: 1266756.125 acc: 0.7521076202392578\n",
      "step: 12860\n",
      "train: loss: 210359.265625 acc: 0.9912933707237244  val: loss: 503257.5625 acc: 0.8728543519973755\n",
      "step: 12865\n",
      "train: loss: 1008333.1875 acc: 0.9627019166946411  val: loss: 953882.125 acc: 0.7618402242660522\n",
      "step: 12870\n",
      "train: loss: 1963696.625 acc: 0.9256484508514404  val: loss: 1476466.75 acc: 0.4073258638381958\n",
      "step: 12875\n",
      "train: loss: 1712675.75 acc: 0.9331211447715759  val: loss: 408904.40625 acc: 0.9290696382522583\n",
      "step: 12880\n",
      "train: loss: 338675.65625 acc: 0.9767923951148987  val: loss: 1233184.75 acc: 0.8794823288917542\n",
      "step: 12885\n",
      "train: loss: 452827.84375 acc: 0.961564838886261  val: loss: 394193.3125 acc: 0.8946644067764282\n",
      "step: 12890\n",
      "train: loss: 238686.328125 acc: 0.9678812026977539  val: loss: 371359.59375 acc: 0.9316877126693726\n",
      "step: 12895\n",
      "train: loss: 1475462.375 acc: 0.8776959776878357  val: loss: 169750.390625 acc: 0.9708731770515442\n",
      "step: 12900\n",
      "train: loss: 1627452.625 acc: 0.6288173198699951  val: loss: 332445.28125 acc: 0.9340628981590271\n",
      "step: 12905\n",
      "train: loss: 2007543.25 acc: 0.6569572687149048  val: loss: 1134219.375 acc: 0.7399009466171265\n",
      "step: 12910\n",
      "train: loss: 898958.0625 acc: 0.680374801158905  val: loss: 1031649.8125 acc: 0.735284686088562\n",
      "step: 12915\n",
      "train: loss: 150998.953125 acc: 0.9278140068054199  val: loss: 1093142.0 acc: 0.8129404187202454\n",
      "step: 12920\n",
      "train: loss: 435722.53125 acc: 0.7903485894203186  val: loss: 313414.65625 acc: 0.8521653413772583\n",
      "step: 12925\n",
      "train: loss: 709375.25 acc: 0.6428985595703125  val: loss: 645005.375 acc: 0.6934690475463867\n",
      "step: 12930\n",
      "train: loss: 880024.375 acc: 0.7098007798194885  val: loss: 285232.96875 acc: 0.5590136051177979\n",
      "step: 12935\n",
      "train: loss: 57937.62890625 acc: 0.9506739377975464  val: loss: 1514364.375 acc: 0.6610634326934814\n",
      "step: 12940\n",
      "train: loss: 73251.515625 acc: 0.9283584356307983  val: loss: 694094.375 acc: 0.7160595059394836\n",
      "step: 12945\n",
      "train: loss: 22647.7265625 acc: 0.9822113513946533  val: loss: 710584.1875 acc: 0.6997641324996948\n",
      "step: 12950\n",
      "train: loss: 20874.1640625 acc: 0.9829418659210205  val: loss: 1598789.25 acc: 0.6406935453414917\n",
      "step: 12955\n",
      "train: loss: 284623.34375 acc: 0.8543621897697449  val: loss: 830433.875 acc: 0.7012225389480591\n",
      "step: 12960\n",
      "train: loss: 398441.84375 acc: 0.8007516860961914  val: loss: 849449.625 acc: 0.6766514778137207\n",
      "step: 12965\n",
      "train: loss: 25424.015625 acc: 0.9771734476089478  val: loss: 2094277.25 acc: 0.5925090312957764\n",
      "step: 12970\n",
      "train: loss: 329619.53125 acc: 0.8515137434005737  val: loss: 1690549.75 acc: 0.6805264949798584\n",
      "step: 12975\n",
      "train: loss: 68611.3515625 acc: 0.9098418951034546  val: loss: 575871.125 acc: 0.7241430878639221\n",
      "step: 12980\n",
      "train: loss: 121422.25 acc: 0.9027814269065857  val: loss: 4164360.75 acc: 0.5283323526382446\n",
      "step: 12985\n",
      "train: loss: 90676.46875 acc: 0.9245477318763733  val: loss: 1750946.625 acc: 0.5712932348251343\n",
      "step: 12990\n",
      "train: loss: 629297.9375 acc: 0.7044152617454529  val: loss: 1255425.125 acc: 0.6520580053329468\n",
      "step: 12995\n",
      "train: loss: 747282.3125 acc: 0.6530429124832153  val: loss: 1763712.625 acc: 0.5857893228530884\n",
      "step: 13000\n",
      "train: loss: 71375.7421875 acc: 0.9339195489883423  val: loss: 1894772.625 acc: 0.597969114780426\n",
      "step: 13005\n",
      "train: loss: 307335.28125 acc: 0.8148971199989319  val: loss: 2518641.75 acc: 0.5336464643478394\n",
      "step: 13010\n",
      "train: loss: 1495209.25 acc: 0.6961443424224854  val: loss: 625332.75 acc: 0.7341715097427368\n",
      "step: 13015\n",
      "train: loss: 1127151.25 acc: 0.83077472448349  val: loss: 853861.875 acc: 0.6820282936096191\n",
      "step: 13020\n",
      "train: loss: 855730.1875 acc: 0.8832421898841858  val: loss: 436369.8125 acc: 0.9263725280761719\n",
      "step: 13025\n",
      "train: loss: 300131.875 acc: 0.9639409184455872  val: loss: 2340918.0 acc: 0.5494071245193481\n",
      "step: 13030\n",
      "train: loss: 245959.3125 acc: 0.9692663550376892  val: loss: 943766.8125 acc: 0.760265588760376\n",
      "step: 13035\n",
      "train: loss: 67132.0703125 acc: 0.9817774295806885  val: loss: 369962.375 acc: 0.9281156659126282\n",
      "step: 13040\n",
      "train: loss: 195170.015625 acc: 0.9827841520309448  val: loss: 780528.6875 acc: 0.9038112759590149\n",
      "step: 13045\n",
      "train: loss: 135601.1875 acc: 0.9892643690109253  val: loss: 615100.5 acc: 0.9317265152931213\n",
      "step: 13050\n",
      "train: loss: 161462.609375 acc: 0.9880316257476807  val: loss: 176501.203125 acc: 0.9529940485954285\n",
      "step: 13055\n",
      "train: loss: 68186.7578125 acc: 0.9923951029777527  val: loss: 603313.75 acc: 0.9191027879714966\n",
      "step: 13060\n",
      "train: loss: 35743.21875 acc: 0.9927170872688293  val: loss: 813094.125 acc: 0.9141219258308411\n",
      "step: 13065\n",
      "train: loss: 49137.4453125 acc: 0.9886233806610107  val: loss: 175296.4375 acc: 0.9470446705818176\n",
      "step: 13070\n",
      "train: loss: 11797.8134765625 acc: 0.9589759707450867  val: loss: 1403195.625 acc: 0.5156427621841431\n",
      "step: 13075\n",
      "train: loss: 10420.9853515625 acc: 0.9678969383239746  val: loss: 676192.6875 acc: 0.8642964363098145\n",
      "step: 13080\n",
      "train: loss: 24871.26953125 acc: 0.98636794090271  val: loss: 1054287.875 acc: 0.5767133235931396\n",
      "step: 13085\n",
      "train: loss: 15573.1396484375 acc: 0.9901908040046692  val: loss: 1032746.0 acc: 0.9198532104492188\n",
      "step: 13090\n",
      "train: loss: 19082.517578125 acc: 0.9888200759887695  val: loss: 1118666.5 acc: 0.8529068231582642\n",
      "step: 13095\n",
      "train: loss: 4594.22998046875 acc: 0.9853475689888  val: loss: 1957506.5 acc: 0.4096912741661072\n",
      "step: 13100\n",
      "train: loss: 46597.06640625 acc: 0.959367036819458  val: loss: 1763332.75 acc: 0.23336845636367798\n",
      "step: 13105\n",
      "train: loss: 14660.8740234375 acc: 0.962759256362915  val: loss: 1622476.75 acc: 0.6840941905975342\n",
      "step: 13110\n",
      "train: loss: 5998.31005859375 acc: 0.9802159070968628  val: loss: 2388306.0 acc: 0.6733286380767822\n",
      "step: 13115\n",
      "train: loss: 52034.61328125 acc: 0.9823994040489197  val: loss: 780669.875 acc: 0.7531738877296448\n",
      "step: 13120\n",
      "train: loss: 16806.3515625 acc: 0.9886257648468018  val: loss: 1267206.0 acc: 0.8493530750274658\n",
      "step: 13125\n",
      "train: loss: 13555.390625 acc: 0.9913921356201172  val: loss: 1231388.75 acc: 0.34627729654312134\n",
      "step: 13130\n",
      "train: loss: 18360.345703125 acc: 0.9908415675163269  val: loss: 923221.0 acc: 0.5356862545013428\n",
      "step: 13135\n",
      "train: loss: 26031.876953125 acc: 0.9876833558082581  val: loss: 655430.25 acc: 0.9043645262718201\n",
      "step: 13140\n",
      "train: loss: 9353.10546875 acc: 0.987428605556488  val: loss: 1605008.5 acc: 0.7603562474250793\n",
      "step: 13145\n",
      "train: loss: 6281.92529296875 acc: 0.9951955676078796  val: loss: 2206725.25 acc: 0.7763782143592834\n",
      "step: 13150\n",
      "train: loss: 12861.1611328125 acc: 0.9958689212799072  val: loss: 792273.0625 acc: 0.8798086643218994\n",
      "step: 13155\n",
      "train: loss: 30792.95703125 acc: 0.9896541237831116  val: loss: 2229595.75 acc: 0.6131529808044434\n",
      "step: 13160\n",
      "train: loss: 35766.31640625 acc: 0.9874932169914246  val: loss: 1934799.25 acc: 0.38945358991622925\n",
      "step: 13165\n",
      "train: loss: 19422.224609375 acc: 0.9855871200561523  val: loss: 2275869.75 acc: 0.17630064487457275\n",
      "step: 13170\n",
      "train: loss: 67438.6953125 acc: 0.9823306202888489  val: loss: 3012435.75 acc: -0.05971837043762207\n",
      "step: 13175\n",
      "train: loss: 88754.8359375 acc: 0.9668926000595093  val: loss: 819062.5 acc: 0.924588680267334\n",
      "step: 13180\n",
      "train: loss: 58476.609375 acc: 0.9760981202125549  val: loss: 3291373.0 acc: 0.030293643474578857\n",
      "step: 13185\n",
      "train: loss: 197634.8125 acc: 0.9630624651908875  val: loss: 1344365.375 acc: 0.7884012460708618\n",
      "step: 13190\n",
      "train: loss: 385526.15625 acc: 0.9616416692733765  val: loss: 1196434.5 acc: 0.5810638666152954\n",
      "step: 13195\n",
      "train: loss: 565440.375 acc: 0.9449788928031921  val: loss: 729652.8125 acc: 0.8944473266601562\n",
      "step: 13200\n",
      "train: loss: 124330.2734375 acc: 0.9855654239654541  val: loss: 1190327.25 acc: 0.8585823774337769\n",
      "step: 13205\n",
      "train: loss: 120043.6796875 acc: 0.9794787764549255  val: loss: 518708.125 acc: 0.9015554189682007\n",
      "step: 13210\n",
      "train: loss: 138824.671875 acc: 0.9807619452476501  val: loss: 532237.0 acc: 0.893681526184082\n",
      "step: 13215\n",
      "train: loss: 877623.5625 acc: 0.911159336566925  val: loss: 1418986.75 acc: 0.6695727109909058\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 13220\n",
      "train: loss: 2041665.75 acc: 0.8271687626838684  val: loss: 2496250.75 acc: 0.2663151025772095\n",
      "step: 13225\n",
      "train: loss: 676448.125 acc: 0.9419204592704773  val: loss: 560332.25 acc: 0.8731526732444763\n",
      "step: 13230\n",
      "train: loss: 960078.25 acc: 0.9621513485908508  val: loss: 561933.5 acc: 0.8670075535774231\n",
      "step: 13235\n",
      "train: loss: 2021052.75 acc: 0.9387540221214294  val: loss: 208219.0 acc: 0.9556369781494141\n",
      "step: 13240\n",
      "train: loss: 1387245.0 acc: 0.9389880895614624  val: loss: 570055.5 acc: 0.9318717122077942\n",
      "step: 13245\n",
      "train: loss: 561523.125 acc: 0.9587726593017578  val: loss: 184239.484375 acc: 0.9721123576164246\n",
      "step: 13250\n",
      "train: loss: 470220.21875 acc: 0.9490012526512146  val: loss: 222171.1875 acc: 0.8808850646018982\n",
      "step: 13255\n",
      "train: loss: 268446.59375 acc: 0.9646344184875488  val: loss: 1000249.3125 acc: 0.31812626123428345\n",
      "step: 13260\n",
      "train: loss: 441998.125 acc: 0.958173930644989  val: loss: 1335677.625 acc: 0.6936243772506714\n",
      "step: 13265\n",
      "train: loss: 3397175.0 acc: -0.10515260696411133  val: loss: 465020.5 acc: 0.8607240915298462\n",
      "step: 13270\n",
      "train: loss: 902810.875 acc: 0.7556418180465698  val: loss: 344310.375 acc: 0.8882038593292236\n",
      "step: 13275\n",
      "train: loss: 1582108.0 acc: 0.026383280754089355  val: loss: 910850.875 acc: 0.7576491236686707\n",
      "step: 13280\n",
      "train: loss: 1327348.125 acc: 0.7785717844963074  val: loss: 408374.9375 acc: 0.7486878633499146\n",
      "step: 13285\n",
      "train: loss: 1133997.5 acc: 0.5002312660217285  val: loss: 1095072.0 acc: 0.8253253698348999\n",
      "step: 13290\n",
      "train: loss: 1517320.5 acc: 0.1551281213760376  val: loss: 2091520.25 acc: 0.6830912828445435\n",
      "step: 13295\n",
      "train: loss: 444470.46875 acc: 0.8087990283966064  val: loss: 1015065.125 acc: 0.6764315962791443\n",
      "step: 13300\n",
      "train: loss: 805352.625 acc: 0.7938815355300903  val: loss: 4084102.0 acc: 0.5990015268325806\n",
      "step: 13305\n",
      "train: loss: 61939.54296875 acc: 0.9421884417533875  val: loss: 1450243.25 acc: 0.6527847051620483\n",
      "step: 13310\n",
      "train: loss: 149284.984375 acc: 0.8972095251083374  val: loss: 63791.6953125 acc: 0.9182174205780029\n",
      "step: 13315\n",
      "train: loss: 89697.3359375 acc: 0.9361305236816406  val: loss: 1251562.875 acc: 0.6548458337783813\n",
      "step: 13320\n",
      "train: loss: 175439.96875 acc: 0.8687522411346436  val: loss: 2967359.0 acc: 0.5571394562721252\n",
      "step: 13325\n",
      "train: loss: 414633.8125 acc: 0.8220670223236084  val: loss: 1860671.75 acc: 0.6072002649307251\n",
      "step: 13330\n",
      "train: loss: 55188.30859375 acc: 0.9579128623008728  val: loss: 868870.375 acc: 0.6895413398742676\n",
      "step: 13335\n",
      "train: loss: 63746.2421875 acc: 0.9577995538711548  val: loss: 1500123.0 acc: 0.5589361190795898\n",
      "step: 13340\n",
      "train: loss: 39235.33984375 acc: 0.9479709267616272  val: loss: 6568275.0 acc: 0.5373658537864685\n",
      "step: 13345\n",
      "train: loss: 109471.7265625 acc: 0.9156599044799805  val: loss: 3517464.5 acc: 0.5671195983886719\n",
      "step: 13350\n",
      "train: loss: 207531.125 acc: 0.843715250492096  val: loss: 1815245.0 acc: 0.6559718251228333\n",
      "step: 13355\n",
      "train: loss: 94216.28125 acc: 0.9168193936347961  val: loss: 2401302.25 acc: 0.615365207195282\n",
      "step: 13360\n",
      "train: loss: 1009583.4375 acc: 0.6870405673980713  val: loss: 4044737.5 acc: 0.5516053438186646\n",
      "step: 13365\n",
      "train: loss: 36978.43359375 acc: 0.9655027389526367  val: loss: 3036856.25 acc: 0.5897361040115356\n",
      "step: 13370\n",
      "train: loss: 87784.0546875 acc: 0.9087035655975342  val: loss: 523256.21875 acc: 0.7273904085159302\n",
      "step: 13375\n",
      "train: loss: 1314088.0 acc: 0.6446611881256104  val: loss: 848788.0 acc: 0.8042014837265015\n",
      "step: 13380\n",
      "train: loss: 1334530.5 acc: 0.7841753363609314  val: loss: 2570819.0 acc: 0.768001914024353\n",
      "step: 13385\n",
      "train: loss: 1693351.75 acc: 0.800326943397522  val: loss: 905133.875 acc: 0.823698878288269\n",
      "step: 13390\n",
      "train: loss: 290420.0625 acc: 0.9741726517677307  val: loss: 780510.6875 acc: 0.8491873741149902\n",
      "step: 13395\n",
      "train: loss: 573415.625 acc: 0.8715181350708008  val: loss: 1816228.875 acc: 0.7665873169898987\n",
      "step: 13400\n",
      "train: loss: 196954.015625 acc: 0.9672860503196716  val: loss: 163771.671875 acc: 0.9602705836296082\n",
      "step: 13405\n",
      "train: loss: 246226.703125 acc: 0.9784605503082275  val: loss: 1637969.0 acc: 0.7951508164405823\n",
      "step: 13410\n",
      "train: loss: 307351.65625 acc: 0.9707707762718201  val: loss: 876058.5 acc: 0.7939357161521912\n",
      "step: 13415\n",
      "train: loss: 211916.5 acc: 0.9850912094116211  val: loss: 1623944.0 acc: 0.7395087480545044\n",
      "step: 13420\n",
      "train: loss: 94201.84375 acc: 0.988187313079834  val: loss: 1111253.125 acc: 0.7152358293533325\n",
      "step: 13425\n",
      "train: loss: 60160.42578125 acc: 0.9880658388137817  val: loss: 2006164.75 acc: 0.6238499283790588\n",
      "step: 13430\n",
      "train: loss: 28624.658203125 acc: 0.9817081093788147  val: loss: 1293920.125 acc: 0.796147346496582\n",
      "step: 13435\n",
      "train: loss: 14504.7509765625 acc: 0.9837526679039001  val: loss: 1675640.75 acc: 0.7163933515548706\n",
      "step: 13440\n",
      "train: loss: 36449.44921875 acc: 0.9735592007637024  val: loss: 639237.25 acc: 0.8969922065734863\n",
      "step: 13445\n",
      "train: loss: 13685.9892578125 acc: 0.9807364344596863  val: loss: 988315.75 acc: 0.8446817398071289\n",
      "step: 13450\n",
      "train: loss: 22506.564453125 acc: 0.9843170046806335  val: loss: 485681.90625 acc: 0.9502477049827576\n",
      "step: 13455\n",
      "train: loss: 22819.814453125 acc: 0.9473829865455627  val: loss: 2066313.5 acc: 0.4817606210708618\n",
      "step: 13460\n",
      "train: loss: 12897.3125 acc: 0.9742276072502136  val: loss: 2085139.375 acc: 0.2805572748184204\n",
      "step: 13465\n",
      "train: loss: 4525.52197265625 acc: 0.9914575219154358  val: loss: 1047508.125 acc: 0.5778697729110718\n",
      "step: 13470\n",
      "train: loss: 17355.10546875 acc: 0.974768340587616  val: loss: 2260447.25 acc: 0.6790077090263367\n",
      "step: 13475\n",
      "train: loss: 12846.9306640625 acc: 0.9653827548027039  val: loss: 1717625.0 acc: 0.03739362955093384\n",
      "step: 13480\n",
      "train: loss: 47230.2890625 acc: 0.9738161563873291  val: loss: 1558318.875 acc: 0.7671465277671814\n",
      "step: 13485\n",
      "train: loss: 22459.841796875 acc: 0.9848187565803528  val: loss: 730366.625 acc: 0.8892903923988342\n",
      "step: 13490\n",
      "train: loss: 33180.65625 acc: 0.9793345928192139  val: loss: 314064.34375 acc: 0.8756769895553589\n",
      "step: 13495\n",
      "train: loss: 23137.78125 acc: 0.9897738695144653  val: loss: 2174859.0 acc: 0.7363401651382446\n",
      "step: 13500\n",
      "train: loss: 12015.8212890625 acc: 0.9896735548973083  val: loss: 2059432.625 acc: 0.36183494329452515\n",
      "step: 13505\n",
      "train: loss: 6031.63720703125 acc: 0.992305338382721  val: loss: 3439760.5 acc: -0.10487997531890869\n",
      "step: 13510\n",
      "train: loss: 8404.7373046875 acc: 0.9961149096488953  val: loss: 1447452.5 acc: 0.6449875831604004\n",
      "step: 13515\n",
      "train: loss: 17115.751953125 acc: 0.9914708733558655  val: loss: 1123753.5 acc: 0.826000988483429\n",
      "step: 13520\n",
      "train: loss: 27685.908203125 acc: 0.9906079173088074  val: loss: 1144155.375 acc: 0.7283387184143066\n",
      "step: 13525\n",
      "train: loss: 23369.27734375 acc: 0.9930625557899475  val: loss: 316649.28125 acc: 0.9120036363601685\n",
      "step: 13530\n",
      "train: loss: 23577.525390625 acc: 0.9858964681625366  val: loss: 1276655.25 acc: 0.2625235319137573\n",
      "step: 13535\n",
      "train: loss: 205590.5625 acc: 0.9557247757911682  val: loss: 1066416.125 acc: 0.8917708396911621\n",
      "step: 13540\n",
      "train: loss: 44162.56640625 acc: 0.9645111560821533  val: loss: 941122.25 acc: 0.8142505288124084\n",
      "step: 13545\n",
      "train: loss: 179338.5625 acc: 0.904291033744812  val: loss: 148090.109375 acc: 0.9743563532829285\n",
      "step: 13550\n",
      "train: loss: 424099.15625 acc: 0.9185436964035034  val: loss: 235965.0 acc: 0.9610728025436401\n",
      "step: 13555\n",
      "train: loss: 100954.2265625 acc: 0.9872297048568726  val: loss: 3993347.25 acc: -4.293115615844727\n",
      "step: 13560\n",
      "train: loss: 133054.03125 acc: 0.9860689640045166  val: loss: 706817.875 acc: 0.7997990846633911\n",
      "step: 13565\n",
      "train: loss: 78517.0390625 acc: 0.9927850961685181  val: loss: 2773408.5 acc: 0.19096380472183228\n",
      "step: 13570\n",
      "train: loss: 201514.0625 acc: 0.9742074012756348  val: loss: 272950.84375 acc: 0.9421754479408264\n",
      "step: 13575\n",
      "train: loss: 222388.609375 acc: 0.9695999026298523  val: loss: 1684556.5 acc: 0.4381936192512512\n",
      "step: 13580\n",
      "train: loss: 1083230.875 acc: 0.9438753128051758  val: loss: 304660.4375 acc: 0.8581312894821167\n",
      "step: 13585\n",
      "train: loss: 420094.53125 acc: 0.9583794474601746  val: loss: 733979.0625 acc: 0.702221155166626\n",
      "step: 13590\n",
      "train: loss: 281599.71875 acc: 0.9688805937767029  val: loss: 1528418.875 acc: 0.5479450225830078\n",
      "step: 13595\n",
      "train: loss: 1032809.8125 acc: 0.9610860347747803  val: loss: 626700.8125 acc: 0.8418781161308289\n",
      "step: 13600\n",
      "train: loss: 2205280.5 acc: 0.9479677677154541  val: loss: 1813081.5 acc: 0.4465221166610718\n",
      "step: 13605\n",
      "train: loss: 1168856.5 acc: 0.9514471292495728  val: loss: 1193153.625 acc: 0.8471821546554565\n",
      "step: 13610\n",
      "train: loss: 666294.125 acc: 0.9712802171707153  val: loss: 780669.4375 acc: 0.8809294700622559\n",
      "step: 13615\n",
      "train: loss: 830112.5625 acc: 0.9441814422607422  val: loss: 97548.03125 acc: 0.8928326964378357\n",
      "step: 13620\n",
      "train: loss: 400565.46875 acc: 0.9678053259849548  val: loss: 287322.21875 acc: 0.9102705717086792\n",
      "step: 13625\n",
      "train: loss: 409526.9375 acc: 0.9455406069755554  val: loss: 89696.796875 acc: 0.9407113194465637\n",
      "step: 13630\n",
      "train: loss: 2637450.0 acc: 0.46763044595718384  val: loss: 1104023.875 acc: 0.5644389390945435\n",
      "step: 13635\n",
      "train: loss: 482907.09375 acc: 0.8895710706710815  val: loss: 1279240.125 acc: 0.7846840620040894\n",
      "step: 13640\n",
      "train: loss: 416438.125 acc: 0.8878774642944336  val: loss: 526475.875 acc: 0.7979534864425659\n",
      "step: 13645\n",
      "train: loss: 594611.625 acc: 0.7611256837844849  val: loss: 947370.8125 acc: 0.8497873544692993\n",
      "step: 13650\n",
      "train: loss: 610050.1875 acc: 0.8655651211738586  val: loss: 821091.375 acc: 0.8466532230377197\n",
      "step: 13655\n",
      "train: loss: 1299635.25 acc: 0.4685346484184265  val: loss: 1775809.625 acc: 0.7802912592887878\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 13660\n",
      "train: loss: 835424.0 acc: 0.6754528284072876  val: loss: 1054877.0 acc: 0.6850990056991577\n",
      "step: 13665\n",
      "train: loss: 1877831.375 acc: 0.6776474714279175  val: loss: 2783996.5 acc: 0.6569578647613525\n",
      "step: 13670\n",
      "train: loss: 149562.203125 acc: 0.8849815130233765  val: loss: 1779313.0 acc: 0.6255549788475037\n",
      "step: 13675\n",
      "train: loss: 257023.484375 acc: 0.8667266368865967  val: loss: 2913682.5 acc: 0.6467678546905518\n",
      "step: 13680\n",
      "train: loss: 32332.462890625 acc: 0.9721813201904297  val: loss: 5236439.5 acc: 0.6261636018753052\n",
      "step: 13685\n",
      "train: loss: 54617.875 acc: 0.9567514061927795  val: loss: 2377196.25 acc: 0.6738225221633911\n",
      "step: 13690\n",
      "train: loss: 52354.0390625 acc: 0.9585325121879578  val: loss: 1923687.375 acc: 0.7187782526016235\n",
      "step: 13695\n",
      "train: loss: 118827.46875 acc: 0.9088867902755737  val: loss: 1194891.0 acc: 0.68892502784729\n",
      "step: 13700\n",
      "train: loss: 436771.0625 acc: 0.7804005146026611  val: loss: 2714739.75 acc: 0.6048505902290344\n",
      "step: 13705\n",
      "train: loss: 53905.58984375 acc: 0.9459933042526245  val: loss: 6618420.0 acc: 0.5823714733123779\n",
      "step: 13710\n",
      "train: loss: 192029.109375 acc: 0.862470269203186  val: loss: 933927.9375 acc: 0.6339130401611328\n",
      "step: 13715\n",
      "train: loss: 193987.359375 acc: 0.8575257658958435  val: loss: 928535.125 acc: 0.7583911418914795\n",
      "step: 13720\n",
      "train: loss: 352968.0 acc: 0.7198477387428284  val: loss: 2020832.375 acc: 0.6378611326217651\n",
      "step: 13725\n",
      "train: loss: 253916.703125 acc: 0.827141523361206  val: loss: 1656915.625 acc: 0.6435443162918091\n",
      "step: 13730\n",
      "train: loss: 146495.078125 acc: 0.8898731470108032  val: loss: 5248967.0 acc: 0.5693376660346985\n",
      "step: 13735\n",
      "train: loss: 120531.6953125 acc: 0.8843145370483398  val: loss: 2025618.625 acc: 0.6386630535125732\n",
      "step: 13740\n",
      "train: loss: 404222.25 acc: 0.8010924458503723  val: loss: 1988274.0 acc: 0.6232531666755676\n",
      "step: 13745\n",
      "train: loss: 2320618.0 acc: 0.6982828974723816  val: loss: 691960.8125 acc: 0.8455795049667358\n",
      "step: 13750\n",
      "train: loss: 930689.5625 acc: 0.8579071760177612  val: loss: 998710.0 acc: 0.6672404408454895\n",
      "step: 13755\n",
      "train: loss: 559471.5 acc: 0.9588457345962524  val: loss: 906995.0625 acc: 0.7323670387268066\n",
      "step: 13760\n",
      "train: loss: 350242.78125 acc: 0.9552298784255981  val: loss: 1633958.625 acc: 0.57584148645401\n",
      "step: 13765\n",
      "train: loss: 121870.59375 acc: 0.976284921169281  val: loss: 316045.1875 acc: 0.7947229743003845\n",
      "step: 13770\n",
      "train: loss: 163007.671875 acc: 0.978461503982544  val: loss: 1257851.625 acc: 0.6152874231338501\n",
      "step: 13775\n",
      "train: loss: 81410.6484375 acc: 0.9914540648460388  val: loss: 413054.09375 acc: 0.9025912880897522\n",
      "step: 13780\n",
      "train: loss: 80003.5078125 acc: 0.9947681427001953  val: loss: 1495267.0 acc: -0.03280973434448242\n",
      "step: 13785\n",
      "train: loss: 87654.03125 acc: 0.9895398020744324  val: loss: 2123916.0 acc: 0.7239692211151123\n",
      "step: 13790\n",
      "train: loss: 75843.0703125 acc: 0.989744246006012  val: loss: 1074508.25 acc: 0.8786368370056152\n",
      "step: 13795\n",
      "train: loss: 25341.75390625 acc: 0.9901255369186401  val: loss: 995678.9375 acc: 0.7574564218521118\n",
      "step: 13800\n",
      "train: loss: 9417.6767578125 acc: 0.9961621165275574  val: loss: 484238.375 acc: 0.7488499283790588\n",
      "step: 13805\n",
      "train: loss: 3451.4501953125 acc: 0.9922898411750793  val: loss: 298612.96875 acc: 0.950069785118103\n",
      "step: 13810\n",
      "train: loss: 13548.2802734375 acc: 0.9805326461791992  val: loss: 947431.4375 acc: 0.815625011920929\n",
      "step: 13815\n",
      "train: loss: 22823.53125 acc: 0.9446166157722473  val: loss: 375656.09375 acc: 0.9429807066917419\n",
      "step: 13820\n",
      "train: loss: 23372.408203125 acc: 0.9800405502319336  val: loss: 1714242.75 acc: 0.6844085454940796\n",
      "step: 13825\n",
      "train: loss: 29950.595703125 acc: 0.9537709951400757  val: loss: 1687872.25 acc: 0.7917507886886597\n",
      "step: 13830\n",
      "train: loss: 22783.896484375 acc: 0.98623126745224  val: loss: 577586.3125 acc: 0.8733623027801514\n",
      "step: 13835\n",
      "train: loss: 15292.4287109375 acc: 0.9735822081565857  val: loss: 2218150.0 acc: 0.8237792253494263\n",
      "step: 13840\n",
      "train: loss: 4766.1298828125 acc: 0.987212598323822  val: loss: 1169938.375 acc: 0.008383512496948242\n",
      "step: 13845\n",
      "train: loss: 20384.7109375 acc: 0.9733043909072876  val: loss: 801231.8125 acc: 0.812513530254364\n",
      "step: 13850\n",
      "train: loss: 17771.896484375 acc: 0.9885361790657043  val: loss: 1492101.5 acc: 0.6064275503158569\n",
      "step: 13855\n",
      "train: loss: 42193.16796875 acc: 0.9800005555152893  val: loss: 1201886.25 acc: 0.8270578980445862\n",
      "step: 13860\n",
      "train: loss: 27359.13671875 acc: 0.9767726063728333  val: loss: 323637.1875 acc: 0.8729838728904724\n",
      "step: 13865\n",
      "train: loss: 23270.080078125 acc: 0.9830395579338074  val: loss: 595744.875 acc: 0.6418747305870056\n",
      "step: 13870\n",
      "train: loss: 11733.234375 acc: 0.9868068099021912  val: loss: 1387530.5 acc: 0.4506460428237915\n",
      "step: 13875\n",
      "train: loss: 5572.64697265625 acc: 0.9955638647079468  val: loss: 294205.0 acc: 0.9577897191047668\n",
      "step: 13880\n",
      "train: loss: 40206.23828125 acc: 0.9630494117736816  val: loss: 712959.0625 acc: 0.804894208908081\n",
      "step: 13885\n",
      "train: loss: 21197.623046875 acc: 0.9939455986022949  val: loss: 601301.375 acc: 0.9067326784133911\n",
      "step: 13890\n",
      "train: loss: 19464.578125 acc: 0.9947637319564819  val: loss: 2244451.25 acc: -0.4143509864807129\n",
      "step: 13895\n",
      "train: loss: 42651.20703125 acc: 0.9908056855201721  val: loss: 1479345.25 acc: 0.770743727684021\n",
      "step: 13900\n",
      "train: loss: 72186.6796875 acc: 0.9763866662979126  val: loss: 403046.96875 acc: 0.9236152172088623\n",
      "step: 13905\n",
      "train: loss: 91721.0234375 acc: 0.9810219407081604  val: loss: 216151.53125 acc: 0.9494954943656921\n",
      "step: 13910\n",
      "train: loss: 38281.796875 acc: 0.9375160336494446  val: loss: 265687.90625 acc: 0.9383972883224487\n",
      "step: 13915\n",
      "train: loss: 639210.25 acc: 0.8838242888450623  val: loss: 89970.15625 acc: 0.8789606094360352\n",
      "step: 13920\n",
      "train: loss: 112146.3125 acc: 0.9858189225196838  val: loss: 706604.4375 acc: 0.6858536601066589\n",
      "step: 13925\n",
      "train: loss: 353700.84375 acc: 0.9695858955383301  val: loss: 1080323.875 acc: 0.8203218579292297\n",
      "step: 13930\n",
      "train: loss: 49169.765625 acc: 0.994512677192688  val: loss: 2172647.0 acc: 0.5284448862075806\n",
      "step: 13935\n",
      "train: loss: 374564.9375 acc: 0.9570209980010986  val: loss: 396584.1875 acc: 0.8421592116355896\n",
      "step: 13940\n",
      "train: loss: 201378.734375 acc: 0.9646627902984619  val: loss: 198011.546875 acc: 0.9165920615196228\n",
      "step: 13945\n",
      "train: loss: 1004764.0625 acc: 0.9555407166481018  val: loss: 240601.6875 acc: 0.9402858018875122\n",
      "step: 13950\n",
      "train: loss: 351125.5 acc: 0.9622863531112671  val: loss: 409811.53125 acc: 0.8695533275604248\n",
      "step: 13955\n",
      "train: loss: 321458.46875 acc: 0.9691163897514343  val: loss: 264581.625 acc: 0.9132946729660034\n",
      "step: 13960\n",
      "train: loss: 742569.5625 acc: 0.9765854477882385  val: loss: 913984.4375 acc: 0.7459704875946045\n",
      "step: 13965\n",
      "train: loss: 2181341.0 acc: 0.9188253283500671  val: loss: 1433566.625 acc: 0.8380700349807739\n",
      "step: 13970\n",
      "train: loss: 1819957.0 acc: 0.9386675357818604  val: loss: 889220.8125 acc: 0.6627607345581055\n",
      "step: 13975\n",
      "train: loss: 973333.8125 acc: 0.9406020045280457  val: loss: 1194039.5 acc: 0.8305116891860962\n",
      "step: 13980\n",
      "train: loss: 327773.78125 acc: 0.9717864990234375  val: loss: 652389.25 acc: 0.9307090640068054\n",
      "step: 13985\n",
      "train: loss: 1603992.875 acc: 0.8915086388587952  val: loss: 490490.46875 acc: 0.944973886013031\n",
      "step: 13990\n",
      "train: loss: 306515.53125 acc: 0.9649737477302551  val: loss: 335446.5625 acc: 0.8838598728179932\n",
      "step: 13995\n",
      "train: loss: 1489924.125 acc: 0.8171769976615906  val: loss: 688932.125 acc: 0.7793610692024231\n",
      "step: 14000\n",
      "train: loss: 1279012.375 acc: 0.5769506096839905  val: loss: 823527.0 acc: 0.8933824300765991\n",
      "step: 14005\n",
      "train: loss: 1053531.75 acc: 0.6183897256851196  val: loss: 2086951.5 acc: 0.7897833585739136\n",
      "step: 14010\n",
      "train: loss: 187405.90625 acc: 0.8963469862937927  val: loss: 2196105.5 acc: 0.7466443777084351\n",
      "step: 14015\n",
      "train: loss: 925789.8125 acc: 0.7539495229721069  val: loss: 2495784.25 acc: 0.743556022644043\n",
      "step: 14020\n",
      "train: loss: 978721.4375 acc: 0.7122848033905029  val: loss: 2197955.0 acc: 0.7750868797302246\n",
      "step: 14025\n",
      "train: loss: 964029.1875 acc: 0.6641618013381958  val: loss: 2034651.0 acc: 0.6751154661178589\n",
      "step: 14030\n",
      "train: loss: 148637.109375 acc: 0.8865264654159546  val: loss: 667086.3125 acc: 0.6837494373321533\n",
      "step: 14035\n",
      "train: loss: 68485.9453125 acc: 0.934966504573822  val: loss: 3360313.5 acc: 0.5861049294471741\n",
      "step: 14040\n",
      "train: loss: 28672.69140625 acc: 0.9766819477081299  val: loss: 1632108.125 acc: 0.6662236452102661\n",
      "step: 14045\n",
      "train: loss: 27153.650390625 acc: 0.9785472750663757  val: loss: 1413987.0 acc: 0.656873345375061\n",
      "step: 14050\n",
      "train: loss: 91370.84375 acc: 0.9422882199287415  val: loss: 1171530.625 acc: 0.6872360706329346\n",
      "step: 14055\n",
      "train: loss: 240237.03125 acc: 0.8457422256469727  val: loss: 412322.21875 acc: 0.8251218199729919\n",
      "step: 14060\n",
      "train: loss: 265868.6875 acc: 0.8477436304092407  val: loss: 752228.75 acc: 0.7721832990646362\n",
      "step: 14065\n",
      "train: loss: 52363.55859375 acc: 0.9570596814155579  val: loss: 1316709.125 acc: 0.6658103466033936\n",
      "step: 14070\n",
      "train: loss: 22658.171875 acc: 0.977998673915863  val: loss: 452871.71875 acc: 0.7856682538986206\n",
      "step: 14075\n",
      "train: loss: 126125.1015625 acc: 0.893925130367279  val: loss: 1161911.0 acc: 0.6125211715698242\n",
      "step: 14080\n",
      "train: loss: 114382.140625 acc: 0.9056607484817505  val: loss: 4685450.5 acc: 0.5672336220741272\n",
      "step: 14085\n",
      "train: loss: 160861.890625 acc: 0.8300331830978394  val: loss: 506392.875 acc: 0.7430575489997864\n",
      "step: 14090\n",
      "train: loss: 225095.125 acc: 0.8097500205039978  val: loss: 3031688.5 acc: 0.5701301097869873\n",
      "step: 14095\n",
      "train: loss: 52763.9375 acc: 0.9512026906013489  val: loss: 2346583.25 acc: 0.606271505355835\n",
      "step: 14100\n",
      "train: loss: 217811.015625 acc: 0.8326180577278137  val: loss: 599815.9375 acc: 0.7142804265022278\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 14105\n",
      "train: loss: 1072768.375 acc: 0.6845126152038574  val: loss: 174521.765625 acc: 0.8486914038658142\n",
      "step: 14110\n",
      "train: loss: 1159232.25 acc: 0.7029604911804199  val: loss: 1786320.5 acc: 0.7448130249977112\n",
      "step: 14115\n",
      "train: loss: 1477610.75 acc: 0.7436308860778809  val: loss: 2469670.0 acc: 0.8535740375518799\n",
      "step: 14120\n",
      "train: loss: 839066.9375 acc: 0.9256579279899597  val: loss: 367279.5 acc: 0.954848051071167\n",
      "step: 14125\n",
      "train: loss: 910170.75 acc: 0.8924155235290527  val: loss: 1189944.75 acc: 0.8457721471786499\n",
      "step: 14130\n",
      "train: loss: 240734.28125 acc: 0.9685957431793213  val: loss: 874694.125 acc: 0.8459880948066711\n",
      "step: 14135\n",
      "train: loss: 186446.109375 acc: 0.9657509922981262  val: loss: 1520113.5 acc: 0.7098714709281921\n",
      "step: 14140\n",
      "train: loss: 190792.25 acc: 0.9845885634422302  val: loss: 1516618.75 acc: 0.3423857092857361\n",
      "step: 14145\n",
      "train: loss: 153113.84375 acc: 0.9898443222045898  val: loss: 1413049.625 acc: 0.632474958896637\n",
      "step: 14150\n",
      "train: loss: 142723.640625 acc: 0.9858156442642212  val: loss: 1675022.375 acc: 0.7157282829284668\n",
      "step: 14155\n",
      "train: loss: 93781.609375 acc: 0.985751211643219  val: loss: 2120724.0 acc: -1.3214118480682373\n",
      "step: 14160\n",
      "train: loss: 31437.640625 acc: 0.9851459860801697  val: loss: 1714466.0 acc: 0.3321455717086792\n",
      "step: 14165\n",
      "train: loss: 26091.990234375 acc: 0.9888530969619751  val: loss: 245495.71875 acc: 0.9186640977859497\n",
      "step: 14170\n",
      "train: loss: 3841.920166015625 acc: 0.9886558651924133  val: loss: 517360.125 acc: 0.9183927774429321\n",
      "step: 14175\n",
      "train: loss: 45890.05859375 acc: 0.9838724136352539  val: loss: 196462.796875 acc: 0.9169917106628418\n",
      "step: 14180\n",
      "train: loss: 20798.26953125 acc: 0.9894968271255493  val: loss: 324966.25 acc: 0.7565600275993347\n",
      "step: 14185\n",
      "train: loss: 17684.03125 acc: 0.9819459319114685  val: loss: 488648.8125 acc: 0.8434216976165771\n",
      "step: 14190\n",
      "train: loss: 13886.5029296875 acc: 0.974376916885376  val: loss: 486669.25 acc: 0.9026058316230774\n",
      "step: 14195\n",
      "train: loss: 7176.55078125 acc: 0.9771375060081482  val: loss: 1160175.625 acc: 0.13916414976119995\n",
      "step: 14200\n",
      "train: loss: 15594.0361328125 acc: 0.9625503420829773  val: loss: 905962.5 acc: 0.8437517285346985\n",
      "step: 14205\n",
      "train: loss: 15588.166015625 acc: 0.975272536277771  val: loss: 332527.375 acc: 0.9474818706512451\n",
      "step: 14210\n",
      "train: loss: 40137.88671875 acc: 0.9818223118782043  val: loss: 138647.4375 acc: 0.9787644147872925\n",
      "step: 14215\n",
      "train: loss: 12598.181640625 acc: 0.9897401332855225  val: loss: 1489802.5 acc: 0.6365379691123962\n",
      "step: 14220\n",
      "train: loss: 87921.8046875 acc: 0.9583877325057983  val: loss: 662511.75 acc: 0.8042009472846985\n",
      "step: 14225\n",
      "train: loss: 18364.880859375 acc: 0.9902782440185547  val: loss: 225087.390625 acc: 0.9598709940910339\n",
      "step: 14230\n",
      "train: loss: 90878.4453125 acc: 0.9086276292800903  val: loss: 1240472.125 acc: 0.4310639500617981\n",
      "step: 14235\n",
      "train: loss: 34064.03515625 acc: 0.9277417063713074  val: loss: 469827.28125 acc: 0.8004948496818542\n",
      "step: 14240\n",
      "train: loss: 11119.4384765625 acc: 0.9891886115074158  val: loss: 828941.8125 acc: 0.7371814250946045\n",
      "step: 14245\n",
      "train: loss: 27017.953125 acc: 0.9859308004379272  val: loss: 1616001.75 acc: 0.6383758783340454\n",
      "step: 14250\n",
      "train: loss: 31198.80859375 acc: 0.9893789291381836  val: loss: 2203301.25 acc: 0.14429056644439697\n",
      "step: 14255\n",
      "train: loss: 31885.294921875 acc: 0.9903553128242493  val: loss: 1273395.5 acc: 0.7403473854064941\n",
      "step: 14260\n",
      "train: loss: 36243.4921875 acc: 0.991413414478302  val: loss: 497743.40625 acc: 0.9135318398475647\n",
      "step: 14265\n",
      "train: loss: 12973.9794921875 acc: 0.9930292963981628  val: loss: 1092661.375 acc: 0.8800651431083679\n",
      "step: 14270\n",
      "train: loss: 208630.21875 acc: 0.965331494808197  val: loss: 935651.25 acc: 0.6440759897232056\n",
      "step: 14275\n",
      "train: loss: 102578.671875 acc: 0.9571084976196289  val: loss: 739907.3125 acc: 0.8952240943908691\n",
      "step: 14280\n",
      "train: loss: 97502.28125 acc: 0.9786850810050964  val: loss: 1140082.875 acc: 0.875313401222229\n",
      "step: 14285\n",
      "train: loss: 59229.8046875 acc: 0.9849547147750854  val: loss: 108051.6328125 acc: 0.9796162843704224\n",
      "step: 14290\n",
      "train: loss: 129432.0234375 acc: 0.9905457496643066  val: loss: 427806.0 acc: 0.9496163725852966\n",
      "step: 14295\n",
      "train: loss: 129160.4921875 acc: 0.9829192161560059  val: loss: 89107.0546875 acc: 0.9717607498168945\n",
      "step: 14300\n",
      "train: loss: 36816.37109375 acc: 0.9932039380073547  val: loss: 1144582.0 acc: 0.9035162925720215\n",
      "step: 14305\n",
      "train: loss: 64972.92578125 acc: 0.9918715953826904  val: loss: 1292506.625 acc: 0.6904465556144714\n",
      "step: 14310\n",
      "train: loss: 659635.375 acc: 0.9706060290336609  val: loss: 1543836.0 acc: 0.622002124786377\n",
      "step: 14315\n",
      "train: loss: 214121.875 acc: 0.9876765012741089  val: loss: 664481.4375 acc: 0.9490798711776733\n",
      "step: 14320\n",
      "train: loss: 133253.21875 acc: 0.9674445986747742  val: loss: 828366.5625 acc: 0.883816659450531\n",
      "step: 14325\n",
      "train: loss: 1062582.75 acc: 0.9544047713279724  val: loss: 1021303.5625 acc: 0.8102995753288269\n",
      "step: 14330\n",
      "train: loss: 1486468.875 acc: 0.9606383442878723  val: loss: 1797654.0 acc: 0.5284419059753418\n",
      "step: 14335\n",
      "train: loss: 989214.0 acc: 0.9598864316940308  val: loss: 1168150.125 acc: 0.8348360061645508\n",
      "step: 14340\n",
      "train: loss: 1462327.25 acc: 0.9346284866333008  val: loss: 2129520.25 acc: 0.5296023488044739\n",
      "step: 14345\n",
      "train: loss: 522531.65625 acc: 0.9511021971702576  val: loss: 968968.875 acc: 0.7826526761054993\n",
      "step: 14350\n",
      "train: loss: 525900.3125 acc: 0.9736061096191406  val: loss: 1469706.875 acc: 0.6340543031692505\n",
      "step: 14355\n",
      "train: loss: 482538.875 acc: 0.9238153696060181  val: loss: 368589.9375 acc: 0.9484593868255615\n",
      "step: 14360\n",
      "train: loss: 337161.21875 acc: 0.9494259357452393  val: loss: 1201797.375 acc: 0.8095028400421143\n",
      "step: 14365\n",
      "train: loss: 1354002.75 acc: 0.752288818359375  val: loss: 875206.8125 acc: 0.7302820086479187\n",
      "step: 14370\n",
      "train: loss: 506686.40625 acc: 0.7996383309364319  val: loss: 1000537.0 acc: 0.8079216480255127\n",
      "step: 14375\n",
      "train: loss: 890162.9375 acc: 0.5351762771606445  val: loss: 1034240.8125 acc: 0.8068084716796875\n",
      "step: 14380\n",
      "train: loss: 486937.6875 acc: 0.8495010137557983  val: loss: 122247.578125 acc: 0.7849255800247192\n",
      "step: 14385\n",
      "train: loss: 1122585.75 acc: 0.5578205585479736  val: loss: 835488.6875 acc: 0.6316138505935669\n",
      "step: 14390\n",
      "train: loss: 917366.1875 acc: 0.7165732383728027  val: loss: 922121.8125 acc: 0.72059166431427\n",
      "step: 14395\n",
      "train: loss: 78321.328125 acc: 0.9299595952033997  val: loss: 860824.3125 acc: 0.7051337957382202\n",
      "step: 14400\n",
      "train: loss: 49294.3828125 acc: 0.9589731097221375  val: loss: 3293206.75 acc: 0.5582782030105591\n",
      "step: 14405\n",
      "train: loss: 40833.96875 acc: 0.96440190076828  val: loss: 1024217.375 acc: 0.6786801815032959\n",
      "step: 14410\n",
      "train: loss: 143374.90625 acc: 0.9072264432907104  val: loss: 702301.1875 acc: 0.7460544109344482\n",
      "step: 14415\n",
      "train: loss: 155394.234375 acc: 0.9016750454902649  val: loss: 4687403.0 acc: 0.5400133728981018\n",
      "step: 14420\n",
      "train: loss: 530365.9375 acc: 0.8034086227416992  val: loss: 4959014.5 acc: 0.5391399264335632\n",
      "step: 14425\n",
      "train: loss: 80589.0078125 acc: 0.9500190615653992  val: loss: 1671665.375 acc: 0.641289234161377\n",
      "step: 14430\n",
      "train: loss: 131976.25 acc: 0.874078631401062  val: loss: 4069585.75 acc: 0.5269848108291626\n",
      "step: 14435\n",
      "train: loss: 299674.65625 acc: 0.827964186668396  val: loss: 749361.1875 acc: 0.7072629332542419\n",
      "step: 14440\n",
      "train: loss: 192946.65625 acc: 0.8362981081008911  val: loss: 760801.9375 acc: 0.793390154838562\n",
      "step: 14445\n",
      "train: loss: 171491.90625 acc: 0.8848466277122498  val: loss: 1463376.5 acc: 0.7113507986068726\n",
      "step: 14450\n",
      "train: loss: 454035.375 acc: 0.7998219728469849  val: loss: 1395228.5 acc: 0.6301058530807495\n",
      "step: 14455\n",
      "train: loss: 963429.1875 acc: 0.6692056655883789  val: loss: 179835.359375 acc: 0.8424652218818665\n",
      "step: 14460\n",
      "train: loss: 804401.0625 acc: 0.7246708869934082  val: loss: 1520006.625 acc: 0.7000434398651123\n",
      "step: 14465\n",
      "train: loss: 103050.7578125 acc: 0.9001548290252686  val: loss: 2641398.5 acc: 0.6189870834350586\n",
      "step: 14470\n",
      "train: loss: 730763.4375 acc: 0.7331423759460449  val: loss: 960352.375 acc: 0.6612399816513062\n",
      "step: 14475\n",
      "train: loss: 1084327.25 acc: 0.7575085759162903  val: loss: 1001715.125 acc: 0.6774346828460693\n",
      "step: 14480\n",
      "train: loss: 1166480.0 acc: 0.8504429459571838  val: loss: 985062.1875 acc: 0.8777673244476318\n",
      "step: 14485\n",
      "train: loss: 859218.3125 acc: 0.910584568977356  val: loss: 521803.5 acc: 0.8287299871444702\n",
      "step: 14490\n",
      "train: loss: 134764.625 acc: 0.9842820167541504  val: loss: 1135892.625 acc: 0.5537638664245605\n",
      "step: 14495\n",
      "train: loss: 98160.875 acc: 0.9880157709121704  val: loss: 531572.75 acc: 0.7382078766822815\n",
      "step: 14500\n",
      "train: loss: 71319.171875 acc: 0.984356701374054  val: loss: 454001.40625 acc: 0.8915361762046814\n",
      "step: 14505\n",
      "train: loss: 43771.7578125 acc: 0.9952114820480347  val: loss: 579604.875 acc: 0.9204249978065491\n",
      "step: 14510\n",
      "train: loss: 112659.9140625 acc: 0.9923501014709473  val: loss: 642299.0625 acc: 0.912851095199585\n",
      "step: 14515\n",
      "train: loss: 99380.2734375 acc: 0.9900421500205994  val: loss: 635928.375 acc: 0.9164636135101318\n",
      "step: 14520\n",
      "train: loss: 61900.71484375 acc: 0.9920706748962402  val: loss: 856988.3125 acc: 0.7791885733604431\n",
      "step: 14525\n",
      "train: loss: 297405.46875 acc: 0.9184221029281616  val: loss: 122574.6953125 acc: 0.971651017665863\n",
      "step: 14530\n",
      "train: loss: 4972.79833984375 acc: 0.9962751865386963  val: loss: 1094093.375 acc: 0.630124568939209\n",
      "step: 14535\n",
      "train: loss: 17567.19921875 acc: 0.9927340745925903  val: loss: 132430.1875 acc: 0.9493677020072937\n",
      "step: 14540\n",
      "train: loss: 8671.828125 acc: 0.9943798184394836  val: loss: 927955.3125 acc: 0.7334573268890381\n",
      "step: 14545\n",
      "train: loss: 24812.421875 acc: 0.9747886061668396  val: loss: 102538.765625 acc: 0.9579704999923706\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 14550\n",
      "train: loss: 205532.71875 acc: 0.5911126732826233  val: loss: 642787.4375 acc: 0.7658604383468628\n",
      "step: 14555\n",
      "train: loss: 15537.08984375 acc: 0.9673746228218079  val: loss: 798850.625 acc: 0.00880664587020874\n",
      "step: 14560\n",
      "train: loss: 14140.6826171875 acc: 0.9634058475494385  val: loss: 293034.28125 acc: 0.9218186140060425\n",
      "step: 14565\n",
      "train: loss: 3770.091552734375 acc: 0.9901509284973145  val: loss: 196802.828125 acc: 0.9035669565200806\n",
      "step: 14570\n",
      "train: loss: 13503.94921875 acc: 0.9666562676429749  val: loss: 724891.0 acc: 0.8654799461364746\n",
      "step: 14575\n",
      "train: loss: 57854.19921875 acc: 0.9668094515800476  val: loss: 442867.25 acc: 0.8952348232269287\n",
      "step: 14580\n",
      "train: loss: 50812.16015625 acc: 0.9744799733161926  val: loss: 35913.8671875 acc: 0.9872616529464722\n",
      "step: 14585\n",
      "train: loss: 19279.25 acc: 0.9865554571151733  val: loss: 1363056.875 acc: 0.7322782874107361\n",
      "step: 14590\n",
      "train: loss: 37929.1640625 acc: 0.9792881608009338  val: loss: 731442.0625 acc: 0.9327136874198914\n",
      "step: 14595\n",
      "train: loss: 10699.796875 acc: 0.9923251271247864  val: loss: 2080801.875 acc: 0.6967779994010925\n",
      "step: 14600\n",
      "train: loss: 6850.60009765625 acc: 0.9808161854743958  val: loss: 792963.6875 acc: 0.9041077494621277\n",
      "step: 14605\n",
      "train: loss: 22249.18359375 acc: 0.9835352897644043  val: loss: 1191134.875 acc: 0.8379710912704468\n",
      "step: 14610\n",
      "train: loss: 13296.109375 acc: 0.995101273059845  val: loss: 2572126.0 acc: 0.6253732442855835\n",
      "step: 14615\n",
      "train: loss: 20831.943359375 acc: 0.9933680295944214  val: loss: 1414246.125 acc: -0.2742999792098999\n",
      "step: 14620\n",
      "train: loss: 21562.46875 acc: 0.9929744005203247  val: loss: 623940.625 acc: 0.8775389194488525\n",
      "step: 14625\n",
      "train: loss: 27228.34765625 acc: 0.9834880828857422  val: loss: 335469.78125 acc: 0.9708498120307922\n",
      "step: 14630\n",
      "train: loss: 27394.84375 acc: 0.9914035797119141  val: loss: 1703505.625 acc: 0.8949256539344788\n",
      "step: 14635\n",
      "train: loss: 83568.7421875 acc: 0.981508731842041  val: loss: 1141555.5 acc: 0.9113990068435669\n",
      "step: 14640\n",
      "train: loss: 196164.9375 acc: 0.9592072367668152  val: loss: 1079756.875 acc: 0.8876665830612183\n",
      "step: 14645\n",
      "train: loss: 486406.34375 acc: 0.9059838652610779  val: loss: 677756.3125 acc: 0.9080361127853394\n",
      "step: 14650\n",
      "train: loss: 165607.078125 acc: 0.9684139490127563  val: loss: 2695704.5 acc: 0.48663222789764404\n",
      "step: 14655\n",
      "train: loss: 159466.265625 acc: 0.9852638840675354  val: loss: 3462590.0 acc: 0.4518885612487793\n",
      "step: 14660\n",
      "train: loss: 82895.5703125 acc: 0.9910062551498413  val: loss: 1961141.375 acc: -0.10997259616851807\n",
      "step: 14665\n",
      "train: loss: 81828.0390625 acc: 0.9921901226043701  val: loss: 1687020.25 acc: 0.5358089208602905\n",
      "step: 14670\n",
      "train: loss: 150600.0 acc: 0.9698165655136108  val: loss: 2169473.5 acc: 0.5252102613449097\n",
      "step: 14675\n",
      "train: loss: 192801.84375 acc: 0.9821869134902954  val: loss: 2098235.75 acc: 0.6644059419631958\n",
      "step: 14680\n",
      "train: loss: 325492.46875 acc: 0.9835041165351868  val: loss: 971740.375 acc: 0.7176508903503418\n",
      "step: 14685\n",
      "train: loss: 176857.375 acc: 0.9462222456932068  val: loss: 476397.84375 acc: 0.9201498627662659\n",
      "step: 14690\n",
      "train: loss: 1895588.125 acc: 0.8987513780593872  val: loss: 740112.1875 acc: 0.8447367548942566\n",
      "step: 14695\n",
      "train: loss: 1315942.875 acc: 0.9697690606117249  val: loss: 326046.09375 acc: 0.9291353821754456\n",
      "step: 14700\n",
      "train: loss: 1032665.875 acc: 0.9539178609848022  val: loss: 633815.375 acc: 0.8067253828048706\n",
      "step: 14705\n",
      "train: loss: 846396.1875 acc: 0.9465090036392212  val: loss: 2859943.0 acc: 0.3276902437210083\n",
      "step: 14710\n",
      "train: loss: 615799.5625 acc: 0.917172908782959  val: loss: 1569402.0 acc: 0.6763330698013306\n",
      "step: 14715\n",
      "train: loss: 420239.65625 acc: 0.9438009858131409  val: loss: 1116903.375 acc: 0.5535433292388916\n",
      "step: 14720\n",
      "train: loss: 275223.09375 acc: 0.964586079120636  val: loss: 917606.125 acc: 0.8396748900413513\n",
      "step: 14725\n",
      "train: loss: 629428.8125 acc: 0.9522377252578735  val: loss: 752880.5625 acc: 0.8836795091629028\n",
      "step: 14730\n",
      "train: loss: 1485710.625 acc: 0.7831581830978394  val: loss: 703177.3125 acc: 0.7780619263648987\n",
      "step: 14735\n",
      "train: loss: 399385.90625 acc: 0.732841968536377  val: loss: 777488.3125 acc: 0.7572259902954102\n",
      "step: 14740\n",
      "train: loss: 523661.75 acc: 0.8618515729904175  val: loss: 1066537.375 acc: 0.6960837841033936\n",
      "step: 14745\n",
      "train: loss: 501661.15625 acc: 0.7267705202102661  val: loss: 656848.25 acc: 0.7706654071807861\n",
      "step: 14750\n",
      "train: loss: 759497.0 acc: 0.6745082139968872  val: loss: 655970.75 acc: 0.8018193244934082\n",
      "step: 14755\n",
      "train: loss: 559035.375 acc: 0.707128643989563  val: loss: 2963442.25 acc: 0.7052398920059204\n",
      "step: 14760\n",
      "train: loss: 802953.125 acc: 0.7328258156776428  val: loss: 1545515.625 acc: 0.7184606790542603\n",
      "step: 14765\n",
      "train: loss: 105558.953125 acc: 0.9196160435676575  val: loss: 2548346.0 acc: 0.6667152047157288\n",
      "step: 14770\n",
      "train: loss: 47550.25 acc: 0.9592140316963196  val: loss: 127823.078125 acc: 0.8977840542793274\n",
      "step: 14775\n",
      "train: loss: 153181.328125 acc: 0.9045922756195068  val: loss: 4246484.5 acc: 0.5821911692619324\n",
      "step: 14780\n",
      "train: loss: 147648.6875 acc: 0.8981372714042664  val: loss: 1317757.5 acc: 0.6460069417953491\n",
      "step: 14785\n",
      "train: loss: 209401.453125 acc: 0.8876316547393799  val: loss: 1199974.875 acc: 0.6321762800216675\n",
      "step: 14790\n",
      "train: loss: 59389.7421875 acc: 0.9499799609184265  val: loss: 1126354.0 acc: 0.6913213729858398\n",
      "step: 14795\n",
      "train: loss: 360282.46875 acc: 0.8040178418159485  val: loss: 1293944.0 acc: 0.6858875751495361\n",
      "step: 14800\n",
      "train: loss: 29273.494140625 acc: 0.9658606052398682  val: loss: 1632529.0 acc: 0.6040610074996948\n",
      "step: 14805\n",
      "train: loss: 90540.2109375 acc: 0.8879408240318298  val: loss: 5144175.5 acc: 0.5380034446716309\n",
      "step: 14810\n",
      "train: loss: 139905.578125 acc: 0.8739944696426392  val: loss: 1917968.125 acc: 0.5766087770462036\n",
      "step: 14815\n",
      "train: loss: 493242.21875 acc: 0.7668019533157349  val: loss: 1447122.375 acc: 0.6669962406158447\n",
      "step: 14820\n",
      "train: loss: 170290.46875 acc: 0.874728262424469  val: loss: 2431839.5 acc: 0.5561754703521729\n",
      "step: 14825\n",
      "train: loss: 1086782.625 acc: 0.6741536259651184  val: loss: 1764121.75 acc: 0.6444514989852905\n",
      "step: 14830\n",
      "train: loss: 112144.8984375 acc: 0.8821002244949341  val: loss: 580277.4375 acc: 0.73592209815979\n",
      "step: 14835\n",
      "train: loss: 102234.84375 acc: 0.9100032448768616  val: loss: 1883401.625 acc: 0.628483772277832\n",
      "step: 14840\n",
      "train: loss: 2257341.75 acc: 0.6274971961975098  val: loss: 467890.3125 acc: 0.7575475573539734\n",
      "step: 14845\n",
      "train: loss: 1535501.375 acc: 0.7787221074104309  val: loss: 480738.9375 acc: 0.7965734601020813\n",
      "step: 14850\n",
      "train: loss: 499979.8125 acc: 0.9531969428062439  val: loss: 1202560.625 acc: 0.88570237159729\n",
      "step: 14855\n",
      "train: loss: 225174.046875 acc: 0.9794550538063049  val: loss: 616262.0 acc: 0.677590012550354\n",
      "step: 14860\n",
      "train: loss: 150388.234375 acc: 0.9710314869880676  val: loss: 2449520.5 acc: -0.13652539253234863\n",
      "step: 14865\n",
      "train: loss: 130763.6640625 acc: 0.9762147068977356  val: loss: 134545.671875 acc: 0.9619197845458984\n",
      "step: 14870\n",
      "train: loss: 165104.671875 acc: 0.9853372573852539  val: loss: 282884.03125 acc: 0.9120418429374695\n",
      "step: 14875\n",
      "train: loss: 196962.234375 acc: 0.9848419427871704  val: loss: 409463.1875 acc: 0.9157570600509644\n",
      "step: 14880\n",
      "train: loss: 97271.5703125 acc: 0.9936268329620361  val: loss: 334722.46875 acc: 0.9538350105285645\n",
      "step: 14885\n",
      "train: loss: 58628.1796875 acc: 0.9948449730873108  val: loss: 1237641.875 acc: 0.7646246552467346\n",
      "step: 14890\n",
      "train: loss: 79182.203125 acc: 0.987827718257904  val: loss: 1299303.375 acc: 0.250868558883667\n",
      "step: 14895\n",
      "train: loss: 17608.83203125 acc: 0.9881940484046936  val: loss: 548043.0 acc: 0.830571174621582\n",
      "step: 14900\n",
      "train: loss: 19284.69921875 acc: 0.988994836807251  val: loss: 354002.125 acc: 0.9485161304473877\n",
      "step: 14905\n",
      "train: loss: 18212.400390625 acc: 0.9904583096504211  val: loss: 872914.9375 acc: 0.9263960123062134\n",
      "step: 14910\n",
      "train: loss: 10011.36328125 acc: 0.9857112765312195  val: loss: 513891.9375 acc: 0.805266261100769\n",
      "step: 14915\n",
      "train: loss: 202238.0 acc: 0.6366848945617676  val: loss: 228621.3125 acc: 0.9576841592788696\n",
      "step: 14920\n",
      "train: loss: 5442.6357421875 acc: 0.9905247092247009  val: loss: 360105.625 acc: 0.9752553105354309\n",
      "step: 14925\n",
      "train: loss: 4848.58544921875 acc: 0.9852213263511658  val: loss: 803173.875 acc: 0.881636917591095\n",
      "step: 14930\n",
      "train: loss: 12385.4287109375 acc: 0.9523265957832336  val: loss: 752938.375 acc: 0.9424270391464233\n",
      "step: 14935\n",
      "train: loss: 21036.365234375 acc: 0.9744412302970886  val: loss: 1779408.125 acc: 0.906917929649353\n",
      "step: 14940\n",
      "train: loss: 57564.5078125 acc: 0.9750944375991821  val: loss: 1086260.25 acc: 0.8897764682769775\n",
      "step: 14945\n",
      "train: loss: 19269.6171875 acc: 0.9868500828742981  val: loss: 1817780.375 acc: 0.6617456674575806\n",
      "step: 14950\n",
      "train: loss: 15265.05078125 acc: 0.9888114333152771  val: loss: 1519689.375 acc: 0.8645951151847839\n",
      "step: 14955\n",
      "train: loss: 26008.125 acc: 0.9838546514511108  val: loss: 1220715.5 acc: 0.654198408126831\n",
      "step: 14960\n",
      "train: loss: 15902.8740234375 acc: 0.9885913133621216  val: loss: 1236626.375 acc: 0.6756102442741394\n",
      "step: 14965\n",
      "train: loss: 7639.43359375 acc: 0.9828746914863586  val: loss: 2882859.5 acc: 0.5636212825775146\n",
      "step: 14970\n",
      "train: loss: 12419.1015625 acc: 0.9563261270523071  val: loss: 1640758.875 acc: 0.6977614164352417\n",
      "step: 14975\n",
      "train: loss: 7971.8857421875 acc: 0.9967472553253174  val: loss: 2920035.75 acc: 0.4194369912147522\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 14980\n",
      "train: loss: 35667.875 acc: 0.9895463585853577  val: loss: 1958203.625 acc: 0.8280031085014343\n",
      "step: 14985\n",
      "train: loss: 30689.98046875 acc: 0.9901330471038818  val: loss: 2140382.75 acc: 0.6479418277740479\n",
      "step: 14990\n",
      "train: loss: 23273.412109375 acc: 0.9930487871170044  val: loss: 373573.84375 acc: 0.9552769660949707\n",
      "step: 14995\n",
      "train: loss: 42990.4375 acc: 0.9893440008163452  val: loss: 2186779.75 acc: -0.8108508586883545\n",
      "step: 15000\n",
      "train: loss: 171248.703125 acc: 0.9468897581100464  val: loss: 1222066.875 acc: 0.8285968899726868\n",
      "step: 15005\n",
      "train: loss: 85668.3203125 acc: 0.9756487011909485  val: loss: 1089613.0 acc: 0.2461184859275818\n",
      "step: 15010\n",
      "train: loss: 114317.6796875 acc: 0.9780352115631104  val: loss: 1646344.125 acc: 0.4711630940437317\n",
      "step: 15015\n",
      "train: loss: 243510.421875 acc: 0.9710071086883545  val: loss: 1733822.125 acc: 0.722861647605896\n",
      "step: 15020\n",
      "train: loss: 56396.26171875 acc: 0.9940348267555237  val: loss: 1957613.875 acc: 0.5206059217453003\n",
      "step: 15025\n",
      "train: loss: 422127.90625 acc: 0.937980592250824  val: loss: 2277733.75 acc: 0.5100889205932617\n",
      "step: 15030\n",
      "train: loss: 111700.6953125 acc: 0.9854258298873901  val: loss: 3916420.0 acc: -1.3770222663879395\n",
      "step: 15035\n",
      "train: loss: 258359.75 acc: 0.9701777696609497  val: loss: 796557.5 acc: 0.8421766757965088\n",
      "step: 15040\n",
      "train: loss: 258871.609375 acc: 0.9749431610107422  val: loss: 416841.8125 acc: 0.8827636241912842\n",
      "step: 15045\n",
      "train: loss: 474770.1875 acc: 0.974793553352356  val: loss: 308987.90625 acc: 0.8637305498123169\n",
      "step: 15050\n",
      "train: loss: 327526.4375 acc: 0.9641070365905762  val: loss: 4384335.5 acc: -0.1605919599533081\n",
      "step: 15055\n",
      "train: loss: 1255536.625 acc: 0.893714189529419  val: loss: 999856.375 acc: 0.8063595294952393\n",
      "step: 15060\n",
      "train: loss: 1181952.625 acc: 0.9703950881958008  val: loss: 1127489.625 acc: 0.815251350402832\n",
      "step: 15065\n",
      "train: loss: 1509095.0 acc: 0.9599507451057434  val: loss: 510901.5625 acc: 0.8782557845115662\n",
      "step: 15070\n",
      "train: loss: 825659.25 acc: 0.946001410484314  val: loss: 1208569.375 acc: 0.8341064453125\n",
      "step: 15075\n",
      "train: loss: 442764.84375 acc: 0.9591057300567627  val: loss: 828298.125 acc: 0.8831413984298706\n",
      "step: 15080\n",
      "train: loss: 448280.21875 acc: 0.9476726055145264  val: loss: 373565.0625 acc: 0.9429893493652344\n",
      "step: 15085\n",
      "train: loss: 564214.125 acc: 0.9577783942222595  val: loss: 84532.765625 acc: 0.9466135501861572\n",
      "step: 15090\n",
      "train: loss: 287903.8125 acc: 0.9212651252746582  val: loss: 878999.5 acc: 0.7477425336837769\n",
      "step: 15095\n",
      "train: loss: 2347916.5 acc: 0.3885456919670105  val: loss: 318684.0625 acc: 0.9314236640930176\n",
      "step: 15100\n",
      "train: loss: 1056489.25 acc: 0.6991323232650757  val: loss: 277389.75 acc: 0.9212714433670044\n",
      "step: 15105\n",
      "train: loss: 1106400.75 acc: 0.8255112767219543  val: loss: 1082500.625 acc: 0.7946200370788574\n",
      "step: 15110\n",
      "train: loss: 388959.34375 acc: 0.8192395567893982  val: loss: 425722.375 acc: 0.8607692718505859\n",
      "step: 15115\n",
      "train: loss: 834773.25 acc: 0.3135982155799866  val: loss: 546193.625 acc: 0.8628219962120056\n",
      "step: 15120\n",
      "train: loss: 985476.875 acc: 0.6013746857643127  val: loss: 1275067.375 acc: 0.7413294315338135\n",
      "step: 15125\n",
      "train: loss: 348373.53125 acc: 0.7721558809280396  val: loss: 200413.0625 acc: 0.7707350850105286\n",
      "step: 15130\n",
      "train: loss: 191092.75 acc: 0.843103289604187  val: loss: 2280356.0 acc: 0.6091477870941162\n",
      "step: 15135\n",
      "train: loss: 136688.34375 acc: 0.8807253837585449  val: loss: 2192006.5 acc: 0.6912996172904968\n",
      "step: 15140\n",
      "train: loss: 45085.5546875 acc: 0.9645304679870605  val: loss: 746267.625 acc: 0.7097042798995972\n",
      "step: 15145\n",
      "train: loss: 43987.4609375 acc: 0.963446319103241  val: loss: 1932470.625 acc: 0.6464641094207764\n",
      "step: 15150\n",
      "train: loss: 69865.15625 acc: 0.944738507270813  val: loss: 905804.625 acc: 0.6751052141189575\n",
      "step: 15155\n",
      "train: loss: 472199.65625 acc: 0.8156532049179077  val: loss: 2218176.0 acc: 0.629738450050354\n",
      "step: 15160\n",
      "train: loss: 35056.62890625 acc: 0.9697951674461365  val: loss: 757930.9375 acc: 0.6860529184341431\n",
      "step: 15165\n",
      "train: loss: 269169.5625 acc: 0.8472714424133301  val: loss: 100460.8359375 acc: 0.8848742842674255\n",
      "step: 15170\n",
      "train: loss: 152968.703125 acc: 0.8423869609832764  val: loss: 326219.25 acc: 0.7873480319976807\n",
      "step: 15175\n",
      "train: loss: 183998.671875 acc: 0.8862802386283875  val: loss: 597614.0625 acc: 0.7353434562683105\n",
      "step: 15180\n",
      "train: loss: 91763.8984375 acc: 0.9037283658981323  val: loss: 2999478.25 acc: 0.5470854640007019\n",
      "step: 15185\n",
      "train: loss: 124304.96875 acc: 0.9138278365135193  val: loss: 883131.625 acc: 0.6840317249298096\n",
      "step: 15190\n",
      "train: loss: 140035.984375 acc: 0.8749814033508301  val: loss: 922612.9375 acc: 0.6505638360977173\n",
      "step: 15195\n",
      "train: loss: 497877.4375 acc: 0.7574918270111084  val: loss: 138734.34375 acc: 0.8833546042442322\n",
      "step: 15200\n",
      "train: loss: 633359.25 acc: 0.7844725251197815  val: loss: 2171871.0 acc: 0.6200162172317505\n",
      "step: 15205\n",
      "train: loss: 1547116.125 acc: 0.6436471939086914  val: loss: 770272.5 acc: 0.8098890781402588\n",
      "step: 15210\n",
      "train: loss: 901411.5 acc: 0.6838116645812988  val: loss: 1726207.625 acc: 0.7452039122581482\n",
      "step: 15215\n",
      "train: loss: 1147778.125 acc: 0.9016544222831726  val: loss: 1016274.5 acc: 0.8014504313468933\n",
      "step: 15220\n",
      "train: loss: 178164.015625 acc: 0.9751665592193604  val: loss: 473389.0 acc: 0.9259114265441895\n",
      "step: 15225\n",
      "train: loss: 455760.125 acc: 0.9447777271270752  val: loss: 449413.6875 acc: 0.9434188604354858\n",
      "step: 15230\n",
      "train: loss: 98262.8828125 acc: 0.9630464911460876  val: loss: 817957.125 acc: 0.9027457237243652\n",
      "step: 15235\n",
      "train: loss: 179512.59375 acc: 0.9834891557693481  val: loss: 403191.40625 acc: 0.9626365900039673\n",
      "step: 15240\n",
      "train: loss: 64742.52734375 acc: 0.9955220818519592  val: loss: 1421434.875 acc: 0.6218008995056152\n",
      "step: 15245\n",
      "train: loss: 84585.6640625 acc: 0.9929207563400269  val: loss: 809281.25 acc: 0.9259553551673889\n",
      "step: 15250\n",
      "train: loss: 81275.453125 acc: 0.9873071312904358  val: loss: 675211.25 acc: 0.9285686016082764\n",
      "step: 15255\n",
      "train: loss: 54273.48828125 acc: 0.987902045249939  val: loss: 992162.5 acc: 0.9096535444259644\n",
      "step: 15260\n",
      "train: loss: 30270.55078125 acc: 0.987758457660675  val: loss: 456476.0 acc: 0.8156182765960693\n",
      "step: 15265\n",
      "train: loss: 8069.08154296875 acc: 0.9970548152923584  val: loss: 2191573.25 acc: 0.8144137859344482\n",
      "step: 15270\n",
      "train: loss: 40927.1015625 acc: 0.9816009402275085  val: loss: 975687.0 acc: 0.6854275465011597\n",
      "step: 15275\n",
      "train: loss: 22825.884765625 acc: 0.9909168481826782  val: loss: 2552765.25 acc: 0.4042377471923828\n",
      "step: 15280\n",
      "train: loss: 17843.08984375 acc: 0.9634127616882324  val: loss: 1033971.0 acc: 0.6403895616531372\n",
      "step: 15285\n",
      "train: loss: 23698.3359375 acc: 0.9847265481948853  val: loss: 1615085.125 acc: 0.5848156213760376\n",
      "step: 15290\n",
      "train: loss: 8942.14453125 acc: 0.9938766360282898  val: loss: 1334153.0 acc: 0.7595171928405762\n",
      "step: 15295\n",
      "train: loss: 7467.67919921875 acc: 0.9839192628860474  val: loss: 1833324.875 acc: 0.7422332763671875\n",
      "step: 15300\n",
      "train: loss: 6876.00439453125 acc: 0.9851804971694946  val: loss: 630382.25 acc: 0.909737229347229\n",
      "step: 15305\n",
      "train: loss: 6198.11572265625 acc: 0.987030565738678  val: loss: 453706.59375 acc: 0.6656849384307861\n",
      "step: 15310\n",
      "train: loss: 33280.828125 acc: 0.9802917838096619  val: loss: 1034077.125 acc: 0.8578652143478394\n",
      "step: 15315\n",
      "train: loss: 26019.341796875 acc: 0.9841078519821167  val: loss: 929690.875 acc: 0.8501001596450806\n",
      "step: 15320\n",
      "train: loss: 14146.7265625 acc: 0.9916465878486633  val: loss: 986587.0 acc: 0.4013604521751404\n",
      "step: 15325\n",
      "train: loss: 112038.6875 acc: 0.9212356209754944  val: loss: 930428.875 acc: 0.6153820753097534\n",
      "step: 15330\n",
      "train: loss: 12846.0166015625 acc: 0.9878882765769958  val: loss: 1952233.875 acc: 0.6251415610313416\n",
      "step: 15335\n",
      "train: loss: 14007.5810546875 acc: 0.9829012751579285  val: loss: 1616422.625 acc: 0.26276683807373047\n",
      "step: 15340\n",
      "train: loss: 14602.580078125 acc: 0.9890609383583069  val: loss: 1940797.875 acc: 0.7173478603363037\n",
      "step: 15345\n",
      "train: loss: 36732.93359375 acc: 0.9933878183364868  val: loss: 1589341.125 acc: 0.7975221872329712\n",
      "step: 15350\n",
      "train: loss: 17498.3046875 acc: 0.98980313539505  val: loss: 1131462.125 acc: 0.7550801038742065\n",
      "step: 15355\n",
      "train: loss: 24702.765625 acc: 0.9816951155662537  val: loss: 2575976.5 acc: 0.532787561416626\n",
      "step: 15360\n",
      "train: loss: 27442.4609375 acc: 0.9923121333122253  val: loss: 1753954.875 acc: 0.48904258012771606\n",
      "step: 15365\n",
      "train: loss: 229353.375 acc: 0.9214940071105957  val: loss: 2898630.25 acc: -0.19106388092041016\n",
      "step: 15370\n",
      "train: loss: 115322.5078125 acc: 0.9381828308105469  val: loss: 244693.171875 acc: 0.9580011963844299\n",
      "step: 15375\n",
      "train: loss: 44352.37109375 acc: 0.9757198691368103  val: loss: 2036801.875 acc: 0.7045358419418335\n",
      "step: 15380\n",
      "train: loss: 444293.90625 acc: 0.928151547908783  val: loss: 1933275.5 acc: 0.6475932002067566\n",
      "step: 15385\n",
      "train: loss: 40452.61328125 acc: 0.9966371059417725  val: loss: 322573.8125 acc: 0.9376229643821716\n",
      "step: 15390\n",
      "train: loss: 90020.484375 acc: 0.9890653491020203  val: loss: 776366.0625 acc: 0.918868362903595\n",
      "step: 15395\n",
      "train: loss: 160522.1875 acc: 0.9848533272743225  val: loss: 1930619.25 acc: 0.6336669921875\n",
      "step: 15400\n",
      "train: loss: 148791.578125 acc: 0.9777204990386963  val: loss: 320593.125 acc: 0.9437803626060486\n",
      "step: 15405\n",
      "train: loss: 318548.65625 acc: 0.974055826663971  val: loss: 1669580.875 acc: 0.6025235652923584\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 15410\n",
      "train: loss: 325654.15625 acc: 0.9747787714004517  val: loss: 770013.0625 acc: 0.8933000564575195\n",
      "step: 15415\n",
      "train: loss: 279912.34375 acc: 0.9644517302513123  val: loss: 1802846.75 acc: 0.686622142791748\n",
      "step: 15420\n",
      "train: loss: 1332649.5 acc: 0.8757593035697937  val: loss: 4208048.0 acc: 0.33062028884887695\n",
      "step: 15425\n",
      "train: loss: 757948.5625 acc: 0.9646053910255432  val: loss: 428058.59375 acc: 0.8904590606689453\n",
      "step: 15430\n",
      "train: loss: 772581.4375 acc: 0.9721152186393738  val: loss: 354090.90625 acc: 0.9201295375823975\n",
      "step: 15435\n",
      "train: loss: 866872.25 acc: 0.9604585766792297  val: loss: 703472.875 acc: 0.8952396512031555\n",
      "step: 15440\n",
      "train: loss: 481862.03125 acc: 0.9650811553001404  val: loss: 1248926.625 acc: 0.4847274422645569\n",
      "step: 15445\n",
      "train: loss: 405028.09375 acc: 0.9612131118774414  val: loss: 270613.40625 acc: 0.9437936544418335\n",
      "step: 15450\n",
      "train: loss: 781734.4375 acc: 0.8923282027244568  val: loss: 551873.8125 acc: 0.8976178765296936\n",
      "step: 15455\n",
      "train: loss: 148766.125 acc: 0.9741600751876831  val: loss: 562610.1875 acc: 0.6267181634902954\n",
      "step: 15460\n",
      "train: loss: 2008861.0 acc: 0.6563625335693359  val: loss: 575260.3125 acc: 0.767795979976654\n",
      "step: 15465\n",
      "train: loss: 1343216.625 acc: 0.6781982183456421  val: loss: 295140.375 acc: 0.8498207330703735\n",
      "step: 15470\n",
      "train: loss: 505209.46875 acc: 0.8062936067581177  val: loss: 918847.0625 acc: 0.7711646556854248\n",
      "step: 15475\n",
      "train: loss: 650700.75 acc: 0.787509024143219  val: loss: 1617395.0 acc: 0.6671847701072693\n",
      "step: 15480\n",
      "train: loss: 908806.3125 acc: 0.6188679933547974  val: loss: 824724.0 acc: 0.7498080730438232\n",
      "step: 15485\n",
      "train: loss: 1021763.375 acc: 0.7176430225372314  val: loss: 1221927.125 acc: 0.7177752256393433\n",
      "step: 15490\n",
      "train: loss: 210411.984375 acc: 0.8490435481071472  val: loss: 1598990.125 acc: 0.5925514698028564\n",
      "step: 15495\n",
      "train: loss: 115576.8671875 acc: 0.9106292724609375  val: loss: 2286288.25 acc: 0.6143875122070312\n",
      "step: 15500\n",
      "train: loss: 132543.21875 acc: 0.9115617275238037  val: loss: 2847882.25 acc: 0.6119811534881592\n",
      "step: 15505\n",
      "train: loss: 55841.71484375 acc: 0.955608069896698  val: loss: 1322734.5 acc: 0.6390568017959595\n",
      "step: 15510\n",
      "train: loss: 43326.875 acc: 0.9657526016235352  val: loss: 759833.125 acc: 0.7559505701065063\n",
      "step: 15515\n",
      "train: loss: 151361.609375 acc: 0.9103348255157471  val: loss: 2265201.0 acc: 0.5835007429122925\n",
      "step: 15520\n",
      "train: loss: 147069.40625 acc: 0.8925866484642029  val: loss: 1623253.625 acc: 0.5936890244483948\n",
      "step: 15525\n",
      "train: loss: 390297.875 acc: 0.7650986909866333  val: loss: 2756261.25 acc: 0.5611723065376282\n",
      "step: 15530\n",
      "train: loss: 159166.015625 acc: 0.8518819808959961  val: loss: 6409375.5 acc: 0.5057864189147949\n",
      "step: 15535\n",
      "train: loss: 214337.0 acc: 0.8003830909729004  val: loss: 1666276.625 acc: 0.6495408415794373\n",
      "step: 15540\n",
      "train: loss: 175754.328125 acc: 0.8933448791503906  val: loss: 5487746.5 acc: 0.5351884365081787\n",
      "step: 15545\n",
      "train: loss: 801842.125 acc: 0.7374005913734436  val: loss: 3915112.0 acc: 0.6108717918395996\n",
      "step: 15550\n",
      "train: loss: 95505.78125 acc: 0.8968232274055481  val: loss: 5996418.5 acc: 0.5260255932807922\n",
      "step: 15555\n",
      "train: loss: 113272.4921875 acc: 0.913928747177124  val: loss: 1435715.375 acc: 0.6906657218933105\n",
      "step: 15560\n",
      "train: loss: 39472.5625 acc: 0.9483701586723328  val: loss: 897035.5 acc: 0.7044501304626465\n",
      "step: 15565\n",
      "train: loss: 417301.15625 acc: 0.8215353488922119  val: loss: 660627.0 acc: 0.6868101954460144\n",
      "step: 15570\n",
      "train: loss: 1172398.75 acc: 0.5378397703170776  val: loss: 2663417.25 acc: 0.6805684566497803\n",
      "step: 15575\n",
      "train: loss: 1687575.5 acc: 0.7833383083343506  val: loss: 860646.125 acc: 0.8668981790542603\n",
      "step: 15580\n",
      "train: loss: 549359.0625 acc: 0.9358749389648438  val: loss: 1790812.625 acc: 0.3549804091453552\n",
      "step: 15585\n",
      "train: loss: 188251.96875 acc: 0.9817525744438171  val: loss: 724477.9375 acc: 0.7981165647506714\n",
      "step: 15590\n",
      "train: loss: 116779.1484375 acc: 0.9875543117523193  val: loss: 1725100.375 acc: 0.518123984336853\n",
      "step: 15595\n",
      "train: loss: 105257.0078125 acc: 0.9801828861236572  val: loss: 1286458.5 acc: 0.8217547535896301\n",
      "step: 15600\n",
      "train: loss: 82158.2578125 acc: 0.9887524247169495  val: loss: 2136254.5 acc: 0.5413177609443665\n",
      "step: 15605\n",
      "train: loss: 72737.5703125 acc: 0.9947218894958496  val: loss: 1540650.375 acc: 0.6460320353507996\n",
      "step: 15610\n",
      "train: loss: 98085.0859375 acc: 0.9924602508544922  val: loss: 601733.3125 acc: 0.8493599891662598\n",
      "step: 15615\n",
      "train: loss: 60919.984375 acc: 0.9936572909355164  val: loss: 973826.6875 acc: 0.887249767780304\n",
      "step: 15620\n",
      "train: loss: 36509.0859375 acc: 0.9932114481925964  val: loss: 1178155.375 acc: 0.7045822739601135\n",
      "step: 15625\n",
      "train: loss: 34696.34765625 acc: 0.981998860836029  val: loss: 888369.875 acc: 0.5049882531166077\n",
      "step: 15630\n",
      "train: loss: 6111.01513671875 acc: 0.9907101392745972  val: loss: 1154068.5 acc: 0.6263771057128906\n",
      "step: 15635\n",
      "train: loss: 6293.50634765625 acc: 0.985892117023468  val: loss: 1996560.875 acc: -0.17293012142181396\n",
      "step: 15640\n",
      "train: loss: 10723.4931640625 acc: 0.9823094010353088  val: loss: 589298.3125 acc: 0.9165890216827393\n",
      "step: 15645\n",
      "train: loss: 27346.28515625 acc: 0.9431777000427246  val: loss: 1518804.375 acc: -0.43427252769470215\n",
      "step: 15650\n",
      "train: loss: 10232.1142578125 acc: 0.9921842217445374  val: loss: 1033660.1875 acc: 0.3018645644187927\n",
      "step: 15655\n",
      "train: loss: 15636.125 acc: 0.9902088642120361  val: loss: 1242342.375 acc: 0.7324455976486206\n",
      "step: 15660\n",
      "train: loss: 16838.984375 acc: 0.9747543931007385  val: loss: 1197560.125 acc: 0.6690808534622192\n",
      "step: 15665\n",
      "train: loss: 14676.6943359375 acc: 0.9731115102767944  val: loss: 2399316.25 acc: 0.25480717420578003\n",
      "step: 15670\n",
      "train: loss: 16286.87890625 acc: 0.9758229851722717  val: loss: 506396.25 acc: 0.9241047501564026\n",
      "step: 15675\n",
      "train: loss: 16436.3515625 acc: 0.9911706447601318  val: loss: 2041036.25 acc: 0.6635884046554565\n",
      "step: 15680\n",
      "train: loss: 22517.58203125 acc: 0.9714521169662476  val: loss: 1463917.0 acc: 0.43781977891921997\n",
      "step: 15685\n",
      "train: loss: 30881.515625 acc: 0.9733222126960754  val: loss: 1381048.5 acc: 0.684071958065033\n",
      "step: 15690\n",
      "train: loss: 18192.21875 acc: 0.9892038702964783  val: loss: 1487361.375 acc: 0.6429662704467773\n",
      "step: 15695\n",
      "train: loss: 14780.609375 acc: 0.9790079593658447  val: loss: 935620.0625 acc: 0.8312387466430664\n",
      "step: 15700\n",
      "train: loss: 10375.2939453125 acc: 0.9903063178062439  val: loss: 198263.609375 acc: 0.9713759422302246\n",
      "step: 15705\n",
      "train: loss: 13694.9375 acc: 0.9885154366493225  val: loss: 515709.6875 acc: 0.862085223197937\n",
      "step: 15710\n",
      "train: loss: 20108.060546875 acc: 0.9944864511489868  val: loss: 1858080.125 acc: 0.5047398805618286\n",
      "step: 15715\n",
      "train: loss: 32204.669921875 acc: 0.9896117448806763  val: loss: 607808.0 acc: 0.9215536117553711\n",
      "step: 15720\n",
      "train: loss: 20369.388671875 acc: 0.995069682598114  val: loss: 2987917.25 acc: 0.13129210472106934\n",
      "step: 15725\n",
      "train: loss: 9883.5498046875 acc: 0.9947968125343323  val: loss: 3027408.75 acc: 0.08021014928817749\n",
      "step: 15730\n",
      "train: loss: 34104.21484375 acc: 0.9791422486305237  val: loss: 808416.125 acc: 0.8033735156059265\n",
      "step: 15735\n",
      "train: loss: 79048.0859375 acc: 0.957144021987915  val: loss: 1089903.125 acc: 0.7372824549674988\n",
      "step: 15740\n",
      "train: loss: 50404.265625 acc: 0.9855089783668518  val: loss: 466175.375 acc: 0.7313564419746399\n",
      "step: 15745\n",
      "train: loss: 135765.84375 acc: 0.978222131729126  val: loss: 1649807.875 acc: 0.5348681807518005\n",
      "step: 15750\n",
      "train: loss: 62260.40234375 acc: 0.9874704480171204  val: loss: 2772129.0 acc: -0.014755606651306152\n",
      "step: 15755\n",
      "train: loss: 128882.4453125 acc: 0.988353967666626  val: loss: 216661.671875 acc: 0.9454068541526794\n",
      "step: 15760\n",
      "train: loss: 85233.9765625 acc: 0.9887548089027405  val: loss: 1078926.25 acc: 0.8729647397994995\n",
      "step: 15765\n",
      "train: loss: 184131.90625 acc: 0.9851357936859131  val: loss: 919254.5625 acc: 0.4081401824951172\n",
      "step: 15770\n",
      "train: loss: 102102.5625 acc: 0.9888763427734375  val: loss: 158769.1875 acc: 0.9137665629386902\n",
      "step: 15775\n",
      "train: loss: 394002.0 acc: 0.9490018486976624  val: loss: 1156864.125 acc: 0.8008670806884766\n",
      "step: 15780\n",
      "train: loss: 109952.9765625 acc: 0.9841201901435852  val: loss: 607582.5625 acc: 0.8028527498245239\n",
      "step: 15785\n",
      "train: loss: 601126.3125 acc: 0.9312316179275513  val: loss: 279034.6875 acc: 0.9368731379508972\n",
      "step: 15790\n",
      "train: loss: 1299327.875 acc: 0.9637923836708069  val: loss: 676325.6875 acc: 0.8058906197547913\n",
      "step: 15795\n",
      "train: loss: 625665.0625 acc: 0.9810099601745605  val: loss: 268544.1875 acc: 0.9404103755950928\n",
      "step: 15800\n",
      "train: loss: 892773.1875 acc: 0.9677308201789856  val: loss: 509775.6875 acc: 0.7370729446411133\n",
      "step: 15805\n",
      "train: loss: 1043276.625 acc: 0.9135388731956482  val: loss: 907364.9375 acc: 0.9001765847206116\n",
      "step: 15810\n",
      "train: loss: 792167.8125 acc: 0.9052093625068665  val: loss: 694063.125 acc: 0.7425320148468018\n",
      "step: 15815\n",
      "train: loss: 182938.765625 acc: 0.9652689695358276  val: loss: 304591.34375 acc: 0.9098855257034302\n",
      "step: 15820\n",
      "train: loss: 560807.6875 acc: 0.9175689220428467  val: loss: 308460.3125 acc: 0.949129045009613\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 15825\n",
      "train: loss: 1765393.875 acc: 0.7005388736724854  val: loss: 822508.75 acc: 0.6161156296730042\n",
      "step: 15830\n",
      "train: loss: 1212612.375 acc: 0.44175177812576294  val: loss: 262728.71875 acc: 0.866519570350647\n",
      "step: 15835\n",
      "train: loss: 760729.9375 acc: 0.7742191553115845  val: loss: 1810178.5 acc: 0.7718919515609741\n",
      "step: 15840\n",
      "train: loss: 524553.4375 acc: 0.8243587613105774  val: loss: 1606219.375 acc: 0.7818391919136047\n",
      "step: 15845\n",
      "train: loss: 522650.65625 acc: 0.8210160732269287  val: loss: 896822.5 acc: 0.8198135495185852\n",
      "step: 15850\n",
      "train: loss: 508871.375 acc: 0.7615734934806824  val: loss: 3798363.75 acc: 0.6479441523551941\n",
      "step: 15855\n",
      "train: loss: 631567.25 acc: 0.8032444715499878  val: loss: 3742883.25 acc: 0.6222786903381348\n",
      "step: 15860\n",
      "train: loss: 745273.8125 acc: 0.7440682649612427  val: loss: 3386414.75 acc: 0.6095701456069946\n",
      "step: 15865\n",
      "train: loss: 108180.296875 acc: 0.9014416337013245  val: loss: 4282302.5 acc: 0.5568046569824219\n",
      "step: 15870\n",
      "train: loss: 58107.8203125 acc: 0.9593760371208191  val: loss: 2620465.5 acc: 0.6134076714515686\n",
      "step: 15875\n",
      "train: loss: 39211.3359375 acc: 0.9679732918739319  val: loss: 5566334.5 acc: 0.517228364944458\n",
      "step: 15880\n",
      "train: loss: 59280.7578125 acc: 0.9555845260620117  val: loss: 633446.375 acc: 0.7013816833496094\n",
      "step: 15885\n",
      "train: loss: 99824.125 acc: 0.921205461025238  val: loss: 3953333.75 acc: 0.5071861743927002\n",
      "step: 15890\n",
      "train: loss: 79726.2578125 acc: 0.931537389755249  val: loss: 4078030.75 acc: 0.5481293201446533\n",
      "step: 15895\n",
      "train: loss: 208488.9375 acc: 0.8660695552825928  val: loss: 3739652.75 acc: 0.5754477977752686\n",
      "step: 15900\n",
      "train: loss: 69520.0859375 acc: 0.9224722981452942  val: loss: 1133343.125 acc: 0.7139250040054321\n",
      "step: 15905\n",
      "train: loss: 225111.578125 acc: 0.7985060811042786  val: loss: 3709082.25 acc: 0.5958381295204163\n",
      "step: 15910\n",
      "train: loss: 608609.4375 acc: 0.7122830152511597  val: loss: 2098584.0 acc: 0.6294824481010437\n",
      "step: 15915\n",
      "train: loss: 349145.34375 acc: 0.7925388813018799  val: loss: 4346712.5 acc: 0.5063173174858093\n",
      "step: 15920\n",
      "train: loss: 61363.6875 acc: 0.946657121181488  val: loss: 754572.8125 acc: 0.7238518595695496\n",
      "step: 15925\n",
      "train: loss: 292589.5625 acc: 0.8393705487251282  val: loss: 1000826.75 acc: 0.705721378326416\n",
      "step: 15930\n",
      "train: loss: 122296.9296875 acc: 0.8987694382667542  val: loss: 2894978.5 acc: 0.6156378984451294\n",
      "step: 15935\n",
      "train: loss: 1673337.625 acc: 0.5918817520141602  val: loss: 1459433.0 acc: 0.6638349294662476\n",
      "step: 15940\n",
      "train: loss: 763099.625 acc: 0.7077043056488037  val: loss: 2698445.5 acc: 0.7315244674682617\n",
      "step: 15945\n",
      "train: loss: 1684858.875 acc: 0.8536695241928101  val: loss: 1385087.25 acc: 0.7816272974014282\n",
      "step: 15950\n",
      "train: loss: 249551.71875 acc: 0.9799636602401733  val: loss: 617193.75 acc: 0.814358115196228\n",
      "step: 15955\n",
      "train: loss: 145484.40625 acc: 0.9784699082374573  val: loss: 1229075.625 acc: 0.8096957206726074\n",
      "step: 15960\n",
      "train: loss: 250247.5 acc: 0.962369978427887  val: loss: 2149185.75 acc: 0.5449674725532532\n",
      "step: 15965\n",
      "train: loss: 118109.2265625 acc: 0.9863621592521667  val: loss: 1150294.375 acc: 0.15420633554458618\n",
      "step: 15970\n",
      "train: loss: 108303.7421875 acc: 0.992767870426178  val: loss: 1702339.25 acc: 0.6733782291412354\n",
      "step: 15975\n",
      "train: loss: 123102.328125 acc: 0.9917874336242676  val: loss: 1308199.0 acc: 0.47701239585876465\n",
      "step: 15980\n",
      "train: loss: 115137.2421875 acc: 0.9846592545509338  val: loss: 1526762.0 acc: 0.5959521532058716\n",
      "step: 15985\n",
      "train: loss: 47392.5625 acc: 0.99409419298172  val: loss: 1292748.875 acc: 0.8607960939407349\n",
      "step: 15990\n",
      "train: loss: 58189.52734375 acc: 0.9840706586837769  val: loss: 1812918.75 acc: 0.4974880814552307\n",
      "step: 15995\n",
      "train: loss: 38592.25 acc: 0.9832819104194641  val: loss: 899197.0 acc: 0.8388136625289917\n",
      "step: 16000\n",
      "train: loss: 1636.5810546875 acc: 0.9954018592834473  val: loss: 1280617.5 acc: 0.8402995467185974\n",
      "step: 16005\n",
      "train: loss: 14782.2802734375 acc: 0.985837996006012  val: loss: 1886145.25 acc: 0.6527747511863708\n",
      "step: 16010\n",
      "train: loss: 20147.685546875 acc: 0.9626904129981995  val: loss: 831189.6875 acc: 0.8020197749137878\n",
      "step: 16015\n",
      "train: loss: 14181.0126953125 acc: 0.9656997323036194  val: loss: 2291636.75 acc: 0.3753620982170105\n",
      "step: 16020\n",
      "train: loss: 9654.7744140625 acc: 0.9797617793083191  val: loss: 2220100.75 acc: 0.5686143040657043\n",
      "step: 16025\n",
      "train: loss: 13627.4443359375 acc: 0.9920409917831421  val: loss: 2051829.5 acc: 0.5636289119720459\n",
      "step: 16030\n",
      "train: loss: 20774.43359375 acc: 0.9865386486053467  val: loss: 324079.5 acc: 0.8013111352920532\n",
      "step: 16035\n",
      "train: loss: 13489.6904296875 acc: 0.9775062799453735  val: loss: 643032.3125 acc: 0.9047418236732483\n",
      "step: 16040\n",
      "train: loss: 30185.0234375 acc: 0.9859206676483154  val: loss: 965466.4375 acc: 0.8317102193832397\n",
      "step: 16045\n",
      "train: loss: 22205.326171875 acc: 0.9875439405441284  val: loss: 1439780.875 acc: 0.7326286435127258\n",
      "step: 16050\n",
      "train: loss: 33335.1328125 acc: 0.9820570349693298  val: loss: 1357240.25 acc: 0.26341813802719116\n",
      "step: 16055\n",
      "train: loss: 7887.44091796875 acc: 0.9943788051605225  val: loss: 2596816.25 acc: -0.6261301040649414\n",
      "step: 16060\n",
      "train: loss: 20144.607421875 acc: 0.9780420064926147  val: loss: 140488.953125 acc: 0.9619168043136597\n",
      "step: 16065\n",
      "train: loss: 13771.451171875 acc: 0.984478235244751  val: loss: 2362678.0 acc: -0.24491798877716064\n",
      "step: 16070\n",
      "train: loss: 10965.8408203125 acc: 0.9943484663963318  val: loss: 2304528.75 acc: 0.510809063911438\n",
      "step: 16075\n",
      "train: loss: 35376.29296875 acc: 0.9885811805725098  val: loss: 3212908.25 acc: -0.4773324728012085\n",
      "step: 16080\n",
      "train: loss: 37049.15234375 acc: 0.9879255294799805  val: loss: 366854.53125 acc: 0.9419096112251282\n",
      "step: 16085\n",
      "train: loss: 24451.53125 acc: 0.9920819997787476  val: loss: 1818843.125 acc: 0.3989103436470032\n",
      "step: 16090\n",
      "train: loss: 16322.7587890625 acc: 0.9896299839019775  val: loss: 27421.09765625 acc: 0.9869339466094971\n",
      "step: 16095\n",
      "train: loss: 38025.1875 acc: 0.9940099716186523  val: loss: 917732.375 acc: 0.6809127926826477\n",
      "step: 16100\n",
      "train: loss: 69665.4296875 acc: 0.9746383428573608  val: loss: 817148.75 acc: 0.785698652267456\n",
      "step: 16105\n",
      "train: loss: 300879.21875 acc: 0.21588802337646484  val: loss: 171467.140625 acc: 0.9383508563041687\n",
      "step: 16110\n",
      "train: loss: 54071.9453125 acc: 0.990613579750061  val: loss: 478703.59375 acc: 0.9432966113090515\n",
      "step: 16115\n",
      "train: loss: 644286.1875 acc: 0.9009016156196594  val: loss: 500981.9375 acc: 0.8801217079162598\n",
      "step: 16120\n",
      "train: loss: 88354.28125 acc: 0.9906948804855347  val: loss: 1247754.75 acc: 0.8684075474739075\n",
      "step: 16125\n",
      "train: loss: 51242.6640625 acc: 0.9926955699920654  val: loss: 234582.390625 acc: 0.9538999795913696\n",
      "step: 16130\n",
      "train: loss: 55103.69921875 acc: 0.9955171942710876  val: loss: 344118.21875 acc: 0.9475210905075073\n",
      "step: 16135\n",
      "train: loss: 380597.5 acc: 0.9654847383499146  val: loss: 1959601.75 acc: 0.4891306161880493\n",
      "step: 16140\n",
      "train: loss: 983382.625 acc: 0.9362688660621643  val: loss: 728032.1875 acc: 0.695611834526062\n",
      "step: 16145\n",
      "train: loss: 183828.71875 acc: 0.9879692196846008  val: loss: 1091297.5 acc: 0.7417242527008057\n",
      "step: 16150\n",
      "train: loss: 368738.875 acc: 0.9791932702064514  val: loss: 966844.4375 acc: 0.7266854047775269\n",
      "step: 16155\n",
      "train: loss: 583655.6875 acc: 0.9712377190589905  val: loss: 795176.125 acc: 0.8465122580528259\n",
      "step: 16160\n",
      "train: loss: 5819032.0 acc: 0.8079729676246643  val: loss: 375815.4375 acc: 0.9356786012649536\n",
      "step: 16165\n",
      "train: loss: 2841633.75 acc: 0.8708473443984985  val: loss: 625136.5625 acc: 0.9254768490791321\n",
      "step: 16170\n",
      "train: loss: 563907.375 acc: 0.9726288914680481  val: loss: 151694.234375 acc: 0.9696851372718811\n",
      "step: 16175\n",
      "train: loss: 963474.5625 acc: 0.9447017908096313  val: loss: 38241.60546875 acc: 0.9866517186164856\n",
      "step: 16180\n",
      "train: loss: 240716.359375 acc: 0.96451735496521  val: loss: 731882.4375 acc: 0.7606923580169678\n",
      "step: 16185\n",
      "train: loss: 208893.03125 acc: 0.9490407705307007  val: loss: 863603.0625 acc: 0.7755509614944458\n",
      "step: 16190\n",
      "train: loss: 2415034.75 acc: 0.6573066711425781  val: loss: 425027.125 acc: 0.9374979734420776\n",
      "step: 16195\n",
      "train: loss: 959464.625 acc: 0.4764329195022583  val: loss: 1651295.75 acc: 0.8050473928451538\n",
      "step: 16200\n",
      "train: loss: 623466.625 acc: 0.7640380859375  val: loss: 1385548.25 acc: 0.7734073996543884\n",
      "step: 16205\n",
      "train: loss: 350455.9375 acc: 0.8342798948287964  val: loss: 2217850.0 acc: 0.7773579955101013\n",
      "step: 16210\n",
      "train: loss: 578425.25 acc: 0.8520636558532715  val: loss: 761333.5 acc: 0.8375862836837769\n",
      "step: 16215\n",
      "train: loss: 1350352.25 acc: 0.18935853242874146  val: loss: 1018757.1875 acc: 0.7909635305404663\n",
      "step: 16220\n",
      "train: loss: 505160.90625 acc: 0.7718724012374878  val: loss: 3250710.25 acc: 0.6758022308349609\n",
      "step: 16225\n",
      "train: loss: 617815.375 acc: 0.7121127843856812  val: loss: 1613321.75 acc: 0.716844916343689\n",
      "step: 16230\n",
      "train: loss: 135217.015625 acc: 0.8850818276405334  val: loss: 1119990.75 acc: 0.6634491086006165\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 16235\n",
      "train: loss: 276836.5 acc: 0.8343451619148254  val: loss: 2758134.75 acc: 0.6792610883712769\n",
      "step: 16240\n",
      "train: loss: 106202.6015625 acc: 0.9219582080841064  val: loss: 353389.125 acc: 0.7756848931312561\n",
      "step: 16245\n",
      "train: loss: 75609.3515625 acc: 0.9432658553123474  val: loss: 603111.9375 acc: 0.7070362567901611\n",
      "step: 16250\n",
      "train: loss: 44959.54296875 acc: 0.9610785841941833  val: loss: 4500069.5 acc: 0.6120052933692932\n",
      "step: 16255\n",
      "train: loss: 260963.015625 acc: 0.8539948463439941  val: loss: 214192.609375 acc: 0.8805081844329834\n",
      "step: 16260\n",
      "train: loss: 294357.21875 acc: 0.8300082683563232  val: loss: 3581982.75 acc: 0.6123054027557373\n",
      "step: 16265\n",
      "train: loss: 107672.5390625 acc: 0.8642816543579102  val: loss: 1000548.125 acc: 0.6866490840911865\n",
      "step: 16270\n",
      "train: loss: 223290.328125 acc: 0.8545737862586975  val: loss: 544285.0625 acc: 0.7473809123039246\n",
      "step: 16275\n",
      "train: loss: 240963.359375 acc: 0.8123747110366821  val: loss: 1424919.0 acc: 0.6904788017272949\n",
      "step: 16280\n",
      "train: loss: 193380.875 acc: 0.8643263578414917  val: loss: 266605.9375 acc: 0.8229458332061768\n",
      "step: 16285\n",
      "train: loss: 172029.8125 acc: 0.8711775541305542  val: loss: 3936916.25 acc: 0.5062733888626099\n",
      "step: 16290\n",
      "train: loss: 631696.4375 acc: 0.748976469039917  val: loss: 2878936.0 acc: 0.5567353963851929\n",
      "step: 16295\n",
      "train: loss: 77497.7265625 acc: 0.9202107787132263  val: loss: 3149120.25 acc: 0.5304825901985168\n",
      "step: 16300\n",
      "train: loss: 1153416.875 acc: 0.6795096397399902  val: loss: 1033218.5625 acc: 0.6467535495758057\n",
      "step: 16305\n",
      "train: loss: 1049051.5 acc: 0.7073290348052979  val: loss: 970458.75 acc: 0.7244049310684204\n",
      "step: 16310\n",
      "train: loss: 1645397.5 acc: 0.8185968399047852  val: loss: 664868.25 acc: 0.7607199549674988\n",
      "step: 16315\n",
      "train: loss: 863487.5 acc: 0.9125375151634216  val: loss: 825570.0 acc: 0.8453973531723022\n",
      "step: 16320\n",
      "train: loss: 499710.03125 acc: 0.9430426359176636  val: loss: 1479033.25 acc: 0.8027322292327881\n",
      "step: 16325\n",
      "train: loss: 361113.03125 acc: 0.9296034574508667  val: loss: 1262148.75 acc: 0.5795366764068604\n",
      "step: 16330\n",
      "train: loss: 125789.9453125 acc: 0.9791324138641357  val: loss: 180527.546875 acc: 0.9779791831970215\n",
      "step: 16335\n",
      "train: loss: 145125.125 acc: 0.9902044534683228  val: loss: 929755.125 acc: 0.8099364042282104\n",
      "step: 16340\n",
      "train: loss: 132399.0 acc: 0.9901178479194641  val: loss: 1605897.75 acc: 0.7815150618553162\n",
      "step: 16345\n",
      "train: loss: 96961.390625 acc: 0.9910915493965149  val: loss: 984900.875 acc: 0.6745100617408752\n",
      "step: 16350\n",
      "train: loss: 108542.109375 acc: 0.9848973155021667  val: loss: 816128.25 acc: 0.8691861033439636\n",
      "step: 16355\n",
      "train: loss: 71227.4765625 acc: 0.9889360070228577  val: loss: 362819.78125 acc: 0.9593492746353149\n",
      "step: 16360\n",
      "train: loss: 38362.4375 acc: 0.9806891679763794  val: loss: 221048.5 acc: 0.9462892413139343\n",
      "step: 16365\n",
      "train: loss: 12339.5732421875 acc: 0.9921513795852661  val: loss: 1240023.125 acc: 0.10728931427001953\n",
      "step: 16370\n",
      "train: loss: 7942.78662109375 acc: 0.9758959412574768  val: loss: 550938.375 acc: 0.8457269072532654\n",
      "step: 16375\n",
      "train: loss: 7595.83935546875 acc: 0.9939894676208496  val: loss: 221718.4375 acc: 0.9723265171051025\n",
      "step: 16380\n",
      "train: loss: 25819.767578125 acc: 0.989429771900177  val: loss: 449701.46875 acc: 0.9231592416763306\n",
      "step: 16385\n",
      "train: loss: 12612.82421875 acc: 0.9783236384391785  val: loss: 1184643.75 acc: 0.5778642892837524\n",
      "step: 16390\n",
      "train: loss: 21231.46875 acc: 0.9701498746871948  val: loss: 566562.8125 acc: 0.42865943908691406\n",
      "step: 16395\n",
      "train: loss: 11477.19921875 acc: 0.9522845149040222  val: loss: 883024.25 acc: 0.8570900559425354\n",
      "step: 16400\n",
      "train: loss: 11777.9833984375 acc: 0.9772855043411255  val: loss: 970757.8125 acc: 0.75773686170578\n",
      "step: 16405\n",
      "train: loss: 11523.7841796875 acc: 0.9809269309043884  val: loss: 888255.625 acc: 0.7479918599128723\n",
      "step: 16410\n",
      "train: loss: 36326.0 acc: 0.9796819090843201  val: loss: 931325.875 acc: 0.8942906260490417\n",
      "step: 16415\n",
      "train: loss: 33249.92578125 acc: 0.9824292659759521  val: loss: 963563.5 acc: 0.6884607076644897\n",
      "step: 16420\n",
      "train: loss: 23704.029296875 acc: 0.989552915096283  val: loss: 102701.390625 acc: 0.986132800579071\n",
      "step: 16425\n",
      "train: loss: 21287.23828125 acc: 0.9811597466468811  val: loss: 1134992.125 acc: 0.18721026182174683\n",
      "step: 16430\n",
      "train: loss: 10507.431640625 acc: 0.9825915098190308  val: loss: 865468.875 acc: 0.7555279731750488\n",
      "step: 16435\n",
      "train: loss: 14797.2109375 acc: 0.9811437726020813  val: loss: 215511.578125 acc: 0.9225590825080872\n",
      "step: 16440\n",
      "train: loss: 27672.599609375 acc: 0.9912104606628418  val: loss: 128991.796875 acc: 0.9641103148460388\n",
      "step: 16445\n",
      "train: loss: 31269.755859375 acc: 0.9918726086616516  val: loss: 785564.875 acc: 0.7203047275543213\n",
      "step: 16450\n",
      "train: loss: 39605.859375 acc: 0.9829209446907043  val: loss: 529678.9375 acc: 0.8621460795402527\n",
      "step: 16455\n",
      "train: loss: 32631.66796875 acc: 0.9904779195785522  val: loss: 189154.203125 acc: 0.9830276966094971\n",
      "step: 16460\n",
      "train: loss: 370893.875 acc: 0.9171682596206665  val: loss: 2306909.5 acc: 0.7047098875045776\n",
      "step: 16465\n",
      "train: loss: 234386.453125 acc: 0.9480594396591187  val: loss: 640085.5 acc: 0.5520356893539429\n",
      "step: 16470\n",
      "train: loss: 36094.12109375 acc: 0.9888580441474915  val: loss: 299907.75 acc: 0.9476917386054993\n",
      "step: 16475\n",
      "train: loss: 679485.4375 acc: 0.8797138333320618  val: loss: 1072058.0 acc: 0.9301242232322693\n",
      "step: 16480\n",
      "train: loss: 211603.046875 acc: 0.9589743614196777  val: loss: 396071.0625 acc: 0.8267287611961365\n",
      "step: 16485\n",
      "train: loss: 64271.46875 acc: 0.9939230680465698  val: loss: 1615469.25 acc: 0.5931364297866821\n",
      "step: 16490\n",
      "train: loss: 190258.796875 acc: 0.9730154275894165  val: loss: 567699.3125 acc: 0.866957426071167\n",
      "step: 16495\n",
      "train: loss: 277097.03125 acc: 0.9505765438079834  val: loss: 549490.75 acc: 0.9169829487800598\n",
      "step: 16500\n",
      "train: loss: 227908.984375 acc: 0.9805557727813721  val: loss: 1984545.75 acc: 0.831933319568634\n",
      "step: 16505\n",
      "train: loss: 576825.8125 acc: 0.9792032837867737  val: loss: 1185135.5 acc: 0.9104860424995422\n",
      "step: 16510\n",
      "train: loss: 277283.40625 acc: 0.9429975748062134  val: loss: 1755406.5 acc: 0.7621193528175354\n",
      "step: 16515\n",
      "train: loss: 368559.59375 acc: 0.968416154384613  val: loss: 830200.9375 acc: 0.7659816145896912\n",
      "step: 16520\n",
      "train: loss: 1058577.625 acc: 0.9645905494689941  val: loss: 531381.875 acc: 0.7088039517402649\n",
      "step: 16525\n",
      "train: loss: 1135734.25 acc: 0.969174325466156  val: loss: 538186.125 acc: 0.9048692584037781\n",
      "step: 16530\n",
      "train: loss: 1993655.375 acc: 0.8730766773223877  val: loss: 1569275.25 acc: 0.4872554540634155\n",
      "step: 16535\n",
      "train: loss: 932429.375 acc: 0.939967155456543  val: loss: 753307.0 acc: 0.777704656124115\n",
      "step: 16540\n",
      "train: loss: 262591.9375 acc: 0.9653304815292358  val: loss: 945102.0 acc: 0.4303515553474426\n",
      "step: 16545\n",
      "train: loss: 532400.5 acc: 0.9459298253059387  val: loss: 1267674.75 acc: 0.8659963607788086\n",
      "step: 16550\n",
      "train: loss: 676773.8125 acc: 0.9345144629478455  val: loss: 1899926.0 acc: 0.7919291853904724\n",
      "step: 16555\n",
      "train: loss: 766601.3125 acc: 0.7255213260650635  val: loss: 413873.40625 acc: 0.9376638531684875\n",
      "step: 16560\n",
      "train: loss: 1592326.625 acc: 0.4590259790420532  val: loss: 259055.828125 acc: 0.7717933654785156\n",
      "step: 16565\n",
      "train: loss: 1968961.0 acc: 0.6611332297325134  val: loss: 734618.125 acc: 0.7956761121749878\n",
      "step: 16570\n",
      "train: loss: 577540.375 acc: 0.6252130270004272  val: loss: 499553.71875 acc: 0.8233923316001892\n",
      "step: 16575\n",
      "train: loss: 353764.875 acc: 0.8771262168884277  val: loss: 1803363.75 acc: 0.7585547566413879\n",
      "step: 16580\n",
      "train: loss: 752507.25 acc: 0.6290539503097534  val: loss: 626658.5 acc: 0.8379952311515808\n",
      "step: 16585\n",
      "train: loss: 611535.25 acc: 0.6992548704147339  val: loss: 867479.75 acc: 0.7109231948852539\n",
      "step: 16590\n",
      "train: loss: 527395.6875 acc: 0.7549395561218262  val: loss: 6795542.5 acc: 0.6144295930862427\n",
      "step: 16595\n",
      "train: loss: 510074.0625 acc: 0.7335004806518555  val: loss: 694383.375 acc: 0.7200473546981812\n",
      "step: 16600\n",
      "train: loss: 249336.125 acc: 0.8467721939086914  val: loss: 1540993.375 acc: 0.66922926902771\n",
      "step: 16605\n",
      "train: loss: 239378.3125 acc: 0.8682535290718079  val: loss: 1160548.25 acc: 0.7472205758094788\n",
      "step: 16610\n",
      "train: loss: 54536.09765625 acc: 0.9572941660881042  val: loss: 1083261.375 acc: 0.7127541899681091\n",
      "step: 16615\n",
      "train: loss: 192262.703125 acc: 0.8813900947570801  val: loss: 1412511.0 acc: 0.6550109386444092\n",
      "step: 16620\n",
      "train: loss: 118373.3125 acc: 0.9111204743385315  val: loss: 3264822.25 acc: 0.5755995512008667\n",
      "step: 16625\n",
      "train: loss: 174250.578125 acc: 0.8842241764068604  val: loss: 838000.125 acc: 0.7319780588150024\n",
      "step: 16630\n",
      "train: loss: 159076.015625 acc: 0.8367348313331604  val: loss: 2041555.0 acc: 0.6557080745697021\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 16635\n",
      "train: loss: 77148.265625 acc: 0.9129199385643005  val: loss: 1604727.25 acc: 0.6956257820129395\n",
      "step: 16640\n",
      "train: loss: 278566.6875 acc: 0.8225895166397095  val: loss: 1446568.75 acc: 0.7272104024887085\n",
      "step: 16645\n",
      "train: loss: 78516.2890625 acc: 0.9219086170196533  val: loss: 3901745.75 acc: 0.5263097286224365\n",
      "step: 16650\n",
      "train: loss: 798090.75 acc: 0.72818922996521  val: loss: 3892712.75 acc: 0.5586755275726318\n",
      "step: 16655\n",
      "train: loss: 704439.375 acc: 0.7352714538574219  val: loss: 1996049.625 acc: 0.658592939376831\n",
      "step: 16660\n",
      "train: loss: 421308.625 acc: 0.7758998870849609  val: loss: 2063253.25 acc: 0.6144293546676636\n",
      "step: 16665\n",
      "train: loss: 1302026.25 acc: 0.7057372331619263  val: loss: 2597470.75 acc: 0.6339113712310791\n",
      "step: 16670\n",
      "train: loss: 1158164.25 acc: 0.750165581703186  val: loss: 2832009.5 acc: 0.7552253007888794\n",
      "step: 16675\n",
      "train: loss: 1411457.625 acc: 0.8189783096313477  val: loss: 913812.9375 acc: 0.7261862754821777\n",
      "step: 16680\n",
      "train: loss: 828811.25 acc: 0.933578610420227  val: loss: 706527.4375 acc: 0.7835621237754822\n",
      "step: 16685\n",
      "train: loss: 1002313.3125 acc: 0.8511018753051758  val: loss: 509093.1875 acc: 0.8721925020217896\n",
      "step: 16690\n",
      "train: loss: 322194.0 acc: 0.9259018898010254  val: loss: 379607.75 acc: 0.9477322697639465\n",
      "step: 16695\n",
      "train: loss: 181315.609375 acc: 0.9666671752929688  val: loss: 747248.875 acc: 0.5516080260276794\n",
      "step: 16700\n",
      "train: loss: 91602.9609375 acc: 0.9926266670227051  val: loss: 994210.375 acc: 0.6773783564567566\n",
      "step: 16705\n",
      "train: loss: 68616.8515625 acc: 0.9956035017967224  val: loss: 1916185.0 acc: 0.4967842102050781\n",
      "step: 16710\n",
      "train: loss: 96842.2734375 acc: 0.989369809627533  val: loss: 602207.0625 acc: 0.8784259557723999\n",
      "step: 16715\n",
      "train: loss: 93117.1015625 acc: 0.9817839860916138  val: loss: 406368.25 acc: 0.8024623990058899\n",
      "step: 16720\n",
      "train: loss: 19398.599609375 acc: 0.9934676885604858  val: loss: 265569.875 acc: 0.9590169787406921\n",
      "step: 16725\n",
      "train: loss: 15513.7275390625 acc: 0.9903128147125244  val: loss: 391789.4375 acc: 0.9244393706321716\n",
      "step: 16730\n",
      "train: loss: 2100.267333984375 acc: 0.9860661029815674  val: loss: 460261.34375 acc: 0.8710451126098633\n",
      "step: 16735\n",
      "train: loss: 11077.1474609375 acc: 0.9822179675102234  val: loss: 1207589.875 acc: 0.6722487211227417\n",
      "step: 16740\n",
      "train: loss: 217252.671875 acc: 0.7034301161766052  val: loss: 682092.9375 acc: 0.6827912330627441\n",
      "step: 16745\n",
      "train: loss: 26913.78125 acc: 0.9632598161697388  val: loss: 597263.1875 acc: 0.7049429416656494\n",
      "step: 16750\n",
      "train: loss: 5336.37646484375 acc: 0.9841366410255432  val: loss: 292637.03125 acc: 0.9478551149368286\n",
      "step: 16755\n",
      "train: loss: 21772.06640625 acc: 0.9513992667198181  val: loss: 177224.296875 acc: 0.9376971125602722\n",
      "step: 16760\n",
      "train: loss: 17564.158203125 acc: 0.9843834638595581  val: loss: 325035.90625 acc: 0.9084991216659546\n",
      "step: 16765\n",
      "train: loss: 14836.5849609375 acc: 0.9922775626182556  val: loss: 279299.59375 acc: 0.9183387160301208\n",
      "step: 16770\n",
      "train: loss: 54487.1875 acc: 0.9752031564712524  val: loss: 2597705.5 acc: 0.5816351175308228\n",
      "step: 16775\n",
      "train: loss: 29255.33203125 acc: 0.982266366481781  val: loss: 650247.3125 acc: 0.9149941205978394\n",
      "step: 16780\n",
      "train: loss: 52071.2578125 acc: 0.9755524396896362  val: loss: 631551.125 acc: 0.8850648999214172\n",
      "step: 16785\n",
      "train: loss: 54263.91796875 acc: 0.9810394644737244  val: loss: 414727.4375 acc: 0.9069056510925293\n",
      "step: 16790\n",
      "train: loss: 10905.341796875 acc: 0.9926528334617615  val: loss: 1004852.25 acc: 0.8668394684791565\n",
      "step: 16795\n",
      "train: loss: 21743.859375 acc: 0.9773553013801575  val: loss: 1234412.75 acc: 0.8887699842453003\n",
      "step: 16800\n",
      "train: loss: 4062.449462890625 acc: 0.9969052076339722  val: loss: 492416.8125 acc: 0.880911111831665\n",
      "step: 16805\n",
      "train: loss: 65241.83203125 acc: 0.9736766815185547  val: loss: 1397045.625 acc: 0.5933897495269775\n",
      "step: 16810\n",
      "train: loss: 30415.7578125 acc: 0.9901015162467957  val: loss: 1831977.25 acc: 0.7722305059432983\n",
      "step: 16815\n",
      "train: loss: 39664.00390625 acc: 0.9911050200462341  val: loss: 281460.75 acc: 0.8454983234405518\n",
      "step: 16820\n",
      "train: loss: 39984.08203125 acc: 0.9857096672058105  val: loss: 733178.5 acc: 0.8978918790817261\n",
      "step: 16825\n",
      "train: loss: 66619.375 acc: 0.9847502708435059  val: loss: 471691.40625 acc: 0.7166856527328491\n",
      "step: 16830\n",
      "train: loss: 245712.4375 acc: 0.9370705485343933  val: loss: 1970955.5 acc: 0.2496923804283142\n",
      "step: 16835\n",
      "train: loss: 86683.1796875 acc: 0.9698824286460876  val: loss: 1931472.125 acc: 0.6885852813720703\n",
      "step: 16840\n",
      "train: loss: 202263.75 acc: 0.9187133312225342  val: loss: 1547640.5 acc: 0.7646456360816956\n",
      "step: 16845\n",
      "train: loss: 260125.453125 acc: 0.9645366072654724  val: loss: 465049.0 acc: 0.937523365020752\n",
      "step: 16850\n",
      "train: loss: 81646.5859375 acc: 0.992302656173706  val: loss: 1761735.875 acc: 0.8273904323577881\n",
      "step: 16855\n",
      "train: loss: 74423.6015625 acc: 0.9902340769767761  val: loss: 3820220.75 acc: -0.33064496517181396\n",
      "step: 16860\n",
      "train: loss: 54847.7734375 acc: 0.9904212951660156  val: loss: 288152.21875 acc: 0.9420378804206848\n",
      "step: 16865\n",
      "train: loss: 212656.734375 acc: 0.9742603302001953  val: loss: 937763.625 acc: 0.862089991569519\n",
      "step: 16870\n",
      "train: loss: 538181.4375 acc: 0.97748863697052  val: loss: 443801.09375 acc: 0.9168744087219238\n",
      "step: 16875\n",
      "train: loss: 185693.234375 acc: 0.9428437948226929  val: loss: 308536.46875 acc: 0.7647413611412048\n",
      "step: 16880\n",
      "train: loss: 285784.875 acc: 0.9623010158538818  val: loss: 1534238.75 acc: 0.8589950799942017\n",
      "step: 16885\n",
      "train: loss: 804201.4375 acc: 0.9647918939590454  val: loss: 1011692.125 acc: 0.7370266318321228\n",
      "step: 16890\n",
      "train: loss: 2139019.25 acc: 0.9427264332771301  val: loss: 274721.96875 acc: 0.8913825750350952\n",
      "step: 16895\n",
      "train: loss: 3942378.75 acc: 0.838841438293457  val: loss: 622628.625 acc: 0.8897250294685364\n",
      "step: 16900\n",
      "train: loss: 1693217.25 acc: 0.8957130312919617  val: loss: 2300242.25 acc: 0.6085820198059082\n",
      "step: 16905\n",
      "train: loss: 459755.65625 acc: 0.959640383720398  val: loss: 1133249.5 acc: -0.13679325580596924\n",
      "step: 16910\n",
      "train: loss: 285446.8125 acc: 0.9754712581634521  val: loss: 311663.90625 acc: 0.9288399815559387\n",
      "step: 16915\n",
      "train: loss: 1030881.6875 acc: 0.9158791303634644  val: loss: 1789821.5 acc: 0.8707595467567444\n",
      "step: 16920\n",
      "train: loss: 774917.25 acc: 0.854935348033905  val: loss: 468437.28125 acc: 0.8778026103973389\n",
      "step: 16925\n",
      "train: loss: 1264915.625 acc: 0.3912227153778076  val: loss: 464573.84375 acc: 0.8294061422348022\n",
      "step: 16930\n",
      "train: loss: 978349.5 acc: 0.8135523796081543  val: loss: 682682.125 acc: 0.8451299071311951\n",
      "step: 16935\n",
      "train: loss: 374527.78125 acc: 0.846607506275177  val: loss: 593648.75 acc: 0.7736188173294067\n",
      "step: 16940\n",
      "train: loss: 825549.8125 acc: 0.803841233253479  val: loss: 936966.5625 acc: 0.7466094493865967\n",
      "step: 16945\n",
      "train: loss: 957526.5 acc: 0.6144499778747559  val: loss: 1519443.5 acc: 0.7304224967956543\n",
      "step: 16950\n",
      "train: loss: 474003.6875 acc: 0.685706615447998  val: loss: 3032870.5 acc: 0.6557790040969849\n",
      "step: 16955\n",
      "train: loss: 340488.78125 acc: 0.7809509038925171  val: loss: 3048528.75 acc: 0.6446555256843567\n",
      "step: 16960\n",
      "train: loss: 58344.3359375 acc: 0.946468710899353  val: loss: 2425581.75 acc: 0.6582833528518677\n",
      "step: 16965\n",
      "train: loss: 120715.8359375 acc: 0.899395227432251  val: loss: 2336968.5 acc: 0.6633113622665405\n",
      "step: 16970\n",
      "train: loss: 57928.5859375 acc: 0.9525041580200195  val: loss: 753653.375 acc: 0.7391991019248962\n",
      "step: 16975\n",
      "train: loss: 128022.203125 acc: 0.9112944006919861  val: loss: 1342849.0 acc: 0.6836199760437012\n",
      "step: 16980\n",
      "train: loss: 28906.017578125 acc: 0.9789310693740845  val: loss: 2114904.25 acc: 0.6253087520599365\n",
      "step: 16985\n",
      "train: loss: 31363.119140625 acc: 0.9740407466888428  val: loss: 716127.0 acc: 0.7217778563499451\n",
      "step: 16990\n",
      "train: loss: 121477.9765625 acc: 0.900282084941864  val: loss: 689005.4375 acc: 0.7399388551712036\n",
      "step: 16995\n",
      "train: loss: 34888.5625 acc: 0.9679213762283325  val: loss: 916747.25 acc: 0.6724454164505005\n",
      "step: 17000\n",
      "train: loss: 26167.080078125 acc: 0.971447229385376  val: loss: 1603839.0 acc: 0.6186981201171875\n",
      "step: 17005\n",
      "train: loss: 278399.4375 acc: 0.8467227220535278  val: loss: 546482.6875 acc: 0.7437676191329956\n",
      "step: 17010\n",
      "train: loss: 41832.29296875 acc: 0.9554367661476135  val: loss: 374579.84375 acc: 0.7376878261566162\n",
      "step: 17015\n",
      "train: loss: 599781.0625 acc: 0.7434629201889038  val: loss: 790872.25 acc: 0.7540806531906128\n",
      "step: 17020\n",
      "train: loss: 331487.84375 acc: 0.8075410723686218  val: loss: 708498.9375 acc: 0.7029989957809448\n",
      "step: 17025\n",
      "train: loss: 220880.65625 acc: 0.8451228141784668  val: loss: 923828.5625 acc: 0.7238320112228394\n",
      "step: 17030\n",
      "train: loss: 547622.4375 acc: 0.7932349443435669  val: loss: 771800.625 acc: 0.7557051181793213\n",
      "step: 17035\n",
      "train: loss: 1413546.0 acc: 0.7812696099281311  val: loss: 747700.3125 acc: 0.7195330858230591\n",
      "step: 17040\n",
      "train: loss: 1514384.25 acc: 0.7789492607116699  val: loss: 676937.75 acc: 0.8544486165046692\n",
      "step: 17045\n",
      "train: loss: 528142.3125 acc: 0.9612479209899902  val: loss: 801762.875 acc: 0.791495144367218\n",
      "step: 17050\n",
      "train: loss: 846897.1875 acc: 0.8794752359390259  val: loss: 441037.84375 acc: 0.8647937774658203\n",
      "step: 17055\n",
      "train: loss: 214877.390625 acc: 0.9701584577560425  val: loss: 401367.65625 acc: 0.8091516494750977\n",
      "step: 17060\n",
      "train: loss: 187328.0 acc: 0.9742913842201233  val: loss: 305192.78125 acc: 0.960171103477478\n",
      "step: 17065\n",
      "train: loss: 90518.4453125 acc: 0.9926422834396362  val: loss: 299558.65625 acc: 0.9229931831359863\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 17070\n",
      "train: loss: 97086.0 acc: 0.9935159683227539  val: loss: 1040579.3125 acc: 0.7566583156585693\n",
      "step: 17075\n",
      "train: loss: 67175.6484375 acc: 0.9930437207221985  val: loss: 510009.0 acc: 0.8759853839874268\n",
      "step: 17080\n",
      "train: loss: 67692.3125 acc: 0.9910433888435364  val: loss: 666546.0625 acc: 0.8602719306945801\n",
      "step: 17085\n",
      "train: loss: 45232.67578125 acc: 0.9804835319519043  val: loss: 353701.59375 acc: 0.9471211433410645\n",
      "step: 17090\n",
      "train: loss: 29405.177734375 acc: 0.9909893870353699  val: loss: 677220.5625 acc: 0.9360329508781433\n",
      "step: 17095\n",
      "train: loss: 16968.626953125 acc: 0.9952294826507568  val: loss: 872900.875 acc: 0.9457321763038635\n",
      "step: 17100\n",
      "train: loss: 8933.7919921875 acc: 0.977077841758728  val: loss: 423375.0 acc: 0.9640690684318542\n",
      "step: 17105\n",
      "train: loss: 14989.779296875 acc: 0.9720883965492249  val: loss: 505878.34375 acc: 0.9199908375740051\n",
      "step: 17110\n",
      "train: loss: 15267.4365234375 acc: 0.9924668073654175  val: loss: 720543.9375 acc: 0.9205304384231567\n",
      "step: 17115\n",
      "train: loss: 20826.029296875 acc: 0.9891043305397034  val: loss: 453285.28125 acc: 0.8845922350883484\n",
      "step: 17120\n",
      "train: loss: 18153.029296875 acc: 0.9789133667945862  val: loss: 1623147.875 acc: 0.7851429581642151\n",
      "step: 17125\n",
      "train: loss: 16991.44921875 acc: 0.9908148050308228  val: loss: 826302.4375 acc: 0.8688092231750488\n",
      "step: 17130\n",
      "train: loss: 18146.251953125 acc: 0.9615302085876465  val: loss: 842821.8125 acc: 0.8694583177566528\n",
      "step: 17135\n",
      "train: loss: 21856.849609375 acc: 0.9655295610427856  val: loss: 474757.84375 acc: 0.8711680769920349\n",
      "step: 17140\n",
      "train: loss: 12566.7060546875 acc: 0.9932335615158081  val: loss: 1180579.0 acc: 0.8318972587585449\n",
      "step: 17145\n",
      "train: loss: 11925.5009765625 acc: 0.9827674031257629  val: loss: 601556.625 acc: 0.9288713335990906\n",
      "step: 17150\n",
      "train: loss: 4599.0302734375 acc: 0.9960609078407288  val: loss: 1574270.25 acc: 0.10797327756881714\n",
      "step: 17155\n",
      "train: loss: 19042.462890625 acc: 0.989973783493042  val: loss: 1531024.875 acc: 0.8511286973953247\n",
      "step: 17160\n",
      "train: loss: 24640.11328125 acc: 0.9871200919151306  val: loss: 3217212.25 acc: -1.5240590572357178\n",
      "step: 17165\n",
      "train: loss: 84039.4921875 acc: 0.8669081330299377  val: loss: 2219707.25 acc: 0.4824039936065674\n",
      "step: 17170\n",
      "train: loss: 19028.87890625 acc: 0.9930325150489807  val: loss: 2728109.0 acc: 0.5460414886474609\n",
      "step: 17175\n",
      "train: loss: 22523.150390625 acc: 0.9929845333099365  val: loss: 1224291.625 acc: 0.7138534784317017\n",
      "step: 17180\n",
      "train: loss: 20371.103515625 acc: 0.9947950839996338  val: loss: 1501746.5 acc: 0.8025646805763245\n",
      "step: 17185\n",
      "train: loss: 13998.380859375 acc: 0.9936931133270264  val: loss: 617505.6875 acc: 0.8405017256736755\n",
      "step: 17190\n",
      "train: loss: 81690.1953125 acc: 0.9718995690345764  val: loss: 1567173.25 acc: 0.4116330146789551\n",
      "step: 17195\n",
      "train: loss: 94196.5859375 acc: 0.9711316227912903  val: loss: 1088455.0 acc: 0.520491361618042\n",
      "step: 17200\n",
      "train: loss: 102660.4375 acc: 0.9106265902519226  val: loss: 857446.8125 acc: -0.16521048545837402\n",
      "step: 17205\n",
      "train: loss: 148737.90625 acc: 0.9720078706741333  val: loss: 1741968.375 acc: 0.44630128145217896\n",
      "step: 17210\n",
      "train: loss: 119043.375 acc: 0.982225239276886  val: loss: 2939125.25 acc: 0.6841784715652466\n",
      "step: 17215\n",
      "train: loss: 138063.078125 acc: 0.9830366373062134  val: loss: 1739727.125 acc: 0.02754288911819458\n",
      "step: 17220\n",
      "train: loss: 64702.51953125 acc: 0.9930214285850525  val: loss: 1648287.75 acc: 0.8175804615020752\n",
      "step: 17225\n",
      "train: loss: 68028.5859375 acc: 0.980341374874115  val: loss: 1299868.25 acc: 0.6472557783126831\n",
      "step: 17230\n",
      "train: loss: 175618.703125 acc: 0.9647691249847412  val: loss: 320804.625 acc: 0.9728800654411316\n",
      "step: 17235\n",
      "train: loss: 1933552.0 acc: 0.8861119747161865  val: loss: 1983033.625 acc: 0.7562055587768555\n",
      "step: 17240\n",
      "train: loss: 343453.9375 acc: 0.9541133046150208  val: loss: 1195791.875 acc: 0.673026442527771\n",
      "step: 17245\n",
      "train: loss: 223765.484375 acc: 0.9625977277755737  val: loss: 752631.3125 acc: 0.800226628780365\n",
      "step: 17250\n",
      "train: loss: 1284509.375 acc: 0.9391384720802307  val: loss: 695493.3125 acc: 0.6034265756607056\n",
      "step: 17255\n",
      "train: loss: 3411139.0 acc: 0.9000307321548462  val: loss: 482258.53125 acc: 0.9094915390014648\n",
      "step: 17260\n",
      "train: loss: 1565461.875 acc: 0.9498081207275391  val: loss: 1491374.25 acc: 0.46372759342193604\n",
      "step: 17265\n",
      "train: loss: 1940735.125 acc: 0.9296995997428894  val: loss: 1019176.6875 acc: 0.8279105424880981\n",
      "step: 17270\n",
      "train: loss: 570167.375 acc: 0.9686703681945801  val: loss: 663633.125 acc: 0.8021821975708008\n",
      "step: 17275\n",
      "train: loss: 1190118.375 acc: 0.8923496007919312  val: loss: 1215694.0 acc: 0.640209436416626\n",
      "step: 17280\n",
      "train: loss: 235995.515625 acc: 0.9679104685783386  val: loss: 440081.34375 acc: 0.8223559856414795\n",
      "step: 17285\n",
      "train: loss: 200751.6875 acc: 0.9447342157363892  val: loss: 1489693.875 acc: 0.8348276615142822\n",
      "step: 17290\n",
      "train: loss: 1600219.125 acc: 0.3506302833557129  val: loss: 419008.15625 acc: 0.833662211894989\n",
      "step: 17295\n",
      "train: loss: 302845.375 acc: 0.767524003982544  val: loss: 890071.375 acc: 0.7931585311889648\n",
      "step: 17300\n",
      "train: loss: 288390.75 acc: 0.8847824335098267  val: loss: 839780.0625 acc: 0.8611368536949158\n",
      "step: 17305\n",
      "train: loss: 748429.0 acc: 0.8231796026229858  val: loss: 1926462.75 acc: 0.7891917824745178\n",
      "step: 17310\n",
      "train: loss: 771971.4375 acc: 0.7427181005477905  val: loss: 960059.0 acc: 0.8142871856689453\n",
      "step: 17315\n",
      "train: loss: 508013.375 acc: 0.6841530203819275  val: loss: 2149331.0 acc: 0.6417423486709595\n",
      "step: 17320\n",
      "train: loss: 390995.78125 acc: 0.7623850107192993  val: loss: 776509.0625 acc: 0.7324554920196533\n",
      "step: 17325\n",
      "train: loss: 163920.8125 acc: 0.8633152842521667  val: loss: 1322289.75 acc: 0.6137909293174744\n",
      "step: 17330\n",
      "train: loss: 41340.3359375 acc: 0.9606161117553711  val: loss: 1300239.375 acc: 0.702121913433075\n",
      "step: 17335\n",
      "train: loss: 425700.375 acc: 0.802742600440979  val: loss: 1683090.0 acc: 0.6221858263015747\n",
      "step: 17340\n",
      "train: loss: 50212.06640625 acc: 0.959517240524292  val: loss: 996580.1875 acc: 0.6905312538146973\n",
      "step: 17345\n",
      "train: loss: 280599.28125 acc: 0.8323876261711121  val: loss: 2760822.25 acc: 0.585124135017395\n",
      "step: 17350\n",
      "train: loss: 298432.71875 acc: 0.8402864933013916  val: loss: 1287628.125 acc: 0.6421489715576172\n",
      "step: 17355\n",
      "train: loss: 51160.72265625 acc: 0.9573493003845215  val: loss: 631753.6875 acc: 0.7322429418563843\n",
      "step: 17360\n",
      "train: loss: 269972.71875 acc: 0.7176083326339722  val: loss: 2206622.25 acc: 0.6703629493713379\n",
      "step: 17365\n",
      "train: loss: 253852.40625 acc: 0.8239241242408752  val: loss: 1079670.25 acc: 0.700373649597168\n",
      "step: 17370\n",
      "train: loss: 52042.8671875 acc: 0.9455633163452148  val: loss: 821292.0625 acc: 0.740598201751709\n",
      "step: 17375\n",
      "train: loss: 942510.8125 acc: 0.7089841365814209  val: loss: 2174179.75 acc: 0.5989563465118408\n",
      "step: 17380\n",
      "train: loss: 86781.5546875 acc: 0.9105669856071472  val: loss: 1194851.625 acc: 0.6780608296394348\n",
      "step: 17385\n",
      "train: loss: 165898.015625 acc: 0.868496298789978  val: loss: 1011893.9375 acc: 0.6789770722389221\n",
      "step: 17390\n",
      "train: loss: 46157.796875 acc: 0.9489340782165527  val: loss: 1167265.875 acc: 0.641081690788269\n",
      "step: 17395\n",
      "train: loss: 1116857.75 acc: 0.7066377401351929  val: loss: 3165429.0 acc: 0.5824717283248901\n",
      "step: 17400\n",
      "train: loss: 1453222.625 acc: 0.716171383857727  val: loss: 1524509.5 acc: 0.7712060213088989\n",
      "step: 17405\n",
      "train: loss: 1106585.5 acc: 0.7923753261566162  val: loss: 531384.5625 acc: 0.7556484937667847\n",
      "step: 17410\n",
      "train: loss: 687802.625 acc: 0.9373621344566345  val: loss: 1007057.5 acc: 0.8404526710510254\n",
      "step: 17415\n",
      "train: loss: 581873.5 acc: 0.9558128118515015  val: loss: 1445732.625 acc: 0.7752237319946289\n",
      "step: 17420\n",
      "train: loss: 230539.09375 acc: 0.958327054977417  val: loss: 995309.125 acc: 0.8304848074913025\n",
      "step: 17425\n",
      "train: loss: 209243.609375 acc: 0.9732238054275513  val: loss: 664814.4375 acc: 0.954784631729126\n",
      "step: 17430\n",
      "train: loss: 95577.046875 acc: 0.9912741780281067  val: loss: 1760226.125 acc: 0.644156813621521\n",
      "step: 17435\n",
      "train: loss: 69901.984375 acc: 0.9950760006904602  val: loss: 801901.75 acc: 0.9507990479469299\n",
      "step: 17440\n",
      "train: loss: 98244.71875 acc: 0.9924122095108032  val: loss: 784395.6875 acc: 0.8693451881408691\n",
      "step: 17445\n",
      "train: loss: 37891.26953125 acc: 0.995575487613678  val: loss: 1123444.625 acc: 0.7881544828414917\n",
      "step: 17450\n",
      "train: loss: 21513.27734375 acc: 0.9833709597587585  val: loss: 2219952.0 acc: 0.5663467645645142\n",
      "step: 17455\n",
      "train: loss: 6854.52587890625 acc: 0.9889381527900696  val: loss: 770660.5 acc: 0.9021252989768982\n",
      "step: 17460\n",
      "train: loss: 16100.1982421875 acc: 0.9936103820800781  val: loss: 759109.25 acc: 0.8244248032569885\n",
      "step: 17465\n",
      "train: loss: 9526.99609375 acc: 0.9945283532142639  val: loss: 1754968.375 acc: 0.7752906680107117\n",
      "step: 17470\n",
      "train: loss: 3496.947265625 acc: 0.9891499280929565  val: loss: 2656883.25 acc: 0.1444372534751892\n",
      "step: 17475\n",
      "train: loss: 43327.6796875 acc: 0.96197509765625  val: loss: 1413380.25 acc: 0.6228289604187012\n",
      "step: 17480\n",
      "train: loss: 3504.6142578125 acc: 0.992365837097168  val: loss: 2628558.75 acc: 0.3553934097290039\n",
      "step: 17485\n",
      "train: loss: 6551.80517578125 acc: 0.9826974868774414  val: loss: 1811043.75 acc: 0.24711501598358154\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 17490\n",
      "train: loss: 12293.3525390625 acc: 0.9722901582717896  val: loss: 1187882.375 acc: 0.7302143573760986\n",
      "step: 17495\n",
      "train: loss: 11291.1201171875 acc: 0.9659985899925232  val: loss: 330410.9375 acc: 0.9382554292678833\n",
      "step: 17500\n",
      "train: loss: 26159.43359375 acc: 0.9739751815795898  val: loss: 4290419.0 acc: 0.5347709059715271\n",
      "step: 17505\n",
      "train: loss: 20405.74609375 acc: 0.9891613721847534  val: loss: 813028.4375 acc: 0.7611283659934998\n",
      "step: 17510\n",
      "train: loss: 33385.97265625 acc: 0.9826666712760925  val: loss: 2043785.625 acc: -0.32151293754577637\n",
      "step: 17515\n",
      "train: loss: 34548.21875 acc: 0.9850960969924927  val: loss: 1152603.875 acc: 0.7977690696716309\n",
      "step: 17520\n",
      "train: loss: 14231.525390625 acc: 0.992936909198761  val: loss: 2035277.625 acc: 0.5538550615310669\n",
      "step: 17525\n",
      "train: loss: 8750.888671875 acc: 0.9739457964897156  val: loss: 3323310.25 acc: 0.07535618543624878\n",
      "step: 17530\n",
      "train: loss: 9055.7373046875 acc: 0.9895439147949219  val: loss: 3125205.0 acc: -1.0011203289031982\n",
      "step: 17535\n",
      "train: loss: 17582.103515625 acc: 0.9945296049118042  val: loss: 1505489.5 acc: 0.7673912048339844\n",
      "step: 17540\n",
      "train: loss: 44772.71484375 acc: 0.9847230911254883  val: loss: 2005287.375 acc: 0.5147382616996765\n",
      "step: 17545\n",
      "train: loss: 33692.17578125 acc: 0.991411566734314  val: loss: 1420784.625 acc: 0.6848071813583374\n",
      "step: 17550\n",
      "train: loss: 30047.783203125 acc: 0.9867186546325684  val: loss: 2586482.25 acc: -1.228278398513794\n",
      "step: 17555\n",
      "train: loss: 14098.888671875 acc: 0.9945449829101562  val: loss: 1710503.625 acc: 0.8316707015037537\n",
      "step: 17560\n",
      "train: loss: 89850.3828125 acc: 0.9766363501548767  val: loss: 1781787.25 acc: 0.41640347242355347\n",
      "step: 17565\n",
      "train: loss: 49490.2578125 acc: 0.9720070958137512  val: loss: 1255860.0 acc: 0.6632707715034485\n",
      "step: 17570\n",
      "train: loss: 145957.03125 acc: 0.912219226360321  val: loss: 1191954.25 acc: 0.38573604822158813\n",
      "step: 17575\n",
      "train: loss: 390167.84375 acc: 0.9445087909698486  val: loss: 1462509.125 acc: 0.4503616690635681\n",
      "step: 17580\n",
      "train: loss: 707756.5625 acc: 0.9152489304542542  val: loss: 2547226.25 acc: 0.7384771108627319\n",
      "step: 17585\n",
      "train: loss: 508839.34375 acc: 0.9310807585716248  val: loss: 1685335.875 acc: 0.8640179634094238\n",
      "step: 17590\n",
      "train: loss: 32422.59765625 acc: 0.9961299300193787  val: loss: 666679.8125 acc: 0.5903546810150146\n",
      "step: 17595\n",
      "train: loss: 90546.0390625 acc: 0.9889087080955505  val: loss: 313132.125 acc: 0.9470595121383667\n",
      "step: 17600\n",
      "train: loss: 676561.6875 acc: 0.9678130745887756  val: loss: 1824567.875 acc: 0.43978118896484375\n",
      "step: 17605\n",
      "train: loss: 293705.53125 acc: 0.9794677495956421  val: loss: 900872.5 acc: 0.7360026836395264\n",
      "step: 17610\n",
      "train: loss: 481413.8125 acc: 0.9650811553001404  val: loss: 2439358.25 acc: 0.2625446319580078\n",
      "step: 17615\n",
      "train: loss: 1405227.75 acc: 0.9364886283874512  val: loss: 611887.3125 acc: 0.9398854970932007\n",
      "step: 17620\n",
      "train: loss: 665635.0625 acc: 0.9788247346878052  val: loss: 744020.625 acc: 0.8404873013496399\n",
      "step: 17625\n",
      "train: loss: 1311046.625 acc: 0.9605822563171387  val: loss: 807550.1875 acc: 0.7794226408004761\n",
      "step: 17630\n",
      "train: loss: 1625361.75 acc: 0.9442270398139954  val: loss: 453836.5 acc: 0.7857556343078613\n",
      "step: 17635\n",
      "train: loss: 145736.484375 acc: 0.9879587292671204  val: loss: 991479.9375 acc: 0.8479688167572021\n",
      "step: 17640\n",
      "train: loss: 515179.8125 acc: 0.9624532461166382  val: loss: 212293.1875 acc: 0.9583557844161987\n",
      "step: 17645\n",
      "train: loss: 92280.796875 acc: 0.9699036478996277  val: loss: 1416710.875 acc: 0.6258482933044434\n",
      "step: 17650\n",
      "train: loss: 322570.03125 acc: 0.9588266611099243  val: loss: 261436.546875 acc: 0.8742281198501587\n",
      "step: 17655\n",
      "train: loss: 2403395.25 acc: -0.31021976470947266  val: loss: 732488.1875 acc: 0.7770191431045532\n",
      "step: 17660\n",
      "train: loss: 484903.5 acc: 0.6893847584724426  val: loss: 1917344.625 acc: 0.7048908472061157\n",
      "step: 17665\n",
      "train: loss: 216277.921875 acc: 0.9105942249298096  val: loss: 810971.0 acc: 0.8042399883270264\n",
      "step: 17670\n",
      "train: loss: 552254.1875 acc: 0.8134670853614807  val: loss: 775529.375 acc: 0.7935532927513123\n",
      "step: 17675\n",
      "train: loss: 459585.75 acc: 0.779920756816864  val: loss: 290565.0 acc: 0.8824661374092102\n",
      "step: 17680\n",
      "train: loss: 2285390.25 acc: 0.6582810878753662  val: loss: 867093.375 acc: 0.7232012748718262\n",
      "step: 17685\n",
      "train: loss: 718316.625 acc: 0.6714076995849609  val: loss: 1369076.75 acc: 0.6882401704788208\n",
      "step: 17690\n",
      "train: loss: 276083.84375 acc: 0.8323588967323303  val: loss: 1226253.375 acc: 0.6371909379959106\n",
      "step: 17695\n",
      "train: loss: 132476.015625 acc: 0.9076088666915894  val: loss: 990600.0625 acc: 0.7392541170120239\n",
      "step: 17700\n",
      "train: loss: 229069.0 acc: 0.8665074110031128  val: loss: 1166586.0 acc: 0.6803776025772095\n",
      "step: 17705\n",
      "train: loss: 40280.26953125 acc: 0.9650613069534302  val: loss: 2304087.25 acc: 0.602536678314209\n",
      "step: 17710\n",
      "train: loss: 57568.484375 acc: 0.9529728889465332  val: loss: 727144.3125 acc: 0.7322512865066528\n",
      "step: 17715\n",
      "train: loss: 103544.484375 acc: 0.9258143901824951  val: loss: 3401256.25 acc: 0.5807281732559204\n",
      "step: 17720\n",
      "train: loss: 307981.4375 acc: 0.8396493196487427  val: loss: 547575.6875 acc: 0.6633762121200562\n",
      "step: 17725\n",
      "train: loss: 155693.125 acc: 0.8812878131866455  val: loss: 4350736.0 acc: 0.5563638806343079\n",
      "step: 17730\n",
      "train: loss: 157704.96875 acc: 0.87494295835495  val: loss: 3728603.75 acc: 0.599584698677063\n",
      "step: 17735\n",
      "train: loss: 98638.0859375 acc: 0.9286505579948425  val: loss: 4776420.5 acc: 0.5939065217971802\n",
      "step: 17740\n",
      "train: loss: 165073.953125 acc: 0.8563019633293152  val: loss: 4779004.5 acc: 0.5015699863433838\n",
      "step: 17745\n",
      "train: loss: 145035.515625 acc: 0.8409793972969055  val: loss: 384306.84375 acc: 0.7808754444122314\n",
      "step: 17750\n",
      "train: loss: 921962.6875 acc: 0.705536961555481  val: loss: 4940666.5 acc: 0.5610129833221436\n",
      "step: 17755\n",
      "train: loss: 259343.03125 acc: 0.8544031381607056  val: loss: 1918068.5 acc: 0.6756426692008972\n",
      "step: 17760\n",
      "train: loss: 258548.0 acc: 0.7366768717765808  val: loss: 1136697.875 acc: 0.6442009806632996\n",
      "step: 17765\n",
      "train: loss: 2854114.75 acc: 0.6631789207458496  val: loss: 2993898.5 acc: 0.7107486724853516\n",
      "step: 17770\n",
      "train: loss: 1512442.0 acc: 0.7681151628494263  val: loss: 2340630.25 acc: 0.7699329853057861\n",
      "step: 17775\n",
      "train: loss: 1375473.875 acc: 0.8702397346496582  val: loss: 2157137.75 acc: 0.780701756477356\n",
      "step: 17780\n",
      "train: loss: 576620.625 acc: 0.951604425907135  val: loss: 1496875.125 acc: 0.6929076313972473\n",
      "step: 17785\n",
      "train: loss: 324026.8125 acc: 0.9414201378822327  val: loss: 1446510.125 acc: 0.6380107402801514\n",
      "step: 17790\n",
      "train: loss: 514526.4375 acc: 0.8664901256561279  val: loss: 937483.0 acc: 0.8795158863067627\n",
      "step: 17795\n",
      "train: loss: 98411.1171875 acc: 0.9895168542861938  val: loss: 491719.875 acc: 0.9549513459205627\n",
      "step: 17800\n",
      "train: loss: 84204.359375 acc: 0.990896463394165  val: loss: 686482.6875 acc: 0.41837286949157715\n",
      "step: 17805\n",
      "train: loss: 68080.78125 acc: 0.9946736097335815  val: loss: 737880.0625 acc: 0.9159268140792847\n",
      "step: 17810\n",
      "train: loss: 37787.9453125 acc: 0.9927270412445068  val: loss: 2875891.5 acc: 0.08026015758514404\n",
      "step: 17815\n",
      "train: loss: 41428.8359375 acc: 0.991184651851654  val: loss: 1887189.375 acc: 0.5279070734977722\n",
      "step: 17820\n",
      "train: loss: 11661.07421875 acc: 0.995724081993103  val: loss: 2340259.5 acc: -0.19262635707855225\n",
      "step: 17825\n",
      "train: loss: 28149.625 acc: 0.9735170006752014  val: loss: 1341559.375 acc: 0.7254976034164429\n",
      "step: 17830\n",
      "train: loss: 3573.660400390625 acc: 0.9805551767349243  val: loss: 703725.375 acc: 0.6879026889801025\n",
      "step: 17835\n",
      "train: loss: 12398.6044921875 acc: 0.9923350811004639  val: loss: 1145763.5 acc: 0.5083366632461548\n",
      "step: 17840\n",
      "train: loss: 24790.83203125 acc: 0.9887194037437439  val: loss: 2264161.75 acc: -0.2548978328704834\n",
      "step: 17845\n",
      "train: loss: 7966.50390625 acc: 0.9954226613044739  val: loss: 1159160.625 acc: 0.5226919054985046\n",
      "step: 17850\n",
      "train: loss: 20327.28125 acc: 0.9616556763648987  val: loss: 868451.9375 acc: 0.8400217294692993\n",
      "step: 17855\n",
      "train: loss: 10960.1201171875 acc: 0.9814029335975647  val: loss: 1445973.5 acc: 0.5277957916259766\n",
      "step: 17860\n",
      "train: loss: 32593.2421875 acc: 0.967947781085968  val: loss: 832592.875 acc: 0.8723766207695007\n",
      "step: 17865\n",
      "train: loss: 17498.34765625 acc: 0.9810194969177246  val: loss: 918443.5625 acc: 0.7470675706863403\n",
      "step: 17870\n",
      "train: loss: 18203.802734375 acc: 0.9869726896286011  val: loss: 1273003.125 acc: 0.3245008587837219\n",
      "step: 17875\n",
      "train: loss: 10168.22265625 acc: 0.9945040941238403  val: loss: 2664872.5 acc: 0.7624636292457581\n",
      "step: 17880\n",
      "train: loss: 83638.5546875 acc: 0.9344418048858643  val: loss: 1427615.125 acc: 0.7792963981628418\n",
      "step: 17885\n",
      "train: loss: 8916.5810546875 acc: 0.9959762692451477  val: loss: 595125.5625 acc: 0.7724005579948425\n",
      "step: 17890\n",
      "train: loss: 9341.1455078125 acc: 0.9938996434211731  val: loss: 1456531.375 acc: 0.7171311974525452\n",
      "step: 17895\n",
      "train: loss: 10969.052734375 acc: 0.9862297773361206  val: loss: 1917533.875 acc: 0.7145355939865112\n",
      "step: 17900\n",
      "train: loss: 14160.4306640625 acc: 0.9909001588821411  val: loss: 1171405.875 acc: 0.7762672901153564\n",
      "step: 17905\n",
      "train: loss: 23618.787109375 acc: 0.9892078042030334  val: loss: 2105390.25 acc: 0.44416171312332153\n",
      "step: 17910\n",
      "train: loss: 26061.3203125 acc: 0.9855144023895264  val: loss: 2346994.5 acc: 0.21601873636245728\n",
      "step: 17915\n",
      "train: loss: 60599.56640625 acc: 0.9844974875450134  val: loss: 1260947.125 acc: 0.7747589349746704\n",
      "step: 17920\n",
      "train: loss: 19157.517578125 acc: 0.9904385805130005  val: loss: 1149783.5 acc: 0.45480453968048096\n",
      "step: 17925\n",
      "train: loss: 155881.921875 acc: 0.9386335611343384  val: loss: 944801.8125 acc: 0.7836454510688782\n",
      "step: 17930\n",
      "train: loss: 107861.09375 acc: 0.9723564386367798  val: loss: 537597.125 acc: 0.7873645424842834\n",
      "step: 17935\n",
      "train: loss: 135664.484375 acc: 0.9233111143112183  val: loss: 84770.28125 acc: 0.9882327914237976\n",
      "step: 17940\n",
      "train: loss: 877805.6875 acc: 0.8833907842636108  val: loss: 1281686.5 acc: 0.7211726903915405\n",
      "step: 17945\n",
      "train: loss: 415409.5 acc: 0.9604557752609253  val: loss: 1683595.375 acc: 0.6479388475418091\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 17950\n",
      "train: loss: 202892.296875 acc: 0.9823196530342102  val: loss: 1367780.5 acc: 0.42699193954467773\n",
      "step: 17955\n",
      "train: loss: 101040.0625 acc: 0.9887518286705017  val: loss: 653402.125 acc: 0.8028481006622314\n",
      "step: 17960\n",
      "train: loss: 476499.90625 acc: 0.9059945344924927  val: loss: 670087.5 acc: 0.381128191947937\n",
      "step: 17965\n",
      "train: loss: 238214.453125 acc: 0.9765629172325134  val: loss: 1223895.25 acc: 0.8604537844657898\n",
      "step: 17970\n",
      "train: loss: 527985.0625 acc: 0.9702739715576172  val: loss: 371823.28125 acc: 0.8044143915176392\n",
      "step: 17975\n",
      "train: loss: 2118652.5 acc: 0.6883870959281921  val: loss: 962461.8125 acc: 0.7987239360809326\n",
      "step: 17980\n",
      "train: loss: 238166.171875 acc: 0.9450293779373169  val: loss: 290491.125 acc: 0.8445219993591309\n",
      "step: 17985\n",
      "train: loss: 1113112.125 acc: 0.9678906202316284  val: loss: 518516.0625 acc: 0.8594135046005249\n",
      "step: 17990\n",
      "train: loss: 1934542.375 acc: 0.9474049210548401  val: loss: 666882.4375 acc: 0.4571117162704468\n",
      "step: 17995\n",
      "train: loss: 4270527.0 acc: 0.8293792605400085  val: loss: 1065187.375 acc: 0.7534123659133911\n",
      "step: 18000\n",
      "train: loss: 1589665.125 acc: 0.9157764911651611  val: loss: 418866.0625 acc: 0.9039754867553711\n",
      "step: 18005\n",
      "train: loss: 211619.125 acc: 0.979808509349823  val: loss: 427084.6875 acc: 0.87953782081604\n",
      "step: 18010\n",
      "train: loss: 1307646.375 acc: 0.8800142407417297  val: loss: 656699.375 acc: 0.7135369181632996\n",
      "step: 18015\n",
      "train: loss: 719466.6875 acc: 0.8748077154159546  val: loss: 274926.0 acc: 0.9530574679374695\n",
      "step: 18020\n",
      "train: loss: 1412791.25 acc: 0.7378281354904175  val: loss: 950161.3125 acc: 0.8114137649536133\n",
      "step: 18025\n",
      "train: loss: 739431.1875 acc: 0.22262698411941528  val: loss: 3055719.75 acc: 0.7346264719963074\n",
      "step: 18030\n",
      "train: loss: 450580.6875 acc: 0.8456625938415527  val: loss: 2034340.625 acc: 0.6996428370475769\n",
      "step: 18035\n",
      "train: loss: 362138.46875 acc: 0.8281246423721313  val: loss: 684085.9375 acc: 0.8478567600250244\n",
      "step: 18040\n",
      "train: loss: 759748.8125 acc: 0.834919810295105  val: loss: 557230.8125 acc: 0.8276962637901306\n",
      "step: 18045\n",
      "train: loss: 1083107.75 acc: 0.5907820463180542  val: loss: 4031977.25 acc: 0.6929466128349304\n",
      "step: 18050\n",
      "train: loss: 556656.6875 acc: 0.7698336839675903  val: loss: 3921725.5 acc: 0.6515213251113892\n",
      "step: 18055\n",
      "train: loss: 658911.5 acc: 0.7346563339233398  val: loss: 3146410.25 acc: 0.5978091359138489\n",
      "step: 18060\n",
      "train: loss: 186859.796875 acc: 0.8573282361030579  val: loss: 1348067.25 acc: 0.7107734680175781\n",
      "step: 18065\n",
      "train: loss: 287341.0625 acc: 0.8413128852844238  val: loss: 1302871.375 acc: 0.7342697381973267\n",
      "step: 18070\n",
      "train: loss: 57611.18359375 acc: 0.952437698841095  val: loss: 3525425.5 acc: 0.6385761499404907\n",
      "step: 18075\n",
      "train: loss: 354726.125 acc: 0.7955273985862732  val: loss: 2545015.75 acc: 0.6511343717575073\n",
      "step: 18080\n",
      "train: loss: 506946.9375 acc: 0.8101773262023926  val: loss: 1119219.75 acc: 0.7044054269790649\n",
      "step: 18085\n",
      "train: loss: 295108.25 acc: 0.803229033946991  val: loss: 4075101.25 acc: 0.6138724684715271\n",
      "step: 18090\n",
      "train: loss: 112868.3515625 acc: 0.9211564660072327  val: loss: 2711566.75 acc: 0.656141996383667\n",
      "step: 18095\n",
      "train: loss: 115311.03125 acc: 0.8426375985145569  val: loss: 2997459.75 acc: 0.6274691224098206\n",
      "step: 18100\n",
      "train: loss: 93874.65625 acc: 0.9379401206970215  val: loss: 2596686.25 acc: 0.6375048160552979\n",
      "step: 18105\n",
      "train: loss: 403884.78125 acc: 0.8060261011123657  val: loss: 2026513.875 acc: 0.6698458194732666\n",
      "step: 18110\n",
      "train: loss: 175770.828125 acc: 0.8774083852767944  val: loss: 1245727.875 acc: 0.6240466237068176\n",
      "step: 18115\n",
      "train: loss: 50709.33984375 acc: 0.942263126373291  val: loss: 2550462.25 acc: 0.5931655168533325\n",
      "step: 18120\n",
      "train: loss: 150870.140625 acc: 0.8540697693824768  val: loss: 247329.140625 acc: 0.874146580696106\n",
      "step: 18125\n",
      "train: loss: 240146.015625 acc: 0.8181197047233582  val: loss: 1037847.4375 acc: 0.6712826490402222\n",
      "step: 18130\n",
      "train: loss: 2283817.75 acc: 0.6669080257415771  val: loss: 907184.1875 acc: 0.7377715110778809\n",
      "step: 18135\n",
      "train: loss: 723877.0 acc: 0.8284342288970947  val: loss: 1890088.375 acc: 0.7748734354972839\n",
      "step: 18140\n",
      "train: loss: 1444399.875 acc: 0.8854666352272034  val: loss: 986005.25 acc: 0.5975421667098999\n",
      "step: 18145\n",
      "train: loss: 754725.4375 acc: 0.9452451467514038  val: loss: 1648524.25 acc: 0.6462175250053406\n",
      "step: 18150\n",
      "train: loss: 174033.21875 acc: 0.9829570651054382  val: loss: 1847994.625 acc: 0.2848668098449707\n",
      "step: 18155\n",
      "train: loss: 138777.359375 acc: 0.975611686706543  val: loss: 686695.3125 acc: 0.9455959796905518\n",
      "step: 18160\n",
      "train: loss: 156443.40625 acc: 0.9821115136146545  val: loss: 877942.125 acc: 0.572560727596283\n",
      "step: 18165\n",
      "train: loss: 112194.3125 acc: 0.9924983382225037  val: loss: 1661189.5 acc: 0.6600854396820068\n",
      "step: 18170\n",
      "train: loss: 75201.71875 acc: 0.9942198991775513  val: loss: 1139678.5 acc: 0.6049174070358276\n",
      "step: 18175\n",
      "train: loss: 54578.125 acc: 0.9950795769691467  val: loss: 723051.0625 acc: 0.8022995591163635\n",
      "step: 18180\n",
      "train: loss: 108657.359375 acc: 0.986414909362793  val: loss: 1454207.5 acc: 0.6954978704452515\n",
      "step: 18185\n",
      "train: loss: 70091.0 acc: 0.9852858185768127  val: loss: 1423971.875 acc: 0.7344980239868164\n",
      "step: 18190\n",
      "train: loss: 18396.958984375 acc: 0.9907059669494629  val: loss: 968502.125 acc: 0.8490769267082214\n",
      "step: 18195\n",
      "train: loss: 11352.1630859375 acc: 0.9941746592521667  val: loss: 2110561.75 acc: 0.7857803106307983\n",
      "step: 18200\n",
      "train: loss: 26930.51953125 acc: 0.9890592694282532  val: loss: 1359068.875 acc: 0.7694405317306519\n",
      "step: 18205\n",
      "train: loss: 13923.75 acc: 0.9732567667961121  val: loss: 1410137.125 acc: 0.7058135271072388\n",
      "step: 18210\n",
      "train: loss: 29599.12890625 acc: 0.9208499193191528  val: loss: 443227.8125 acc: 0.9241420030593872\n",
      "step: 18215\n",
      "train: loss: 7387.17333984375 acc: 0.9626633524894714  val: loss: 360676.8125 acc: 0.9230704307556152\n",
      "step: 18220\n",
      "train: loss: 14117.904296875 acc: 0.9769819378852844  val: loss: 2347967.25 acc: 0.7193461060523987\n",
      "step: 18225\n",
      "train: loss: 19508.0390625 acc: 0.9637271165847778  val: loss: 1543885.375 acc: 0.44244706630706787\n",
      "step: 18230\n",
      "train: loss: 7402.35107421875 acc: 0.9791172742843628  val: loss: 1083341.125 acc: 0.7899278998374939\n",
      "step: 18235\n",
      "train: loss: 36018.29296875 acc: 0.9826251864433289  val: loss: 141401.4375 acc: 0.9529379606246948\n",
      "step: 18240\n",
      "train: loss: 53062.69921875 acc: 0.9635946750640869  val: loss: 327597.6875 acc: 0.8349848389625549\n",
      "step: 18245\n",
      "train: loss: 14623.9453125 acc: 0.9865537881851196  val: loss: 1475689.625 acc: 0.45130932331085205\n",
      "step: 18250\n",
      "train: loss: 7760.751953125 acc: 0.9964156746864319  val: loss: 1846735.125 acc: 0.628717303276062\n",
      "step: 18255\n",
      "train: loss: 12897.95703125 acc: 0.9837852120399475  val: loss: 2028770.75 acc: 0.5684366822242737\n",
      "step: 18260\n",
      "train: loss: 16571.71484375 acc: 0.956606388092041  val: loss: 2270993.25 acc: 0.22978299856185913\n",
      "step: 18265\n",
      "train: loss: 6650.2265625 acc: 0.9959668517112732  val: loss: 3322512.75 acc: 0.38587403297424316\n",
      "step: 18270\n",
      "train: loss: 38726.59765625 acc: 0.9913678765296936  val: loss: 510829.53125 acc: 0.8924705982208252\n",
      "step: 18275\n",
      "train: loss: 43861.64453125 acc: 0.9842823147773743  val: loss: 308345.71875 acc: 0.8665072917938232\n",
      "step: 18280\n",
      "train: loss: 28681.9609375 acc: 0.9911938905715942  val: loss: 827325.1875 acc: 0.9311874508857727\n",
      "step: 18285\n",
      "train: loss: 53067.3203125 acc: 0.9622269868850708  val: loss: 725635.3125 acc: 0.8119301795959473\n",
      "step: 18290\n",
      "train: loss: 45105.3359375 acc: 0.9724414944648743  val: loss: 327202.59375 acc: 0.8413397669792175\n",
      "step: 18295\n",
      "train: loss: 64577.12109375 acc: 0.9839580655097961  val: loss: 392914.84375 acc: 0.8787400722503662\n",
      "step: 18300\n",
      "train: loss: 76521.9453125 acc: 0.9675832390785217  val: loss: 275638.53125 acc: 0.9038982391357422\n",
      "step: 18305\n",
      "train: loss: 343083.09375 acc: 0.9286444187164307  val: loss: 1026552.25 acc: 0.6794317960739136\n",
      "step: 18310\n",
      "train: loss: 167978.609375 acc: 0.9778646230697632  val: loss: 1089282.25 acc: 0.6846429109573364\n",
      "step: 18315\n",
      "train: loss: 86270.0859375 acc: 0.9908419847488403  val: loss: 392232.875 acc: 0.9046366214752197\n",
      "step: 18320\n",
      "train: loss: 73989.359375 acc: 0.9922198057174683  val: loss: 1585318.875 acc: 0.5769009590148926\n",
      "step: 18325\n",
      "train: loss: 249475.328125 acc: 0.9743139147758484  val: loss: 1246302.375 acc: 0.5818506479263306\n",
      "step: 18330\n",
      "train: loss: 132839.5 acc: 0.9683695435523987  val: loss: 1358283.875 acc: 0.8152159452438354\n",
      "step: 18335\n",
      "train: loss: 599732.375 acc: 0.9592792987823486  val: loss: 1923266.25 acc: 0.8118951320648193\n",
      "step: 18340\n",
      "train: loss: 290790.65625 acc: 0.966471791267395  val: loss: 967451.5625 acc: 0.7789294719696045\n",
      "step: 18345\n",
      "train: loss: 487575.5625 acc: 0.9231602549552917  val: loss: 1064556.75 acc: 0.884735107421875\n",
      "step: 18350\n",
      "train: loss: 438052.875 acc: 0.9779763221740723  val: loss: 1135910.625 acc: 0.9161328673362732\n",
      "step: 18355\n",
      "train: loss: 1928101.0 acc: 0.9105764627456665  val: loss: 521274.15625 acc: 0.9143995642662048\n",
      "step: 18360\n",
      "train: loss: 1733894.25 acc: 0.9276074767112732  val: loss: 707657.8125 acc: 0.9081504940986633\n",
      "step: 18365\n",
      "train: loss: 549306.0625 acc: 0.9769963026046753  val: loss: 247695.125 acc: 0.9660573601722717\n",
      "step: 18370\n",
      "train: loss: 583235.4375 acc: 0.936533510684967  val: loss: 194383.046875 acc: 0.9605187773704529\n",
      "step: 18375\n",
      "train: loss: 792861.9375 acc: 0.9071819186210632  val: loss: 1987257.375 acc: 0.42826300859451294\n",
      "step: 18380\n",
      "train: loss: 206384.0625 acc: 0.9599767327308655  val: loss: 341727.03125 acc: 0.938065767288208\n",
      "step: 18385\n",
      "train: loss: 2279742.0 acc: 0.2559483051300049  val: loss: 712829.1875 acc: 0.8737908005714417\n",
      "step: 18390\n",
      "train: loss: 962036.0625 acc: 0.7720651030540466  val: loss: 1510272.375 acc: 0.7645031213760376\n",
      "step: 18395\n",
      "train: loss: 275062.0 acc: 0.8601553440093994  val: loss: 1328281.0 acc: 0.6682513952255249\n",
      "step: 18400\n",
      "train: loss: 297310.21875 acc: 0.8873478174209595  val: loss: 1554681.375 acc: 0.7676959037780762\n",
      "step: 18405\n",
      "train: loss: 349939.15625 acc: 0.9141409993171692  val: loss: 1008838.0625 acc: 0.8041700124740601\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 18410\n",
      "train: loss: 880273.75 acc: 0.5663262605667114  val: loss: 557580.5 acc: 0.8651236295700073\n",
      "step: 18415\n",
      "train: loss: 739811.1875 acc: 0.6477108001708984  val: loss: 2185376.25 acc: 0.7332106232643127\n",
      "step: 18420\n",
      "train: loss: 442877.78125 acc: 0.7699422240257263  val: loss: 3314880.25 acc: 0.6606272459030151\n",
      "step: 18425\n",
      "train: loss: 211086.53125 acc: 0.8031487464904785  val: loss: 1335903.375 acc: 0.7220324277877808\n",
      "step: 18430\n",
      "train: loss: 67214.984375 acc: 0.9373169541358948  val: loss: 1556579.125 acc: 0.7298195362091064\n",
      "step: 18435\n",
      "train: loss: 124083.5234375 acc: 0.9068514108657837  val: loss: 126591.5859375 acc: 0.8703199028968811\n",
      "step: 18440\n",
      "train: loss: 262419.59375 acc: 0.8538468480110168  val: loss: 2997034.0 acc: 0.6496651768684387\n",
      "step: 18445\n",
      "train: loss: 346006.34375 acc: 0.8082549571990967  val: loss: 606923.5625 acc: 0.6764500141143799\n",
      "step: 18450\n",
      "train: loss: 162557.953125 acc: 0.8698490262031555  val: loss: 249677.453125 acc: 0.8721045255661011\n",
      "step: 18455\n",
      "train: loss: 33028.4296875 acc: 0.9738754034042358  val: loss: 1471385.625 acc: 0.6918802857398987\n",
      "step: 18460\n",
      "train: loss: 34968.1640625 acc: 0.9677060842514038  val: loss: 77698.6640625 acc: 0.8942800760269165\n",
      "step: 18465\n",
      "train: loss: 42682.34765625 acc: 0.952808678150177  val: loss: 823536.3125 acc: 0.6354962587356567\n",
      "step: 18470\n",
      "train: loss: 306898.59375 acc: 0.8512881994247437  val: loss: 782066.8125 acc: 0.7608517408370972\n",
      "step: 18475\n",
      "train: loss: 127502.1171875 acc: 0.8925583362579346  val: loss: 2287818.25 acc: 0.5868361592292786\n",
      "step: 18480\n",
      "train: loss: 596087.875 acc: 0.7004956007003784  val: loss: 322322.8125 acc: 0.824840784072876\n",
      "step: 18485\n",
      "train: loss: 267328.1875 acc: 0.8134716749191284  val: loss: 1313509.125 acc: 0.6688382625579834\n",
      "step: 18490\n",
      "train: loss: 174912.765625 acc: 0.8644002079963684  val: loss: 71990.71875 acc: 0.9325370788574219\n",
      "step: 18495\n",
      "train: loss: 1432926.625 acc: 0.7264595627784729  val: loss: 960334.0 acc: 0.7763311862945557\n",
      "step: 18500\n",
      "train: loss: 1618742.25 acc: 0.730667233467102  val: loss: 1623833.25 acc: 0.8183793425559998\n",
      "step: 18505\n",
      "train: loss: 1082847.875 acc: 0.8790741562843323  val: loss: 670974.625 acc: 0.9071184396743774\n",
      "step: 18510\n",
      "train: loss: 666724.6875 acc: 0.9299526810646057  val: loss: 1065701.375 acc: 0.6822957992553711\n",
      "step: 18515\n",
      "train: loss: 342684.03125 acc: 0.9633674621582031  val: loss: 2038785.0 acc: 0.513550877571106\n",
      "step: 18520\n",
      "train: loss: 155320.4375 acc: 0.972544252872467  val: loss: 931560.9375 acc: 0.8128560781478882\n",
      "step: 18525\n",
      "train: loss: 277701.3125 acc: 0.9696304798126221  val: loss: 1717999.625 acc: 0.7911043167114258\n",
      "step: 18530\n",
      "train: loss: 146239.03125 acc: 0.9894536137580872  val: loss: 1515344.375 acc: 0.7153713703155518\n",
      "step: 18535\n",
      "train: loss: 110859.9453125 acc: 0.9923261404037476  val: loss: 783605.375 acc: 0.7301136255264282\n",
      "step: 18540\n",
      "train: loss: 97892.1875 acc: 0.9870151281356812  val: loss: 960157.625 acc: 0.49066847562789917\n",
      "step: 18545\n",
      "train: loss: 94732.5703125 acc: 0.9876216650009155  val: loss: 208195.234375 acc: 0.8586560487747192\n",
      "step: 18550\n",
      "train: loss: 24054.3046875 acc: 0.9916781187057495  val: loss: 811707.5 acc: 0.7584723830223083\n",
      "step: 18555\n",
      "train: loss: 28073.591796875 acc: 0.9813663363456726  val: loss: 148894.6875 acc: 0.9454678893089294\n",
      "step: 18560\n",
      "train: loss: 22872.232421875 acc: 0.9759838581085205  val: loss: 338371.09375 acc: 0.9225570559501648\n",
      "step: 18565\n",
      "train: loss: 28193.4296875 acc: 0.9833083152770996  val: loss: 1022273.8125 acc: 0.8167429566383362\n",
      "step: 18570\n",
      "train: loss: 15118.958984375 acc: 0.9916979074478149  val: loss: 2211957.0 acc: -0.01986229419708252\n",
      "step: 18575\n",
      "train: loss: 11487.8857421875 acc: 0.9896122217178345  val: loss: 921943.75 acc: 0.5960866808891296\n",
      "step: 18580\n",
      "train: loss: 10855.1162109375 acc: 0.9739145636558533  val: loss: 713663.3125 acc: 0.5125640630722046\n",
      "step: 18585\n",
      "train: loss: 3491.950927734375 acc: 0.9894603490829468  val: loss: 386411.0625 acc: 0.9183194637298584\n",
      "step: 18590\n",
      "train: loss: 7903.88037109375 acc: 0.9861782193183899  val: loss: 662271.125 acc: 0.8077685832977295\n",
      "step: 18595\n",
      "train: loss: 6673.439453125 acc: 0.9686971306800842  val: loss: 1589370.25 acc: 0.5809325575828552\n",
      "step: 18600\n",
      "train: loss: 46261.8359375 acc: 0.9736153483390808  val: loss: 1918518.375 acc: 0.6966587901115417\n",
      "step: 18605\n",
      "train: loss: 19078.220703125 acc: 0.9807466864585876  val: loss: 323949.4375 acc: 0.952443540096283\n",
      "step: 18610\n",
      "train: loss: 36767.53515625 acc: 0.9728362560272217  val: loss: 465479.28125 acc: 0.9014445543289185\n",
      "step: 18615\n",
      "train: loss: 31460.9921875 acc: 0.9858587384223938  val: loss: 379548.28125 acc: 0.9184675216674805\n",
      "step: 18620\n",
      "train: loss: 18507.126953125 acc: 0.9859276413917542  val: loss: 1003063.25 acc: 0.8063328266143799\n",
      "step: 18625\n",
      "train: loss: 8474.76953125 acc: 0.9730179309844971  val: loss: 1544307.625 acc: 0.506676971912384\n",
      "step: 18630\n",
      "train: loss: 3242.799072265625 acc: 0.9976630210876465  val: loss: 327154.03125 acc: 0.9090030193328857\n",
      "step: 18635\n",
      "train: loss: 36208.84375 acc: 0.9897058606147766  val: loss: 303795.5 acc: 0.8957070112228394\n",
      "step: 18640\n",
      "train: loss: 35652.37109375 acc: 0.9890641570091248  val: loss: 1042811.9375 acc: 0.8768741488456726\n",
      "step: 18645\n",
      "train: loss: 14951.1689453125 acc: 0.9942061901092529  val: loss: 778698.4375 acc: 0.6714156270027161\n",
      "step: 18650\n",
      "train: loss: 38270.9765625 acc: 0.9886524677276611  val: loss: 444608.25 acc: 0.898682713508606\n",
      "step: 18655\n",
      "train: loss: 63849.4453125 acc: 0.9831247925758362  val: loss: 834459.4375 acc: 0.6764293909072876\n",
      "step: 18660\n",
      "train: loss: 183072.671875 acc: 0.9517554640769958  val: loss: 1465548.125 acc: 0.8710538744926453\n",
      "step: 18665\n",
      "train: loss: 46213.9296875 acc: 0.9592282772064209  val: loss: 633059.9375 acc: 0.9240536093711853\n",
      "step: 18670\n",
      "train: loss: 124772.8671875 acc: 0.9685556888580322  val: loss: 257132.671875 acc: 0.9460408687591553\n",
      "step: 18675\n",
      "train: loss: 54089.4921875 acc: 0.9890292882919312  val: loss: 1178603.625 acc: 0.8317793607711792\n",
      "step: 18680\n",
      "train: loss: 553220.8125 acc: 0.9546753168106079  val: loss: 1838607.375 acc: 0.6019120216369629\n",
      "step: 18685\n",
      "train: loss: 46919.16796875 acc: 0.9956493973731995  val: loss: 1324517.125 acc: 0.5762410163879395\n",
      "step: 18690\n",
      "train: loss: 114105.390625 acc: 0.9745045900344849  val: loss: 483114.03125 acc: 0.8592644333839417\n",
      "step: 18695\n",
      "train: loss: 275685.40625 acc: 0.9664008021354675  val: loss: 471425.1875 acc: 0.925421953201294\n",
      "step: 18700\n",
      "train: loss: 643842.875 acc: 0.9627839922904968  val: loss: 999243.6875 acc: 0.4251822233200073\n",
      "step: 18705\n",
      "train: loss: 322826.53125 acc: 0.9684010148048401  val: loss: 1091339.875 acc: 0.8820922374725342\n",
      "step: 18710\n",
      "train: loss: 237902.96875 acc: 0.9501244425773621  val: loss: 4048835.5 acc: 0.7360753417015076\n",
      "step: 18715\n",
      "train: loss: 989055.25 acc: 0.9610821604728699  val: loss: 1978192.75 acc: 0.8665039539337158\n",
      "step: 18720\n",
      "train: loss: 676263.1875 acc: 0.9769476056098938  val: loss: 905526.6875 acc: 0.6937545537948608\n",
      "step: 18725\n",
      "train: loss: 843534.125 acc: 0.9574385285377502  val: loss: 288417.8125 acc: 0.9732004404067993\n",
      "step: 18730\n",
      "train: loss: 1376981.875 acc: 0.9232264757156372  val: loss: 1091385.875 acc: 0.6608890295028687\n",
      "step: 18735\n",
      "train: loss: 780580.625 acc: 0.9412224888801575  val: loss: 971149.875 acc: 0.8513019680976868\n",
      "step: 18740\n",
      "train: loss: 201168.4375 acc: 0.9831863045692444  val: loss: 2243162.0 acc: 0.39415496587753296\n",
      "step: 18745\n",
      "train: loss: 371128.65625 acc: 0.9544743299484253  val: loss: 941271.9375 acc: 0.6952214241027832\n",
      "step: 18750\n",
      "train: loss: 2226691.25 acc: 0.42395704984664917  val: loss: 1078497.5 acc: 0.8756669759750366\n",
      "step: 18755\n",
      "train: loss: 638915.9375 acc: 0.49574851989746094  val: loss: 850678.9375 acc: 0.8590848445892334\n",
      "step: 18760\n",
      "train: loss: 206059.953125 acc: 0.8787935972213745  val: loss: 1673133.5 acc: 0.7703492641448975\n",
      "step: 18765\n",
      "train: loss: 541209.0 acc: 0.781037449836731  val: loss: 473627.28125 acc: 0.7533561587333679\n",
      "step: 18770\n",
      "train: loss: 693278.25 acc: 0.8315094113349915  val: loss: 871984.25 acc: 0.5178823471069336\n",
      "step: 18775\n",
      "train: loss: 697479.25 acc: 0.43970227241516113  val: loss: 2361783.5 acc: 0.8335726261138916\n",
      "step: 18780\n",
      "train: loss: 452563.375 acc: 0.7181631326675415  val: loss: 2537097.25 acc: 0.6367020606994629\n",
      "step: 18785\n",
      "train: loss: 169893.40625 acc: 0.8852313160896301  val: loss: 2158413.0 acc: 0.5979722738265991\n",
      "step: 18790\n",
      "train: loss: 128842.9296875 acc: 0.8908478021621704  val: loss: 2255805.75 acc: 0.6350706815719604\n",
      "step: 18795\n",
      "train: loss: 273312.0625 acc: 0.8219338655471802  val: loss: 1326467.625 acc: 0.6956852674484253\n",
      "step: 18800\n",
      "train: loss: 77714.5 acc: 0.9416951537132263  val: loss: 1607432.75 acc: 0.6811231970787048\n",
      "step: 18805\n",
      "train: loss: 58679.90234375 acc: 0.9544709920883179  val: loss: 1403211.875 acc: 0.6099744439125061\n",
      "step: 18810\n",
      "train: loss: 159624.65625 acc: 0.8847882747650146  val: loss: 1273689.0 acc: 0.6470123529434204\n",
      "step: 18815\n",
      "train: loss: 335333.21875 acc: 0.8297058939933777  val: loss: 1854102.875 acc: 0.6948801279067993\n",
      "step: 18820\n",
      "train: loss: 100193.0390625 acc: 0.9290829300880432  val: loss: 756586.625 acc: 0.7437386512756348\n",
      "step: 18825\n",
      "train: loss: 24595.419921875 acc: 0.9750701189041138  val: loss: 857770.5 acc: 0.7438219785690308\n",
      "step: 18830\n",
      "train: loss: 13397.17578125 acc: 0.9811803698539734  val: loss: 2845846.75 acc: 0.6001682281494141\n",
      "step: 18835\n",
      "train: loss: 105805.2109375 acc: 0.9159427285194397  val: loss: 913932.625 acc: 0.6389129161834717\n",
      "step: 18840\n",
      "train: loss: 348733.53125 acc: 0.8228026628494263  val: loss: 771624.625 acc: 0.6761555671691895\n",
      "step: 18845\n",
      "train: loss: 210905.234375 acc: 0.8404225707054138  val: loss: 1686148.375 acc: 0.6955243349075317\n",
      "step: 18850\n",
      "train: loss: 776443.1875 acc: 0.6900609135627747  val: loss: 563123.8125 acc: 0.7532350420951843\n",
      "step: 18855\n",
      "train: loss: 401709.40625 acc: 0.7892165184020996  val: loss: 3368081.75 acc: 0.6276628971099854\n",
      "step: 18860\n",
      "train: loss: 590117.375 acc: 0.7383873462677002  val: loss: 1160428.0 acc: 0.6637085676193237\n",
      "step: 18865\n",
      "train: loss: 2202928.75 acc: 0.7574483156204224  val: loss: 1280969.625 acc: 0.6373706459999084\n",
      "step: 18870\n",
      "train: loss: 1542234.25 acc: 0.8504869341850281  val: loss: 967868.6875 acc: 0.8163629770278931\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 18875\n",
      "train: loss: 1082683.875 acc: 0.912177324295044  val: loss: 696695.25 acc: 0.09531986713409424\n",
      "step: 18880\n",
      "train: loss: 725289.875 acc: 0.8597891330718994  val: loss: 848949.1875 acc: 0.8960453271865845\n",
      "step: 18885\n",
      "train: loss: 222246.328125 acc: 0.9502971768379211  val: loss: 345637.15625 acc: 0.8382499814033508\n",
      "step: 18890\n",
      "train: loss: 240064.265625 acc: 0.9705629944801331  val: loss: 518011.21875 acc: 0.8889288306236267\n",
      "step: 18895\n",
      "train: loss: 106974.5859375 acc: 0.9895592927932739  val: loss: 669142.875 acc: 0.7913933992385864\n",
      "step: 18900\n",
      "train: loss: 151679.390625 acc: 0.9895567893981934  val: loss: 472222.59375 acc: 0.8042263388633728\n",
      "step: 18905\n",
      "train: loss: 97866.984375 acc: 0.9875293374061584  val: loss: 1497955.125 acc: 0.7611680626869202\n",
      "step: 18910\n",
      "train: loss: 92379.1953125 acc: 0.9824944734573364  val: loss: 829963.625 acc: 0.7787669897079468\n",
      "step: 18915\n",
      "train: loss: 28825.447265625 acc: 0.9928252100944519  val: loss: 1794771.625 acc: 0.2784577012062073\n",
      "step: 18920\n",
      "train: loss: 48450.87109375 acc: 0.9892546534538269  val: loss: 153396.6875 acc: 0.9565820693969727\n",
      "step: 18925\n",
      "train: loss: 4601.8642578125 acc: 0.9965755343437195  val: loss: 199533.359375 acc: 0.9598195552825928\n",
      "step: 18930\n",
      "train: loss: 10800.4833984375 acc: 0.9836066365242004  val: loss: 637507.75 acc: 0.8473244905471802\n",
      "step: 18935\n",
      "train: loss: 32500.5625 acc: 0.9806126952171326  val: loss: 748901.25 acc: 0.818943440914154\n",
      "step: 18940\n",
      "train: loss: 192775.625 acc: 0.6557760834693909  val: loss: 1000456.5625 acc: 0.8489950299263\n",
      "step: 18945\n",
      "train: loss: 17337.2109375 acc: 0.9667388796806335  val: loss: 553515.75 acc: 0.8528391718864441\n",
      "step: 18950\n",
      "train: loss: 7790.23583984375 acc: 0.9677129983901978  val: loss: 519421.3125 acc: 0.9287735223770142\n",
      "step: 18955\n",
      "train: loss: 11899.7470703125 acc: 0.9712245464324951  val: loss: 349888.46875 acc: 0.9288427233695984\n",
      "step: 18960\n",
      "train: loss: 13309.4150390625 acc: 0.9778059124946594  val: loss: 317149.65625 acc: 0.9167546033859253\n",
      "step: 18965\n",
      "train: loss: 56369.51953125 acc: 0.9783828854560852  val: loss: 268575.125 acc: 0.9712758660316467\n",
      "step: 18970\n",
      "train: loss: 38900.90625 acc: 0.9822306036949158  val: loss: 1899186.375 acc: 0.8071721792221069\n",
      "step: 18975\n",
      "train: loss: 23711.46484375 acc: 0.9907935857772827  val: loss: 401197.15625 acc: 0.8438141345977783\n",
      "step: 18980\n",
      "train: loss: 18823.1171875 acc: 0.990926206111908  val: loss: 280644.5 acc: 0.9844279885292053\n",
      "step: 18985\n",
      "train: loss: 16301.837890625 acc: 0.990189254283905  val: loss: 311563.28125 acc: 0.9539108872413635\n",
      "step: 18990\n",
      "train: loss: 11560.21484375 acc: 0.9810574650764465  val: loss: 1367873.5 acc: 0.6772575378417969\n",
      "step: 18995\n",
      "train: loss: 5434.173828125 acc: 0.9937290549278259  val: loss: 1196775.125 acc: 0.8239158987998962\n",
      "step: 19000\n",
      "train: loss: 53140.82421875 acc: 0.9769774675369263  val: loss: 384977.71875 acc: 0.9336029291152954\n",
      "step: 19005\n",
      "train: loss: 30920.576171875 acc: 0.9925237894058228  val: loss: 565837.0625 acc: 0.9541885852813721\n",
      "step: 19010\n",
      "train: loss: 62238.88671875 acc: 0.9863041639328003  val: loss: 2605563.75 acc: 0.6331484913825989\n",
      "step: 19015\n",
      "train: loss: 51133.83984375 acc: 0.9845362901687622  val: loss: 2384978.25 acc: 0.4852285385131836\n",
      "step: 19020\n",
      "train: loss: 257449.25 acc: 0.9155580997467041  val: loss: 2066150.0 acc: -0.011348485946655273\n",
      "step: 19025\n",
      "train: loss: 110713.5546875 acc: 0.9628496766090393  val: loss: 2059587.125 acc: -0.07478916645050049\n",
      "step: 19030\n",
      "train: loss: 43417.33203125 acc: 0.9744956493377686  val: loss: 281255.375 acc: 0.9539535641670227\n",
      "step: 19035\n",
      "train: loss: 988502.625 acc: 0.8329825401306152  val: loss: 1445935.625 acc: 0.8747194409370422\n",
      "step: 19040\n",
      "train: loss: 153833.875 acc: 0.9769793152809143  val: loss: 516621.53125 acc: 0.9019782543182373\n",
      "step: 19045\n",
      "train: loss: 217916.046875 acc: 0.980067253112793  val: loss: 3000422.25 acc: 0.6235487461090088\n",
      "step: 19050\n",
      "train: loss: 51565.6171875 acc: 0.9949548244476318  val: loss: 2328092.5 acc: 0.37756723165512085\n",
      "step: 19055\n",
      "train: loss: 264513.78125 acc: 0.9650314450263977  val: loss: 494017.5 acc: 0.871178925037384\n",
      "step: 19060\n",
      "train: loss: 228415.15625 acc: 0.9840682744979858  val: loss: 455145.125 acc: 0.8959025144577026\n",
      "step: 19065\n",
      "train: loss: 273486.84375 acc: 0.9486441612243652  val: loss: 1630104.875 acc: 0.6767297983169556\n",
      "step: 19070\n",
      "train: loss: 567483.75 acc: 0.9648022055625916  val: loss: 566585.0625 acc: 0.8357709050178528\n",
      "step: 19075\n",
      "train: loss: 333216.3125 acc: 0.962554395198822  val: loss: 937557.1875 acc: 0.673431396484375\n",
      "step: 19080\n",
      "train: loss: 1592608.375 acc: 0.9324370622634888  val: loss: 2103446.75 acc: 0.7893893122673035\n",
      "step: 19085\n",
      "train: loss: 1209892.375 acc: 0.960606575012207  val: loss: 561168.1875 acc: 0.6469229459762573\n",
      "step: 19090\n",
      "train: loss: 2345991.25 acc: 0.9166873693466187  val: loss: 1524010.625 acc: -0.07649469375610352\n",
      "step: 19095\n",
      "train: loss: 895300.625 acc: 0.9514257311820984  val: loss: 977342.625 acc: 0.42938435077667236\n",
      "step: 19100\n",
      "train: loss: 446733.40625 acc: 0.9406499266624451  val: loss: 1459459.0 acc: 0.628183126449585\n",
      "step: 19105\n",
      "train: loss: 464273.65625 acc: 0.9256817102432251  val: loss: 206368.421875 acc: 0.9507414102554321\n",
      "step: 19110\n",
      "train: loss: 311876.46875 acc: 0.9429887533187866  val: loss: 910080.0 acc: 0.8567134141921997\n",
      "step: 19115\n",
      "train: loss: 693161.0 acc: 0.8511945009231567  val: loss: 1076305.375 acc: 0.8204208612442017\n",
      "step: 19120\n",
      "train: loss: 1233155.875 acc: 0.5714807510375977  val: loss: 640035.375 acc: 0.7255622148513794\n",
      "step: 19125\n",
      "train: loss: 588370.875 acc: 0.7781356573104858  val: loss: 1975017.375 acc: 0.7520357966423035\n",
      "step: 19130\n",
      "train: loss: 337732.625 acc: 0.8261539936065674  val: loss: 400776.3125 acc: 0.8125400543212891\n",
      "step: 19135\n",
      "train: loss: 416981.6875 acc: 0.8436496257781982  val: loss: 947961.625 acc: 0.7948105931282043\n",
      "step: 19140\n",
      "train: loss: 1091075.625 acc: 0.5905532240867615  val: loss: 1027919.0 acc: 0.7704315781593323\n",
      "step: 19145\n",
      "train: loss: 728050.375 acc: 0.577970027923584  val: loss: 1936062.125 acc: 0.7294963002204895\n",
      "step: 19150\n",
      "train: loss: 505811.84375 acc: 0.7289894819259644  val: loss: 2086257.25 acc: 0.657957911491394\n",
      "step: 19155\n",
      "train: loss: 226402.578125 acc: 0.8183078169822693  val: loss: 259902.9375 acc: 0.7033529877662659\n",
      "step: 19160\n",
      "train: loss: 116337.765625 acc: 0.920394778251648  val: loss: 1871252.625 acc: 0.6939425468444824\n",
      "step: 19165\n",
      "train: loss: 101452.59375 acc: 0.9349232316017151  val: loss: 740002.375 acc: 0.6866040825843811\n",
      "step: 19170\n",
      "train: loss: 159867.03125 acc: 0.900448739528656  val: loss: 987497.875 acc: 0.6942592263221741\n",
      "step: 19175\n",
      "train: loss: 270904.5625 acc: 0.8613969087600708  val: loss: 535904.1875 acc: 0.7211394906044006\n",
      "step: 19180\n",
      "train: loss: 441790.0625 acc: 0.8177931308746338  val: loss: 2878045.75 acc: 0.605109691619873\n",
      "step: 19185\n",
      "train: loss: 187304.984375 acc: 0.8602586984634399  val: loss: 882580.625 acc: 0.7157623767852783\n",
      "step: 19190\n",
      "train: loss: 100864.640625 acc: 0.9095578789710999  val: loss: 2261919.0 acc: 0.6225768327713013\n",
      "step: 19195\n",
      "train: loss: 167190.09375 acc: 0.8342041969299316  val: loss: 1531062.625 acc: 0.6812119483947754\n",
      "step: 19200\n",
      "train: loss: 432331.53125 acc: 0.8110728859901428  val: loss: 2044809.125 acc: 0.6520103812217712\n",
      "step: 19205\n",
      "train: loss: 492638.875 acc: 0.7050042152404785  val: loss: 1133661.125 acc: 0.6416059732437134\n",
      "step: 19210\n",
      "train: loss: 856215.125 acc: 0.6972824931144714  val: loss: 689363.0 acc: 0.7262740135192871\n",
      "step: 19215\n",
      "train: loss: 173346.515625 acc: 0.8119906783103943  val: loss: 331141.96875 acc: 0.7856737375259399\n",
      "step: 19220\n",
      "train: loss: 85214.6484375 acc: 0.9383682012557983  val: loss: 1675414.75 acc: 0.6297754645347595\n",
      "step: 19225\n",
      "train: loss: 490930.8125 acc: 0.7765835523605347  val: loss: 1768957.5 acc: 0.648991048336029\n",
      "step: 19230\n",
      "train: loss: 1446673.75 acc: 0.7809005975723267  val: loss: 1833478.25 acc: 0.7940017580986023\n",
      "step: 19235\n",
      "train: loss: 2438430.0 acc: 0.8037475943565369  val: loss: 281648.875 acc: 0.9037318825721741\n",
      "step: 19240\n",
      "train: loss: 723020.625 acc: 0.918968141078949  val: loss: 1221000.0 acc: 0.8498832583427429\n",
      "step: 19245\n",
      "train: loss: 728047.1875 acc: 0.9105154275894165  val: loss: 1423633.875 acc: 0.7715297341346741\n",
      "step: 19250\n",
      "train: loss: 125242.390625 acc: 0.9852714538574219  val: loss: 473444.84375 acc: 0.9054408073425293\n",
      "step: 19255\n",
      "train: loss: 121704.3203125 acc: 0.9848939180374146  val: loss: 583482.25 acc: 0.8774054646492004\n",
      "step: 19260\n",
      "train: loss: 104126.4140625 acc: 0.9919735193252563  val: loss: 413387.78125 acc: 0.8707656860351562\n",
      "step: 19265\n",
      "train: loss: 69875.03125 acc: 0.994817316532135  val: loss: 696613.6875 acc: 0.800561785697937\n",
      "step: 19270\n",
      "train: loss: 84535.234375 acc: 0.9902146458625793  val: loss: 1430961.75 acc: 0.5709576606750488\n",
      "step: 19275\n",
      "train: loss: 99869.8671875 acc: 0.987896203994751  val: loss: 468417.3125 acc: 0.9524264335632324\n",
      "step: 19280\n",
      "train: loss: 17041.58984375 acc: 0.9944448471069336  val: loss: 419931.03125 acc: 0.9531151652336121\n",
      "step: 19285\n",
      "train: loss: 13044.5771484375 acc: 0.9938404560089111  val: loss: 1198137.375 acc: 0.8269448280334473\n",
      "step: 19290\n",
      "train: loss: 2438.4052734375 acc: 0.9898106455802917  val: loss: 121665.484375 acc: 0.9880938529968262\n",
      "step: 19295\n",
      "train: loss: 15924.5439453125 acc: 0.9882141947746277  val: loss: 1627768.25 acc: 0.8849195241928101\n",
      "step: 19300\n",
      "train: loss: 23726.068359375 acc: 0.9735639095306396  val: loss: 2068571.75 acc: 0.8722836971282959\n",
      "step: 19305\n",
      "train: loss: 9459.2080078125 acc: 0.9928703904151917  val: loss: 504028.59375 acc: 0.9558186531066895\n",
      "step: 19310\n",
      "train: loss: 32073.462890625 acc: 0.9735912084579468  val: loss: 656658.25 acc: 0.9429476261138916\n",
      "step: 19315\n",
      "train: loss: 17267.814453125 acc: 0.9627131223678589  val: loss: 1257587.875 acc: 0.7226052284240723\n",
      "step: 19320\n",
      "train: loss: 6977.869140625 acc: 0.9878885746002197  val: loss: 563189.4375 acc: 0.8589807748794556\n",
      "step: 19325\n",
      "train: loss: 12030.4580078125 acc: 0.9837328195571899  val: loss: 297048.71875 acc: 0.9471036791801453\n",
      "step: 19330\n",
      "train: loss: 7511.50927734375 acc: 0.993416428565979  val: loss: 1155053.125 acc: 0.8722106218338013\n",
      "step: 19335\n",
      "train: loss: 26170.2890625 acc: 0.9744433760643005  val: loss: 1832145.75 acc: 0.6902159452438354\n",
      "step: 19340\n",
      "train: loss: 12804.3740234375 acc: 0.9934350252151489  val: loss: 1076231.875 acc: 0.5320984125137329\n",
      "step: 19345\n",
      "train: loss: 17764.84375 acc: 0.9933817982673645  val: loss: 1679815.75 acc: 0.6234350800514221\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 19350\n",
      "train: loss: 13269.7529296875 acc: 0.9844176769256592  val: loss: 403930.28125 acc: 0.9151198863983154\n",
      "step: 19355\n",
      "train: loss: 15103.8154296875 acc: 0.9890302419662476  val: loss: 3265953.75 acc: 0.4675947427749634\n",
      "step: 19360\n",
      "train: loss: 5249.14892578125 acc: 0.9946267604827881  val: loss: 3391624.75 acc: 0.4100514054298401\n",
      "step: 19365\n",
      "train: loss: 27972.3671875 acc: 0.9901838302612305  val: loss: 1871169.625 acc: 0.588640570640564\n",
      "step: 19370\n",
      "train: loss: 26962.150390625 acc: 0.992680013179779  val: loss: 2085968.625 acc: 0.3781408667564392\n",
      "step: 19375\n",
      "train: loss: 39968.9453125 acc: 0.9908351898193359  val: loss: 2116892.0 acc: 0.6976628303527832\n",
      "step: 19380\n",
      "train: loss: 35142.28515625 acc: 0.9851239323616028  val: loss: 1985562.0 acc: 0.032610177993774414\n",
      "step: 19385\n",
      "train: loss: 26488.908203125 acc: 0.9915885329246521  val: loss: 1552965.625 acc: 0.21363884210586548\n",
      "step: 19390\n",
      "train: loss: 146681.859375 acc: 0.9679853916168213  val: loss: 1008895.75 acc: 0.8479403257369995\n",
      "step: 19395\n",
      "train: loss: 336878.78125 acc: 0.8296436071395874  val: loss: 2426948.5 acc: 0.5719688534736633\n",
      "step: 19400\n",
      "train: loss: 482023.75 acc: 0.919405460357666  val: loss: 2082720.625 acc: 0.4912627935409546\n",
      "step: 19405\n",
      "train: loss: 79941.921875 acc: 0.9764940738677979  val: loss: 3221644.0 acc: -0.013174772262573242\n",
      "step: 19410\n",
      "train: loss: 153315.015625 acc: 0.9809941053390503  val: loss: 3291012.5 acc: 0.5162440538406372\n",
      "step: 19415\n",
      "train: loss: 495750.5 acc: 0.9423906803131104  val: loss: 2198810.25 acc: 0.2823737859725952\n",
      "step: 19420\n",
      "train: loss: 93539.1796875 acc: 0.990418553352356  val: loss: 1145486.125 acc: 0.6802088618278503\n",
      "step: 19425\n",
      "train: loss: 329088.09375 acc: 0.97442227602005  val: loss: 1071886.5 acc: 0.8577877879142761\n",
      "step: 19430\n",
      "train: loss: 210613.3125 acc: 0.9791121482849121  val: loss: 905046.5 acc: 0.8605085611343384\n",
      "step: 19435\n",
      "train: loss: 560513.8125 acc: 0.9283342361450195  val: loss: 1139239.75 acc: 0.8485455513000488\n",
      "step: 19440\n",
      "train: loss: 165147.59375 acc: 0.9768223762512207  val: loss: 820995.75 acc: 0.8022597432136536\n",
      "step: 19445\n",
      "train: loss: 947533.3125 acc: 0.9582219123840332  val: loss: 652074.5 acc: 0.839686930179596\n",
      "step: 19450\n",
      "train: loss: 1933197.125 acc: 0.943332314491272  val: loss: 594787.125 acc: 0.8389077186584473\n",
      "step: 19455\n",
      "train: loss: 1601586.375 acc: 0.9452101588249207  val: loss: 939810.0 acc: 0.8378599882125854\n",
      "step: 19460\n",
      "train: loss: 1621562.125 acc: 0.9182454943656921  val: loss: 832758.9375 acc: 0.8008195161819458\n",
      "step: 19465\n",
      "train: loss: 765992.4375 acc: 0.9556105136871338  val: loss: 178481.6875 acc: 0.9377766251564026\n",
      "step: 19470\n",
      "train: loss: 646463.6875 acc: 0.9556512236595154  val: loss: 1753117.625 acc: 0.5492489337921143\n",
      "step: 19475\n",
      "train: loss: 123045.125 acc: 0.974398672580719  val: loss: 1825245.875 acc: 0.5901252031326294\n",
      "step: 19480\n",
      "train: loss: 665391.75 acc: 0.9234967231750488  val: loss: 232965.0625 acc: 0.9559739828109741\n",
      "step: 19485\n",
      "train: loss: 588109.4375 acc: 0.8837229609489441  val: loss: 297190.3125 acc: 0.8596033453941345\n",
      "step: 19490\n",
      "train: loss: 551113.6875 acc: 0.7465837001800537  val: loss: 380140.96875 acc: 0.6350933909416199\n",
      "step: 19495\n",
      "train: loss: 134128.15625 acc: 0.9241344928741455  val: loss: 524196.875 acc: 0.7378984093666077\n",
      "step: 19500\n",
      "train: loss: 521284.8125 acc: 0.8505516052246094  val: loss: 594273.4375 acc: 0.8445367813110352\n",
      "step: 19505\n",
      "train: loss: 1106986.0 acc: 0.633017897605896  val: loss: 1423692.125 acc: 0.7382814884185791\n",
      "step: 19510\n",
      "train: loss: 841590.5625 acc: 0.7122859954833984  val: loss: 1337808.625 acc: 0.6807988882064819\n",
      "step: 19515\n",
      "train: loss: 344031.53125 acc: 0.7638196349143982  val: loss: 347254.9375 acc: 0.7035516500473022\n",
      "step: 19520\n",
      "train: loss: 506206.09375 acc: 0.7604703903198242  val: loss: 1328760.5 acc: 0.5643527507781982\n",
      "step: 19525\n",
      "train: loss: 196546.296875 acc: 0.8531770706176758  val: loss: 767994.375 acc: 0.6717514991760254\n",
      "step: 19530\n",
      "train: loss: 69969.1171875 acc: 0.9374828338623047  val: loss: 854251.9375 acc: 0.7112059593200684\n",
      "step: 19535\n",
      "train: loss: 42371.86328125 acc: 0.9647113680839539  val: loss: 532685.75 acc: 0.7786133289337158\n",
      "step: 19540\n",
      "train: loss: 91885.265625 acc: 0.9167106747627258  val: loss: 431821.84375 acc: 0.7369846701622009\n",
      "step: 19545\n",
      "train: loss: 332395.625 acc: 0.8433424234390259  val: loss: 1094431.875 acc: 0.7025001049041748\n",
      "step: 19550\n",
      "train: loss: 22427.578125 acc: 0.9824569225311279  val: loss: 847697.9375 acc: 0.6779203414916992\n",
      "step: 19555\n",
      "train: loss: 201048.578125 acc: 0.7910304069519043  val: loss: 1084354.625 acc: 0.6067887544631958\n",
      "step: 19560\n",
      "train: loss: 55786.5546875 acc: 0.9434044361114502  val: loss: 2599648.25 acc: 0.6242339015007019\n",
      "step: 19565\n",
      "train: loss: 440495.09375 acc: 0.6546428203582764  val: loss: 1654123.0 acc: 0.6564164757728577\n",
      "step: 19570\n",
      "train: loss: 505318.5 acc: 0.7690566778182983  val: loss: 2695975.0 acc: 0.5950115919113159\n",
      "step: 19575\n",
      "train: loss: 583846.8125 acc: 0.7410587668418884  val: loss: 3328643.5 acc: 0.5352785587310791\n",
      "step: 19580\n",
      "train: loss: 59486.55859375 acc: 0.9264299273490906  val: loss: 544650.1875 acc: 0.7586515545845032\n",
      "step: 19585\n",
      "train: loss: 451128.59375 acc: 0.6963903307914734  val: loss: 2610801.25 acc: 0.6542099714279175\n",
      "step: 19590\n",
      "train: loss: 379439.4375 acc: 0.8212790489196777  val: loss: 1542252.375 acc: 0.6729605197906494\n",
      "step: 19595\n",
      "train: loss: 2207957.75 acc: 0.7268896102905273  val: loss: 3012696.75 acc: 0.7067419290542603\n",
      "step: 19600\n",
      "train: loss: 1070301.375 acc: 0.7933630347251892  val: loss: 1499937.875 acc: 0.8128562569618225\n",
      "step: 19605\n",
      "train: loss: 1285364.875 acc: 0.8878971338272095  val: loss: 707158.3125 acc: 0.7752712965011597\n",
      "step: 19610\n",
      "train: loss: 1011499.4375 acc: 0.8771520853042603  val: loss: 839396.25 acc: 0.8927997350692749\n",
      "step: 19615\n",
      "train: loss: 379899.375 acc: 0.9361860752105713  val: loss: 1408648.75 acc: -0.22930967807769775\n",
      "step: 19620\n",
      "train: loss: 262415.46875 acc: 0.9344730973243713  val: loss: 2380715.25 acc: 0.3168451189994812\n",
      "step: 19625\n",
      "train: loss: 60933.28125 acc: 0.9955877065658569  val: loss: 490704.125 acc: 0.9546345472335815\n",
      "step: 19630\n",
      "train: loss: 88342.9609375 acc: 0.9942064881324768  val: loss: 859093.5625 acc: 0.8969890475273132\n",
      "step: 19635\n",
      "train: loss: 44803.45703125 acc: 0.9964730143547058  val: loss: 1684953.125 acc: 0.7609758973121643\n",
      "step: 19640\n",
      "train: loss: 66699.671875 acc: 0.9910869002342224  val: loss: 3842700.5 acc: 0.075888991355896\n",
      "step: 19645\n",
      "train: loss: 40990.71484375 acc: 0.9873320460319519  val: loss: 1520289.75 acc: 0.8385047912597656\n",
      "step: 19650\n",
      "train: loss: 14481.1474609375 acc: 0.996209979057312  val: loss: 1748372.5 acc: 0.5350965261459351\n",
      "step: 19655\n",
      "train: loss: 11140.6083984375 acc: 0.9945850372314453  val: loss: 1224238.0 acc: 0.8067107200622559\n",
      "step: 19660\n",
      "train: loss: 4397.9892578125 acc: 0.9971526861190796  val: loss: 585614.0 acc: 0.8724541664123535\n",
      "step: 19665\n",
      "train: loss: 14675.5458984375 acc: 0.9731563329696655  val: loss: 1381068.5 acc: 0.8118868470191956\n",
      "step: 19670\n",
      "train: loss: 5607.83349609375 acc: 0.9906087517738342  val: loss: 2158945.0 acc: 0.6549148559570312\n",
      "step: 19675\n",
      "train: loss: 18973.640625 acc: 0.9391669631004333  val: loss: 1039600.5625 acc: 0.8993586301803589\n",
      "step: 19680\n",
      "train: loss: 20950.90234375 acc: 0.9560563564300537  val: loss: 1368948.375 acc: 0.7474088668823242\n",
      "step: 19685\n",
      "train: loss: 20032.466796875 acc: 0.978100597858429  val: loss: 875949.25 acc: 0.8833406567573547\n",
      "step: 19690\n",
      "train: loss: 2639.169921875 acc: 0.9944866895675659  val: loss: 839287.625 acc: 0.7633914947509766\n",
      "step: 19695\n",
      "train: loss: 44490.81640625 acc: 0.970229983329773  val: loss: 1605536.375 acc: 0.44944554567337036\n",
      "step: 19700\n",
      "train: loss: 17622.93359375 acc: 0.985842764377594  val: loss: 1005734.875 acc: -0.05693364143371582\n",
      "step: 19705\n",
      "train: loss: 139476.75 acc: 0.9089166522026062  val: loss: 1566203.5 acc: 0.6110806465148926\n",
      "step: 19710\n",
      "train: loss: 85931.9765625 acc: 0.9599732756614685  val: loss: 1819897.125 acc: 0.03592967987060547\n",
      "step: 19715\n",
      "train: loss: 10882.9482421875 acc: 0.9939320087432861  val: loss: 2287684.25 acc: 0.34772825241088867\n",
      "step: 19720\n",
      "train: loss: 17245.322265625 acc: 0.9766851663589478  val: loss: 2643189.0 acc: 0.5752605199813843\n",
      "step: 19725\n",
      "train: loss: 13556.365234375 acc: 0.9908508062362671  val: loss: 1324524.0 acc: 0.6342709064483643\n",
      "step: 19730\n",
      "train: loss: 38269.2890625 acc: 0.9894165992736816  val: loss: 1868706.375 acc: 0.6228020191192627\n",
      "step: 19735\n",
      "train: loss: 24901.34375 acc: 0.9949091076850891  val: loss: 2352018.75 acc: 0.4030914306640625\n",
      "step: 19740\n",
      "train: loss: 39747.56640625 acc: 0.9904993772506714  val: loss: 1300000.5 acc: 0.7459774613380432\n",
      "step: 19745\n",
      "train: loss: 39560.5859375 acc: 0.9781365394592285  val: loss: 1167089.625 acc: 0.7842918038368225\n",
      "step: 19750\n",
      "train: loss: 53815.3984375 acc: 0.9877447485923767  val: loss: 1635709.75 acc: 0.5201345086097717\n",
      "step: 19755\n",
      "train: loss: 235461.921875 acc: 0.9366124272346497  val: loss: 2215202.0 acc: 0.46486949920654297\n",
      "step: 19760\n",
      "train: loss: 103067.4375 acc: 0.9651107788085938  val: loss: 1946417.875 acc: 0.4827457666397095\n",
      "step: 19765\n",
      "train: loss: 33041.01953125 acc: 0.9649832248687744  val: loss: 1192480.25 acc: 0.7692450284957886\n",
      "step: 19770\n",
      "train: loss: 612240.1875 acc: 0.8664019107818604  val: loss: 1446762.25 acc: 0.8363350629806519\n",
      "step: 19775\n",
      "train: loss: 115194.5078125 acc: 0.9887372255325317  val: loss: 1065744.5 acc: 0.7876715660095215\n",
      "step: 19780\n",
      "train: loss: 364304.65625 acc: 0.9404286742210388  val: loss: 1285300.125 acc: 0.8664520978927612\n",
      "step: 19785\n",
      "train: loss: 241007.484375 acc: 0.9713068008422852  val: loss: 1494967.375 acc: 0.8109621405601501\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 19790\n",
      "train: loss: 235185.734375 acc: 0.9716891646385193  val: loss: 1323778.375 acc: 0.6229581832885742\n",
      "step: 19795\n",
      "train: loss: 967113.5625 acc: 0.956853449344635  val: loss: 2817391.5 acc: -0.6919118165969849\n",
      "step: 19800\n",
      "train: loss: 2686367.25 acc: 0.865311861038208  val: loss: 319453.0 acc: 0.9148862361907959\n",
      "step: 19805\n",
      "train: loss: 297172.9375 acc: 0.9740714430809021  val: loss: 642780.3125 acc: 0.8118841052055359\n",
      "step: 19810\n",
      "train: loss: 1178612.875 acc: 0.9352005124092102  val: loss: 1014744.125 acc: 0.7796545028686523\n",
      "step: 19815\n",
      "train: loss: 498411.5 acc: 0.9821699261665344  val: loss: 1909695.25 acc: 0.381117582321167\n",
      "step: 19820\n",
      "train: loss: 1027108.75 acc: 0.9685204029083252  val: loss: 306337.6875 acc: 0.9209821820259094\n",
      "step: 19825\n",
      "train: loss: 1707725.0 acc: 0.9288578033447266  val: loss: 716313.9375 acc: 0.9082401394844055\n",
      "step: 19830\n",
      "train: loss: 997249.9375 acc: 0.9463947415351868  val: loss: 310406.4375 acc: 0.8862462043762207\n",
      "step: 19835\n",
      "train: loss: 704873.8125 acc: 0.9389517307281494  val: loss: 289191.5 acc: 0.9270975589752197\n",
      "step: 19840\n",
      "train: loss: 364117.6875 acc: 0.8909227252006531  val: loss: 842428.125 acc: 0.873343288898468\n",
      "step: 19845\n",
      "train: loss: 107227.703125 acc: 0.9652180075645447  val: loss: 369979.875 acc: 0.9266531467437744\n",
      "step: 19850\n",
      "train: loss: 1538084.125 acc: 0.507060170173645  val: loss: 685700.0 acc: 0.8443828821182251\n",
      "step: 19855\n",
      "train: loss: 692789.6875 acc: 0.5585391521453857  val: loss: 1493230.5 acc: 0.8143312931060791\n",
      "step: 19860\n",
      "train: loss: 659980.0 acc: 0.6946609020233154  val: loss: 737204.0 acc: 0.7698216438293457\n",
      "step: 19865\n",
      "train: loss: 956106.6875 acc: 0.7866612672805786  val: loss: 774090.6875 acc: 0.815934419631958\n",
      "step: 19870\n",
      "train: loss: 724364.875 acc: 0.7432945966720581  val: loss: 920379.6875 acc: 0.7768773436546326\n",
      "step: 19875\n",
      "train: loss: 733142.3125 acc: 0.571516752243042  val: loss: 425119.9375 acc: 0.7213287949562073\n",
      "step: 19880\n",
      "train: loss: 543137.9375 acc: 0.7304223775863647  val: loss: 931006.0625 acc: 0.6335176229476929\n",
      "step: 19885\n",
      "train: loss: 100799.9921875 acc: 0.9126025438308716  val: loss: 2678780.75 acc: 0.5697761178016663\n",
      "step: 19890\n",
      "train: loss: 207651.921875 acc: 0.8515275120735168  val: loss: 2387516.0 acc: 0.6384782791137695\n",
      "step: 19895\n",
      "train: loss: 102247.3125 acc: 0.913151741027832  val: loss: 2051525.875 acc: 0.6516985297203064\n",
      "step: 19900\n",
      "train: loss: 43857.48046875 acc: 0.9631600379943848  val: loss: 2751903.0 acc: 0.6072015762329102\n",
      "step: 19905\n",
      "train: loss: 60518.71484375 acc: 0.9447610378265381  val: loss: 2463099.0 acc: 0.614150881767273\n",
      "step: 19910\n",
      "train: loss: 636115.0625 acc: 0.7578628659248352  val: loss: 3524800.0 acc: 0.6090099215507507\n",
      "step: 19915\n",
      "train: loss: 38981.92578125 acc: 0.9670588970184326  val: loss: 3637552.5 acc: 0.5615872740745544\n",
      "step: 19920\n",
      "train: loss: 136187.65625 acc: 0.9119440317153931  val: loss: 3012543.0 acc: 0.6288914680480957\n",
      "step: 19925\n",
      "train: loss: 547908.8125 acc: 0.6884231567382812  val: loss: 2521591.5 acc: 0.6592073440551758\n",
      "step: 19930\n",
      "train: loss: 111148.7265625 acc: 0.8911240696907043  val: loss: 1978514.75 acc: 0.6987500190734863\n",
      "step: 19935\n",
      "train: loss: 101157.9765625 acc: 0.9237101674079895  val: loss: 1264986.625 acc: 0.5849013328552246\n",
      "step: 19940\n",
      "train: loss: 260329.296875 acc: 0.8059583902359009  val: loss: 2761008.5 acc: 0.6561384201049805\n",
      "step: 19945\n",
      "train: loss: 216765.84375 acc: 0.8404675722122192  val: loss: 3525200.0 acc: 0.555992603302002\n",
      "step: 19950\n",
      "train: loss: 596331.875 acc: 0.7241773009300232  val: loss: 1770744.75 acc: 0.6762638092041016\n",
      "step: 19955\n",
      "train: loss: 873565.0 acc: 0.7350231409072876  val: loss: 675979.8125 acc: 0.712492823600769\n",
      "step: 19960\n",
      "train: loss: 2082495.0 acc: 0.690233588218689  val: loss: 2990556.5 acc: 0.7367391586303711\n",
      "step: 19965\n",
      "train: loss: 1429260.375 acc: 0.7549574375152588  val: loss: 561599.4375 acc: 0.9029331207275391\n",
      "step: 19970\n",
      "train: loss: 1115686.875 acc: 0.8600620031356812  val: loss: 616658.4375 acc: 0.7844685912132263\n",
      "step: 19975\n",
      "train: loss: 258605.765625 acc: 0.9812080264091492  val: loss: 1048201.5 acc: 0.6789218187332153\n",
      "step: 19980\n",
      "train: loss: 670884.9375 acc: 0.8430054187774658  val: loss: 1541276.25 acc: 0.6700361967086792\n",
      "step: 19985\n",
      "train: loss: 443631.40625 acc: 0.9271800518035889  val: loss: 1402003.25 acc: 0.4558577537536621\n",
      "step: 19990\n",
      "train: loss: 179031.34375 acc: 0.9820601940155029  val: loss: 738213.4375 acc: 0.5215936899185181\n",
      "step: 19995\n",
      "train: loss: 85006.921875 acc: 0.9941160082817078  val: loss: 2149412.75 acc: 0.6320513486862183\n",
      "step: 20000\n",
      "train: loss: 106578.265625 acc: 0.9928991198539734  val: loss: 1757734.25 acc: 0.7820932865142822\n",
      "step: 20005\n",
      "train: loss: 88571.7890625 acc: 0.9834089279174805  val: loss: 1135828.75 acc: 0.7364846467971802\n",
      "step: 20010\n",
      "train: loss: 35567.203125 acc: 0.9916647672653198  val: loss: 1806304.75 acc: 0.4220868945121765\n",
      "step: 20015\n",
      "train: loss: 12632.75390625 acc: 0.9968584179878235  val: loss: 1154472.875 acc: 0.8606643676757812\n",
      "step: 20020\n",
      "train: loss: 10362.830078125 acc: 0.9923211932182312  val: loss: 267660.90625 acc: 0.9668993949890137\n",
      "step: 20025\n",
      "train: loss: 49910.1640625 acc: 0.9759963154792786  val: loss: 1554282.75 acc: 0.29746848344802856\n",
      "step: 20030\n",
      "train: loss: 6787.2978515625 acc: 0.988327145576477  val: loss: 1857954.125 acc: 0.471901535987854\n",
      "step: 20035\n",
      "train: loss: 16756.49609375 acc: 0.964246928691864  val: loss: 688889.8125 acc: 0.7288687229156494\n",
      "step: 20040\n",
      "train: loss: 35306.96484375 acc: 0.9595141410827637  val: loss: 3216458.0 acc: 0.6747251749038696\n",
      "step: 20045\n",
      "train: loss: 10579.7744140625 acc: 0.9721888899803162  val: loss: 1846435.75 acc: 0.24232345819473267\n",
      "step: 20050\n",
      "train: loss: 12897.9267578125 acc: 0.9575183987617493  val: loss: 3298628.25 acc: -1.4386441707611084\n",
      "step: 20055\n",
      "train: loss: 4812.25537109375 acc: 0.9834276437759399  val: loss: 1623397.375 acc: 0.7824344038963318\n",
      "step: 20060\n",
      "train: loss: 19992.1796875 acc: 0.9649048447608948  val: loss: 1873449.0 acc: 0.5475023984909058\n",
      "step: 20065\n",
      "train: loss: 10325.185546875 acc: 0.9947038888931274  val: loss: 598591.875 acc: 0.9222078919410706\n",
      "step: 20070\n",
      "train: loss: 25568.697265625 acc: 0.985992968082428  val: loss: 1983505.125 acc: 0.5534111261367798\n",
      "step: 20075\n",
      "train: loss: 10738.7548828125 acc: 0.9946869611740112  val: loss: 2471461.5 acc: 0.6655868291854858\n",
      "step: 20080\n",
      "train: loss: 31547.12109375 acc: 0.9869371056556702  val: loss: 2767283.75 acc: 0.5638109445571899\n",
      "step: 20085\n",
      "train: loss: 85626.4375 acc: 0.897904634475708  val: loss: 1448644.5 acc: 0.7059109210968018\n",
      "step: 20090\n",
      "train: loss: 5152.93701171875 acc: 0.9904000759124756  val: loss: 2067644.125 acc: 0.530876636505127\n",
      "step: 20095\n",
      "train: loss: 8407.56640625 acc: 0.9970826506614685  val: loss: 2221544.25 acc: 0.27571433782577515\n",
      "step: 20100\n",
      "train: loss: 46909.3515625 acc: 0.9877632260322571  val: loss: 1979448.75 acc: 0.5178405046463013\n",
      "step: 20105\n",
      "train: loss: 26822.80859375 acc: 0.9909291863441467  val: loss: 290656.28125 acc: 0.9684481620788574\n",
      "step: 20110\n",
      "train: loss: 28053.10546875 acc: 0.9881632328033447  val: loss: 825366.625 acc: 0.8149641752243042\n",
      "step: 20115\n",
      "train: loss: 51153.38671875 acc: 0.9794374108314514  val: loss: 865606.25 acc: 0.8284040093421936\n",
      "step: 20120\n",
      "train: loss: 262489.96875 acc: 0.932944655418396  val: loss: 1878765.5 acc: 0.3113911747932434\n",
      "step: 20125\n",
      "train: loss: 162294.578125 acc: 0.9534940719604492  val: loss: 704947.625 acc: 0.6086194515228271\n",
      "step: 20130\n",
      "train: loss: 80532.7578125 acc: 0.9579506516456604  val: loss: 1011332.5 acc: 0.4349691867828369\n",
      "step: 20135\n",
      "train: loss: 120194.609375 acc: 0.9834808707237244  val: loss: 329914.625 acc: 0.9281831383705139\n",
      "step: 20140\n",
      "train: loss: 117872.8359375 acc: 0.9897428750991821  val: loss: 179840.109375 acc: 0.9469639658927917\n",
      "step: 20145\n",
      "train: loss: 99311.8046875 acc: 0.9866629242897034  val: loss: 1137672.5 acc: 0.6792221665382385\n",
      "step: 20150\n",
      "train: loss: 30591.251953125 acc: 0.9955780506134033  val: loss: 451101.0625 acc: 0.8365879654884338\n",
      "step: 20155\n",
      "train: loss: 313974.5625 acc: 0.9392524361610413  val: loss: 532745.5625 acc: 0.8777456879615784\n",
      "step: 20160\n",
      "train: loss: 175998.9375 acc: 0.9849951267242432  val: loss: 1354976.625 acc: 0.7232479453086853\n",
      "step: 20165\n",
      "train: loss: 683294.8125 acc: 0.9684238433837891  val: loss: 616973.1875 acc: 0.8434297442436218\n",
      "step: 20170\n",
      "train: loss: 330880.8125 acc: 0.9610564112663269  val: loss: 1138123.375 acc: 0.7756645679473877\n",
      "step: 20175\n",
      "train: loss: 279722.4375 acc: 0.9555813074111938  val: loss: 532796.0625 acc: 0.8264198303222656\n",
      "step: 20180\n",
      "train: loss: 377892.5625 acc: 0.9828550815582275  val: loss: 773579.1875 acc: 0.8880447745323181\n",
      "step: 20185\n",
      "train: loss: 1050357.0 acc: 0.9683498740196228  val: loss: 825790.6875 acc: 0.8906606435775757\n",
      "step: 20190\n",
      "train: loss: 1630755.625 acc: 0.9192350506782532  val: loss: 494465.9375 acc: 0.8782309293746948\n",
      "step: 20195\n",
      "train: loss: 1439863.5 acc: 0.9403998851776123  val: loss: 444171.25 acc: 0.900753378868103\n",
      "step: 20200\n",
      "train: loss: 385045.5 acc: 0.9587237238883972  val: loss: 738041.4375 acc: 0.9205245971679688\n",
      "step: 20205\n",
      "train: loss: 178563.25 acc: 0.9750038981437683  val: loss: 191758.4375 acc: 0.9498466849327087\n",
      "step: 20210\n",
      "train: loss: 551219.5 acc: 0.8588994741439819  val: loss: 509609.875 acc: 0.9234915971755981\n",
      "step: 20215\n",
      "train: loss: 1847593.75 acc: 0.2609652876853943  val: loss: 2072406.0 acc: 0.7891611456871033\n",
      "step: 20220\n",
      "train: loss: 740708.625 acc: 0.845935046672821  val: loss: 696095.9375 acc: 0.746497631072998\n",
      "step: 20225\n",
      "train: loss: 603366.3125 acc: 0.7544717788696289  val: loss: 1074590.0 acc: 0.8418307900428772\n",
      "step: 20230\n",
      "train: loss: 524978.25 acc: 0.8246123194694519  val: loss: 422787.125 acc: 0.8410040736198425\n",
      "step: 20235\n",
      "train: loss: 559506.125 acc: 0.8457865118980408  val: loss: 385791.03125 acc: 0.8428826928138733\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 20240\n",
      "train: loss: 845988.6875 acc: 0.632681667804718  val: loss: 2031368.125 acc: 0.7783678770065308\n",
      "step: 20245\n",
      "train: loss: 282349.0625 acc: 0.8472402095794678  val: loss: 638025.1875 acc: 0.6167521476745605\n",
      "step: 20250\n",
      "train: loss: 189599.984375 acc: 0.8673464059829712  val: loss: 4471292.5 acc: 0.580050528049469\n",
      "step: 20255\n",
      "train: loss: 269074.0 acc: 0.8201180696487427  val: loss: 6078112.0 acc: 0.6054097414016724\n",
      "step: 20260\n",
      "train: loss: 202451.65625 acc: 0.8568370342254639  val: loss: 3594194.0 acc: 0.6593519449234009\n",
      "step: 20265\n",
      "train: loss: 118572.6484375 acc: 0.9210208654403687  val: loss: 1642929.5 acc: 0.6619173288345337\n",
      "step: 20270\n",
      "train: loss: 128256.2421875 acc: 0.8885339498519897  val: loss: 1211356.25 acc: 0.7170824408531189\n",
      "step: 20275\n",
      "train: loss: 107608.875 acc: 0.9259740710258484  val: loss: 3437863.0 acc: 0.5918520092964172\n",
      "step: 20280\n",
      "train: loss: 274339.0625 acc: 0.8303462266921997  val: loss: 2909177.25 acc: 0.619563102722168\n",
      "step: 20285\n",
      "train: loss: 122894.6484375 acc: 0.8692348599433899  val: loss: 550725.125 acc: 0.7138040065765381\n",
      "step: 20290\n",
      "train: loss: 85181.59375 acc: 0.9276286959648132  val: loss: 4634709.0 acc: 0.5914100408554077\n",
      "step: 20295\n",
      "train: loss: 367323.125 acc: 0.8389948010444641  val: loss: 525607.0625 acc: 0.7934038043022156\n",
      "step: 20300\n",
      "train: loss: 461729.4375 acc: 0.7794603109359741  val: loss: 106134.8359375 acc: 0.9133332967758179\n",
      "step: 20305\n",
      "train: loss: 231838.875 acc: 0.8413504362106323  val: loss: 2524486.0 acc: 0.5988187193870544\n",
      "step: 20310\n",
      "train: loss: 469610.28125 acc: 0.7164124846458435  val: loss: 1662587.75 acc: 0.680594265460968\n",
      "step: 20315\n",
      "train: loss: 36122.8671875 acc: 0.9575517177581787  val: loss: 1096196.25 acc: 0.6534630656242371\n",
      "step: 20320\n",
      "train: loss: 455846.78125 acc: 0.7763091325759888  val: loss: 1325631.25 acc: 0.6010582447052002\n",
      "step: 20325\n",
      "train: loss: 1695695.0 acc: 0.6993929147720337  val: loss: 1297570.0 acc: 0.6994756460189819\n",
      "step: 20330\n",
      "train: loss: 1036256.375 acc: 0.788650393486023  val: loss: 893441.5 acc: 0.7158046960830688\n",
      "step: 20335\n",
      "train: loss: 890339.3125 acc: 0.9198538661003113  val: loss: 1111860.25 acc: 0.8240744471549988\n",
      "step: 20340\n",
      "train: loss: 348317.09375 acc: 0.968608558177948  val: loss: 2639975.75 acc: 0.20851922035217285\n",
      "step: 20345\n",
      "train: loss: 361924.21875 acc: 0.943718671798706  val: loss: 1187408.75 acc: 0.5110800266265869\n",
      "step: 20350\n",
      "train: loss: 364546.84375 acc: 0.9497241377830505  val: loss: 1678594.25 acc: 0.4251580834388733\n",
      "step: 20355\n",
      "train: loss: 162953.546875 acc: 0.9806973934173584  val: loss: 686965.375 acc: 0.7763203978538513\n",
      "step: 20360\n",
      "train: loss: 137703.140625 acc: 0.9872732758522034  val: loss: 1427591.125 acc: 0.8024982213973999\n",
      "step: 20365\n",
      "train: loss: 75327.0703125 acc: 0.993354856967926  val: loss: 1304479.0 acc: 0.8285437822341919\n",
      "step: 20370\n",
      "train: loss: 336137.8125 acc: 0.9546754360198975  val: loss: 647726.75 acc: 0.8493030667304993\n",
      "step: 20375\n",
      "train: loss: 52204.18359375 acc: 0.9928973913192749  val: loss: 2486463.0 acc: 0.7884543538093567\n",
      "step: 20380\n",
      "train: loss: 22050.326171875 acc: 0.9893982410430908  val: loss: 1789899.875 acc: 0.6203442811965942\n",
      "step: 20385\n",
      "train: loss: 8828.9287109375 acc: 0.972226083278656  val: loss: 2406167.25 acc: 0.2803407311439514\n",
      "step: 20390\n",
      "train: loss: 11308.4794921875 acc: 0.9795451164245605  val: loss: 1775651.75 acc: 0.6323540806770325\n",
      "step: 20395\n",
      "train: loss: 3826.537353515625 acc: 0.9918548464775085  val: loss: 315590.5 acc: 0.9554412961006165\n",
      "step: 20400\n",
      "train: loss: 195974.40625 acc: 0.5914063453674316  val: loss: 426849.46875 acc: 0.9581506848335266\n",
      "step: 20405\n",
      "train: loss: 23928.693359375 acc: 0.968949556350708  val: loss: 486506.65625 acc: 0.9373922944068909\n",
      "step: 20410\n",
      "train: loss: 16523.837890625 acc: 0.9416977167129517  val: loss: 1450121.5 acc: 0.7306677103042603\n",
      "step: 20415\n",
      "train: loss: 10708.6494140625 acc: 0.9248783588409424  val: loss: 1428442.75 acc: 0.4471641182899475\n",
      "step: 20420\n",
      "train: loss: 11517.052734375 acc: 0.9838207364082336  val: loss: 675529.75 acc: 0.8791877031326294\n",
      "step: 20425\n",
      "train: loss: 53229.8515625 acc: 0.9548520445823669  val: loss: 1245840.875 acc: 0.7093596458435059\n",
      "step: 20430\n",
      "train: loss: 20938.705078125 acc: 0.984060525894165  val: loss: 617634.25 acc: 0.7689430117607117\n",
      "step: 20435\n",
      "train: loss: 22795.982421875 acc: 0.9800550937652588  val: loss: 916720.3125 acc: 0.5714635848999023\n",
      "step: 20440\n",
      "train: loss: 25450.767578125 acc: 0.9824977517127991  val: loss: 1565661.0 acc: 0.7282341718673706\n",
      "step: 20445\n",
      "train: loss: 12117.2998046875 acc: 0.988982617855072  val: loss: 753546.1875 acc: 0.43158960342407227\n",
      "step: 20450\n",
      "train: loss: 6525.77734375 acc: 0.99376380443573  val: loss: 524763.4375 acc: 0.8708492517471313\n",
      "step: 20455\n",
      "train: loss: 9764.7548828125 acc: 0.990824282169342  val: loss: 425958.28125 acc: 0.950238823890686\n",
      "step: 20460\n",
      "train: loss: 27716.294921875 acc: 0.9886167049407959  val: loss: 295070.15625 acc: 0.9043895602226257\n",
      "step: 20465\n",
      "train: loss: 52861.01171875 acc: 0.9852821230888367  val: loss: 1268364.125 acc: 0.6455146074295044\n",
      "step: 20470\n",
      "train: loss: 26649.853515625 acc: 0.9870827198028564  val: loss: 1282676.375 acc: 0.8149734139442444\n",
      "step: 20475\n",
      "train: loss: 45554.546875 acc: 0.981193482875824  val: loss: 912273.375 acc: 0.7700854539871216\n",
      "step: 20480\n",
      "train: loss: 39900.546875 acc: 0.9866133332252502  val: loss: 108677.53125 acc: 0.9781962633132935\n",
      "step: 20485\n",
      "train: loss: 148421.890625 acc: 0.958630383014679  val: loss: 113568.1875 acc: 0.9849228858947754\n",
      "step: 20490\n",
      "train: loss: 69810.4296875 acc: 0.9624236822128296  val: loss: 272739.375 acc: 0.9139692783355713\n",
      "step: 20495\n",
      "train: loss: 266609.9375 acc: 0.9138039350509644  val: loss: 718150.1875 acc: 0.790276825428009\n",
      "step: 20500\n",
      "train: loss: 566722.125 acc: 0.9253267645835876  val: loss: 1705683.375 acc: -0.458290696144104\n",
      "step: 20505\n",
      "train: loss: 116484.125 acc: 0.9845506548881531  val: loss: 1037756.6875 acc: 0.5794858932495117\n",
      "step: 20510\n",
      "train: loss: 117977.1796875 acc: 0.9814392924308777  val: loss: 346617.1875 acc: 0.9519264698028564\n",
      "step: 20515\n",
      "train: loss: 331258.46875 acc: 0.9456851482391357  val: loss: 397264.4375 acc: 0.9154612421989441\n",
      "step: 20520\n",
      "train: loss: 159116.234375 acc: 0.9591758251190186  val: loss: 372553.6875 acc: 0.9252761602401733\n",
      "step: 20525\n",
      "train: loss: 1698007.375 acc: 0.8416990637779236  val: loss: 369296.84375 acc: 0.9470360279083252\n",
      "step: 20530\n",
      "train: loss: 1100575.25 acc: 0.8491926789283752  val: loss: 975930.125 acc: 0.8555302619934082\n",
      "step: 20535\n",
      "train: loss: 474104.21875 acc: 0.9757841229438782  val: loss: 406799.8125 acc: 0.7807274460792542\n",
      "step: 20540\n",
      "train: loss: 196129.03125 acc: 0.9804130792617798  val: loss: 592689.1875 acc: 0.8651047945022583\n",
      "step: 20545\n",
      "train: loss: 1619613.5 acc: 0.9557156562805176  val: loss: 536698.9375 acc: 0.9480083584785461\n",
      "step: 20550\n",
      "train: loss: 888456.5 acc: 0.9736577868461609  val: loss: 517926.9375 acc: 0.9615892171859741\n",
      "step: 20555\n",
      "train: loss: 1959691.0 acc: 0.8982303142547607  val: loss: 471167.6875 acc: 0.9142889380455017\n",
      "step: 20560\n",
      "train: loss: 570312.5 acc: 0.9656455516815186  val: loss: 1228762.5 acc: 0.7942739129066467\n",
      "step: 20565\n",
      "train: loss: 263349.65625 acc: 0.9631573557853699  val: loss: 955169.875 acc: 0.7982608079910278\n",
      "step: 20570\n",
      "train: loss: 665272.6875 acc: 0.9444420337677002  val: loss: 653584.75 acc: 0.933663010597229\n",
      "step: 20575\n",
      "train: loss: 773020.0625 acc: 0.9199075698852539  val: loss: 940609.5625 acc: 0.9117610454559326\n",
      "step: 20580\n",
      "train: loss: 1862354.75 acc: 0.7019745707511902  val: loss: 232707.640625 acc: 0.8799065947532654\n",
      "step: 20585\n",
      "train: loss: 512927.0625 acc: 0.6899271011352539  val: loss: 652912.0625 acc: 0.8426378965377808\n",
      "step: 20590\n",
      "train: loss: 663033.3125 acc: 0.7943923473358154  val: loss: 539218.8125 acc: 0.7577861547470093\n",
      "step: 20595\n",
      "train: loss: 660024.8125 acc: 0.7958454489707947  val: loss: 3249969.0 acc: 0.7442045211791992\n",
      "step: 20600\n",
      "train: loss: 345477.96875 acc: 0.8902157545089722  val: loss: 599817.125 acc: 0.8686785697937012\n",
      "step: 20605\n",
      "train: loss: 1047989.125 acc: 0.4427907466888428  val: loss: 887905.6875 acc: 0.7854625582695007\n",
      "step: 20610\n",
      "train: loss: 360423.0625 acc: 0.7804064750671387  val: loss: 4557584.5 acc: 0.5854935646057129\n",
      "step: 20615\n",
      "train: loss: 102285.296875 acc: 0.9130373001098633  val: loss: 3175623.25 acc: 0.6335166692733765\n",
      "step: 20620\n",
      "train: loss: 61872.7890625 acc: 0.9373570084571838  val: loss: 531012.5625 acc: 0.7716463208198547\n",
      "step: 20625\n",
      "train: loss: 54703.57421875 acc: 0.9560383558273315  val: loss: 1735994.75 acc: 0.7108449935913086\n",
      "step: 20630\n",
      "train: loss: 24783.798828125 acc: 0.9788427352905273  val: loss: 871699.625 acc: 0.7543765902519226\n",
      "step: 20635\n",
      "train: loss: 32864.03515625 acc: 0.972861647605896  val: loss: 987223.1875 acc: 0.7347694635391235\n",
      "step: 20640\n",
      "train: loss: 25537.314453125 acc: 0.9789836406707764  val: loss: 410590.65625 acc: 0.7865745425224304\n",
      "step: 20645\n",
      "train: loss: 121095.6875 acc: 0.8907039165496826  val: loss: 878250.9375 acc: 0.6550799608230591\n",
      "step: 20650\n",
      "train: loss: 320138.40625 acc: 0.8012322187423706  val: loss: 252819.421875 acc: 0.8492296934127808\n",
      "step: 20655\n",
      "train: loss: 348545.46875 acc: 0.7988469004631042  val: loss: 1595429.875 acc: 0.665228009223938\n",
      "step: 20660\n",
      "train: loss: 39478.72265625 acc: 0.9357039928436279  val: loss: 1517564.5 acc: 0.7310394048690796\n",
      "step: 20665\n",
      "train: loss: 30779.654296875 acc: 0.9680586457252502  val: loss: 2404456.0 acc: 0.581967830657959\n",
      "step: 20670\n",
      "train: loss: 64150.88671875 acc: 0.9260848760604858  val: loss: 901413.1875 acc: 0.6852973699569702\n",
      "step: 20675\n",
      "train: loss: 510754.09375 acc: 0.7534488439559937  val: loss: 941435.9375 acc: 0.711124062538147\n",
      "step: 20680\n",
      "train: loss: 178552.65625 acc: 0.8314352035522461  val: loss: 966956.3125 acc: 0.7313265800476074\n",
      "step: 20685\n",
      "train: loss: 125839.3984375 acc: 0.8975602388381958  val: loss: 730092.3125 acc: 0.7418612241744995\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 20690\n",
      "train: loss: 1860829.75 acc: 0.6726049184799194  val: loss: 1076807.875 acc: 0.749779462814331\n",
      "step: 20695\n",
      "train: loss: 1426079.0 acc: 0.7551799416542053  val: loss: 1569493.25 acc: 0.7558574676513672\n",
      "step: 20700\n",
      "train: loss: 1321458.125 acc: 0.8149030804634094  val: loss: 968422.875 acc: 0.7719559073448181\n",
      "step: 20705\n",
      "train: loss: 557503.25 acc: 0.946831464767456  val: loss: 1333240.75 acc: 0.5556224584579468\n",
      "step: 20710\n",
      "train: loss: 366785.28125 acc: 0.9276989102363586  val: loss: 1479580.75 acc: 0.6910997629165649\n",
      "step: 20715\n",
      "train: loss: 212238.0625 acc: 0.9678899049758911  val: loss: 2059857.75 acc: -0.04424643516540527\n",
      "step: 20720\n",
      "train: loss: 123937.5078125 acc: 0.9801058769226074  val: loss: 754717.0 acc: 0.8762098550796509\n",
      "step: 20725\n",
      "train: loss: 148481.8125 acc: 0.9880878925323486  val: loss: 1158777.75 acc: 0.8107484579086304\n",
      "step: 20730\n",
      "train: loss: 55221.328125 acc: 0.9955904483795166  val: loss: 901278.8125 acc: 0.8494699597358704\n",
      "step: 20735\n",
      "train: loss: 57177.359375 acc: 0.9943850636482239  val: loss: 518364.71875 acc: 0.9398221969604492\n",
      "step: 20740\n",
      "train: loss: 23939.158203125 acc: 0.9956216812133789  val: loss: 520281.84375 acc: 0.8770748376846313\n",
      "step: 20745\n",
      "train: loss: 22624.0390625 acc: 0.9905939698219299  val: loss: 267855.34375 acc: 0.9458168745040894\n",
      "step: 20750\n",
      "train: loss: 19362.671875 acc: 0.9940381050109863  val: loss: 449307.5625 acc: 0.8158135414123535\n",
      "step: 20755\n",
      "train: loss: 20587.65625 acc: 0.9930457472801208  val: loss: 1059713.75 acc: 0.7802174687385559\n",
      "step: 20760\n",
      "train: loss: 12128.9462890625 acc: 0.981281578540802  val: loss: 181145.703125 acc: 0.979499340057373\n",
      "step: 20765\n",
      "train: loss: 14208.0244140625 acc: 0.9891199469566345  val: loss: 1154721.25 acc: 0.7034109830856323\n",
      "step: 20770\n",
      "train: loss: 15352.8544921875 acc: 0.9765534996986389  val: loss: 130673.6875 acc: 0.9710585474967957\n",
      "step: 20775\n",
      "train: loss: 4538.3544921875 acc: 0.9863671064376831  val: loss: 257366.28125 acc: 0.9528826475143433\n",
      "step: 20780\n",
      "train: loss: 6655.9208984375 acc: 0.9840381741523743  val: loss: 106765.3203125 acc: 0.9848679900169373\n",
      "step: 20785\n",
      "train: loss: 12230.4833984375 acc: 0.9672954082489014  val: loss: 1252743.125 acc: 0.600792646408081\n",
      "step: 20790\n",
      "train: loss: 15112.6943359375 acc: 0.9711191058158875  val: loss: 338797.96875 acc: 0.943073570728302\n",
      "step: 20795\n",
      "train: loss: 23806.994140625 acc: 0.9844756722450256  val: loss: 962061.0 acc: 0.32084494829177856\n",
      "step: 20800\n",
      "train: loss: 38077.02734375 acc: 0.9780309796333313  val: loss: 1456325.25 acc: 0.32670485973358154\n",
      "step: 20805\n",
      "train: loss: 18214.40234375 acc: 0.989324688911438  val: loss: 983615.625 acc: 0.7619103193283081\n",
      "step: 20810\n",
      "train: loss: 15197.7041015625 acc: 0.9824430346488953  val: loss: 2223646.0 acc: 0.24419569969177246\n",
      "step: 20815\n",
      "train: loss: 14109.798828125 acc: 0.9937818050384521  val: loss: 1340250.5 acc: 0.49039226770401\n",
      "step: 20820\n",
      "train: loss: 6573.44580078125 acc: 0.9902939200401306  val: loss: 2071003.75 acc: -0.03181004524230957\n",
      "step: 20825\n",
      "train: loss: 6978.60400390625 acc: 0.9936795830726624  val: loss: 2041238.125 acc: -0.40286409854888916\n",
      "step: 20830\n",
      "train: loss: 21230.943359375 acc: 0.99323570728302  val: loss: 1831603.75 acc: 0.26541638374328613\n",
      "step: 20835\n",
      "train: loss: 33617.8046875 acc: 0.991925060749054  val: loss: 273567.8125 acc: 0.9574154019355774\n",
      "step: 20840\n",
      "train: loss: 29312.216796875 acc: 0.9935441613197327  val: loss: 669710.5 acc: 0.8338587284088135\n",
      "step: 20845\n",
      "train: loss: 9799.767578125 acc: 0.9956835508346558  val: loss: 955581.5625 acc: 0.8370139598846436\n",
      "step: 20850\n",
      "train: loss: 51683.9296875 acc: 0.9883813261985779  val: loss: 527879.5 acc: 0.9120239019393921\n",
      "step: 20855\n",
      "train: loss: 319049.40625 acc: 0.9313248991966248  val: loss: 1210501.375 acc: 0.7780898809432983\n",
      "step: 20860\n",
      "train: loss: 119964.7265625 acc: 0.9522871375083923  val: loss: 941293.5 acc: 0.658390998840332\n",
      "step: 20865\n",
      "train: loss: 93300.1640625 acc: 0.9825535416603088  val: loss: 309187.90625 acc: 0.9763205647468567\n",
      "step: 20870\n",
      "train: loss: 162496.265625 acc: 0.9783774018287659  val: loss: 1203041.25 acc: 0.8167214393615723\n",
      "step: 20875\n",
      "train: loss: 87102.8046875 acc: 0.9929760694503784  val: loss: 728504.875 acc: 0.9333940148353577\n",
      "step: 20880\n",
      "train: loss: 124319.8671875 acc: 0.985974133014679  val: loss: 1589119.0 acc: 0.43369704484939575\n",
      "step: 20885\n",
      "train: loss: 86548.5546875 acc: 0.992219865322113  val: loss: 1284280.0 acc: 0.8461685180664062\n",
      "step: 20890\n",
      "train: loss: 160413.9375 acc: 0.9840868711471558  val: loss: 694464.6875 acc: 0.8758595585823059\n",
      "step: 20895\n",
      "train: loss: 272973.78125 acc: 0.9904714822769165  val: loss: 657130.5 acc: 0.9068131446838379\n",
      "step: 20900\n",
      "train: loss: 96623.296875 acc: 0.9919434785842896  val: loss: 597130.625 acc: 0.8308601379394531\n",
      "step: 20905\n",
      "train: loss: 194451.296875 acc: 0.9585411548614502  val: loss: 1129110.25 acc: 0.7869057655334473\n",
      "step: 20910\n",
      "train: loss: 644770.75 acc: 0.9741498231887817  val: loss: 883910.375 acc: 0.548008382320404\n",
      "step: 20915\n",
      "train: loss: 1632246.25 acc: 0.9386822581291199  val: loss: 3620058.25 acc: -1.277289867401123\n",
      "step: 20920\n",
      "train: loss: 1415564.75 acc: 0.9457486867904663  val: loss: 828062.25 acc: 0.9210265278816223\n",
      "step: 20925\n",
      "train: loss: 3147488.75 acc: 0.7785594463348389  val: loss: 1368203.5 acc: 0.826377272605896\n",
      "step: 20930\n",
      "train: loss: 1758364.5 acc: 0.8943963050842285  val: loss: 855219.6875 acc: 0.7874676585197449\n",
      "step: 20935\n",
      "train: loss: 246693.015625 acc: 0.9493850469589233  val: loss: 710325.5 acc: 0.7922031283378601\n",
      "step: 20940\n",
      "train: loss: 311222.09375 acc: 0.9670376777648926  val: loss: 741752.5 acc: 0.7480190992355347\n",
      "step: 20945\n",
      "train: loss: 1952570.125 acc: 0.3586841821670532  val: loss: 1062526.0 acc: 0.8742856979370117\n",
      "step: 20950\n",
      "train: loss: 1057678.125 acc: 0.6642802953720093  val: loss: 1747399.625 acc: 0.8252283334732056\n",
      "step: 20955\n",
      "train: loss: 1065561.125 acc: 0.7595025300979614  val: loss: 1888970.0 acc: 0.7183843851089478\n",
      "step: 20960\n",
      "train: loss: 314860.46875 acc: 0.8586219549179077  val: loss: 424595.09375 acc: 0.8699966669082642\n",
      "step: 20965\n",
      "train: loss: 359990.9375 acc: 0.8632397651672363  val: loss: 419318.34375 acc: 0.8650980591773987\n",
      "step: 20970\n",
      "train: loss: 1004259.5625 acc: 0.6165183782577515  val: loss: 2508320.0 acc: 0.7049552202224731\n",
      "step: 20975\n",
      "train: loss: 920952.125 acc: 0.692566454410553  val: loss: 1692530.625 acc: 0.6927376389503479\n",
      "step: 20980\n",
      "train: loss: 270140.25 acc: 0.8182936310768127  val: loss: 1621311.0 acc: 0.6915412545204163\n",
      "step: 20985\n",
      "train: loss: 236748.03125 acc: 0.8509643077850342  val: loss: 1478199.25 acc: 0.6990403532981873\n",
      "step: 20990\n",
      "train: loss: 53619.078125 acc: 0.9583417177200317  val: loss: 1055713.375 acc: 0.7442560195922852\n",
      "step: 20995\n",
      "train: loss: 56881.046875 acc: 0.9582090377807617  val: loss: 1013801.6875 acc: 0.6998376846313477\n",
      "step: 21000\n",
      "train: loss: 90504.671875 acc: 0.9220211505889893  val: loss: 926301.6875 acc: 0.7042114734649658\n",
      "step: 21005\n",
      "train: loss: 142939.84375 acc: 0.8897303938865662  val: loss: 2072738.5 acc: 0.6126471161842346\n",
      "step: 21010\n",
      "train: loss: 897657.25 acc: 0.7168726921081543  val: loss: 1880694.25 acc: 0.6583787798881531\n",
      "step: 21015\n",
      "train: loss: 22917.228515625 acc: 0.9779332876205444  val: loss: 4508894.5 acc: 0.5651391744613647\n",
      "step: 21020\n",
      "train: loss: 73832.5390625 acc: 0.9279982447624207  val: loss: 1743187.25 acc: 0.6472939252853394\n",
      "step: 21025\n",
      "train: loss: 435046.34375 acc: 0.7635320425033569  val: loss: 1095945.375 acc: 0.7023329734802246\n",
      "step: 21030\n",
      "train: loss: 157969.59375 acc: 0.8843623995780945  val: loss: 3621344.0 acc: 0.5737707614898682\n",
      "step: 21035\n",
      "train: loss: 361565.90625 acc: 0.7743971943855286  val: loss: 664428.75 acc: 0.7210984230041504\n",
      "step: 21040\n",
      "train: loss: 243622.671875 acc: 0.7581269145011902  val: loss: 5591852.5 acc: 0.5227625370025635\n",
      "step: 21045\n",
      "train: loss: 130683.0 acc: 0.9029899835586548  val: loss: 1521294.75 acc: 0.6477289199829102\n",
      "step: 21050\n",
      "train: loss: 705109.4375 acc: 0.7409471869468689  val: loss: 2770434.0 acc: 0.6153544187545776\n",
      "step: 21055\n",
      "train: loss: 713758.0 acc: 0.7260734438896179  val: loss: 1298484.375 acc: 0.6893576383590698\n",
      "step: 21060\n",
      "train: loss: 983107.6875 acc: 0.8007314801216125  val: loss: 693797.75 acc: 0.860945463180542\n",
      "step: 21065\n",
      "train: loss: 996117.5 acc: 0.8520117998123169  val: loss: 739191.0 acc: 0.7564753293991089\n",
      "step: 21070\n",
      "train: loss: 1331125.0 acc: 0.8691951036453247  val: loss: 1571323.5 acc: 0.5718299150466919\n",
      "step: 21075\n",
      "train: loss: 630435.3125 acc: 0.8905149698257446  val: loss: 779319.25 acc: 0.8623395562171936\n",
      "step: 21080\n",
      "train: loss: 259679.0625 acc: 0.9515615105628967  val: loss: 1917354.25 acc: 0.702853798866272\n",
      "step: 21085\n",
      "train: loss: 301481.1875 acc: 0.9489243626594543  val: loss: 536596.5625 acc: 0.86322021484375\n",
      "step: 21090\n",
      "train: loss: 210468.375 acc: 0.9794460535049438  val: loss: 500982.90625 acc: 0.8704729080200195\n",
      "step: 21095\n",
      "train: loss: 171372.046875 acc: 0.9867452383041382  val: loss: 1766008.25 acc: 0.3168179988861084\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 21100\n",
      "train: loss: 164373.578125 acc: 0.9807732105255127  val: loss: 349627.46875 acc: 0.8829417824745178\n",
      "step: 21105\n",
      "train: loss: 77658.3984375 acc: 0.9882920384407043  val: loss: 359078.9375 acc: 0.8322563767433167\n",
      "step: 21110\n",
      "train: loss: 48456.90625 acc: 0.9892616271972656  val: loss: 969153.8125 acc: 0.3902285695075989\n",
      "step: 21115\n",
      "train: loss: 8858.8056640625 acc: 0.9951391816139221  val: loss: 444791.0 acc: 0.8334675431251526\n",
      "step: 21120\n",
      "train: loss: 8145.57763671875 acc: 0.9939177632331848  val: loss: 940785.625 acc: 0.7812451124191284\n",
      "step: 21125\n",
      "train: loss: 31797.76953125 acc: 0.9441431760787964  val: loss: 175787.0625 acc: 0.9444620013237\n",
      "step: 21130\n",
      "train: loss: 28875.16796875 acc: 0.940928041934967  val: loss: 748061.75 acc: 0.7883763313293457\n",
      "step: 21135\n",
      "train: loss: 15241.248046875 acc: 0.9689985513687134  val: loss: 1019952.0 acc: 0.7883966565132141\n",
      "step: 21140\n",
      "train: loss: 18518.875 acc: 0.9706186652183533  val: loss: 1287017.5 acc: 0.6272907257080078\n",
      "step: 21145\n",
      "train: loss: 7947.1982421875 acc: 0.9806000590324402  val: loss: 184446.78125 acc: 0.927453875541687\n",
      "step: 21150\n",
      "train: loss: 31945.55859375 acc: 0.9320257902145386  val: loss: 418066.09375 acc: 0.9263087511062622\n",
      "step: 21155\n",
      "train: loss: 33499.5078125 acc: 0.9577076435089111  val: loss: 713231.0625 acc: 0.9319403767585754\n",
      "step: 21160\n",
      "train: loss: 39152.9296875 acc: 0.9791313409805298  val: loss: 377325.0625 acc: 0.9169163703918457\n",
      "step: 21165\n",
      "train: loss: 48601.14453125 acc: 0.9761833548545837  val: loss: 1173033.625 acc: 0.841999351978302\n",
      "step: 21170\n",
      "train: loss: 22728.1015625 acc: 0.9853048324584961  val: loss: 1447292.625 acc: 0.13584822416305542\n",
      "step: 21175\n",
      "train: loss: 14755.341796875 acc: 0.9911002516746521  val: loss: 1542384.125 acc: 0.8076073527336121\n",
      "step: 21180\n",
      "train: loss: 7307.59326171875 acc: 0.9919753670692444  val: loss: 893596.3125 acc: 0.8488917350769043\n",
      "step: 21185\n",
      "train: loss: 10894.1923828125 acc: 0.9846394658088684  val: loss: 1138678.625 acc: 0.7738255858421326\n",
      "step: 21190\n",
      "train: loss: 90518.0859375 acc: 0.8992393016815186  val: loss: 2147817.75 acc: 0.74788498878479\n",
      "step: 21195\n",
      "train: loss: 54496.9296875 acc: 0.9700559377670288  val: loss: 1155191.5 acc: 0.890713095664978\n",
      "step: 21200\n",
      "train: loss: 40038.9375 acc: 0.986813485622406  val: loss: 1295882.0 acc: 0.4827672243118286\n",
      "step: 21205\n",
      "train: loss: 14248.9140625 acc: 0.9951322674751282  val: loss: 148981.15625 acc: 0.9661209583282471\n",
      "step: 21210\n",
      "train: loss: 10988.88671875 acc: 0.9952619671821594  val: loss: 1738922.875 acc: 0.6002756357192993\n",
      "step: 21215\n",
      "train: loss: 222778.0625 acc: 0.9105064272880554  val: loss: 1275494.5 acc: 0.732667863368988\n",
      "step: 21220\n",
      "train: loss: 71127.7578125 acc: 0.9781689643859863  val: loss: 3198327.75 acc: 0.6193494200706482\n",
      "step: 21225\n",
      "train: loss: 55597.38671875 acc: 0.9707950353622437  val: loss: 1443353.625 acc: 0.8687393069267273\n",
      "step: 21230\n",
      "train: loss: 292549.6875 acc: 0.881752073764801  val: loss: 1190754.375 acc: 0.6747069954872131\n",
      "step: 21235\n",
      "train: loss: 129718.921875 acc: 0.9712948203086853  val: loss: 3403025.75 acc: 0.6371561288833618\n",
      "step: 21240\n",
      "train: loss: 781342.4375 acc: 0.9162791967391968  val: loss: 335451.8125 acc: 0.9536359906196594\n",
      "step: 21245\n",
      "train: loss: 125695.2734375 acc: 0.981035590171814  val: loss: 1117745.5 acc: 0.8743722438812256\n",
      "step: 21250\n",
      "train: loss: 59444.20703125 acc: 0.9779782891273499  val: loss: 882397.75 acc: 0.8041298389434814\n",
      "step: 21255\n",
      "train: loss: 98324.375 acc: 0.9917952418327332  val: loss: 438466.4375 acc: 0.8769124746322632\n",
      "step: 21260\n",
      "train: loss: 254487.78125 acc: 0.9788633584976196  val: loss: 1153327.125 acc: 0.8396686315536499\n",
      "step: 21265\n",
      "train: loss: 1716467.0 acc: 0.933038592338562  val: loss: 1301713.5 acc: 0.4556318521499634\n",
      "step: 21270\n",
      "train: loss: 96271.734375 acc: 0.9530894160270691  val: loss: 408653.5625 acc: 0.9260232448577881\n",
      "step: 21275\n",
      "train: loss: 461349.3125 acc: 0.9715784192085266  val: loss: 250485.015625 acc: 0.9306970834732056\n",
      "step: 21280\n",
      "train: loss: 738522.0 acc: 0.9531241059303284  val: loss: 1212832.25 acc: 0.43868565559387207\n",
      "step: 21285\n",
      "train: loss: 1450352.625 acc: 0.9648401737213135  val: loss: 882776.125 acc: 0.8269146680831909\n",
      "step: 21290\n",
      "train: loss: 1201609.375 acc: 0.91147780418396  val: loss: 1085834.25 acc: 0.5744584798812866\n",
      "step: 21295\n",
      "train: loss: 2017549.5 acc: 0.8495166301727295  val: loss: 1403451.75 acc: 0.762478232383728\n",
      "step: 21300\n",
      "train: loss: 502015.65625 acc: 0.9357587695121765  val: loss: 380519.9375 acc: 0.9565942883491516\n",
      "step: 21305\n",
      "train: loss: 228301.671875 acc: 0.9466316103935242  val: loss: 1265230.0 acc: 0.14257079362869263\n",
      "step: 21310\n",
      "train: loss: 1716919.5 acc: 0.5203828811645508  val: loss: 2205580.0 acc: 0.6818754076957703\n",
      "step: 21315\n",
      "train: loss: 1497272.125 acc: 0.8057527542114258  val: loss: 691073.0 acc: 0.812598705291748\n",
      "step: 21320\n",
      "train: loss: 615439.75 acc: 0.770370364189148  val: loss: 642410.75 acc: 0.859609842300415\n",
      "step: 21325\n",
      "train: loss: 602806.0625 acc: 0.7380645871162415  val: loss: 1708820.875 acc: 0.7884985208511353\n",
      "step: 21330\n",
      "train: loss: 620089.0 acc: 0.7740523219108582  val: loss: 610093.0625 acc: 0.8226264119148254\n",
      "step: 21335\n",
      "train: loss: 989026.0 acc: 0.667383074760437  val: loss: 489251.5625 acc: 0.8397575616836548\n",
      "step: 21340\n",
      "train: loss: 526821.1875 acc: 0.7131997346878052  val: loss: 1071992.0 acc: 0.716214656829834\n",
      "step: 21345\n",
      "train: loss: 885489.125 acc: 0.6780486106872559  val: loss: 1621774.625 acc: 0.6854492425918579\n",
      "step: 21350\n",
      "train: loss: 404295.5625 acc: 0.7904855012893677  val: loss: 1117109.25 acc: 0.7004896402359009\n",
      "step: 21355\n",
      "train: loss: 136165.5625 acc: 0.8908554911613464  val: loss: 4173577.25 acc: 0.625846803188324\n",
      "step: 21360\n",
      "train: loss: 58433.765625 acc: 0.952037513256073  val: loss: 1177113.375 acc: 0.7177074551582336\n",
      "step: 21365\n",
      "train: loss: 33926.7734375 acc: 0.9732216596603394  val: loss: 1445064.5 acc: 0.6489013433456421\n",
      "step: 21370\n",
      "train: loss: 196149.046875 acc: 0.8528820276260376  val: loss: 1056957.5 acc: 0.6748440861701965\n",
      "step: 21375\n",
      "train: loss: 211864.65625 acc: 0.8694015741348267  val: loss: 2411964.75 acc: 0.6205986142158508\n",
      "step: 21380\n",
      "train: loss: 266907.6875 acc: 0.8338578343391418  val: loss: 1413931.875 acc: 0.6790392398834229\n",
      "step: 21385\n",
      "train: loss: 145190.046875 acc: 0.8853074908256531  val: loss: 494739.0 acc: 0.7167384624481201\n",
      "step: 21390\n",
      "train: loss: 187737.375 acc: 0.8674315810203552  val: loss: 2418803.0 acc: 0.6613901257514954\n",
      "step: 21395\n",
      "train: loss: 183010.53125 acc: 0.8851077556610107  val: loss: 3061226.5 acc: 0.6155707836151123\n",
      "step: 21400\n",
      "train: loss: 501478.46875 acc: 0.7757716774940491  val: loss: 942333.625 acc: 0.6815028190612793\n",
      "step: 21405\n",
      "train: loss: 136102.546875 acc: 0.8552665114402771  val: loss: 1296486.625 acc: 0.6392979621887207\n",
      "step: 21410\n",
      "train: loss: 228368.875 acc: 0.852135181427002  val: loss: 1164818.125 acc: 0.6637197732925415\n",
      "step: 21415\n",
      "train: loss: 615304.0 acc: 0.7641898393630981  val: loss: 955549.0 acc: 0.732588529586792\n",
      "step: 21420\n",
      "train: loss: 622166.125 acc: 0.7783511877059937  val: loss: 1792132.5 acc: 0.7057058811187744\n",
      "step: 21425\n",
      "train: loss: 1504235.375 acc: 0.8420150876045227  val: loss: 1144688.25 acc: 0.7130252122879028\n",
      "step: 21430\n",
      "train: loss: 1098806.25 acc: 0.8556431531906128  val: loss: 617941.3125 acc: 0.79216068983078\n",
      "step: 21435\n",
      "train: loss: 335993.75 acc: 0.9712944030761719  val: loss: 716680.125 acc: 0.8029234409332275\n",
      "step: 21440\n",
      "train: loss: 599737.875 acc: 0.9447813034057617  val: loss: 550024.3125 acc: 0.7805408239364624\n",
      "step: 21445\n",
      "train: loss: 261196.15625 acc: 0.9533814787864685  val: loss: 101107.015625 acc: 0.9827923774719238\n",
      "step: 21450\n",
      "train: loss: 143382.296875 acc: 0.9800501465797424  val: loss: 429242.46875 acc: 0.8914414644241333\n",
      "step: 21455\n",
      "train: loss: 60764.0390625 acc: 0.9950105547904968  val: loss: 884413.8125 acc: 0.754279613494873\n",
      "step: 21460\n",
      "train: loss: 73418.84375 acc: 0.9939250349998474  val: loss: 1324855.25 acc: 0.6050673723220825\n",
      "step: 21465\n",
      "train: loss: 68239.375 acc: 0.9920757412910461  val: loss: 819366.5 acc: 0.9580376148223877\n",
      "step: 21470\n",
      "train: loss: 48257.109375 acc: 0.9953804016113281  val: loss: 937194.9375 acc: 0.8932135105133057\n",
      "step: 21475\n",
      "train: loss: 15838.044921875 acc: 0.9865984320640564  val: loss: 911603.25 acc: 0.8094030022621155\n",
      "step: 21480\n",
      "train: loss: 9586.0224609375 acc: 0.9963784217834473  val: loss: 776157.875 acc: 0.8670474290847778\n",
      "step: 21485\n",
      "train: loss: 16960.62890625 acc: 0.9905200004577637  val: loss: 213119.359375 acc: 0.9551165103912354\n",
      "step: 21490\n",
      "train: loss: 7293.45068359375 acc: 0.9974303841590881  val: loss: 1380721.25 acc: 0.8790773153305054\n",
      "step: 21495\n",
      "train: loss: 21708.1015625 acc: 0.9910070896148682  val: loss: 990850.5 acc: 0.5774459838867188\n",
      "step: 21500\n",
      "train: loss: 44105.9453125 acc: 0.9423500895500183  val: loss: 669021.25 acc: 0.9365020394325256\n",
      "step: 21505\n",
      "train: loss: 26709.44921875 acc: 0.9537701606750488  val: loss: 190346.8125 acc: 0.9641215205192566\n",
      "step: 21510\n",
      "train: loss: 12365.7431640625 acc: 0.96934974193573  val: loss: 950688.625 acc: 0.8784804344177246\n",
      "step: 21515\n",
      "train: loss: 6381.3095703125 acc: 0.9832603931427002  val: loss: 2221262.5 acc: 0.8660845756530762\n",
      "step: 21520\n",
      "train: loss: 11841.5595703125 acc: 0.9825764894485474  val: loss: 1887698.5 acc: 0.7097747325897217\n",
      "step: 21525\n",
      "train: loss: 12028.7919921875 acc: 0.9911865592002869  val: loss: 1579869.75 acc: 0.6634194850921631\n",
      "step: 21530\n",
      "train: loss: 21430.005859375 acc: 0.9875824451446533  val: loss: 1281438.875 acc: 0.5920016765594482\n",
      "step: 21535\n",
      "train: loss: 23908.67578125 acc: 0.9876607656478882  val: loss: 219834.671875 acc: 0.9299185276031494\n",
      "step: 21540\n",
      "train: loss: 33115.44921875 acc: 0.9889583587646484  val: loss: 2329670.25 acc: 0.621602475643158\n",
      "step: 21545\n",
      "train: loss: 17148.31640625 acc: 0.9860903024673462  val: loss: 1200057.75 acc: 0.8623056411743164\n",
      "step: 21550\n",
      "train: loss: 17100.916015625 acc: 0.9709225296974182  val: loss: 1970254.625 acc: 0.7627446055412292\n",
      "step: 21555\n",
      "train: loss: 20649.509765625 acc: 0.9904016256332397  val: loss: 2278642.0 acc: 0.44057703018188477\n",
      "step: 21560\n",
      "train: loss: 98533.6328125 acc: 0.9597873091697693  val: loss: 1317490.5 acc: 0.6685073375701904\n",
      "step: 21565\n",
      "train: loss: 40082.1953125 acc: 0.9928527474403381  val: loss: 896801.3125 acc: 0.37116628885269165\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 21570\n",
      "train: loss: 62480.25390625 acc: 0.9792141318321228  val: loss: 1741737.5 acc: 0.6872756481170654\n",
      "step: 21575\n",
      "train: loss: 21707.3828125 acc: 0.9831060171127319  val: loss: 830062.125 acc: 0.7526710033416748\n",
      "step: 21580\n",
      "train: loss: 41287.03515625 acc: 0.9815104007720947  val: loss: 1049311.5 acc: 0.9036397933959961\n",
      "step: 21585\n",
      "train: loss: 88029.171875 acc: 0.9741607904434204  val: loss: 1711423.0 acc: 0.30933690071105957\n",
      "step: 21590\n",
      "train: loss: 56349.5546875 acc: 0.9779008030891418  val: loss: 2420969.5 acc: 0.2001470923423767\n",
      "step: 21595\n",
      "train: loss: 160449.359375 acc: 0.9682325720787048  val: loss: 1979105.25 acc: -0.6566601991653442\n",
      "step: 21600\n",
      "train: loss: 105576.4921875 acc: 0.9724242091178894  val: loss: 2796743.5 acc: 0.6425160765647888\n",
      "step: 21605\n",
      "train: loss: 189965.421875 acc: 0.9815481901168823  val: loss: 1633133.5 acc: 0.6956347823143005\n",
      "step: 21610\n",
      "train: loss: 72291.1875 acc: 0.9919269680976868  val: loss: 1590246.625 acc: 0.7095427513122559\n",
      "step: 21615\n",
      "train: loss: 299151.625 acc: 0.9621747732162476  val: loss: 1829164.25 acc: 0.43951791524887085\n",
      "step: 21620\n",
      "train: loss: 160984.21875 acc: 0.983880877494812  val: loss: 1765036.375 acc: 0.6497540473937988\n",
      "step: 21625\n",
      "train: loss: 330239.09375 acc: 0.9796593189239502  val: loss: 456019.03125 acc: 0.9118439555168152\n",
      "step: 21630\n",
      "train: loss: 757610.25 acc: 0.9751409888267517  val: loss: 3316983.5 acc: 0.06651228666305542\n",
      "step: 21635\n",
      "train: loss: 230925.125 acc: 0.9668501615524292  val: loss: 800672.25 acc: 0.8227329850196838\n",
      "step: 21640\n",
      "train: loss: 1129598.75 acc: 0.9376661777496338  val: loss: 726049.625 acc: 0.9027194976806641\n",
      "step: 21645\n",
      "train: loss: 1118075.0 acc: 0.9602600336074829  val: loss: 776891.125 acc: 0.8940160870552063\n",
      "step: 21650\n",
      "train: loss: 1139970.25 acc: 0.9244937300682068  val: loss: 868561.1875 acc: 0.6886938810348511\n",
      "step: 21655\n",
      "train: loss: 434746.8125 acc: 0.9743289351463318  val: loss: 1776084.875 acc: 0.3625766634941101\n",
      "step: 21660\n",
      "train: loss: 468553.5 acc: 0.9685139656066895  val: loss: 185800.0 acc: 0.9517316818237305\n",
      "step: 21665\n",
      "train: loss: 557302.0 acc: 0.9652824997901917  val: loss: 806987.125 acc: 0.8549813628196716\n",
      "step: 21670\n",
      "train: loss: 273505.625 acc: 0.9197293519973755  val: loss: 232152.78125 acc: 0.9431955814361572\n",
      "step: 21675\n",
      "train: loss: 1269983.375 acc: 0.25792282819747925  val: loss: 738329.6875 acc: 0.8486018180847168\n",
      "step: 21680\n",
      "train: loss: 637793.125 acc: 0.8272593021392822  val: loss: 1024118.9375 acc: 0.6708296537399292\n",
      "step: 21685\n",
      "train: loss: 205618.53125 acc: 0.8073745965957642  val: loss: 674631.1875 acc: 0.7685448527336121\n",
      "step: 21690\n",
      "train: loss: 515834.875 acc: 0.8214828968048096  val: loss: 411859.15625 acc: 0.8380354642868042\n",
      "step: 21695\n",
      "train: loss: 577657.5 acc: 0.8312111496925354  val: loss: 1083921.5 acc: 0.8069638013839722\n",
      "step: 21700\n",
      "train: loss: 1027258.3125 acc: 0.7386293411254883  val: loss: 1344393.5 acc: 0.832456648349762\n",
      "step: 21705\n",
      "train: loss: 319248.71875 acc: 0.7478676438331604  val: loss: 416872.125 acc: 0.7553413510322571\n",
      "step: 21710\n",
      "train: loss: 177756.5625 acc: 0.8388726711273193  val: loss: 1107995.25 acc: 0.6081891059875488\n",
      "step: 21715\n",
      "train: loss: 508306.59375 acc: 0.7615429759025574  val: loss: 933501.6875 acc: 0.6965771913528442\n",
      "step: 21720\n",
      "train: loss: 34050.67578125 acc: 0.9713450074195862  val: loss: 1492837.875 acc: 0.6519634127616882\n",
      "step: 21725\n",
      "train: loss: 77698.3515625 acc: 0.9433963298797607  val: loss: 1934581.0 acc: 0.6111948490142822\n",
      "step: 21730\n",
      "train: loss: 73707.8828125 acc: 0.9390425086021423  val: loss: 351654.1875 acc: 0.8227582573890686\n",
      "step: 21735\n",
      "train: loss: 165321.96875 acc: 0.9001694917678833  val: loss: 294856.9375 acc: 0.7492563724517822\n",
      "step: 21740\n",
      "train: loss: 260228.296875 acc: 0.8743102550506592  val: loss: 525355.6875 acc: 0.775600790977478\n",
      "step: 21745\n",
      "train: loss: 58642.3046875 acc: 0.9598509669303894  val: loss: 1004338.3125 acc: 0.6726226806640625\n",
      "step: 21750\n",
      "train: loss: 609581.25 acc: 0.7548061609268188  val: loss: 678720.0625 acc: 0.6853466629981995\n",
      "step: 21755\n",
      "train: loss: 52477.83203125 acc: 0.9375708103179932  val: loss: 1025845.25 acc: 0.7044291496276855\n",
      "step: 21760\n",
      "train: loss: 215736.671875 acc: 0.8474898338317871  val: loss: 1662062.0 acc: 0.6933853626251221\n",
      "step: 21765\n",
      "train: loss: 223824.796875 acc: 0.8196076154708862  val: loss: 613554.1875 acc: 0.7146241664886475\n",
      "step: 21770\n",
      "train: loss: 181229.796875 acc: 0.8261769413948059  val: loss: 1013396.8125 acc: 0.6581904888153076\n",
      "step: 21775\n",
      "train: loss: 672357.125 acc: 0.6395949125289917  val: loss: 2813085.0 acc: 0.6655241847038269\n",
      "step: 21780\n",
      "train: loss: 716059.375 acc: 0.7073286771774292  val: loss: 2996254.5 acc: 0.6255804300308228\n",
      "step: 21785\n",
      "train: loss: 174660.171875 acc: 0.8824234008789062  val: loss: 2418716.25 acc: 0.6496346592903137\n",
      "step: 21790\n",
      "train: loss: 2597717.75 acc: 0.727000892162323  val: loss: 2400182.5 acc: 0.7647562623023987\n",
      "step: 21795\n",
      "train: loss: 1734269.375 acc: 0.7809755802154541  val: loss: 1452576.75 acc: 0.8139444589614868\n",
      "step: 21800\n",
      "train: loss: 591828.1875 acc: 0.9426986575126648  val: loss: 904892.125 acc: 0.5787363648414612\n",
      "step: 21805\n",
      "train: loss: 320145.09375 acc: 0.9720776081085205  val: loss: 1174640.625 acc: 0.8566657304763794\n",
      "step: 21810\n",
      "train: loss: 230254.640625 acc: 0.9714431762695312  val: loss: 1385097.5 acc: 0.8649183511734009\n",
      "step: 21815\n",
      "train: loss: 224186.859375 acc: 0.9646888375282288  val: loss: 249269.46875 acc: 0.9358909130096436\n",
      "step: 21820\n",
      "train: loss: 105538.3984375 acc: 0.9929361939430237  val: loss: 1260271.25 acc: 0.8627292513847351\n",
      "step: 21825\n",
      "train: loss: 56636.9609375 acc: 0.9955151081085205  val: loss: 1273492.875 acc: 0.7572125792503357\n",
      "step: 21830\n",
      "train: loss: 71978.4453125 acc: 0.9922810792922974  val: loss: 1887094.625 acc: 0.5720415115356445\n",
      "step: 21835\n",
      "train: loss: 49153.05078125 acc: 0.9918435215950012  val: loss: 609863.3125 acc: 0.9096212983131409\n",
      "step: 21840\n",
      "train: loss: 22089.21484375 acc: 0.9946677088737488  val: loss: 1239380.75 acc: 0.8496309518814087\n",
      "step: 21845\n",
      "train: loss: 25595.462890625 acc: 0.9782817363739014  val: loss: 731808.75 acc: 0.6546733379364014\n",
      "step: 21850\n",
      "train: loss: 16288.4580078125 acc: 0.9869488477706909  val: loss: 419607.28125 acc: 0.9582213163375854\n",
      "step: 21855\n",
      "train: loss: 9874.0966796875 acc: 0.9768979549407959  val: loss: 464303.03125 acc: 0.8063851594924927\n",
      "step: 21860\n",
      "train: loss: 16301.3486328125 acc: 0.9754058122634888  val: loss: 1247849.25 acc: 0.5477980375289917\n",
      "step: 21865\n",
      "train: loss: 11412.564453125 acc: 0.9642548561096191  val: loss: 1054501.5 acc: 0.8445783853530884\n",
      "step: 21870\n",
      "train: loss: 11122.6171875 acc: 0.9829447269439697  val: loss: 1327683.75 acc: 0.5829570293426514\n",
      "step: 21875\n",
      "train: loss: 8881.9521484375 acc: 0.986427903175354  val: loss: 1453065.0 acc: 0.7265356779098511\n",
      "step: 21880\n",
      "train: loss: 7920.697265625 acc: 0.955639123916626  val: loss: 1111137.5 acc: 0.8541672229766846\n",
      "step: 21885\n",
      "train: loss: 27957.693359375 acc: 0.9706364274024963  val: loss: 2959726.75 acc: -0.8385530710220337\n",
      "step: 21890\n",
      "train: loss: 6600.99365234375 acc: 0.9910727739334106  val: loss: 1436019.0 acc: 0.8496599197387695\n",
      "step: 21895\n",
      "train: loss: 18071.552734375 acc: 0.985737681388855  val: loss: 2523697.0 acc: 0.4237101674079895\n",
      "step: 21900\n",
      "train: loss: 22606.453125 acc: 0.9881153702735901  val: loss: 3656771.0 acc: -0.336861252784729\n",
      "step: 21905\n",
      "train: loss: 105526.9609375 acc: 0.9340042471885681  val: loss: 4032076.0 acc: 0.6118330955505371\n",
      "step: 21910\n",
      "train: loss: 24393.32421875 acc: 0.9818168878555298  val: loss: 2036557.875 acc: 0.5852450132369995\n",
      "step: 21915\n",
      "train: loss: 9276.150390625 acc: 0.9624996185302734  val: loss: 2157721.0 acc: 0.6963294744491577\n",
      "step: 21920\n",
      "train: loss: 7513.04833984375 acc: 0.991840124130249  val: loss: 1558815.75 acc: 0.7575603723526001\n",
      "step: 21925\n",
      "train: loss: 46584.828125 acc: 0.9815965294837952  val: loss: 3780894.5 acc: -0.1458737850189209\n",
      "step: 21930\n",
      "train: loss: 25787.703125 acc: 0.9916985630989075  val: loss: 2815874.0 acc: 0.26112955808639526\n",
      "step: 21935\n",
      "train: loss: 44306.859375 acc: 0.9864276051521301  val: loss: 253813.796875 acc: 0.8144361972808838\n",
      "step: 21940\n",
      "train: loss: 60021.84375 acc: 0.9776628017425537  val: loss: 746106.4375 acc: 0.862210750579834\n",
      "step: 21945\n",
      "train: loss: 20727.873046875 acc: 0.9898019433021545  val: loss: 1842684.875 acc: 0.737033486366272\n",
      "step: 21950\n",
      "train: loss: 112619.15625 acc: 0.9772844910621643  val: loss: 2449523.75 acc: 0.39479732513427734\n",
      "step: 21955\n",
      "train: loss: 106873.5546875 acc: 0.9585895538330078  val: loss: 1523067.5 acc: 0.5488659143447876\n",
      "step: 21960\n",
      "train: loss: 56508.19921875 acc: 0.9699293375015259  val: loss: 1580421.5 acc: 0.8784037232398987\n",
      "step: 21965\n",
      "train: loss: 166276.734375 acc: 0.9739089012145996  val: loss: 420805.375 acc: 0.8896756768226624\n",
      "step: 21970\n",
      "train: loss: 578459.9375 acc: 0.9452013373374939  val: loss: 648958.375 acc: 0.8220882415771484\n",
      "step: 21975\n",
      "train: loss: 45782.82421875 acc: 0.9932452440261841  val: loss: 572371.875 acc: 0.9410115480422974\n",
      "step: 21980\n",
      "train: loss: 37010.953125 acc: 0.993047297000885  val: loss: 1696646.25 acc: 0.7052931785583496\n",
      "step: 21985\n",
      "train: loss: 330375.1875 acc: 0.970268964767456  val: loss: 1668061.375 acc: 0.3593566417694092\n",
      "step: 21990\n",
      "train: loss: 362839.03125 acc: 0.9788822531700134  val: loss: 3383454.0 acc: 0.16075259447097778\n",
      "step: 21995\n",
      "train: loss: 335642.84375 acc: 0.9744188189506531  val: loss: 926587.5 acc: 0.6965056657791138\n",
      "step: 22000\n",
      "train: loss: 319418.9375 acc: 0.9826194047927856  val: loss: 1101348.75 acc: 0.6611873507499695\n",
      "step: 22005\n",
      "train: loss: 688378.375 acc: 0.9369635581970215  val: loss: 374244.6875 acc: 0.8769744038581848\n",
      "step: 22010\n",
      "train: loss: 1333547.875 acc: 0.9532184600830078  val: loss: 259393.640625 acc: 0.8167437314987183\n",
      "step: 22015\n",
      "train: loss: 966873.375 acc: 0.9480357766151428  val: loss: 804332.4375 acc: 0.30349087715148926\n",
      "step: 22020\n",
      "train: loss: 997752.0 acc: 0.9539511799812317  val: loss: 695944.125 acc: 0.924416720867157\n",
      "step: 22025\n",
      "train: loss: 486326.125 acc: 0.9684845805168152  val: loss: 189529.5625 acc: 0.9069443941116333\n",
      "step: 22030\n",
      "train: loss: 501917.71875 acc: 0.9494743943214417  val: loss: 137206.71875 acc: 0.9227503538131714\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 22035\n",
      "train: loss: 637351.5 acc: 0.939462423324585  val: loss: 796620.125 acc: 0.6013141870498657\n",
      "step: 22040\n",
      "train: loss: 228859.8125 acc: 0.9513403177261353  val: loss: 525119.5 acc: 0.7469069957733154\n",
      "step: 22045\n",
      "train: loss: 1278905.5 acc: 0.05491822957992554  val: loss: 806589.0 acc: 0.8609558343887329\n",
      "step: 22050\n",
      "train: loss: 670134.4375 acc: 0.7236244678497314  val: loss: 1299154.5 acc: 0.8111816048622131\n",
      "step: 22055\n",
      "train: loss: 573817.875 acc: 0.8306788802146912  val: loss: 1470933.25 acc: 0.766645073890686\n",
      "step: 22060\n",
      "train: loss: 865227.625 acc: 0.7162249088287354  val: loss: 682276.0 acc: 0.8341752886772156\n",
      "step: 22065\n",
      "train: loss: 583375.875 acc: 0.7400256395339966  val: loss: 1610688.0 acc: 0.7832272052764893\n",
      "step: 22070\n",
      "train: loss: 812894.875 acc: 0.7032155990600586  val: loss: 1045348.9375 acc: 0.6719697713851929\n",
      "step: 22075\n",
      "train: loss: 521231.03125 acc: 0.7433647513389587  val: loss: 1031768.0 acc: 0.6662892699241638\n",
      "step: 22080\n",
      "train: loss: 268907.9375 acc: 0.8217370510101318  val: loss: 1270757.25 acc: 0.7314159870147705\n",
      "step: 22085\n",
      "train: loss: 88252.328125 acc: 0.9144456386566162  val: loss: 1169359.25 acc: 0.7411925792694092\n",
      "step: 22090\n",
      "train: loss: 89027.25 acc: 0.9218913316726685  val: loss: 4902281.5 acc: 0.602939248085022\n",
      "step: 22095\n",
      "train: loss: 39809.2578125 acc: 0.9659234881401062  val: loss: 2186540.5 acc: 0.6111046075820923\n",
      "step: 22100\n",
      "train: loss: 217547.21875 acc: 0.8252279758453369  val: loss: 2122064.75 acc: 0.6678234338760376\n",
      "step: 22105\n",
      "train: loss: 424286.96875 acc: 0.8003937602043152  val: loss: 1086809.625 acc: 0.6976426839828491\n",
      "step: 22110\n",
      "train: loss: 408576.5 acc: 0.7978433966636658  val: loss: 3570487.0 acc: 0.5790407657623291\n",
      "step: 22115\n",
      "train: loss: 198523.3125 acc: 0.8477594256401062  val: loss: 771756.5 acc: 0.7280812859535217\n",
      "step: 22120\n",
      "train: loss: 312734.5 acc: 0.7547401189804077  val: loss: 4155447.25 acc: 0.5743488073348999\n",
      "step: 22125\n",
      "train: loss: 209310.75 acc: 0.7134238481521606  val: loss: 3642968.5 acc: 0.604154109954834\n",
      "step: 22130\n",
      "train: loss: 361119.75 acc: 0.7975805997848511  val: loss: 2558542.0 acc: 0.6003224849700928\n",
      "step: 22135\n",
      "train: loss: 135927.96875 acc: 0.8983694911003113  val: loss: 4732811.0 acc: 0.6164119839668274\n",
      "step: 22140\n",
      "train: loss: 336055.625 acc: 0.827247142791748  val: loss: 3066140.25 acc: 0.5784288644790649\n",
      "step: 22145\n",
      "train: loss: 90689.40625 acc: 0.9156443476676941  val: loss: 5943346.0 acc: 0.49757814407348633\n",
      "step: 22150\n",
      "train: loss: 250513.125 acc: 0.7974305748939514  val: loss: 2388023.0 acc: 0.6324307918548584\n",
      "step: 22155\n",
      "train: loss: 1060351.5 acc: 0.763296365737915  val: loss: 1894516.125 acc: 0.7179367542266846\n",
      "step: 22160\n",
      "train: loss: 921987.5625 acc: 0.7540833950042725  val: loss: 1015752.1875 acc: 0.7109221816062927\n",
      "step: 22165\n",
      "train: loss: 1317649.75 acc: 0.9136865735054016  val: loss: 1259678.25 acc: 0.7057448625564575\n",
      "step: 22170\n",
      "train: loss: 516978.75 acc: 0.9557058811187744  val: loss: 1495100.375 acc: 0.7464690208435059\n",
      "step: 22175\n",
      "train: loss: 417034.3125 acc: 0.9357032179832458  val: loss: 1575186.375 acc: 0.3981435298919678\n",
      "step: 22180\n",
      "train: loss: 127460.359375 acc: 0.9703914523124695  val: loss: 1129291.75 acc: 0.6298340559005737\n",
      "step: 22185\n",
      "train: loss: 131305.796875 acc: 0.9879112243652344  val: loss: 1439397.5 acc: 0.8667194247245789\n",
      "step: 22190\n",
      "train: loss: 81894.6484375 acc: 0.9944059252738953  val: loss: 1564102.25 acc: 0.6841837167739868\n",
      "step: 22195\n",
      "train: loss: 265910.0625 acc: 0.9731228947639465  val: loss: 1618379.75 acc: 0.5477705001831055\n",
      "step: 22200\n",
      "train: loss: 50183.23828125 acc: 0.995244026184082  val: loss: 670677.5625 acc: 0.7991000413894653\n",
      "step: 22205\n",
      "train: loss: 13611.900390625 acc: 0.9950673580169678  val: loss: 1648355.625 acc: 0.4842793345451355\n",
      "step: 22210\n",
      "train: loss: 8022.9638671875 acc: 0.9927098155021667  val: loss: 703202.5 acc: 0.6845758557319641\n",
      "step: 22215\n",
      "train: loss: 20820.15625 acc: 0.994327187538147  val: loss: 538255.0 acc: 0.8855547308921814\n",
      "step: 22220\n",
      "train: loss: 3080.7255859375 acc: 0.9952353239059448  val: loss: 894167.375 acc: 0.8723183274269104\n",
      "step: 22225\n",
      "train: loss: 13222.54296875 acc: 0.981002688407898  val: loss: 1188914.25 acc: 0.7627585530281067\n",
      "step: 22230\n",
      "train: loss: 27362.48046875 acc: 0.9788228869438171  val: loss: 707134.4375 acc: 0.8591500520706177\n",
      "step: 22235\n",
      "train: loss: 12305.5791015625 acc: 0.9895964860916138  val: loss: 2544661.5 acc: -0.38517773151397705\n",
      "step: 22240\n",
      "train: loss: 10225.2294921875 acc: 0.9723823666572571  val: loss: 987734.4375 acc: 0.41061097383499146\n",
      "step: 22245\n",
      "train: loss: 12300.3525390625 acc: 0.959321916103363  val: loss: 1474066.0 acc: 0.6500723361968994\n",
      "step: 22250\n",
      "train: loss: 15401.263671875 acc: 0.9699585437774658  val: loss: 1739003.0 acc: 0.7965620756149292\n",
      "step: 22255\n",
      "train: loss: 25654.482421875 acc: 0.9865758419036865  val: loss: 1295709.5 acc: 0.8392539024353027\n",
      "step: 22260\n",
      "train: loss: 26593.248046875 acc: 0.9886509776115417  val: loss: 2141181.75 acc: 0.3107508420944214\n",
      "step: 22265\n",
      "train: loss: 101529.3046875 acc: 0.9487118124961853  val: loss: 1915730.25 acc: 0.5823521018028259\n",
      "step: 22270\n",
      "train: loss: 11562.3134765625 acc: 0.9926047325134277  val: loss: 1251833.0 acc: 0.5798748731613159\n",
      "step: 22275\n",
      "train: loss: 15952.8720703125 acc: 0.9887335896492004  val: loss: 2046732.875 acc: 0.12499511241912842\n",
      "step: 22280\n",
      "train: loss: 6223.99609375 acc: 0.9903172850608826  val: loss: 1661526.75 acc: 0.7347315549850464\n",
      "step: 22285\n",
      "train: loss: 18560.1875 acc: 0.9887116551399231  val: loss: 1756798.0 acc: 0.3854972720146179\n",
      "step: 22290\n",
      "train: loss: 22811.775390625 acc: 0.9912275671958923  val: loss: 1855752.875 acc: 0.34364694356918335\n",
      "step: 22295\n",
      "train: loss: 35083.015625 acc: 0.9901350736618042  val: loss: 2341015.25 acc: 0.4513680934906006\n",
      "step: 22300\n",
      "train: loss: 27472.173828125 acc: 0.9933204054832458  val: loss: 1369378.125 acc: 0.7243746519088745\n",
      "step: 22305\n",
      "train: loss: 17892.05859375 acc: 0.9844123721122742  val: loss: 528742.6875 acc: 0.8990896344184875\n",
      "step: 22310\n",
      "train: loss: 23535.576171875 acc: 0.9893604516983032  val: loss: 1283513.0 acc: 0.6161211729049683\n",
      "step: 22315\n",
      "train: loss: 176549.046875 acc: 0.9092748165130615  val: loss: 287987.15625 acc: 0.9409523606300354\n",
      "step: 22320\n",
      "train: loss: 109312.84375 acc: 0.9715929627418518  val: loss: 695374.9375 acc: 0.4542052149772644\n",
      "step: 22325\n",
      "train: loss: 143310.234375 acc: 0.9297962784767151  val: loss: 429347.46875 acc: 0.9365012645721436\n",
      "step: 22330\n",
      "train: loss: 156039.421875 acc: 0.9766799807548523  val: loss: 1133252.375 acc: 0.6670068502426147\n",
      "step: 22335\n",
      "train: loss: 81172.3828125 acc: 0.9922800064086914  val: loss: 2292664.5 acc: 0.6498368978500366\n",
      "step: 22340\n",
      "train: loss: 161363.484375 acc: 0.9811052680015564  val: loss: 376545.90625 acc: 0.9219663739204407\n",
      "step: 22345\n",
      "train: loss: 106286.3359375 acc: 0.9729272127151489  val: loss: 1869629.25 acc: 0.26943331956863403\n",
      "step: 22350\n",
      "train: loss: 128746.5 acc: 0.9596813321113586  val: loss: 2156497.25 acc: 0.6841890811920166\n",
      "step: 22355\n",
      "train: loss: 518489.28125 acc: 0.9679231643676758  val: loss: 406733.03125 acc: 0.8185545802116394\n",
      "step: 22360\n",
      "train: loss: 239482.78125 acc: 0.9748260974884033  val: loss: 322262.8125 acc: 0.8456385731697083\n",
      "step: 22365\n",
      "train: loss: 274044.53125 acc: 0.9763250350952148  val: loss: 535561.9375 acc: 0.6782132387161255\n",
      "step: 22370\n",
      "train: loss: 2010737.75 acc: 0.9034831523895264  val: loss: 1617527.125 acc: 0.6624628305435181\n",
      "step: 22375\n",
      "train: loss: 1156880.125 acc: 0.9642578363418579  val: loss: 668003.1875 acc: 0.8616325259208679\n",
      "step: 22380\n",
      "train: loss: 1068881.5 acc: 0.9662333130836487  val: loss: 356652.53125 acc: 0.8451541066169739\n",
      "step: 22385\n",
      "train: loss: 763269.625 acc: 0.9606306552886963  val: loss: 320603.53125 acc: 0.9471669793128967\n",
      "step: 22390\n",
      "train: loss: 339266.09375 acc: 0.9705777168273926  val: loss: 2074557.5 acc: 0.724668025970459\n",
      "step: 22395\n",
      "train: loss: 193634.59375 acc: 0.9845282435417175  val: loss: 216647.765625 acc: 0.9315137267112732\n",
      "step: 22400\n",
      "train: loss: 264929.28125 acc: 0.9455680847167969  val: loss: 824274.125 acc: 0.9047141671180725\n",
      "step: 22405\n",
      "train: loss: 346699.90625 acc: 0.9515731334686279  val: loss: 243245.15625 acc: 0.9416681528091431\n",
      "step: 22410\n",
      "train: loss: 855405.3125 acc: 0.852243185043335  val: loss: 835566.125 acc: 0.9006653428077698\n",
      "step: 22415\n",
      "train: loss: 490701.8125 acc: 0.9290504455566406  val: loss: 327256.84375 acc: 0.8576582074165344\n",
      "step: 22420\n",
      "train: loss: 203287.109375 acc: 0.9124611020088196  val: loss: 747607.875 acc: 0.8913626074790955\n",
      "step: 22425\n",
      "train: loss: 416342.46875 acc: 0.767825722694397  val: loss: 1735071.375 acc: 0.8535681962966919\n",
      "step: 22430\n",
      "train: loss: 1008552.1875 acc: 0.5983082056045532  val: loss: 1805974.75 acc: 0.8172474503517151\n",
      "step: 22435\n",
      "train: loss: 593427.5 acc: 0.6875635385513306  val: loss: 1913252.5 acc: 0.6994408369064331\n",
      "step: 22440\n",
      "train: loss: 413888.34375 acc: 0.753411054611206  val: loss: 1346037.5 acc: 0.6657085418701172\n",
      "step: 22445\n",
      "train: loss: 160379.671875 acc: 0.8668262958526611  val: loss: 1695026.0 acc: 0.6471595764160156\n",
      "step: 22450\n",
      "train: loss: 199607.09375 acc: 0.8662043809890747  val: loss: 2760733.75 acc: 0.6932069659233093\n",
      "step: 22455\n",
      "train: loss: 94313.765625 acc: 0.9297897815704346  val: loss: 2454112.25 acc: 0.6513795256614685\n",
      "step: 22460\n",
      "train: loss: 56643.953125 acc: 0.9500551819801331  val: loss: 2054480.0 acc: 0.6501899361610413\n",
      "step: 22465\n",
      "train: loss: 99194.28125 acc: 0.9267442226409912  val: loss: 1027505.1875 acc: 0.7021784782409668\n",
      "step: 22470\n",
      "train: loss: 116626.234375 acc: 0.9023136496543884  val: loss: 4780328.0 acc: 0.5724847316741943\n",
      "step: 22475\n",
      "train: loss: 80858.9140625 acc: 0.9210836887359619  val: loss: 766308.0625 acc: 0.7422069311141968\n",
      "step: 22480\n",
      "train: loss: 34707.05078125 acc: 0.9638704061508179  val: loss: 4132028.75 acc: 0.6024942994117737\n",
      "step: 22485\n",
      "train: loss: 235757.25 acc: 0.7380291223526001  val: loss: 2351525.0 acc: 0.6343377828598022\n",
      "step: 22490\n",
      "train: loss: 229917.453125 acc: 0.8298450112342834  val: loss: 1420769.75 acc: 0.6392765045166016\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 22495\n",
      "train: loss: 606193.375 acc: 0.7880870699882507  val: loss: 300916.4375 acc: 0.7793267369270325\n",
      "step: 22500\n",
      "train: loss: 211667.140625 acc: 0.7799583077430725  val: loss: 3816579.0 acc: 0.6391817331314087\n",
      "step: 22505\n",
      "train: loss: 52805.23828125 acc: 0.9477335810661316  val: loss: 698055.3125 acc: 0.7060794830322266\n",
      "step: 22510\n",
      "train: loss: 892530.4375 acc: 0.6697217226028442  val: loss: 3808141.0 acc: 0.5443783402442932\n",
      "step: 22515\n",
      "train: loss: 229550.90625 acc: 0.8665900230407715  val: loss: 1527511.625 acc: 0.7062546610832214\n",
      "step: 22520\n",
      "train: loss: 2459855.0 acc: 0.6820769309997559  val: loss: 906989.5625 acc: 0.6873303651809692\n",
      "step: 22525\n",
      "train: loss: 879704.25 acc: 0.8281493782997131  val: loss: 1707725.125 acc: 0.7700601816177368\n",
      "step: 22530\n",
      "train: loss: 790127.5625 acc: 0.8914018273353577  val: loss: 1313924.625 acc: 0.8061100244522095\n",
      "step: 22535\n",
      "train: loss: 391639.4375 acc: 0.9566915035247803  val: loss: 1802945.125 acc: -0.3202645778656006\n",
      "step: 22540\n",
      "train: loss: 387324.375 acc: 0.9557830095291138  val: loss: 645739.3125 acc: 0.8559994697570801\n",
      "step: 22545\n",
      "train: loss: 150314.34375 acc: 0.9616410732269287  val: loss: 1382244.125 acc: 0.8286749720573425\n",
      "step: 22550\n",
      "train: loss: 294676.0625 acc: 0.9596186280250549  val: loss: 1721343.625 acc: 0.15114307403564453\n",
      "step: 22555\n",
      "train: loss: 240741.875 acc: 0.9797749519348145  val: loss: 872348.3125 acc: 0.6455715298652649\n",
      "step: 22560\n",
      "train: loss: 63199.171875 acc: 0.9956134557723999  val: loss: 2377419.0 acc: 0.5390793085098267\n",
      "step: 22565\n",
      "train: loss: 72068.2578125 acc: 0.9933118224143982  val: loss: 1474118.875 acc: 0.7961249947547913\n",
      "step: 22570\n",
      "train: loss: 32405.0390625 acc: 0.9881096482276917  val: loss: 883509.5625 acc: 0.8334731459617615\n",
      "step: 22575\n",
      "train: loss: 27732.630859375 acc: 0.9934954047203064  val: loss: 451844.34375 acc: 0.9147011637687683\n",
      "step: 22580\n",
      "train: loss: 3272.369140625 acc: 0.992640495300293  val: loss: 638431.1875 acc: 0.9175491333007812\n",
      "step: 22585\n",
      "train: loss: 7935.18603515625 acc: 0.9966907501220703  val: loss: 2191626.0 acc: 0.6582079529762268\n",
      "step: 22590\n",
      "train: loss: 10626.53515625 acc: 0.9934729337692261  val: loss: 1961365.125 acc: 0.3501628041267395\n",
      "step: 22595\n",
      "train: loss: 14146.09765625 acc: 0.9818102121353149  val: loss: 48967.3671875 acc: 0.9927659630775452\n",
      "step: 22600\n",
      "train: loss: 26761.875 acc: 0.9600803852081299  val: loss: 1644068.25 acc: 0.5485858917236328\n",
      "step: 22605\n",
      "train: loss: 7991.16455078125 acc: 0.994002103805542  val: loss: 453878.375 acc: 0.9029960036277771\n",
      "step: 22610\n",
      "train: loss: 10468.5810546875 acc: 0.97772216796875  val: loss: 964280.8125 acc: 0.8040735721588135\n",
      "step: 22615\n",
      "train: loss: 13111.0048828125 acc: 0.9801141023635864  val: loss: 926447.9375 acc: 0.9025183916091919\n",
      "step: 22620\n",
      "train: loss: 7884.90087890625 acc: 0.9824516773223877  val: loss: 78334.7109375 acc: 0.9796655774116516\n",
      "step: 22625\n",
      "train: loss: 30480.4765625 acc: 0.9771900177001953  val: loss: 45216.24609375 acc: 0.9895009994506836\n",
      "step: 22630\n",
      "train: loss: 32035.7578125 acc: 0.9899716377258301  val: loss: 771718.5 acc: 0.13203096389770508\n",
      "step: 22635\n",
      "train: loss: 15457.7939453125 acc: 0.9889420866966248  val: loss: 2098818.75 acc: 0.6163899302482605\n",
      "step: 22640\n",
      "train: loss: 17636.025390625 acc: 0.976818323135376  val: loss: 889470.125 acc: 0.861528754234314\n",
      "step: 22645\n",
      "train: loss: 10968.5947265625 acc: 0.9907136559486389  val: loss: 1346835.625 acc: 0.49378645420074463\n",
      "step: 22650\n",
      "train: loss: 12287.8125 acc: 0.9880176186561584  val: loss: 1493063.75 acc: 0.6380497217178345\n",
      "step: 22655\n",
      "train: loss: 19785.76171875 acc: 0.990935206413269  val: loss: 920284.5625 acc: 0.8886786699295044\n",
      "step: 22660\n",
      "train: loss: 31518.525390625 acc: 0.9899113774299622  val: loss: 602521.3125 acc: 0.8295698761940002\n",
      "step: 22665\n",
      "train: loss: 33331.4296875 acc: 0.9858150482177734  val: loss: 422463.625 acc: 0.8087010383605957\n",
      "step: 22670\n",
      "train: loss: 38405.61328125 acc: 0.9834609627723694  val: loss: 1222373.375 acc: 0.5269527435302734\n",
      "step: 22675\n",
      "train: loss: 6824.00390625 acc: 0.9972299933433533  val: loss: 1180155.625 acc: 0.8445348143577576\n",
      "step: 22680\n",
      "train: loss: 65031.17578125 acc: 0.9755359888076782  val: loss: 1895259.75 acc: 0.5185129642486572\n",
      "step: 22685\n",
      "train: loss: 104486.5 acc: 0.9637799859046936  val: loss: 1711024.25 acc: 0.7261557579040527\n",
      "step: 22690\n",
      "train: loss: 25075.8046875 acc: 0.9533321261405945  val: loss: 128248.234375 acc: 0.9720038175582886\n",
      "step: 22695\n",
      "train: loss: 237083.0 acc: 0.9677274823188782  val: loss: 262334.09375 acc: 0.9553821682929993\n",
      "step: 22700\n",
      "train: loss: 738678.875 acc: 0.8918436169624329  val: loss: 1043867.875 acc: 0.812404453754425\n",
      "step: 22705\n",
      "train: loss: 51677.40625 acc: 0.993293821811676  val: loss: 332434.875 acc: 0.7821664810180664\n",
      "step: 22710\n",
      "train: loss: 46072.984375 acc: 0.9926394820213318  val: loss: 323996.4375 acc: 0.9419494867324829\n",
      "step: 22715\n",
      "train: loss: 174485.421875 acc: 0.9829674959182739  val: loss: 441512.28125 acc: 0.733986496925354\n",
      "step: 22720\n",
      "train: loss: 98544.2265625 acc: 0.9894879460334778  val: loss: 747605.5625 acc: 0.8728638291358948\n",
      "step: 22725\n",
      "train: loss: 291254.53125 acc: 0.9764699339866638  val: loss: 2888943.5 acc: 0.3955550193786621\n",
      "step: 22730\n",
      "train: loss: 313205.71875 acc: 0.9592664837837219  val: loss: 1885841.0 acc: -0.2564185857772827\n",
      "step: 22735\n",
      "train: loss: 584196.9375 acc: 0.9287364482879639  val: loss: 722920.4375 acc: 0.9001220464706421\n",
      "step: 22740\n",
      "train: loss: 1112657.0 acc: 0.9604575037956238  val: loss: 969731.3125 acc: 0.8707963824272156\n",
      "step: 22745\n",
      "train: loss: 972268.5625 acc: 0.9694173336029053  val: loss: 521455.71875 acc: 0.9173715710639954\n",
      "step: 22750\n",
      "train: loss: 1791821.25 acc: 0.9196972846984863  val: loss: 719947.125 acc: 0.8702754378318787\n",
      "step: 22755\n",
      "train: loss: 748295.875 acc: 0.9466560482978821  val: loss: 194268.296875 acc: 0.8931901454925537\n",
      "step: 22760\n",
      "train: loss: 143846.890625 acc: 0.9735536575317383  val: loss: 224857.484375 acc: 0.9728080630302429\n",
      "step: 22765\n",
      "train: loss: 448988.40625 acc: 0.9352145791053772  val: loss: 920838.0625 acc: 0.868889331817627\n",
      "step: 22770\n",
      "train: loss: 488595.59375 acc: 0.9070512056350708  val: loss: 1066936.25 acc: 0.9204809069633484\n",
      "step: 22775\n",
      "train: loss: 2101428.5 acc: 0.4921618103981018  val: loss: 923914.6875 acc: 0.7867600321769714\n",
      "step: 22780\n",
      "train: loss: 990225.3125 acc: 0.7623008489608765  val: loss: 3095796.0 acc: 0.6963423490524292\n",
      "step: 22785\n",
      "train: loss: 476024.5 acc: 0.7998213171958923  val: loss: 2184536.25 acc: 0.730546236038208\n",
      "step: 22790\n",
      "train: loss: 729709.3125 acc: 0.7700924277305603  val: loss: 1662062.625 acc: 0.7953009009361267\n",
      "step: 22795\n",
      "train: loss: 704874.0625 acc: 0.7686920762062073  val: loss: 573104.4375 acc: 0.8762430548667908\n",
      "step: 22800\n",
      "train: loss: 854972.9375 acc: 0.70672208070755  val: loss: 1111440.0 acc: 0.7240316867828369\n",
      "step: 22805\n",
      "train: loss: 470007.25 acc: 0.7535555362701416  val: loss: 1937838.75 acc: 0.6039215326309204\n",
      "step: 22810\n",
      "train: loss: 956655.625 acc: 0.7351868152618408  val: loss: 4660279.0 acc: 0.5663212537765503\n",
      "step: 22815\n",
      "train: loss: 358451.8125 acc: 0.7948503494262695  val: loss: 129509.4296875 acc: 0.8724550008773804\n",
      "step: 22820\n",
      "train: loss: 36636.0703125 acc: 0.9692835807800293  val: loss: 822852.9375 acc: 0.6982607841491699\n",
      "step: 22825\n",
      "train: loss: 65807.015625 acc: 0.9466092586517334  val: loss: 777813.25 acc: 0.7732447981834412\n",
      "step: 22830\n",
      "train: loss: 21349.515625 acc: 0.9832398891448975  val: loss: 227540.84375 acc: 0.8315934538841248\n",
      "step: 22835\n",
      "train: loss: 539699.1875 acc: 0.7923629879951477  val: loss: 910385.75 acc: 0.7213184833526611\n",
      "step: 22840\n",
      "train: loss: 77270.7421875 acc: 0.9296867847442627  val: loss: 686163.9375 acc: 0.7193503975868225\n",
      "step: 22845\n",
      "train: loss: 67114.7734375 acc: 0.9437450766563416  val: loss: 1775553.875 acc: 0.6311477422714233\n",
      "step: 22850\n",
      "train: loss: 14491.2158203125 acc: 0.9823393225669861  val: loss: 3200366.5 acc: 0.578331470489502\n",
      "step: 22855\n",
      "train: loss: 375020.375 acc: 0.7193601131439209  val: loss: 1751570.5 acc: 0.667030930519104\n",
      "step: 22860\n",
      "train: loss: 379123.09375 acc: 0.8112908005714417  val: loss: 1103768.5 acc: 0.6353266835212708\n",
      "step: 22865\n",
      "train: loss: 566355.75 acc: 0.703637957572937  val: loss: 143170.03125 acc: 0.8631528615951538\n",
      "step: 22870\n",
      "train: loss: 113105.0 acc: 0.9221819043159485  val: loss: 740237.875 acc: 0.694418728351593\n",
      "step: 22875\n",
      "train: loss: 266213.625 acc: 0.8051971793174744  val: loss: 953257.125 acc: 0.7037708759307861\n",
      "step: 22880\n",
      "train: loss: 195526.375 acc: 0.864254355430603  val: loss: 1696761.625 acc: 0.5997875928878784\n",
      "step: 22885\n",
      "train: loss: 2151388.75 acc: 0.6881600618362427  val: loss: 500438.25 acc: 0.7720987200737\n",
      "step: 22890\n",
      "train: loss: 965520.3125 acc: 0.8397238254547119  val: loss: 1376258.875 acc: 0.8761791586875916\n",
      "step: 22895\n",
      "train: loss: 831032.875 acc: 0.936282753944397  val: loss: 1683581.375 acc: 0.4531598687171936\n",
      "step: 22900\n",
      "train: loss: 724628.0 acc: 0.9372798204421997  val: loss: 1930043.125 acc: 0.6254760026931763\n",
      "step: 22905\n",
      "train: loss: 348092.0625 acc: 0.963837742805481  val: loss: 970435.9375 acc: 0.8637915253639221\n",
      "step: 22910\n",
      "train: loss: 409388.5 acc: 0.9254037737846375  val: loss: 958370.9375 acc: 0.804193913936615\n",
      "step: 22915\n",
      "train: loss: 152577.828125 acc: 0.9816024303436279  val: loss: 946460.6875 acc: 0.5232739448547363\n",
      "step: 22920\n",
      "train: loss: 222101.984375 acc: 0.9809548854827881  val: loss: 1090626.5 acc: 0.8900466561317444\n",
      "step: 22925\n",
      "train: loss: 134942.1875 acc: 0.9913525581359863  val: loss: 1385360.875 acc: 0.8177554607391357\n",
      "step: 22930\n",
      "train: loss: 80488.34375 acc: 0.9916200637817383  val: loss: 469902.625 acc: 0.9269907474517822\n",
      "step: 22935\n",
      "train: loss: 61392.62890625 acc: 0.9904881715774536  val: loss: 487126.8125 acc: 0.8856054544448853\n",
      "step: 22940\n",
      "train: loss: 17504.5546875 acc: 0.9929839372634888  val: loss: 1061303.5 acc: 0.4976077675819397\n",
      "step: 22945\n",
      "train: loss: 37084.62109375 acc: 0.9868112206459045  val: loss: 772500.875 acc: 0.511891782283783\n",
      "step: 22950\n",
      "train: loss: 14394.6494140625 acc: 0.9674458503723145  val: loss: 60130.09375 acc: 0.9882504343986511\n",
      "step: 22955\n",
      "train: loss: 9813.2060546875 acc: 0.989381730556488  val: loss: 2321612.5 acc: -0.30759286880493164\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 22960\n",
      "train: loss: 14427.6650390625 acc: 0.9694569110870361  val: loss: 139531.84375 acc: 0.9534289240837097\n",
      "step: 22965\n",
      "train: loss: 25334.40625 acc: 0.9667654037475586  val: loss: 1440211.125 acc: 0.6785318851470947\n",
      "step: 22970\n",
      "train: loss: 23686.451171875 acc: 0.9624886512756348  val: loss: 117624.078125 acc: 0.9709329009056091\n",
      "step: 22975\n",
      "train: loss: 12597.5693359375 acc: 0.9819696545600891  val: loss: 1682999.375 acc: 0.12259495258331299\n",
      "step: 22980\n",
      "train: loss: 16136.5244140625 acc: 0.9662685394287109  val: loss: 335345.40625 acc: 0.9242298603057861\n",
      "step: 22985\n",
      "train: loss: 32517.099609375 acc: 0.9534373879432678  val: loss: 577723.25 acc: 0.822669506072998\n",
      "step: 22990\n",
      "train: loss: 9481.9501953125 acc: 0.9909217953681946  val: loss: 65511.40234375 acc: 0.9858859181404114\n",
      "step: 22995\n",
      "train: loss: 13298.1162109375 acc: 0.9929138422012329  val: loss: 188921.21875 acc: 0.9639410376548767\n",
      "step: 23000\n",
      "train: loss: 23556.791015625 acc: 0.9830886721611023  val: loss: 1403380.75 acc: 0.8156813979148865\n",
      "step: 23005\n",
      "train: loss: 23122.576171875 acc: 0.9850620627403259  val: loss: 893639.0 acc: 0.7161681056022644\n",
      "step: 23010\n",
      "train: loss: 78343.5234375 acc: 0.9292629957199097  val: loss: 1023704.4375 acc: 0.8231813907623291\n",
      "step: 23015\n",
      "train: loss: 6089.12548828125 acc: 0.9926368594169617  val: loss: 1231327.75 acc: 0.5360667705535889\n",
      "step: 23020\n",
      "train: loss: 12293.4150390625 acc: 0.995374858379364  val: loss: 1077515.75 acc: 0.7746726274490356\n",
      "step: 23025\n",
      "train: loss: 24358.46484375 acc: 0.9932726621627808  val: loss: 1412856.75 acc: 0.8070125579833984\n",
      "step: 23030\n",
      "train: loss: 25996.326171875 acc: 0.9877446889877319  val: loss: 1794865.375 acc: 0.8382383584976196\n",
      "step: 23035\n",
      "train: loss: 25543.361328125 acc: 0.9876993894577026  val: loss: 149471.1875 acc: 0.9791592955589294\n",
      "step: 23040\n",
      "train: loss: 43045.95703125 acc: 0.9545056819915771  val: loss: 1139156.5 acc: 0.8497238755226135\n",
      "step: 23045\n",
      "train: loss: 134161.125 acc: 0.9571644067764282  val: loss: 609407.3125 acc: 0.9345698356628418\n",
      "step: 23050\n",
      "train: loss: 76105.671875 acc: 0.9734045267105103  val: loss: 490658.40625 acc: 0.9593607187271118\n",
      "step: 23055\n",
      "train: loss: 120697.4765625 acc: 0.9527233839035034  val: loss: 85213.9765625 acc: 0.9637467265129089\n",
      "step: 23060\n",
      "train: loss: 687993.0 acc: 0.897119402885437  val: loss: 1867876.125 acc: 0.8567302227020264\n",
      "step: 23065\n",
      "train: loss: 429421.4375 acc: 0.943650484085083  val: loss: 477338.84375 acc: 0.9487175941467285\n",
      "step: 23070\n",
      "train: loss: 155893.734375 acc: 0.9853317737579346  val: loss: 1599640.875 acc: 0.5201985836029053\n",
      "step: 23075\n",
      "train: loss: 584159.1875 acc: 0.8775675296783447  val: loss: 714481.1875 acc: 0.9029949903488159\n",
      "step: 23080\n",
      "train: loss: 192826.640625 acc: 0.9589048027992249  val: loss: 236063.5625 acc: 0.8872610330581665\n",
      "step: 23085\n",
      "train: loss: 70809.9765625 acc: 0.9904927611351013  val: loss: 530220.5 acc: 0.9401111602783203\n",
      "step: 23090\n",
      "train: loss: 378942.625 acc: 0.9807167053222656  val: loss: 713899.5625 acc: 0.8964851498603821\n",
      "step: 23095\n",
      "train: loss: 228798.78125 acc: 0.9704908132553101  val: loss: 622477.1875 acc: 0.8978127241134644\n",
      "step: 23100\n",
      "train: loss: 444307.15625 acc: 0.9504502415657043  val: loss: 705658.0 acc: 0.8426886796951294\n",
      "step: 23105\n",
      "train: loss: 327493.65625 acc: 0.987187385559082  val: loss: 893866.75 acc: 0.6564152240753174\n",
      "step: 23110\n",
      "train: loss: 2001844.0 acc: 0.9348632097244263  val: loss: 582568.5 acc: 0.7660191655158997\n",
      "step: 23115\n",
      "train: loss: 940536.75 acc: 0.9589667320251465  val: loss: 804200.625 acc: 0.8418645858764648\n",
      "step: 23120\n",
      "train: loss: 302904.0625 acc: 0.9508954286575317  val: loss: 695381.5 acc: 0.7287452220916748\n",
      "step: 23125\n",
      "train: loss: 1288052.375 acc: 0.9252805709838867  val: loss: 158933.59375 acc: 0.9350565671920776\n",
      "step: 23130\n",
      "train: loss: 690825.125 acc: 0.9116868376731873  val: loss: 1048324.0625 acc: 0.853743314743042\n",
      "step: 23135\n",
      "train: loss: 1034496.6875 acc: 0.8376474976539612  val: loss: 2121310.0 acc: 0.35077810287475586\n",
      "step: 23140\n",
      "train: loss: 2106812.25 acc: 0.7077534198760986  val: loss: 338278.96875 acc: 0.8357818722724915\n",
      "step: 23145\n",
      "train: loss: 497878.6875 acc: 0.8618056774139404  val: loss: 991299.0625 acc: 0.8024997115135193\n",
      "step: 23150\n",
      "train: loss: 665088.625 acc: 0.7952715754508972  val: loss: 1926316.25 acc: 0.7503316402435303\n",
      "step: 23155\n",
      "train: loss: 860586.5 acc: 0.5594784021377563  val: loss: 240550.390625 acc: 0.7136477828025818\n",
      "step: 23160\n",
      "train: loss: 676777.5 acc: 0.7533395886421204  val: loss: 747686.125 acc: 0.72612464427948\n",
      "step: 23165\n",
      "train: loss: 968006.3125 acc: 0.5516960620880127  val: loss: 148889.390625 acc: 0.8670367002487183\n",
      "step: 23170\n",
      "train: loss: 946303.0625 acc: 0.6821665167808533  val: loss: 882816.4375 acc: 0.7260355949401855\n",
      "step: 23175\n",
      "train: loss: 226471.28125 acc: 0.8559678196907043  val: loss: 2868887.0 acc: 0.6076020002365112\n",
      "step: 23180\n",
      "train: loss: 427754.75 acc: 0.7480312585830688  val: loss: 3396027.0 acc: 0.6079171895980835\n",
      "step: 23185\n",
      "train: loss: 84209.5703125 acc: 0.9330915808677673  val: loss: 2661786.75 acc: 0.6272279024124146\n",
      "step: 23190\n",
      "train: loss: 63668.62890625 acc: 0.9510827660560608  val: loss: 1837033.625 acc: 0.6210528612136841\n",
      "step: 23195\n",
      "train: loss: 55351.421875 acc: 0.9548752307891846  val: loss: 4571264.5 acc: 0.575974702835083\n",
      "step: 23200\n",
      "train: loss: 367111.3125 acc: 0.8156578540802002  val: loss: 545518.4375 acc: 0.7554711103439331\n",
      "step: 23205\n",
      "train: loss: 474632.96875 acc: 0.8008257150650024  val: loss: 741785.25 acc: 0.7772858738899231\n",
      "step: 23210\n",
      "train: loss: 394448.8125 acc: 0.8118383288383484  val: loss: 1547455.375 acc: 0.6643877029418945\n",
      "step: 23215\n",
      "train: loss: 41306.5625 acc: 0.9585446119308472  val: loss: 617388.125 acc: 0.7265206575393677\n",
      "step: 23220\n",
      "train: loss: 299031.15625 acc: 0.8418886661529541  val: loss: 979952.6875 acc: 0.7412971258163452\n",
      "step: 23225\n",
      "train: loss: 531620.0 acc: 0.7724800109863281  val: loss: 1473597.75 acc: 0.6522136330604553\n",
      "step: 23230\n",
      "train: loss: 450290.03125 acc: 0.7909919023513794  val: loss: 2069678.875 acc: 0.5600908398628235\n",
      "step: 23235\n",
      "train: loss: 800385.1875 acc: 0.6481540203094482  val: loss: 449918.9375 acc: 0.7309387922286987\n",
      "step: 23240\n",
      "train: loss: 280286.25 acc: 0.821395993232727  val: loss: 4238225.0 acc: 0.5426753759384155\n",
      "step: 23245\n",
      "train: loss: 497883.96875 acc: 0.7053797245025635  val: loss: 2651525.5 acc: 0.5978507995605469\n",
      "step: 23250\n",
      "train: loss: 2833953.5 acc: 0.6464720964431763  val: loss: 960507.9375 acc: 0.7828317880630493\n",
      "step: 23255\n",
      "train: loss: 855297.8125 acc: 0.7790443897247314  val: loss: 881484.25 acc: 0.7197035551071167\n",
      "step: 23260\n",
      "train: loss: 1453384.375 acc: 0.8566582202911377  val: loss: 239471.09375 acc: 0.7495216131210327\n",
      "step: 23265\n",
      "train: loss: 318675.125 acc: 0.9684172868728638  val: loss: 449562.875 acc: 0.9057444334030151\n",
      "step: 23270\n",
      "train: loss: 236784.1875 acc: 0.9687479138374329  val: loss: 794736.1875 acc: 0.7427970767021179\n",
      "step: 23275\n",
      "train: loss: 101940.7890625 acc: 0.984324038028717  val: loss: 251059.078125 acc: 0.8525377511978149\n",
      "step: 23280\n",
      "train: loss: 80326.71875 acc: 0.9915840029716492  val: loss: 241511.0 acc: 0.9588167071342468\n",
      "step: 23285\n",
      "train: loss: 60451.671875 acc: 0.9950287938117981  val: loss: 365995.90625 acc: 0.9554803371429443\n",
      "step: 23290\n",
      "train: loss: 120800.1640625 acc: 0.9919192790985107  val: loss: 1528278.125 acc: 0.7996199131011963\n",
      "step: 23295\n",
      "train: loss: 45936.21484375 acc: 0.9955510497093201  val: loss: 1294449.5 acc: 0.2665640711784363\n",
      "step: 23300\n",
      "train: loss: 979487.0 acc: 0.7353764772415161  val: loss: 1421138.75 acc: 0.5174390077590942\n",
      "step: 23305\n",
      "train: loss: 26455.521484375 acc: 0.9954596757888794  val: loss: 444716.375 acc: 0.8625422716140747\n",
      "step: 23310\n",
      "train: loss: 13148.26953125 acc: 0.9946751594543457  val: loss: 589510.75 acc: 0.7187743186950684\n",
      "step: 23315\n",
      "train: loss: 7766.71630859375 acc: 0.9978389739990234  val: loss: 1900718.625 acc: 0.5834476947784424\n",
      "step: 23320\n",
      "train: loss: 29479.84375 acc: 0.9676045179367065  val: loss: 797548.375 acc: 0.8083093166351318\n",
      "step: 23325\n",
      "train: loss: 15033.625 acc: 0.9762545228004456  val: loss: 1082397.375 acc: 0.820672333240509\n",
      "step: 23330\n",
      "train: loss: 6798.7138671875 acc: 0.9740335941314697  val: loss: 187731.015625 acc: 0.9718177318572998\n",
      "step: 23335\n",
      "train: loss: 15485.173828125 acc: 0.9629112482070923  val: loss: 1351914.125 acc: 0.5777813792228699\n",
      "step: 23340\n",
      "train: loss: 8963.2001953125 acc: 0.9827743172645569  val: loss: 1202224.375 acc: 0.8580480217933655\n",
      "step: 23345\n",
      "train: loss: 10395.4755859375 acc: 0.9848361611366272  val: loss: 553579.9375 acc: 0.8896659016609192\n",
      "step: 23350\n",
      "train: loss: 9123.12109375 acc: 0.9851821660995483  val: loss: 1736237.0 acc: 0.7747638821601868\n",
      "step: 23355\n",
      "train: loss: 37762.84375 acc: 0.9694886207580566  val: loss: 707859.25 acc: 0.8299717903137207\n",
      "step: 23360\n",
      "train: loss: 18030.716796875 acc: 0.991649329662323  val: loss: 767398.5625 acc: 0.9018875956535339\n",
      "step: 23365\n",
      "train: loss: 11119.0263671875 acc: 0.9883323311805725  val: loss: 1645644.125 acc: 0.6939135789871216\n",
      "step: 23370\n",
      "train: loss: 16955.9609375 acc: 0.9907553791999817  val: loss: 1821078.625 acc: 0.3287614583969116\n",
      "step: 23375\n",
      "train: loss: 12062.64453125 acc: 0.9731597900390625  val: loss: 200979.75 acc: 0.9770805835723877\n",
      "step: 23380\n",
      "train: loss: 13862.4130859375 acc: 0.9858993887901306  val: loss: 2277800.75 acc: 0.5986437797546387\n",
      "step: 23385\n",
      "train: loss: 23559.693359375 acc: 0.9904090166091919  val: loss: 954147.125 acc: 0.8761432766914368\n",
      "step: 23390\n",
      "train: loss: 21213.30078125 acc: 0.9935299158096313  val: loss: 454627.8125 acc: 0.939582347869873\n",
      "step: 23395\n",
      "train: loss: 31276.515625 acc: 0.9914732575416565  val: loss: 1818851.875 acc: 0.2667609453201294\n",
      "step: 23400\n",
      "train: loss: 27039.265625 acc: 0.9906119704246521  val: loss: 1941659.375 acc: 0.7899919152259827\n",
      "step: 23405\n",
      "train: loss: 23196.73046875 acc: 0.989667534828186  val: loss: 1557539.875 acc: 0.639054536819458\n",
      "step: 23410\n",
      "train: loss: 252803.421875 acc: 0.9095368981361389  val: loss: 1690751.875 acc: 0.8759530782699585\n",
      "step: 23415\n",
      "train: loss: 193719.640625 acc: 0.9377403259277344  val: loss: 873614.6875 acc: 0.9290010929107666\n",
      "step: 23420\n",
      "train: loss: 96097.6953125 acc: 0.9557982683181763  val: loss: 2916748.5 acc: 0.4932141900062561\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 23425\n",
      "train: loss: 80948.6796875 acc: 0.98460853099823  val: loss: 1613720.625 acc: 0.830970287322998\n",
      "step: 23430\n",
      "train: loss: 271535.65625 acc: 0.9661449193954468  val: loss: 494416.8125 acc: 0.8949970602989197\n",
      "step: 23435\n",
      "train: loss: 1071513.125 acc: 0.868602991104126  val: loss: 712562.125 acc: 0.9134987592697144\n",
      "step: 23440\n",
      "train: loss: 70289.234375 acc: 0.9910659193992615  val: loss: 936539.75 acc: 0.8609322905540466\n",
      "step: 23445\n",
      "train: loss: 876006.0 acc: 0.8568188548088074  val: loss: 1628266.0 acc: 0.7783126831054688\n",
      "step: 23450\n",
      "train: loss: 109790.796875 acc: 0.9890195727348328  val: loss: 951124.0625 acc: 0.8428384065628052\n",
      "step: 23455\n",
      "train: loss: 582190.3125 acc: 0.9615139961242676  val: loss: 4638737.5 acc: -1.1586229801177979\n",
      "step: 23460\n",
      "train: loss: 1052075.125 acc: 0.920048713684082  val: loss: 966307.75 acc: 0.6305745840072632\n",
      "step: 23465\n",
      "train: loss: 645731.5625 acc: 0.8805614113807678  val: loss: 318103.15625 acc: 0.7400004863739014\n",
      "step: 23470\n",
      "train: loss: 847431.6875 acc: 0.9655949473381042  val: loss: 289024.15625 acc: 0.9032849073410034\n",
      "step: 23475\n",
      "train: loss: 982323.5 acc: 0.9569998979568481  val: loss: 365333.09375 acc: 0.9451324343681335\n",
      "step: 23480\n",
      "train: loss: 1785711.0 acc: 0.9326149225234985  val: loss: 356875.25 acc: 0.8829007744789124\n",
      "step: 23485\n",
      "train: loss: 684263.5 acc: 0.9631639122962952  val: loss: 828698.1875 acc: 0.8327616453170776\n",
      "step: 23490\n",
      "train: loss: 682391.1875 acc: 0.9465310573577881  val: loss: 798693.5625 acc: 0.5216624140739441\n",
      "step: 23495\n",
      "train: loss: 210653.296875 acc: 0.9699220657348633  val: loss: 1112051.0 acc: 0.8078455924987793\n",
      "step: 23500\n",
      "train: loss: 434822.375 acc: 0.9487982392311096  val: loss: 2504310.5 acc: 0.745226263999939\n",
      "step: 23505\n",
      "train: loss: 2049607.625 acc: 0.7249703407287598  val: loss: 486003.1875 acc: 0.9487223625183105\n",
      "step: 23510\n",
      "train: loss: 924410.6875 acc: 0.7945684194564819  val: loss: 937461.4375 acc: 0.6909831762313843\n",
      "step: 23515\n",
      "train: loss: 3098335.75 acc: 0.36702507734298706  val: loss: 1721084.25 acc: 0.7813605666160583\n",
      "step: 23520\n",
      "train: loss: 253913.25 acc: 0.8006752729415894  val: loss: 1179970.75 acc: 0.716679036617279\n",
      "step: 23525\n",
      "train: loss: 689263.875 acc: 0.7752386927604675  val: loss: 872457.1875 acc: 0.8318468332290649\n",
      "step: 23530\n",
      "train: loss: 1000048.5625 acc: 0.5624246001243591  val: loss: 1011501.5625 acc: 0.7425845861434937\n",
      "step: 23535\n",
      "train: loss: 257384.234375 acc: 0.7767508029937744  val: loss: 604123.6875 acc: 0.716335654258728\n",
      "step: 23540\n",
      "train: loss: 443618.84375 acc: 0.7872632145881653  val: loss: 1206718.875 acc: 0.6664100885391235\n",
      "step: 23545\n",
      "train: loss: 102692.734375 acc: 0.8994173407554626  val: loss: 1097069.625 acc: 0.6567984819412231\n",
      "step: 23550\n",
      "train: loss: 62267.78515625 acc: 0.9514876008033752  val: loss: 585765.0625 acc: 0.720241367816925\n",
      "step: 23555\n",
      "train: loss: 75745.71875 acc: 0.945336639881134  val: loss: 1185478.0 acc: 0.6758983135223389\n",
      "step: 23560\n",
      "train: loss: 237220.96875 acc: 0.8643354773521423  val: loss: 934813.125 acc: 0.7015372514724731\n",
      "step: 23565\n",
      "train: loss: 34368.203125 acc: 0.9716914892196655  val: loss: 1579980.75 acc: 0.6248157024383545\n",
      "step: 23570\n",
      "train: loss: 399639.65625 acc: 0.8187229633331299  val: loss: 983123.375 acc: 0.6917197704315186\n",
      "step: 23575\n",
      "train: loss: 384618.875 acc: 0.8175779581069946  val: loss: 1064465.25 acc: 0.6966678500175476\n",
      "step: 23580\n",
      "train: loss: 23370.154296875 acc: 0.9749846458435059  val: loss: 563621.625 acc: 0.7348231077194214\n",
      "step: 23585\n",
      "train: loss: 96836.125 acc: 0.9221954941749573  val: loss: 1715538.25 acc: 0.6683855056762695\n",
      "step: 23590\n",
      "train: loss: 357523.0625 acc: 0.8315528035163879  val: loss: 2687329.25 acc: 0.5952881574630737\n",
      "step: 23595\n",
      "train: loss: 533592.9375 acc: 0.757871150970459  val: loss: 1004376.8125 acc: 0.6751452088356018\n",
      "step: 23600\n",
      "train: loss: 494590.03125 acc: 0.7260748147964478  val: loss: 785492.6875 acc: 0.6851307153701782\n",
      "step: 23605\n",
      "train: loss: 1582443.875 acc: 0.6963456869125366  val: loss: 1867733.25 acc: 0.7087641954421997\n",
      "step: 23610\n",
      "train: loss: 672208.4375 acc: 0.7435604929924011  val: loss: 1064509.125 acc: 0.7178817987442017\n",
      "step: 23615\n",
      "train: loss: 1621956.125 acc: 0.6716156005859375  val: loss: 1120029.375 acc: 0.7175174951553345\n",
      "step: 23620\n",
      "train: loss: 1128881.375 acc: 0.8126707077026367  val: loss: 353537.34375 acc: 0.7845628261566162\n",
      "step: 23625\n",
      "train: loss: 1781349.75 acc: 0.7967376708984375  val: loss: 777445.75 acc: 0.8539308905601501\n",
      "step: 23630\n",
      "train: loss: 547078.4375 acc: 0.9555091857910156  val: loss: 1092910.0 acc: 0.7629385590553284\n",
      "step: 23635\n",
      "train: loss: 229563.390625 acc: 0.9736260771751404  val: loss: 649206.1875 acc: 0.7895078659057617\n",
      "step: 23640\n",
      "train: loss: 651828.75 acc: 0.9410350322723389  val: loss: 958612.1875 acc: 0.667775571346283\n",
      "step: 23645\n",
      "train: loss: 100737.9375 acc: 0.9906036853790283  val: loss: 278926.71875 acc: 0.9577842950820923\n",
      "step: 23650\n",
      "train: loss: 37613.40234375 acc: 0.9969784021377563  val: loss: 258425.96875 acc: 0.9648037552833557\n",
      "step: 23655\n",
      "train: loss: 42851.59765625 acc: 0.9974174499511719  val: loss: 619589.0 acc: 0.9118123054504395\n",
      "step: 23660\n",
      "train: loss: 63674.13671875 acc: 0.9943867921829224  val: loss: 1575216.75 acc: 0.8277633786201477\n",
      "step: 23665\n",
      "train: loss: 23393.876953125 acc: 0.9940440058708191  val: loss: 67539.7265625 acc: 0.9874981641769409\n",
      "step: 23670\n",
      "train: loss: 34002.65234375 acc: 0.9944478869438171  val: loss: 415389.5 acc: 0.970658540725708\n",
      "step: 23675\n",
      "train: loss: 4824.6318359375 acc: 0.9963099956512451  val: loss: 398999.90625 acc: 0.9079433083534241\n",
      "step: 23680\n",
      "train: loss: 3924.867919921875 acc: 0.9971868395805359  val: loss: 1196058.125 acc: 0.7516611814498901\n",
      "step: 23685\n",
      "train: loss: 43235.640625 acc: 0.9723445773124695  val: loss: 808171.5 acc: 0.8754973411560059\n",
      "step: 23690\n",
      "train: loss: 13163.1220703125 acc: 0.9820951223373413  val: loss: 686912.0 acc: 0.8938887715339661\n",
      "step: 23695\n",
      "train: loss: 9250.2265625 acc: 0.9571692943572998  val: loss: 2028681.375 acc: 0.3856312036514282\n",
      "step: 23700\n",
      "train: loss: 18182.009765625 acc: 0.9652372598648071  val: loss: 3108556.5 acc: 0.5239689350128174\n",
      "step: 23705\n",
      "train: loss: 11821.658203125 acc: 0.9902284145355225  val: loss: 885085.125 acc: 0.7114984393119812\n",
      "step: 23710\n",
      "train: loss: 13501.9375 acc: 0.9531733989715576  val: loss: 2322155.75 acc: 0.606840193271637\n",
      "step: 23715\n",
      "train: loss: 8080.21728515625 acc: 0.9858256578445435  val: loss: 2432710.0 acc: 0.0047653913497924805\n",
      "step: 23720\n",
      "train: loss: 39309.4921875 acc: 0.9811230301856995  val: loss: 1615299.125 acc: 0.7541517615318298\n",
      "step: 23725\n",
      "train: loss: 57683.828125 acc: 0.9640331864356995  val: loss: 1882130.125 acc: 0.737652063369751\n",
      "step: 23730\n",
      "train: loss: 35416.7578125 acc: 0.978600800037384  val: loss: 1126033.5 acc: 0.8508982062339783\n",
      "step: 23735\n",
      "train: loss: 13648.4501953125 acc: 0.9928253293037415  val: loss: 2879447.5 acc: 0.6989688873291016\n",
      "step: 23740\n",
      "train: loss: 13054.876953125 acc: 0.9902461767196655  val: loss: 2341433.5 acc: 0.7215477228164673\n",
      "step: 23745\n",
      "train: loss: 7981.5751953125 acc: 0.9914371371269226  val: loss: 1648609.25 acc: 0.8348737955093384\n",
      "step: 23750\n",
      "train: loss: 3946.1875 acc: 0.9976997375488281  val: loss: 2881516.5 acc: 0.43805456161499023\n",
      "step: 23755\n",
      "train: loss: 55460.2890625 acc: 0.9818594455718994  val: loss: 1637026.875 acc: 0.615014910697937\n",
      "step: 23760\n",
      "train: loss: 32272.234375 acc: 0.9917887449264526  val: loss: 2058370.875 acc: 0.661921501159668\n",
      "step: 23765\n",
      "train: loss: 17533.982421875 acc: 0.9942001104354858  val: loss: 1915807.125 acc: 0.6797640323638916\n",
      "step: 23770\n",
      "train: loss: 37969.68359375 acc: 0.9845300316810608  val: loss: 704508.8125 acc: 0.701754629611969\n",
      "step: 23775\n",
      "train: loss: 282039.40625 acc: 0.9098678827285767  val: loss: 911565.4375 acc: 0.8543251752853394\n",
      "step: 23780\n",
      "train: loss: 95292.5546875 acc: 0.970919132232666  val: loss: 1859545.125 acc: 0.5464532375335693\n",
      "step: 23785\n",
      "train: loss: 152188.5 acc: 0.96128910779953  val: loss: 866887.1875 acc: 0.814039945602417\n",
      "step: 23790\n",
      "train: loss: 662783.375 acc: 0.8757681846618652  val: loss: 2333951.75 acc: -0.8716146945953369\n",
      "step: 23795\n",
      "train: loss: 105774.9140625 acc: 0.9789286851882935  val: loss: 599335.625 acc: 0.8785249590873718\n",
      "step: 23800\n",
      "train: loss: 513751.375 acc: 0.9496824741363525  val: loss: 434030.71875 acc: 0.9110099077224731\n",
      "step: 23805\n",
      "train: loss: 55977.5078125 acc: 0.9927173852920532  val: loss: 2147355.75 acc: -0.3596585988998413\n",
      "step: 23810\n",
      "train: loss: 116883.1640625 acc: 0.9683429598808289  val: loss: 574037.5 acc: 0.8639060854911804\n",
      "step: 23815\n",
      "train: loss: 126142.7578125 acc: 0.9859583377838135  val: loss: 308973.21875 acc: 0.8685370683670044\n",
      "step: 23820\n",
      "train: loss: 670380.1875 acc: 0.9649279117584229  val: loss: 3575382.5 acc: -0.015952348709106445\n",
      "step: 23825\n",
      "train: loss: 184063.71875 acc: 0.9872674345970154  val: loss: 847574.1875 acc: 0.6889790296554565\n",
      "step: 23830\n",
      "train: loss: 131372.90625 acc: 0.9822733998298645  val: loss: 879324.5 acc: 0.8247202038764954\n",
      "step: 23835\n",
      "train: loss: 740754.0625 acc: 0.9774875044822693  val: loss: 556499.1875 acc: 0.8574947118759155\n",
      "step: 23840\n",
      "train: loss: 324456.6875 acc: 0.989870548248291  val: loss: 1037242.4375 acc: 0.858342707157135\n",
      "step: 23845\n",
      "train: loss: 1143788.0 acc: 0.9688665866851807  val: loss: 1234771.375 acc: 0.8013807535171509\n",
      "step: 23850\n",
      "train: loss: 733817.1875 acc: 0.9583064913749695  val: loss: 145356.5625 acc: 0.9597375392913818\n",
      "step: 23855\n",
      "train: loss: 746266.375 acc: 0.9440559148788452  val: loss: 549951.3125 acc: 0.8032351732254028\n",
      "step: 23860\n",
      "train: loss: 471772.28125 acc: 0.9565958976745605  val: loss: 549547.5 acc: 0.8688154816627502\n",
      "step: 23865\n",
      "train: loss: 370863.65625 acc: 0.9494728446006775  val: loss: 1097809.625 acc: 0.8636555075645447\n",
      "step: 23870\n",
      "train: loss: 991604.0625 acc: 0.8724498748779297  val: loss: 1688713.125 acc: 0.32401609420776367\n",
      "step: 23875\n",
      "train: loss: 898324.4375 acc: 0.6733332872390747  val: loss: 1571846.875 acc: 0.8050435185432434\n",
      "step: 23880\n",
      "train: loss: 741899.6875 acc: 0.8524171710014343  val: loss: 1175004.125 acc: 0.8011048436164856\n",
      "step: 23885\n",
      "train: loss: 994730.875 acc: 0.6737267971038818  val: loss: 1658579.5 acc: 0.805997908115387\n",
      "step: 23890\n",
      "train: loss: 511812.90625 acc: 0.8257369995117188  val: loss: 732695.375 acc: 0.7720315456390381\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 23895\n",
      "train: loss: 740622.875 acc: 0.7322863340377808  val: loss: 1703157.25 acc: 0.7447873950004578\n",
      "step: 23900\n",
      "train: loss: 460629.5 acc: 0.7757403254508972  val: loss: 2807298.5 acc: 0.6666461229324341\n",
      "step: 23905\n",
      "train: loss: 451516.96875 acc: 0.7304962277412415  val: loss: 1138211.5 acc: 0.6723983287811279\n",
      "step: 23910\n",
      "train: loss: 171186.46875 acc: 0.8680745959281921  val: loss: 1736563.25 acc: 0.6153913140296936\n",
      "step: 23915\n",
      "train: loss: 104324.265625 acc: 0.9211808443069458  val: loss: 149688.125 acc: 0.8697327375411987\n",
      "step: 23920\n",
      "train: loss: 150605.890625 acc: 0.9043926000595093  val: loss: 606340.1875 acc: 0.7659018039703369\n",
      "step: 23925\n",
      "train: loss: 81874.0625 acc: 0.9461886286735535  val: loss: 4023180.0 acc: 0.5353050827980042\n",
      "step: 23930\n",
      "train: loss: 276343.65625 acc: 0.8454015851020813  val: loss: 615699.5 acc: 0.7743134498596191\n",
      "step: 23935\n",
      "train: loss: 131640.890625 acc: 0.910128116607666  val: loss: 1776436.125 acc: 0.6421581506729126\n",
      "step: 23940\n",
      "train: loss: 71222.15625 acc: 0.9465307593345642  val: loss: 2232651.75 acc: 0.574790358543396\n",
      "step: 23945\n",
      "train: loss: 126234.015625 acc: 0.8916029930114746  val: loss: 2568645.0 acc: 0.5388576984405518\n",
      "step: 23950\n",
      "train: loss: 44289.95703125 acc: 0.9512701034545898  val: loss: 1335883.625 acc: 0.6962152719497681\n",
      "step: 23955\n",
      "train: loss: 400033.21875 acc: 0.7662758231163025  val: loss: 4575802.5 acc: 0.555181086063385\n",
      "step: 23960\n",
      "train: loss: 102811.4921875 acc: 0.9222369194030762  val: loss: 1904535.125 acc: 0.6174691915512085\n",
      "step: 23965\n",
      "train: loss: 557864.4375 acc: 0.7388550043106079  val: loss: 4028754.25 acc: 0.5610305070877075\n",
      "step: 23970\n",
      "train: loss: 1073231.625 acc: 0.6631442308425903  val: loss: 3277722.5 acc: 0.6117152571678162\n",
      "step: 23975\n",
      "train: loss: 246948.796875 acc: 0.7716064453125  val: loss: 2711057.5 acc: 0.6100842952728271\n",
      "step: 23980\n",
      "train: loss: 445506.25 acc: 0.8051292896270752  val: loss: 5182730.5 acc: 0.6153491735458374\n",
      "step: 23985\n",
      "train: loss: 1009620.625 acc: 0.7636311054229736  val: loss: 1020600.625 acc: 0.7963437438011169\n",
      "step: 23990\n",
      "train: loss: 1277646.75 acc: 0.8442884087562561  val: loss: 713960.375 acc: 0.8600431680679321\n",
      "step: 23995\n",
      "train: loss: 461177.875 acc: 0.945686936378479  val: loss: 636356.75 acc: 0.8368412852287292\n",
      "step: 24000\n",
      "train: loss: 245241.640625 acc: 0.9713804125785828  val: loss: 1148920.625 acc: 0.8699599504470825\n",
      "step: 24005\n",
      "train: loss: 159957.6875 acc: 0.9748486280441284  val: loss: 377947.75 acc: 0.9457298517227173\n",
      "step: 24010\n",
      "train: loss: 89039.7421875 acc: 0.9882111549377441  val: loss: 1028095.125 acc: 0.7869320511817932\n",
      "step: 24015\n",
      "train: loss: 135353.640625 acc: 0.9875949621200562  val: loss: 1387658.125 acc: 0.8112479448318481\n",
      "step: 24020\n",
      "train: loss: 53760.50390625 acc: 0.9963905215263367  val: loss: 4057609.5 acc: -0.4883512258529663\n",
      "step: 24025\n",
      "train: loss: 60395.6953125 acc: 0.9917553663253784  val: loss: 1522042.25 acc: 0.1644095778465271\n",
      "step: 24030\n",
      "train: loss: 39821.2734375 acc: 0.9934073090553284  val: loss: 358680.0 acc: 0.9502594470977783\n",
      "step: 24035\n",
      "train: loss: 32471.548828125 acc: 0.9927721619606018  val: loss: 1832147.5 acc: 0.7680193781852722\n",
      "step: 24040\n",
      "train: loss: 6337.40478515625 acc: 0.9958133101463318  val: loss: 1598016.75 acc: 0.831403911113739\n",
      "step: 24045\n",
      "train: loss: 2247.441650390625 acc: 0.9983853101730347  val: loss: 2309384.75 acc: 0.39434218406677246\n",
      "step: 24050\n",
      "train: loss: 33034.34765625 acc: 0.9873669743537903  val: loss: 1236788.625 acc: 0.43136918544769287\n",
      "step: 24055\n",
      "train: loss: 21375.203125 acc: 0.9872112274169922  val: loss: 1684341.875 acc: 0.7299482822418213\n",
      "step: 24060\n",
      "train: loss: 19924.4609375 acc: 0.9730098247528076  val: loss: 1886304.875 acc: -0.464943528175354\n",
      "step: 24065\n",
      "train: loss: 24473.1640625 acc: 0.9662830233573914  val: loss: 836316.4375 acc: 0.49027329683303833\n",
      "step: 24070\n",
      "train: loss: 20137.865234375 acc: 0.9308876991271973  val: loss: 603921.9375 acc: 0.7143573760986328\n",
      "step: 24075\n",
      "train: loss: 8452.193359375 acc: 0.9789282083511353  val: loss: 652182.875 acc: 0.41571784019470215\n",
      "step: 24080\n",
      "train: loss: 4346.00341796875 acc: 0.9873937368392944  val: loss: 1531109.375 acc: 0.6132450103759766\n",
      "step: 24085\n",
      "train: loss: 63289.3046875 acc: 0.9736304879188538  val: loss: 1742375.25 acc: 0.04361313581466675\n",
      "step: 24090\n",
      "train: loss: 46928.1484375 acc: 0.9704279899597168  val: loss: 3269215.5 acc: 0.7396262884140015\n",
      "step: 24095\n",
      "train: loss: 89978.8671875 acc: 0.9574869275093079  val: loss: 2039088.625 acc: 0.11550521850585938\n",
      "step: 24100\n",
      "train: loss: 16163.8037109375 acc: 0.9941378235816956  val: loss: 961014.125 acc: 0.7432215809822083\n",
      "step: 24105\n",
      "train: loss: 14954.7626953125 acc: 0.9933406710624695  val: loss: 1330642.75 acc: 0.8587813377380371\n",
      "step: 24110\n",
      "train: loss: 16114.5458984375 acc: 0.9831444621086121  val: loss: 4510125.5 acc: -0.08209407329559326\n",
      "step: 24115\n",
      "train: loss: 11281.6611328125 acc: 0.9870275259017944  val: loss: 4614287.0 acc: -0.9080052375793457\n",
      "step: 24120\n",
      "train: loss: 27525.912109375 acc: 0.9908425211906433  val: loss: 461254.25 acc: 0.8498892188072205\n",
      "step: 24125\n",
      "train: loss: 73715.484375 acc: 0.9839392304420471  val: loss: 1162989.125 acc: 0.8423583507537842\n",
      "step: 24130\n",
      "train: loss: 49070.20703125 acc: 0.981370210647583  val: loss: 803655.375 acc: 0.8814229369163513\n",
      "step: 24135\n",
      "train: loss: 28981.787109375 acc: 0.9922223091125488  val: loss: 1758392.5 acc: 0.4549568295478821\n",
      "step: 24140\n",
      "train: loss: 31418.138671875 acc: 0.9924463033676147  val: loss: 1505544.25 acc: 0.43624281883239746\n",
      "step: 24145\n",
      "train: loss: 46004.83203125 acc: 0.987102210521698  val: loss: 1498714.125 acc: 0.7012307643890381\n",
      "step: 24150\n",
      "train: loss: 114562.7734375 acc: 0.9680324792861938  val: loss: 1285069.125 acc: 0.34448009729385376\n",
      "step: 24155\n",
      "train: loss: 127692.5703125 acc: 0.9684017300605774  val: loss: 1483955.125 acc: 0.8147631883621216\n",
      "step: 24160\n",
      "train: loss: 157550.1875 acc: 0.9616855382919312  val: loss: 358472.21875 acc: 0.9450232982635498\n",
      "step: 24165\n",
      "train: loss: 729740.125 acc: 0.934147834777832  val: loss: 389373.71875 acc: 0.8981299996376038\n",
      "step: 24170\n",
      "train: loss: 117030.734375 acc: 0.987474799156189  val: loss: 991678.875 acc: 0.8880941867828369\n",
      "step: 24175\n",
      "train: loss: 114102.328125 acc: 0.977500319480896  val: loss: 366848.28125 acc: 0.9474390745162964\n",
      "step: 24180\n",
      "train: loss: 389037.46875 acc: 0.9718762636184692  val: loss: 903594.0625 acc: 0.6980980038642883\n",
      "step: 24185\n",
      "train: loss: 728413.625 acc: 0.9665634632110596  val: loss: 636313.625 acc: 0.9453744292259216\n",
      "step: 24190\n",
      "train: loss: 284376.625 acc: 0.9715478420257568  val: loss: 1789311.0 acc: 0.38492351770401\n",
      "step: 24195\n",
      "train: loss: 397288.53125 acc: 0.9701146483421326  val: loss: 1518947.5 acc: 0.7154115438461304\n",
      "step: 24200\n",
      "train: loss: 2560414.75 acc: 0.901861310005188  val: loss: 675186.875 acc: 0.8721913695335388\n",
      "step: 24205\n",
      "train: loss: 330416.59375 acc: 0.987153947353363  val: loss: 479789.15625 acc: 0.8043776750564575\n",
      "step: 24210\n",
      "train: loss: 2034987.5 acc: 0.9059866070747375  val: loss: 665566.1875 acc: 0.8301023840904236\n",
      "step: 24215\n",
      "train: loss: 584123.25 acc: 0.9709459543228149  val: loss: 262384.1875 acc: 0.9602858424186707\n",
      "step: 24220\n",
      "train: loss: 437198.6875 acc: 0.9718926548957825  val: loss: 117934.4296875 acc: 0.9579541087150574\n",
      "step: 24225\n",
      "train: loss: 321583.25 acc: 0.9757285714149475  val: loss: 179195.8125 acc: 0.9702516198158264\n",
      "step: 24230\n",
      "train: loss: 333769.59375 acc: 0.9282141923904419  val: loss: 314092.40625 acc: 0.5513700246810913\n",
      "step: 24235\n",
      "train: loss: 716921.8125 acc: 0.8276386857032776  val: loss: 310788.25 acc: 0.836536169052124\n",
      "step: 24240\n",
      "train: loss: 727085.25 acc: 0.6437698602676392  val: loss: 447194.78125 acc: 0.8937874436378479\n",
      "step: 24245\n",
      "train: loss: 495198.53125 acc: 0.8418600559234619  val: loss: 1569937.375 acc: 0.6458001136779785\n",
      "step: 24250\n",
      "train: loss: 343584.90625 acc: 0.8312556743621826  val: loss: 614363.3125 acc: 0.8356272578239441\n",
      "step: 24255\n",
      "train: loss: 352648.8125 acc: 0.8807017803192139  val: loss: 1715181.875 acc: 0.7836652994155884\n",
      "step: 24260\n",
      "train: loss: 1007468.625 acc: 0.6211544275283813  val: loss: 1604686.875 acc: 0.7512609362602234\n",
      "step: 24265\n",
      "train: loss: 278949.21875 acc: 0.8124592304229736  val: loss: 2783584.75 acc: 0.5932786464691162\n",
      "step: 24270\n",
      "train: loss: 532309.1875 acc: 0.7728413939476013  val: loss: 878084.1875 acc: 0.6875433921813965\n",
      "step: 24275\n",
      "train: loss: 213461.3125 acc: 0.8163015842437744  val: loss: 722929.125 acc: 0.6987396478652954\n",
      "step: 24280\n",
      "train: loss: 126990.875 acc: 0.9055339694023132  val: loss: 390240.15625 acc: 0.8010545969009399\n",
      "step: 24285\n",
      "train: loss: 54631.4296875 acc: 0.9526767134666443  val: loss: 5708731.0 acc: 0.6111268997192383\n",
      "step: 24290\n",
      "train: loss: 269902.65625 acc: 0.8558634519577026  val: loss: 8247115.0 acc: 0.5232234001159668\n",
      "step: 24295\n",
      "train: loss: 58634.71875 acc: 0.9558846950531006  val: loss: 2958840.0 acc: 0.5928276777267456\n",
      "step: 24300\n",
      "train: loss: 205377.015625 acc: 0.8872071504592896  val: loss: 836040.3125 acc: 0.7642651796340942\n",
      "step: 24305\n",
      "train: loss: 68843.8125 acc: 0.9508218765258789  val: loss: 2007934.375 acc: 0.589974045753479\n",
      "step: 24310\n",
      "train: loss: 20187.64453125 acc: 0.9772435426712036  val: loss: 3465695.75 acc: 0.5818551778793335\n",
      "step: 24315\n",
      "train: loss: 97405.671875 acc: 0.9186005592346191  val: loss: 4431339.5 acc: 0.5901466012001038\n",
      "step: 24320\n",
      "train: loss: 39813.94921875 acc: 0.9548169374465942  val: loss: 2503156.0 acc: 0.6621127128601074\n",
      "step: 24325\n",
      "train: loss: 137374.0 acc: 0.8999563455581665  val: loss: 2818637.75 acc: 0.6098725199699402\n",
      "step: 24330\n",
      "train: loss: 309862.75 acc: 0.8266261219978333  val: loss: 3476294.5 acc: 0.5850965976715088\n",
      "step: 24335\n",
      "train: loss: 727450.4375 acc: 0.7142348289489746  val: loss: 737819.25 acc: 0.7626113891601562\n",
      "step: 24340\n",
      "train: loss: 56807.12109375 acc: 0.9353711605072021  val: loss: 562816.0 acc: 0.7189881801605225\n",
      "step: 24345\n",
      "train: loss: 1395981.75 acc: 0.663887083530426  val: loss: 3283230.0 acc: 0.6449437141418457\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 24350\n",
      "train: loss: 1291643.875 acc: 0.7857214212417603  val: loss: 1085818.125 acc: 0.6080495119094849\n",
      "step: 24355\n",
      "train: loss: 1156975.125 acc: 0.7973856925964355  val: loss: 787951.0625 acc: 0.8886687755584717\n",
      "step: 24360\n",
      "train: loss: 1204415.0 acc: 0.8588356375694275  val: loss: 1771959.125 acc: 0.41871583461761475\n",
      "step: 24365\n",
      "train: loss: 436757.78125 acc: 0.9505976438522339  val: loss: 1255124.75 acc: 0.8205599188804626\n",
      "step: 24370\n",
      "train: loss: 150825.4375 acc: 0.9800782203674316  val: loss: 1832673.0 acc: 0.7388136982917786\n",
      "step: 24375\n",
      "train: loss: 144330.390625 acc: 0.9805383682250977  val: loss: 800091.6875 acc: 0.6850511431694031\n",
      "step: 24380\n",
      "train: loss: 125241.25 acc: 0.9854565262794495  val: loss: 1400688.25 acc: -0.2989966869354248\n",
      "step: 24385\n",
      "train: loss: 52949.1953125 acc: 0.996488094329834  val: loss: 913151.0 acc: 0.5768967866897583\n",
      "step: 24390\n",
      "train: loss: 64015.69140625 acc: 0.9940147399902344  val: loss: 430583.125 acc: 0.8766673803329468\n",
      "step: 24395\n",
      "train: loss: 561821.9375 acc: 0.924222469329834  val: loss: 1272159.125 acc: 0.8677009344100952\n",
      "step: 24400\n",
      "train: loss: 15817.884765625 acc: 0.9783854484558105  val: loss: 2449853.75 acc: 0.12344080209732056\n",
      "step: 24405\n",
      "train: loss: 63266.91796875 acc: 0.9753629565238953  val: loss: 594853.625 acc: 0.7831001877784729\n",
      "step: 24410\n",
      "train: loss: 6378.21044921875 acc: 0.9570689797401428  val: loss: 946735.625 acc: 0.8772794008255005\n",
      "step: 24415\n",
      "train: loss: 12356.0322265625 acc: 0.9911766052246094  val: loss: 660037.25 acc: 0.8285831212997437\n",
      "step: 24420\n",
      "train: loss: 21552.52734375 acc: 0.9784979820251465  val: loss: 1027078.6875 acc: 0.7722659707069397\n",
      "step: 24425\n",
      "train: loss: 16578.537109375 acc: 0.9725387692451477  val: loss: 1507188.375 acc: 0.35050803422927856\n",
      "step: 24430\n",
      "train: loss: 29725.900390625 acc: 0.9864431023597717  val: loss: 1173304.0 acc: 0.795516848564148\n",
      "step: 24435\n",
      "train: loss: 5704.1279296875 acc: 0.9865997433662415  val: loss: 2541819.5 acc: 0.6448075175285339\n",
      "step: 24440\n",
      "train: loss: 5418.48291015625 acc: 0.979223906993866  val: loss: 2092307.75 acc: -0.5246666669845581\n",
      "step: 24445\n",
      "train: loss: 13932.19140625 acc: 0.9761436581611633  val: loss: 778982.625 acc: 0.6831040978431702\n",
      "step: 24450\n",
      "train: loss: 35385.0703125 acc: 0.9839417934417725  val: loss: 1959240.125 acc: 0.6030864715576172\n",
      "step: 24455\n",
      "train: loss: 8621.3916015625 acc: 0.991980254650116  val: loss: 2987299.25 acc: 0.2243291735649109\n",
      "step: 24460\n",
      "train: loss: 18868.23828125 acc: 0.9844814538955688  val: loss: 2626228.75 acc: 0.140180766582489\n",
      "step: 24465\n",
      "train: loss: 42359.91015625 acc: 0.9791528582572937  val: loss: 1320906.0 acc: 0.7135910391807556\n",
      "step: 24470\n",
      "train: loss: 10145.109375 acc: 0.996892511844635  val: loss: 833680.1875 acc: 0.8249548673629761\n",
      "step: 24475\n",
      "train: loss: 8935.59375 acc: 0.9926019906997681  val: loss: 1038786.6875 acc: 0.7887049913406372\n",
      "step: 24480\n",
      "train: loss: 8952.4111328125 acc: 0.9941396713256836  val: loss: 3214558.75 acc: 0.3346714377403259\n",
      "step: 24485\n",
      "train: loss: 18582.78125 acc: 0.9931793212890625  val: loss: 1818700.375 acc: 0.4906771183013916\n",
      "step: 24490\n",
      "train: loss: 34139.58203125 acc: 0.9844404458999634  val: loss: 1837608.625 acc: 0.6355640888214111\n",
      "step: 24495\n",
      "train: loss: 20447.44140625 acc: 0.9931313395500183  val: loss: 1776956.625 acc: 0.5114607810974121\n",
      "step: 24500\n",
      "train: loss: 29781.041015625 acc: 0.9898698329925537  val: loss: 1089405.375 acc: 0.39426547288894653\n",
      "step: 24505\n",
      "train: loss: 31815.17578125 acc: 0.9821130037307739  val: loss: 294891.0 acc: 0.9546272158622742\n",
      "step: 24510\n",
      "train: loss: 79116.578125 acc: 0.9781657457351685  val: loss: 1471141.875 acc: 0.4866384267807007\n",
      "step: 24515\n",
      "train: loss: 43359.73828125 acc: 0.9677092432975769  val: loss: 1010042.0 acc: 0.7052640914916992\n",
      "step: 24520\n",
      "train: loss: 106983.6796875 acc: 0.9207881093025208  val: loss: 690511.75 acc: 0.839894711971283\n",
      "step: 24525\n",
      "train: loss: 672679.0625 acc: 0.8922841548919678  val: loss: 233458.96875 acc: 0.9341347813606262\n",
      "step: 24530\n",
      "train: loss: 551319.875 acc: 0.9378319382667542  val: loss: 228776.28125 acc: 0.898555338382721\n",
      "step: 24535\n",
      "train: loss: 91094.890625 acc: 0.9876344203948975  val: loss: 1287879.25 acc: 0.7417135238647461\n",
      "step: 24540\n",
      "train: loss: 793195.875 acc: 0.9019366502761841  val: loss: 220273.515625 acc: 0.9033425450325012\n",
      "step: 24545\n",
      "train: loss: 145041.203125 acc: 0.9781040549278259  val: loss: 1587893.125 acc: 0.4112154245376587\n",
      "step: 24550\n",
      "train: loss: 801770.1875 acc: 0.9640301465988159  val: loss: 552299.375 acc: 0.7765747308731079\n",
      "step: 24555\n",
      "train: loss: 300135.125 acc: 0.9766342043876648  val: loss: 692455.9375 acc: 0.8848650455474854\n",
      "step: 24560\n",
      "train: loss: 196574.28125 acc: 0.9698997735977173  val: loss: 3049983.75 acc: 0.5531289577484131\n",
      "step: 24565\n",
      "train: loss: 1125055.5 acc: 0.9060673713684082  val: loss: 1011806.75 acc: 0.8040253520011902\n",
      "step: 24570\n",
      "train: loss: 1560267.75 acc: 0.9665576219558716  val: loss: 761959.5 acc: 0.7940950989723206\n",
      "step: 24575\n",
      "train: loss: 1532021.75 acc: 0.948584258556366  val: loss: 275503.125 acc: 0.9506139159202576\n",
      "step: 24580\n",
      "train: loss: 1728801.625 acc: 0.8720256090164185  val: loss: 455092.28125 acc: 0.9202150106430054\n",
      "step: 24585\n",
      "train: loss: 532679.625 acc: 0.9465875625610352  val: loss: 528473.375 acc: 0.9580035209655762\n",
      "step: 24590\n",
      "train: loss: 1227529.5 acc: 0.8830507397651672  val: loss: 574272.125 acc: 0.9429412484169006\n",
      "step: 24595\n",
      "train: loss: 392385.125 acc: 0.9608654379844666  val: loss: 353828.78125 acc: 0.9365384578704834\n",
      "step: 24600\n",
      "train: loss: 182873.859375 acc: 0.951233983039856  val: loss: 616701.25 acc: 0.9549927711486816\n",
      "step: 24605\n",
      "train: loss: 1900662.625 acc: 0.631344735622406  val: loss: 704429.8125 acc: 0.8103071451187134\n",
      "step: 24610\n",
      "train: loss: 758501.25 acc: 0.3424801826477051  val: loss: 1063498.25 acc: 0.7642735242843628\n",
      "step: 24615\n",
      "train: loss: 785542.0 acc: 0.7877160310745239  val: loss: 2596437.75 acc: 0.7307499051094055\n",
      "step: 24620\n",
      "train: loss: 320137.28125 acc: 0.8588571548461914  val: loss: 1140007.5 acc: 0.8163162469863892\n",
      "step: 24625\n",
      "train: loss: 773724.375 acc: 0.7374573945999146  val: loss: 2213758.5 acc: 0.7560714483261108\n",
      "step: 24630\n",
      "train: loss: 764326.875 acc: 0.7276704907417297  val: loss: 1275355.75 acc: 0.7138498425483704\n",
      "step: 24635\n",
      "train: loss: 685150.875 acc: 0.7250226736068726  val: loss: 3972577.0 acc: 0.5938721299171448\n",
      "step: 24640\n",
      "train: loss: 112346.09375 acc: 0.9217836856842041  val: loss: 2357545.5 acc: 0.5949999094009399\n",
      "step: 24645\n",
      "train: loss: 111255.015625 acc: 0.9213454723358154  val: loss: 1658766.375 acc: 0.6341694593429565\n",
      "step: 24650\n",
      "train: loss: 45751.17578125 acc: 0.9639627933502197  val: loss: 679423.8125 acc: 0.7576022744178772\n",
      "step: 24655\n",
      "train: loss: 19129.328125 acc: 0.9847092628479004  val: loss: 1236678.125 acc: 0.6350674033164978\n",
      "step: 24660\n",
      "train: loss: 116027.7109375 acc: 0.9212414026260376  val: loss: 2229394.5 acc: 0.5847645998001099\n",
      "step: 24665\n",
      "train: loss: 529548.25 acc: 0.7773410081863403  val: loss: 2132472.25 acc: 0.6041858792304993\n",
      "step: 24670\n",
      "train: loss: 216701.90625 acc: 0.861791729927063  val: loss: 2251009.25 acc: 0.6702206134796143\n",
      "step: 24675\n",
      "train: loss: 42944.83984375 acc: 0.9532804489135742  val: loss: 85384.7578125 acc: 0.9223836660385132\n",
      "step: 24680\n",
      "train: loss: 121769.5703125 acc: 0.9071932435035706  val: loss: 2711975.25 acc: 0.6502068042755127\n",
      "step: 24685\n",
      "train: loss: 187396.3125 acc: 0.8523225784301758  val: loss: 1535659.5 acc: 0.6308223009109497\n",
      "step: 24690\n",
      "train: loss: 333350.0625 acc: 0.8266083002090454  val: loss: 360544.0 acc: 0.8313211798667908\n",
      "step: 24695\n",
      "train: loss: 547618.4375 acc: 0.7228185534477234  val: loss: 1969153.125 acc: 0.6342189908027649\n",
      "step: 24700\n",
      "train: loss: 37430.671875 acc: 0.9576958417892456  val: loss: 922667.75 acc: 0.7362720370292664\n",
      "step: 24705\n",
      "train: loss: 272559.9375 acc: 0.8127237558364868  val: loss: 1969620.625 acc: 0.6586270332336426\n",
      "step: 24710\n",
      "train: loss: 245849.078125 acc: 0.8401563167572021  val: loss: 904901.875 acc: 0.7256556749343872\n",
      "step: 24715\n",
      "train: loss: 2619323.25 acc: 0.6807506680488586  val: loss: 922390.0 acc: 0.7950066328048706\n",
      "step: 24720\n",
      "train: loss: 1019323.625 acc: 0.8621675968170166  val: loss: 1730775.25 acc: -0.27484655380249023\n",
      "step: 24725\n",
      "train: loss: 760385.1875 acc: 0.9442968368530273  val: loss: 979697.5 acc: 0.6587961912155151\n",
      "step: 24730\n",
      "train: loss: 489511.90625 acc: 0.9466034173965454  val: loss: 1770174.625 acc: 0.5922102928161621\n",
      "step: 24735\n",
      "train: loss: 127237.0 acc: 0.9827282428741455  val: loss: 1658023.125 acc: 0.6594700813293457\n",
      "step: 24740\n",
      "train: loss: 110237.5703125 acc: 0.9821217656135559  val: loss: 2001965.125 acc: 0.46490150690078735\n",
      "step: 24745\n",
      "train: loss: 127812.5 acc: 0.9861284494400024  val: loss: 880266.9375 acc: 0.8818117380142212\n",
      "step: 24750\n",
      "train: loss: 226489.09375 acc: 0.9839433431625366  val: loss: 844245.9375 acc: 0.8245150446891785\n",
      "step: 24755\n",
      "train: loss: 43948.109375 acc: 0.9968612790107727  val: loss: 956016.5625 acc: 0.662558913230896\n",
      "step: 24760\n",
      "train: loss: 70203.1171875 acc: 0.9925702214241028  val: loss: 531339.0 acc: 0.9160972833633423\n",
      "step: 24765\n",
      "train: loss: 52153.96875 acc: 0.9877601861953735  val: loss: 491508.375 acc: 0.9509475827217102\n",
      "step: 24770\n",
      "train: loss: 25533.087890625 acc: 0.994189441204071  val: loss: 1473653.625 acc: 0.654599666595459\n",
      "step: 24775\n",
      "train: loss: 5992.9794921875 acc: 0.9819034337997437  val: loss: 525662.0 acc: 0.4530521631240845\n",
      "step: 24780\n",
      "train: loss: 9839.0810546875 acc: 0.9778726100921631  val: loss: 601396.8125 acc: 0.8649529814720154\n",
      "step: 24785\n",
      "train: loss: 7563.318359375 acc: 0.9851928353309631  val: loss: 162411.46875 acc: 0.948900580406189\n",
      "step: 24790\n",
      "train: loss: 11013.88671875 acc: 0.9821684956550598  val: loss: 840607.5625 acc: 0.9148581624031067\n",
      "step: 24795\n",
      "train: loss: 55559.578125 acc: 0.9606292247772217  val: loss: 838035.125 acc: 0.30703020095825195\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 24800\n",
      "train: loss: 18940.93359375 acc: 0.9722115397453308  val: loss: 1641245.625 acc: 0.710861325263977\n",
      "step: 24805\n",
      "train: loss: 8429.5380859375 acc: 0.96453458070755  val: loss: 918225.5625 acc: 0.879917562007904\n",
      "step: 24810\n",
      "train: loss: 16436.283203125 acc: 0.967384397983551  val: loss: 1405188.125 acc: 0.5999491214752197\n",
      "step: 24815\n",
      "train: loss: 14812.4287109375 acc: 0.9833343625068665  val: loss: 1135127.75 acc: 0.7293026447296143\n",
      "step: 24820\n",
      "train: loss: 36979.8671875 acc: 0.9711759686470032  val: loss: 125656.3203125 acc: 0.9757832884788513\n",
      "step: 24825\n",
      "train: loss: 42583.48046875 acc: 0.9746423959732056  val: loss: 2294651.75 acc: 0.441622793674469\n",
      "step: 24830\n",
      "train: loss: 7790.71435546875 acc: 0.9943087697029114  val: loss: 124398.0625 acc: 0.9685641527175903\n",
      "step: 24835\n",
      "train: loss: 44458.26953125 acc: 0.966235876083374  val: loss: 480647.90625 acc: 0.8436620235443115\n",
      "step: 24840\n",
      "train: loss: 16628.037109375 acc: 0.9872328042984009  val: loss: 480051.625 acc: 0.8485934138298035\n",
      "step: 24845\n",
      "train: loss: 10289.994140625 acc: 0.98915696144104  val: loss: 1836687.625 acc: 0.2491866946220398\n",
      "step: 24850\n",
      "train: loss: 11951.587890625 acc: 0.9929403066635132  val: loss: 303127.75 acc: 0.8935989141464233\n",
      "step: 24855\n",
      "train: loss: 30705.796875 acc: 0.9901009202003479  val: loss: 246455.640625 acc: 0.9750910401344299\n",
      "step: 24860\n",
      "train: loss: 28918.1875 acc: 0.9913110136985779  val: loss: 756387.0 acc: 0.8834242224693298\n",
      "step: 24865\n",
      "train: loss: 42358.94140625 acc: 0.9857497215270996  val: loss: 401955.75 acc: 0.823047935962677\n",
      "step: 24870\n",
      "train: loss: 40860.27734375 acc: 0.9927144050598145  val: loss: 375196.90625 acc: 0.9415444135665894\n",
      "step: 24875\n",
      "train: loss: 108760.8046875 acc: 0.9686534404754639  val: loss: 984492.3125 acc: 0.7836378216743469\n",
      "step: 24880\n",
      "train: loss: 111120.109375 acc: 0.9790170788764954  val: loss: 175566.5 acc: 0.9008418321609497\n",
      "step: 24885\n",
      "train: loss: 33504.37890625 acc: 0.9572724103927612  val: loss: 1837662.875 acc: 0.7012206315994263\n",
      "step: 24890\n",
      "train: loss: 321232.96875 acc: 0.9453718066215515  val: loss: 265807.9375 acc: 0.9327964782714844\n",
      "step: 24895\n",
      "train: loss: 308810.34375 acc: 0.9715544581413269  val: loss: 1814814.75 acc: 0.5873728394508362\n",
      "step: 24900\n",
      "train: loss: 547486.75 acc: 0.9071406126022339  val: loss: 748934.75 acc: 0.8650614619255066\n",
      "step: 24905\n",
      "train: loss: 128347.6875 acc: 0.98382568359375  val: loss: 246854.171875 acc: 0.9505895376205444\n",
      "step: 24910\n",
      "train: loss: 165993.484375 acc: 0.9609237909317017  val: loss: 284805.75 acc: 0.943059504032135\n",
      "step: 24915\n",
      "train: loss: 665733.75 acc: 0.9688608646392822  val: loss: 342448.53125 acc: 0.9581943154335022\n",
      "step: 24920\n",
      "train: loss: 497298.0625 acc: 0.969419538974762  val: loss: 660934.875 acc: 0.9211705327033997\n",
      "step: 24925\n",
      "train: loss: 165920.453125 acc: 0.9857431650161743  val: loss: 516902.40625 acc: 0.8153196573257446\n",
      "step: 24930\n",
      "train: loss: 1283716.375 acc: 0.9226058721542358  val: loss: 1499799.375 acc: 0.6249302625656128\n",
      "step: 24935\n",
      "train: loss: 898341.4375 acc: 0.9709348678588867  val: loss: 1190031.375 acc: 0.7206819653511047\n",
      "step: 24940\n",
      "train: loss: 1170297.625 acc: 0.9652238488197327  val: loss: 1294935.625 acc: 0.8299645185470581\n",
      "step: 24945\n",
      "train: loss: 1198580.625 acc: 0.9455187916755676  val: loss: 1470526.25 acc: 0.7372196316719055\n",
      "step: 24950\n",
      "train: loss: 794868.5 acc: 0.9314414858818054  val: loss: 1688417.625 acc: 0.7668861150741577\n",
      "step: 24955\n",
      "train: loss: 167818.84375 acc: 0.9840043783187866  val: loss: 1771538.875 acc: 0.6833500862121582\n",
      "step: 24960\n",
      "train: loss: 201867.296875 acc: 0.9502494931221008  val: loss: 1327577.5 acc: 0.9128137230873108\n",
      "step: 24965\n",
      "train: loss: 524501.375 acc: 0.8898681998252869  val: loss: 1000429.375 acc: 0.893847644329071\n",
      "step: 24970\n",
      "train: loss: 1080027.5 acc: 0.6227519512176514  val: loss: 678439.625 acc: 0.7446370124816895\n",
      "step: 24975\n",
      "train: loss: 487385.75 acc: 0.8170759677886963  val: loss: 1648479.75 acc: 0.7627513408660889\n",
      "step: 24980\n",
      "train: loss: 626685.125 acc: 0.8578436970710754  val: loss: 718630.0625 acc: 0.7442856431007385\n",
      "step: 24985\n",
      "train: loss: 648584.4375 acc: 0.8412420749664307  val: loss: 483600.09375 acc: 0.8142131567001343\n",
      "step: 24990\n",
      "train: loss: 930231.1875 acc: 0.6559991836547852  val: loss: 2170443.25 acc: 0.7146890163421631\n",
      "step: 24995\n",
      "train: loss: 493331.40625 acc: 0.6497392654418945  val: loss: 1926795.125 acc: 0.6823176145553589\n",
      "step: 25000\n",
      "train: loss: 70347.40625 acc: 0.945442795753479  val: loss: 208394.109375 acc: 0.8411833047866821\n",
      "step: 25005\n",
      "train: loss: 561094.3125 acc: 0.7691037058830261  val: loss: 2404563.5 acc: 0.6141583919525146\n",
      "step: 25010\n",
      "train: loss: 310098.84375 acc: 0.8168966770172119  val: loss: 2740517.25 acc: 0.5721191167831421\n",
      "step: 25015\n",
      "train: loss: 59917.5703125 acc: 0.955886721611023  val: loss: 1961608.875 acc: 0.643302321434021\n",
      "step: 25020\n",
      "train: loss: 33617.1171875 acc: 0.9727859497070312  val: loss: 2500340.0 acc: 0.5484702587127686\n",
      "step: 25025\n",
      "train: loss: 53385.90625 acc: 0.9543908834457397  val: loss: 263227.125 acc: 0.8531084656715393\n",
      "step: 25030\n",
      "train: loss: 70596.5078125 acc: 0.9387293457984924  val: loss: 4607494.0 acc: 0.4898766875267029\n",
      "step: 25035\n",
      "train: loss: 221333.25 acc: 0.862866997718811  val: loss: 690903.375 acc: 0.7340203523635864\n",
      "step: 25040\n",
      "train: loss: 37576.84765625 acc: 0.9631996154785156  val: loss: 2129789.25 acc: 0.6328373551368713\n",
      "step: 25045\n",
      "train: loss: 35685.09375 acc: 0.9608100056648254  val: loss: 1166955.625 acc: 0.6611384749412537\n",
      "step: 25050\n",
      "train: loss: 542994.3125 acc: 0.7513432502746582  val: loss: 104519.546875 acc: 0.9071073532104492\n",
      "step: 25055\n",
      "train: loss: 125826.1484375 acc: 0.9020557999610901  val: loss: 1537536.875 acc: 0.6344913244247437\n",
      "step: 25060\n",
      "train: loss: 157894.3125 acc: 0.8867748975753784  val: loss: 1029795.0625 acc: 0.6870899200439453\n",
      "step: 25065\n",
      "train: loss: 75818.0234375 acc: 0.9366596341133118  val: loss: 913400.375 acc: 0.672113835811615\n",
      "step: 25070\n",
      "train: loss: 218260.390625 acc: 0.82984459400177  val: loss: 913784.75 acc: 0.7230188250541687\n",
      "step: 25075\n",
      "train: loss: 253796.40625 acc: 0.8638377785682678  val: loss: 1009301.5 acc: 0.6581320762634277\n",
      "step: 25080\n",
      "train: loss: 2642659.75 acc: 0.6971521377563477  val: loss: 951895.375 acc: 0.7942380905151367\n",
      "step: 25085\n",
      "train: loss: 459408.65625 acc: 0.7222691774368286  val: loss: 885117.0625 acc: 0.8560172915458679\n",
      "step: 25090\n",
      "train: loss: 685484.3125 acc: 0.9560906887054443  val: loss: 614378.0625 acc: 0.8810522556304932\n",
      "step: 25095\n",
      "train: loss: 142601.875 acc: 0.9859020113945007  val: loss: 1743853.75 acc: 0.8502620458602905\n",
      "step: 25100\n",
      "train: loss: 148129.390625 acc: 0.9834067225456238  val: loss: 1239825.125 acc: 0.715836763381958\n",
      "step: 25105\n",
      "train: loss: 63914.71875 acc: 0.9870421290397644  val: loss: 1262752.875 acc: 0.8538839817047119\n",
      "step: 25110\n",
      "train: loss: 65307.01171875 acc: 0.9946894645690918  val: loss: 1239469.375 acc: 0.811576783657074\n",
      "step: 25115\n",
      "train: loss: 44901.8046875 acc: 0.9965851306915283  val: loss: 653098.6875 acc: 0.9418257474899292\n",
      "step: 25120\n",
      "train: loss: 79536.6015625 acc: 0.9921242594718933  val: loss: 651235.5 acc: 0.718979001045227\n",
      "step: 25125\n",
      "train: loss: 50566.04296875 acc: 0.9945005178451538  val: loss: 1658783.375 acc: 0.7067514657974243\n",
      "step: 25130\n",
      "train: loss: 43254.984375 acc: 0.9915709495544434  val: loss: 262126.859375 acc: 0.8616403937339783\n",
      "step: 25135\n",
      "train: loss: 13920.4267578125 acc: 0.9942947030067444  val: loss: 605774.875 acc: 0.8387743234634399\n",
      "step: 25140\n",
      "train: loss: 24056.859375 acc: 0.993243932723999  val: loss: 584717.5625 acc: 0.8305648565292358\n",
      "step: 25145\n",
      "train: loss: 7582.59765625 acc: 0.9674781560897827  val: loss: 707020.6875 acc: 0.8715580701828003\n",
      "step: 25150\n",
      "train: loss: 14260.8603515625 acc: 0.9734532833099365  val: loss: 828898.8125 acc: 0.2867708206176758\n",
      "step: 25155\n",
      "train: loss: 48132.74609375 acc: 0.9788066148757935  val: loss: 119673.09375 acc: 0.9800553917884827\n",
      "step: 25160\n",
      "train: loss: 16289.4560546875 acc: 0.9885078072547913  val: loss: 68520.0078125 acc: 0.9840261936187744\n",
      "step: 25165\n",
      "train: loss: 5127.5859375 acc: 0.9820738434791565  val: loss: 180227.0625 acc: 0.9785816073417664\n",
      "step: 25170\n",
      "train: loss: 23921.771484375 acc: 0.9681245684623718  val: loss: 52424.1640625 acc: 0.978314220905304\n",
      "step: 25175\n",
      "train: loss: 15083.515625 acc: 0.9838946461677551  val: loss: 290036.46875 acc: 0.8452401161193848\n",
      "step: 25180\n",
      "train: loss: 26109.98828125 acc: 0.9590619802474976  val: loss: 1188766.125 acc: 0.8204840421676636\n",
      "step: 25185\n",
      "train: loss: 16683.876953125 acc: 0.9904335141181946  val: loss: 1003650.0625 acc: 0.8445900082588196\n",
      "step: 25190\n",
      "train: loss: 15957.1015625 acc: 0.9855886697769165  val: loss: 662490.75 acc: 0.8195693492889404\n",
      "step: 25195\n",
      "train: loss: 21979.595703125 acc: 0.9893592000007629  val: loss: 995051.25 acc: 0.8842232823371887\n",
      "step: 25200\n",
      "train: loss: 50845.984375 acc: 0.9822123646736145  val: loss: 1650790.625 acc: 0.8056890368461609\n",
      "step: 25205\n",
      "train: loss: 8382.7119140625 acc: 0.9860865473747253  val: loss: 1211383.625 acc: 0.7961649894714355\n",
      "step: 25210\n",
      "train: loss: 13146.3974609375 acc: 0.9817091822624207  val: loss: 1122188.375 acc: 0.8326382040977478\n",
      "step: 25215\n",
      "train: loss: 15486.6181640625 acc: 0.9939747452735901  val: loss: 609829.875 acc: 0.9329957962036133\n",
      "step: 25220\n",
      "train: loss: 29039.54296875 acc: 0.9940628409385681  val: loss: 699996.6875 acc: 0.811115026473999\n",
      "step: 25225\n",
      "train: loss: 33859.46875 acc: 0.9907399415969849  val: loss: 409260.96875 acc: 0.9264321327209473\n",
      "step: 25230\n",
      "train: loss: 17245.015625 acc: 0.9872986078262329  val: loss: 1243320.25 acc: 0.5559932589530945\n",
      "step: 25235\n",
      "train: loss: 42593.1875 acc: 0.9753426313400269  val: loss: 796186.1875 acc: 0.9281907081604004\n",
      "step: 25240\n",
      "train: loss: 40726.25390625 acc: 0.9901098608970642  val: loss: 1156303.875 acc: 0.897243320941925\n",
      "step: 25245\n",
      "train: loss: 265791.21875 acc: 0.8989541530609131  val: loss: 727822.8125 acc: 0.7966868877410889\n",
      "step: 25250\n",
      "train: loss: 95584.4453125 acc: 0.9546093344688416  val: loss: 1416689.875 acc: 0.9062700867652893\n",
      "step: 25255\n",
      "train: loss: 1036825.8125 acc: 0.7286670207977295  val: loss: 620095.625 acc: 0.8713626265525818\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 25260\n",
      "train: loss: 75759.5859375 acc: 0.9916302561759949  val: loss: 613490.3125 acc: 0.8430150747299194\n",
      "step: 25265\n",
      "train: loss: 89456.0 acc: 0.9908896684646606  val: loss: 1018098.8125 acc: 0.8338440656661987\n",
      "step: 25270\n",
      "train: loss: 533439.0 acc: 0.8704487085342407  val: loss: 892013.3125 acc: 0.7007685303688049\n",
      "step: 25275\n",
      "train: loss: 56947.41796875 acc: 0.9866078495979309  val: loss: 2668867.5 acc: 0.3561713695526123\n",
      "step: 25280\n",
      "train: loss: 97037.1171875 acc: 0.9899765849113464  val: loss: 2291143.75 acc: 0.6236350536346436\n",
      "step: 25285\n",
      "train: loss: 901075.5625 acc: 0.8809593915939331  val: loss: 1633098.5 acc: 0.5980767011642456\n",
      "step: 25290\n",
      "train: loss: 285264.3125 acc: 0.9734640121459961  val: loss: 571609.9375 acc: 0.9132070541381836\n",
      "step: 25295\n",
      "train: loss: 545815.75 acc: 0.9543900489807129  val: loss: 2862409.0 acc: 0.49628323316574097\n",
      "step: 25300\n",
      "train: loss: 871525.375 acc: 0.9685202836990356  val: loss: 186393.71875 acc: 0.9409163594245911\n",
      "step: 25305\n",
      "train: loss: 853518.6875 acc: 0.9718270301818848  val: loss: 1084852.0 acc: 0.3116714358329773\n",
      "step: 25310\n",
      "train: loss: 832256.125 acc: 0.9610850214958191  val: loss: 752606.75 acc: 0.8978438377380371\n",
      "step: 25315\n",
      "train: loss: 342667.03125 acc: 0.9813174605369568  val: loss: 367513.90625 acc: 0.8911124467849731\n",
      "step: 25320\n",
      "train: loss: 277240.59375 acc: 0.9715802073478699  val: loss: 643205.1875 acc: 0.9247307777404785\n",
      "step: 25325\n",
      "train: loss: 440640.34375 acc: 0.8686909079551697  val: loss: 752777.375 acc: 0.6287413835525513\n",
      "step: 25330\n",
      "train: loss: 359323.40625 acc: 0.9011142253875732  val: loss: 177486.375 acc: 0.8884438276290894\n",
      "step: 25335\n",
      "train: loss: 2656083.0 acc: 0.7463114857673645  val: loss: 1319294.0 acc: 0.8623107671737671\n",
      "step: 25340\n",
      "train: loss: 1043985.5 acc: 0.7268545627593994  val: loss: 1121262.875 acc: 0.6996790170669556\n",
      "step: 25345\n",
      "train: loss: 259989.765625 acc: 0.8928554058074951  val: loss: 353216.28125 acc: 0.8237298727035522\n",
      "step: 25350\n",
      "train: loss: 461518.71875 acc: 0.8257534503936768  val: loss: 1150385.5 acc: 0.8381571769714355\n",
      "step: 25355\n",
      "train: loss: 1002317.9375 acc: 0.690009593963623  val: loss: 1023061.5 acc: 0.7655969858169556\n",
      "step: 25360\n",
      "train: loss: 657552.0625 acc: 0.713935911655426  val: loss: 176656.96875 acc: 0.8486968278884888\n",
      "step: 25365\n",
      "train: loss: 150006.59375 acc: 0.874936580657959  val: loss: 1562200.625 acc: 0.6479288339614868\n",
      "step: 25370\n",
      "train: loss: 111614.34375 acc: 0.9263159036636353  val: loss: 4095888.75 acc: 0.523646354675293\n",
      "step: 25375\n",
      "train: loss: 95560.140625 acc: 0.9302365779876709  val: loss: 123179.765625 acc: 0.8630427122116089\n",
      "step: 25380\n",
      "train: loss: 77317.25 acc: 0.9322448968887329  val: loss: 1610729.625 acc: 0.6967809796333313\n",
      "step: 25385\n",
      "train: loss: 45891.8046875 acc: 0.9649583697319031  val: loss: 1508604.875 acc: 0.673957884311676\n",
      "step: 25390\n",
      "train: loss: 78470.984375 acc: 0.943917453289032  val: loss: 1293658.625 acc: 0.6924127340316772\n",
      "step: 25395\n",
      "train: loss: 398887.96875 acc: 0.7751826643943787  val: loss: 1773379.875 acc: 0.6588163375854492\n",
      "step: 25400\n",
      "train: loss: 71369.8359375 acc: 0.9293253421783447  val: loss: 2453739.75 acc: 0.584510326385498\n",
      "step: 25405\n",
      "train: loss: 478571.0625 acc: 0.7706611156463623  val: loss: 949421.75 acc: 0.7174842357635498\n",
      "step: 25410\n",
      "train: loss: 38640.87890625 acc: 0.9654374718666077  val: loss: 951089.5 acc: 0.680970311164856\n",
      "step: 25415\n",
      "train: loss: 259510.140625 acc: 0.839165210723877  val: loss: 1478562.375 acc: 0.6275854110717773\n",
      "step: 25420\n",
      "train: loss: 569968.6875 acc: 0.7383543848991394  val: loss: 979458.4375 acc: 0.6739259958267212\n",
      "step: 25425\n",
      "train: loss: 462902.65625 acc: 0.7868373394012451  val: loss: 7431350.0 acc: 0.5260433554649353\n",
      "step: 25430\n",
      "train: loss: 220834.65625 acc: 0.8214200735092163  val: loss: 437349.03125 acc: 0.7165526151657104\n",
      "step: 25435\n",
      "train: loss: 221587.84375 acc: 0.8160139918327332  val: loss: 67446.203125 acc: 0.9088122844696045\n",
      "step: 25440\n",
      "train: loss: 171866.734375 acc: 0.8761768937110901  val: loss: 3104131.25 acc: 0.6010041236877441\n",
      "step: 25445\n",
      "train: loss: 1463425.625 acc: 0.7013013362884521  val: loss: 1807877.375 acc: 0.6358488202095032\n",
      "step: 25450\n",
      "train: loss: 1046449.0 acc: 0.7567154169082642  val: loss: 723995.5 acc: 0.7717064619064331\n",
      "step: 25455\n",
      "train: loss: 770569.1875 acc: 0.938932478427887  val: loss: 1174693.375 acc: 0.6720262765884399\n",
      "step: 25460\n",
      "train: loss: 236379.171875 acc: 0.9773868918418884  val: loss: 1328737.375 acc: 0.5040873289108276\n",
      "step: 25465\n",
      "train: loss: 884520.0 acc: 0.9017410278320312  val: loss: 541567.8125 acc: 0.7349725961685181\n",
      "step: 25470\n",
      "train: loss: 137518.84375 acc: 0.9836710095405579  val: loss: 1193796.25 acc: 0.06518179178237915\n",
      "step: 25475\n",
      "train: loss: 126987.078125 acc: 0.9879117608070374  val: loss: 422353.3125 acc: 0.8796191215515137\n",
      "step: 25480\n",
      "train: loss: 88545.6484375 acc: 0.991291880607605  val: loss: 2177629.5 acc: 0.18788129091262817\n",
      "step: 25485\n",
      "train: loss: 18958.458984375 acc: 0.9986929893493652  val: loss: 784586.4375 acc: 0.9051290154457092\n",
      "step: 25490\n",
      "train: loss: 21908.17578125 acc: 0.996759831905365  val: loss: 294003.9375 acc: 0.8996691107749939\n",
      "step: 25495\n",
      "train: loss: 26685.98828125 acc: 0.9955253601074219  val: loss: 79284.921875 acc: 0.978025496006012\n",
      "step: 25500\n",
      "train: loss: 7914.7412109375 acc: 0.9964374303817749  val: loss: 907256.8125 acc: 0.8122715353965759\n",
      "step: 25505\n",
      "train: loss: 17041.080078125 acc: 0.996013879776001  val: loss: 1869452.625 acc: 0.6559491157531738\n",
      "step: 25510\n",
      "train: loss: 17818.033203125 acc: 0.9925693273544312  val: loss: 1457224.5 acc: 0.6790388822555542\n",
      "step: 25515\n",
      "train: loss: 14158.7802734375 acc: 0.9929723739624023  val: loss: 286085.53125 acc: 0.9339063167572021\n",
      "step: 25520\n",
      "train: loss: 11675.7373046875 acc: 0.993078351020813  val: loss: 856060.5 acc: 0.6818702220916748\n",
      "step: 25525\n",
      "train: loss: 245051.90625 acc: 0.6409229040145874  val: loss: 786725.4375 acc: 0.8842226266860962\n",
      "step: 25530\n",
      "train: loss: 8114.65283203125 acc: 0.978676438331604  val: loss: 1191850.125 acc: 0.8284391164779663\n",
      "step: 25535\n",
      "train: loss: 18802.126953125 acc: 0.9810202717781067  val: loss: 235121.625 acc: 0.9474246501922607\n",
      "step: 25540\n",
      "train: loss: 18992.943359375 acc: 0.9784505367279053  val: loss: 962238.0 acc: 0.9370773434638977\n",
      "step: 25545\n",
      "train: loss: 27493.236328125 acc: 0.9493528008460999  val: loss: 330366.71875 acc: 0.955344021320343\n",
      "step: 25550\n",
      "train: loss: 34304.8125 acc: 0.9547748565673828  val: loss: 1162131.875 acc: 0.8752692937850952\n",
      "step: 25555\n",
      "train: loss: 25240.275390625 acc: 0.9744776487350464  val: loss: 1034714.5625 acc: 0.8202056288719177\n",
      "step: 25560\n",
      "train: loss: 14447.2119140625 acc: 0.9880942702293396  val: loss: 1700320.25 acc: 0.7835192084312439\n",
      "step: 25565\n",
      "train: loss: 15342.78125 acc: 0.9891612529754639  val: loss: 1344070.0 acc: 0.8476765751838684\n",
      "step: 25570\n",
      "train: loss: 7638.19775390625 acc: 0.9940074682235718  val: loss: 1616671.625 acc: 0.80616295337677\n",
      "step: 25575\n",
      "train: loss: 10087.5390625 acc: 0.9889558553695679  val: loss: 1643462.375 acc: 0.7322767376899719\n",
      "step: 25580\n",
      "train: loss: 28624.484375 acc: 0.988167941570282  val: loss: 1229913.875 acc: 0.5950523614883423\n",
      "step: 25585\n",
      "train: loss: 21470.71875 acc: 0.9906312227249146  val: loss: 621842.6875 acc: 0.9425591230392456\n",
      "step: 25590\n",
      "train: loss: 11130.7978515625 acc: 0.9955500364303589  val: loss: 2306446.75 acc: 0.5202927589416504\n",
      "step: 25595\n",
      "train: loss: 44857.76171875 acc: 0.9874792695045471  val: loss: 264515.09375 acc: 0.9527484774589539\n",
      "step: 25600\n",
      "train: loss: 19848.5859375 acc: 0.990677535533905  val: loss: 1809471.75 acc: 0.8157662153244019\n",
      "step: 25605\n",
      "train: loss: 39463.8515625 acc: 0.9889383316040039  val: loss: 1840085.125 acc: 0.49060338735580444\n",
      "step: 25610\n",
      "train: loss: 109526.7578125 acc: 0.9706673622131348  val: loss: 1122654.125 acc: 0.6702837347984314\n",
      "step: 25615\n",
      "train: loss: 90009.2109375 acc: 0.967188835144043  val: loss: 1109961.375 acc: 0.846223771572113\n",
      "step: 25620\n",
      "train: loss: 54391.203125 acc: 0.9845579266548157  val: loss: 1505131.0 acc: 0.8596478700637817\n",
      "step: 25625\n",
      "train: loss: 63283.796875 acc: 0.9936001300811768  val: loss: 1508695.75 acc: 0.050235629081726074\n",
      "step: 25630\n",
      "train: loss: 159227.96875 acc: 0.9832524061203003  val: loss: 692100.0625 acc: 0.47144418954849243\n",
      "step: 25635\n",
      "train: loss: 189077.703125 acc: 0.9734108448028564  val: loss: 370302.53125 acc: 0.9577481746673584\n",
      "step: 25640\n",
      "train: loss: 73180.75 acc: 0.9932997822761536  val: loss: 1596536.125 acc: 0.7124927639961243\n",
      "step: 25645\n",
      "train: loss: 185186.3125 acc: 0.9845237731933594  val: loss: 3214686.5 acc: -0.19489407539367676\n",
      "step: 25650\n",
      "train: loss: 287120.625 acc: 0.9785130620002747  val: loss: 2751341.0 acc: -1.3195257186889648\n",
      "step: 25655\n",
      "train: loss: 323945.53125 acc: 0.9591509103775024  val: loss: 688721.625 acc: 0.7030878067016602\n",
      "step: 25660\n",
      "train: loss: 238993.859375 acc: 0.9716442227363586  val: loss: 1984763.125 acc: 0.44763773679733276\n",
      "step: 25665\n",
      "train: loss: 257668.296875 acc: 0.9849365949630737  val: loss: 1865821.125 acc: 0.27856457233428955\n",
      "step: 25670\n",
      "train: loss: 854511.3125 acc: 0.9693261981010437  val: loss: 842300.4375 acc: 0.9325894117355347\n",
      "step: 25675\n",
      "train: loss: 1138231.0 acc: 0.9357199668884277  val: loss: 1670861.125 acc: 0.7699393630027771\n",
      "step: 25680\n",
      "train: loss: 341706.21875 acc: 0.9721481204032898  val: loss: 1407883.75 acc: 0.5916991829872131\n",
      "step: 25685\n",
      "train: loss: 419147.59375 acc: 0.9676356315612793  val: loss: 230444.0625 acc: 0.9692032933235168\n",
      "step: 25690\n",
      "train: loss: 775988.4375 acc: 0.8674677610397339  val: loss: 142890.875 acc: 0.9581966996192932\n",
      "step: 25695\n",
      "train: loss: 666039.75 acc: 0.8483995795249939  val: loss: 959676.4375 acc: 0.8778462409973145\n",
      "step: 25700\n",
      "train: loss: 1614035.375 acc: 0.6469768285751343  val: loss: 715086.5 acc: 0.6954991817474365\n",
      "step: 25705\n",
      "train: loss: 1794287.875 acc: 0.7156356573104858  val: loss: 968720.375 acc: 0.7953826189041138\n",
      "step: 25710\n",
      "train: loss: 1259049.75 acc: 0.7200722694396973  val: loss: 1066845.625 acc: 0.7894818186759949\n",
      "step: 25715\n",
      "train: loss: 307641.96875 acc: 0.8457292914390564  val: loss: 999736.5 acc: 0.8213081955909729\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 25720\n",
      "train: loss: 466586.25 acc: 0.85867840051651  val: loss: 597818.5625 acc: 0.8587031364440918\n",
      "step: 25725\n",
      "train: loss: 1021737.75 acc: 0.44367992877960205  val: loss: 826302.9375 acc: 0.8030651807785034\n",
      "step: 25730\n",
      "train: loss: 459546.25 acc: 0.8099657893180847  val: loss: 2599806.75 acc: 0.5813248753547668\n",
      "step: 25735\n",
      "train: loss: 241588.59375 acc: 0.872236430644989  val: loss: 4139021.0 acc: 0.5701540112495422\n",
      "step: 25740\n",
      "train: loss: 75303.9765625 acc: 0.939452588558197  val: loss: 1016768.9375 acc: 0.7168093919754028\n",
      "step: 25745\n",
      "train: loss: 57856.96484375 acc: 0.9523242712020874  val: loss: 545855.875 acc: 0.723136305809021\n",
      "step: 25750\n",
      "train: loss: 41453.29296875 acc: 0.9604071378707886  val: loss: 915864.4375 acc: 0.7063003778457642\n",
      "step: 25755\n",
      "train: loss: 152095.5 acc: 0.8965250849723816  val: loss: 2282169.75 acc: 0.5384054183959961\n",
      "step: 25760\n",
      "train: loss: 363856.5 acc: 0.836076021194458  val: loss: 2679010.5 acc: 0.6134268045425415\n",
      "step: 25765\n",
      "train: loss: 61185.61328125 acc: 0.9544938206672668  val: loss: 636182.9375 acc: 0.6755258440971375\n",
      "step: 25770\n",
      "train: loss: 171647.140625 acc: 0.8601145148277283  val: loss: 1574886.125 acc: 0.676250696182251\n",
      "step: 25775\n",
      "train: loss: 320161.0625 acc: 0.7966567873954773  val: loss: 2314606.75 acc: 0.5866439342498779\n",
      "step: 25780\n",
      "train: loss: 68842.375 acc: 0.9338454008102417  val: loss: 422532.03125 acc: 0.7491670846939087\n",
      "step: 25785\n",
      "train: loss: 196306.890625 acc: 0.887362003326416  val: loss: 1730384.25 acc: 0.6363668441772461\n",
      "step: 25790\n",
      "train: loss: 190225.390625 acc: 0.8837586045265198  val: loss: 1663035.875 acc: 0.6868002414703369\n",
      "step: 25795\n",
      "train: loss: 113352.1953125 acc: 0.877241313457489  val: loss: 697867.4375 acc: 0.7106947898864746\n",
      "step: 25800\n",
      "train: loss: 116182.359375 acc: 0.9133633971214294  val: loss: 2862609.25 acc: 0.5644288659095764\n",
      "step: 25805\n",
      "train: loss: 134141.578125 acc: 0.8809821009635925  val: loss: 959841.5625 acc: 0.7048174142837524\n",
      "step: 25810\n",
      "train: loss: 639182.4375 acc: 0.784430205821991  val: loss: 462644.28125 acc: 0.7863156199455261\n",
      "step: 25815\n",
      "train: loss: 2046246.5 acc: 0.806887686252594  val: loss: 912640.125 acc: 0.7021781802177429\n",
      "step: 25820\n",
      "train: loss: 766116.0625 acc: 0.8924589157104492  val: loss: 1124531.625 acc: 0.7615633010864258\n",
      "step: 25825\n",
      "train: loss: 684162.5 acc: 0.945950448513031  val: loss: 228016.65625 acc: 0.9546496868133545\n",
      "step: 25830\n",
      "train: loss: 97943.5078125 acc: 0.9887337684631348  val: loss: 532113.4375 acc: 0.8924067616462708\n",
      "step: 25835\n",
      "train: loss: 260638.21875 acc: 0.9626924395561218  val: loss: 175019.421875 acc: 0.9682819843292236\n",
      "step: 25840\n",
      "train: loss: 110768.859375 acc: 0.9885085821151733  val: loss: 1232840.375 acc: 0.7275476455688477\n",
      "step: 25845\n",
      "train: loss: 68258.9296875 acc: 0.9941346645355225  val: loss: 1019373.8125 acc: 0.9336660504341125\n",
      "step: 25850\n",
      "train: loss: 58854.625 acc: 0.9960992932319641  val: loss: 485107.375 acc: 0.9100534915924072\n",
      "step: 25855\n",
      "train: loss: 82645.3828125 acc: 0.9920997619628906  val: loss: 1367715.5 acc: 0.9014284610748291\n",
      "step: 25860\n",
      "train: loss: 25656.1796875 acc: 0.9926589131355286  val: loss: 1409154.0 acc: 0.8580508232116699\n",
      "step: 25865\n",
      "train: loss: 17015.91015625 acc: 0.9967526197433472  val: loss: 752620.875 acc: 0.8883861303329468\n",
      "step: 25870\n",
      "train: loss: 8182.40087890625 acc: 0.99733567237854  val: loss: 1363016.0 acc: 0.5222150087356567\n",
      "step: 25875\n",
      "train: loss: 6762.59423828125 acc: 0.997105062007904  val: loss: 887350.0 acc: 0.9160926342010498\n",
      "step: 25880\n",
      "train: loss: 21191.79296875 acc: 0.9677265882492065  val: loss: 1571473.375 acc: 0.6275774240493774\n",
      "step: 25885\n",
      "train: loss: 12568.4580078125 acc: 0.9918665289878845  val: loss: 969637.4375 acc: 0.9050023555755615\n",
      "step: 25890\n",
      "train: loss: 12527.5498046875 acc: 0.9672353267669678  val: loss: 1231049.625 acc: 0.8305740356445312\n",
      "step: 25895\n",
      "train: loss: 7392.390625 acc: 0.9793707728385925  val: loss: 1278032.625 acc: 0.8314527273178101\n",
      "step: 25900\n",
      "train: loss: 14999.4267578125 acc: 0.9661908149719238  val: loss: 991491.625 acc: 0.8859919309616089\n",
      "step: 25905\n",
      "train: loss: 12225.3212890625 acc: 0.9516475200653076  val: loss: 1236014.125 acc: 0.7360180020332336\n",
      "step: 25910\n",
      "train: loss: 4809.87255859375 acc: 0.9918192028999329  val: loss: 1680091.375 acc: 0.44851964712142944\n",
      "step: 25915\n",
      "train: loss: 31871.904296875 acc: 0.9847524762153625  val: loss: 1547397.25 acc: 0.23100793361663818\n",
      "step: 25920\n",
      "train: loss: 23511.646484375 acc: 0.9821444749832153  val: loss: 3114715.25 acc: -1.0678472518920898\n",
      "step: 25925\n",
      "train: loss: 86164.359375 acc: 0.9247543215751648  val: loss: 2607818.75 acc: -0.7690533399581909\n",
      "step: 25930\n",
      "train: loss: 24792.251953125 acc: 0.9900252819061279  val: loss: 1610288.125 acc: 0.7491083145141602\n",
      "step: 25935\n",
      "train: loss: 27183.390625 acc: 0.9848724007606506  val: loss: 920719.6875 acc: 0.8297995328903198\n",
      "step: 25940\n",
      "train: loss: 9361.1083984375 acc: 0.9764703512191772  val: loss: 1361656.375 acc: 0.6697723865509033\n",
      "step: 25945\n",
      "train: loss: 10469.0322265625 acc: 0.9946373105049133  val: loss: 2676697.5 acc: 0.6962474584579468\n",
      "step: 25950\n",
      "train: loss: 32094.166015625 acc: 0.9877021312713623  val: loss: 2661043.25 acc: -0.7898615598678589\n",
      "step: 25955\n",
      "train: loss: 31623.19140625 acc: 0.9935259222984314  val: loss: 1808534.25 acc: 0.506269097328186\n",
      "step: 25960\n",
      "train: loss: 27431.302734375 acc: 0.992148220539093  val: loss: 2026859.625 acc: 0.5332363843917847\n",
      "step: 25965\n",
      "train: loss: 20199.349609375 acc: 0.9938739538192749  val: loss: 2117345.5 acc: -0.038498878479003906\n",
      "step: 25970\n",
      "train: loss: 354513.21875 acc: 0.7644490003585815  val: loss: 2407260.0 acc: 0.41305649280548096\n",
      "step: 25975\n",
      "train: loss: 95633.0 acc: 0.9800026416778564  val: loss: 2134632.5 acc: 0.7494059801101685\n",
      "step: 25980\n",
      "train: loss: 89662.5625 acc: 0.9529218673706055  val: loss: 1712001.125 acc: 0.06953245401382446\n",
      "step: 25985\n",
      "train: loss: 430408.9375 acc: 0.9288762807846069  val: loss: 1811809.625 acc: 0.1960827112197876\n",
      "step: 25990\n",
      "train: loss: 139642.78125 acc: 0.9853134751319885  val: loss: 2519935.5 acc: 0.5756145715713501\n",
      "step: 25995\n",
      "train: loss: 547291.25 acc: 0.9420158267021179  val: loss: 1778699.25 acc: 0.5524622797966003\n",
      "step: 26000\n",
      "train: loss: 1054564.375 acc: 0.8658567070960999  val: loss: 410654.28125 acc: 0.9196875095367432\n",
      "step: 26005\n",
      "train: loss: 489140.8125 acc: 0.9541999697685242  val: loss: 1247761.625 acc: 0.8616877198219299\n",
      "step: 26010\n",
      "train: loss: 299496.625 acc: 0.9656885266304016  val: loss: 807329.9375 acc: 0.9246914982795715\n",
      "step: 26015\n",
      "train: loss: 303862.75 acc: 0.9833604693412781  val: loss: 448113.375 acc: 0.9371858835220337\n",
      "step: 26020\n",
      "train: loss: 985746.8125 acc: 0.9351024031639099  val: loss: 3841216.5 acc: 0.1333330273628235\n",
      "step: 26025\n",
      "train: loss: 272653.25 acc: 0.9349274635314941  val: loss: 490583.0 acc: 0.9228951930999756\n",
      "step: 26030\n",
      "train: loss: 206084.5 acc: 0.9907258152961731  val: loss: 312495.75 acc: 0.8402805924415588\n",
      "step: 26035\n",
      "train: loss: 993725.9375 acc: 0.9626690149307251  val: loss: 675696.875 acc: 0.8132763504981995\n",
      "step: 26040\n",
      "train: loss: 721363.4375 acc: 0.9760941863059998  val: loss: 730888.1875 acc: 0.6773325204849243\n",
      "step: 26045\n",
      "train: loss: 1505725.375 acc: 0.8499106168746948  val: loss: 73701.5625 acc: 0.9572012424468994\n",
      "step: 26050\n",
      "train: loss: 260146.328125 acc: 0.9505938291549683  val: loss: 1302335.0 acc: 0.5857374668121338\n",
      "step: 26055\n",
      "train: loss: 677272.25 acc: 0.9353893399238586  val: loss: 351418.75 acc: 0.9500552415847778\n",
      "step: 26060\n",
      "train: loss: 610904.4375 acc: 0.9267851114273071  val: loss: 339220.0625 acc: 0.8704187273979187\n",
      "step: 26065\n",
      "train: loss: 1839112.125 acc: 0.44133758544921875  val: loss: 727949.5625 acc: 0.7734864354133606\n",
      "step: 26070\n",
      "train: loss: 1196770.875 acc: 0.7218237519264221  val: loss: 616256.4375 acc: 0.7328256368637085\n",
      "step: 26075\n",
      "train: loss: 1537170.375 acc: 0.7938889265060425  val: loss: 941093.125 acc: 0.7074153423309326\n",
      "step: 26080\n",
      "train: loss: 431861.0 acc: 0.7951603531837463  val: loss: 741484.5625 acc: 0.8608304262161255\n",
      "step: 26085\n",
      "train: loss: 1154305.0 acc: 0.8389345407485962  val: loss: 455655.8125 acc: 0.8651635646820068\n",
      "step: 26090\n",
      "train: loss: 1039419.0625 acc: 0.6750240921974182  val: loss: 677921.9375 acc: 0.7709033489227295\n",
      "step: 26095\n",
      "train: loss: 508968.5 acc: 0.7572993636131287  val: loss: 832798.875 acc: 0.7306303977966309\n",
      "step: 26100\n",
      "train: loss: 516211.25 acc: 0.7646275758743286  val: loss: 268571.15625 acc: 0.8196821212768555\n",
      "step: 26105\n",
      "train: loss: 81437.1015625 acc: 0.9323801398277283  val: loss: 1221395.5 acc: 0.7308443784713745\n",
      "step: 26110\n",
      "train: loss: 54175.2265625 acc: 0.9522834420204163  val: loss: 1415517.0 acc: 0.6646837592124939\n",
      "step: 26115\n",
      "train: loss: 18475.6953125 acc: 0.9851123690605164  val: loss: 1267618.625 acc: 0.6765717267990112\n",
      "step: 26120\n",
      "train: loss: 82117.453125 acc: 0.9453349113464355  val: loss: 1791989.375 acc: 0.641686201095581\n",
      "step: 26125\n",
      "train: loss: 95558.890625 acc: 0.9157156348228455  val: loss: 1353350.375 acc: 0.6365777254104614\n",
      "step: 26130\n",
      "train: loss: 236792.015625 acc: 0.8659630417823792  val: loss: 1988107.875 acc: 0.6497529745101929\n",
      "step: 26135\n",
      "train: loss: 137976.71875 acc: 0.8709691762924194  val: loss: 915091.6875 acc: 0.705182671546936\n",
      "step: 26140\n",
      "train: loss: 26345.51171875 acc: 0.9717758893966675  val: loss: 566732.3125 acc: 0.7907705903053284\n",
      "step: 26145\n",
      "train: loss: 53131.2109375 acc: 0.9550614953041077  val: loss: 2410705.5 acc: 0.6296834945678711\n",
      "step: 26150\n",
      "train: loss: 264377.625 acc: 0.7784681916236877  val: loss: 2784490.0 acc: 0.5948677659034729\n",
      "step: 26155\n",
      "train: loss: 95803.578125 acc: 0.929154634475708  val: loss: 6187785.0 acc: 0.5691993236541748\n",
      "step: 26160\n",
      "train: loss: 326969.0 acc: 0.7815364599227905  val: loss: 1651267.75 acc: 0.7315083742141724\n",
      "step: 26165\n",
      "train: loss: 235287.765625 acc: 0.8212504982948303  val: loss: 1840503.125 acc: 0.6424074172973633\n",
      "step: 26170\n",
      "train: loss: 518767.125 acc: 0.7461101412773132  val: loss: 1881890.5 acc: 0.6700558662414551\n",
      "step: 26175\n",
      "train: loss: 1216101.25 acc: 0.6264347434043884  val: loss: 2731971.5 acc: 0.5603100657463074\n",
      "step: 26180\n",
      "train: loss: 1647697.0 acc: 0.7918482422828674  val: loss: 1596561.5 acc: 0.790042519569397\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 26185\n",
      "train: loss: 801316.4375 acc: 0.9075503945350647  val: loss: 383656.5625 acc: 0.8154777884483337\n",
      "step: 26190\n",
      "train: loss: 549509.3125 acc: 0.9672378301620483  val: loss: 588514.0625 acc: 0.9336615204811096\n",
      "step: 26195\n",
      "train: loss: 464459.25 acc: 0.8723311424255371  val: loss: 422000.46875 acc: 0.8987969756126404\n",
      "step: 26200\n",
      "train: loss: 239681.640625 acc: 0.9589382410049438  val: loss: 805906.375 acc: 0.5795369148254395\n",
      "step: 26205\n",
      "train: loss: 93732.875 acc: 0.978972852230072  val: loss: 193129.046875 acc: 0.9450209736824036\n",
      "step: 26210\n",
      "train: loss: 98574.390625 acc: 0.990943431854248  val: loss: 545027.875 acc: 0.798857569694519\n",
      "step: 26215\n",
      "train: loss: 108455.859375 acc: 0.9923226833343506  val: loss: 1205727.375 acc: 0.17946386337280273\n",
      "step: 26220\n",
      "train: loss: 112038.8125 acc: 0.989253580570221  val: loss: 3282786.75 acc: 0.39719825983047485\n",
      "step: 26225\n",
      "train: loss: 130736.5 acc: 0.9830206036567688  val: loss: 1356578.0 acc: 0.6075048446655273\n",
      "step: 26230\n",
      "train: loss: 24823.599609375 acc: 0.9934346079826355  val: loss: 1080557.25 acc: 0.9153100848197937\n",
      "step: 26235\n",
      "train: loss: 6129.01708984375 acc: 0.9946516156196594  val: loss: 1339431.5 acc: 0.8432891368865967\n",
      "step: 26240\n",
      "train: loss: 10929.6591796875 acc: 0.9770544767379761  val: loss: 740112.3125 acc: 0.6772733926773071\n",
      "step: 26245\n",
      "train: loss: 3828.665771484375 acc: 0.9914999604225159  val: loss: 135499.734375 acc: 0.8984456062316895\n",
      "step: 26250\n",
      "train: loss: 27983.6953125 acc: 0.9854365587234497  val: loss: 966751.1875 acc: 0.8236166834831238\n",
      "step: 26255\n",
      "train: loss: 248820.53125 acc: 0.5748116970062256  val: loss: 1165430.875 acc: 0.8101596832275391\n",
      "step: 26260\n",
      "train: loss: 14569.0166015625 acc: 0.9679409265518188  val: loss: 1242351.75 acc: 0.768441915512085\n",
      "step: 26265\n",
      "train: loss: 8613.1044921875 acc: 0.975751519203186  val: loss: 1170429.5 acc: 0.2826434373855591\n",
      "step: 26270\n",
      "train: loss: 8461.310546875 acc: 0.9591065645217896  val: loss: 1477080.5 acc: 0.8505587577819824\n",
      "step: 26275\n",
      "train: loss: 18294.251953125 acc: 0.9643737077713013  val: loss: 1278825.75 acc: 0.5959351062774658\n",
      "step: 26280\n",
      "train: loss: 33403.67578125 acc: 0.9739965200424194  val: loss: 813913.6875 acc: 0.8454345464706421\n",
      "step: 26285\n",
      "train: loss: 15537.8271484375 acc: 0.9879695177078247  val: loss: 1481296.25 acc: 0.7297301292419434\n",
      "step: 26290\n",
      "train: loss: 78023.40625 acc: 0.9487513303756714  val: loss: 638524.625 acc: 0.7185684442520142\n",
      "step: 26295\n",
      "train: loss: 9634.1953125 acc: 0.9955627918243408  val: loss: 3912915.75 acc: 0.03115677833557129\n",
      "step: 26300\n",
      "train: loss: 11837.873046875 acc: 0.984915554523468  val: loss: 3471342.5 acc: 0.18304169178009033\n",
      "step: 26305\n",
      "train: loss: 23085.6484375 acc: 0.9788402318954468  val: loss: 831759.3125 acc: 0.7307719588279724\n",
      "step: 26310\n",
      "train: loss: 11651.6982421875 acc: 0.9938240051269531  val: loss: 1266612.625 acc: 0.823951005935669\n",
      "step: 26315\n",
      "train: loss: 38666.35546875 acc: 0.9894521832466125  val: loss: 3186378.25 acc: 0.14458894729614258\n",
      "step: 26320\n",
      "train: loss: 41258.5234375 acc: 0.9870445728302002  val: loss: 1714527.75 acc: 0.7917553782463074\n",
      "step: 26325\n",
      "train: loss: 39999.67578125 acc: 0.9927400946617126  val: loss: 2252702.5 acc: 0.2095302939414978\n",
      "step: 26330\n",
      "train: loss: 23181.427734375 acc: 0.9880360960960388  val: loss: 407966.21875 acc: 0.9450885653495789\n",
      "step: 26335\n",
      "train: loss: 18493.599609375 acc: 0.9926750063896179  val: loss: 1534442.625 acc: 0.5451449155807495\n",
      "step: 26340\n",
      "train: loss: 82626.0625 acc: 0.9809352159500122  val: loss: 220656.953125 acc: 0.9566195011138916\n",
      "step: 26345\n",
      "train: loss: 139340.734375 acc: 0.9671238660812378  val: loss: 1675645.0 acc: 0.6946396827697754\n",
      "step: 26350\n",
      "train: loss: 47518.40234375 acc: 0.9587695598602295  val: loss: 1765691.25 acc: 0.5986613035202026\n",
      "step: 26355\n",
      "train: loss: 305406.78125 acc: 0.9465585947036743  val: loss: 561350.625 acc: 0.9076064825057983\n",
      "step: 26360\n",
      "train: loss: 70005.59375 acc: 0.9940413236618042  val: loss: 1867723.5 acc: 0.7720469236373901\n",
      "step: 26365\n",
      "train: loss: 199907.96875 acc: 0.9818128943443298  val: loss: 3310984.25 acc: -0.7514430284500122\n",
      "step: 26370\n",
      "train: loss: 90220.7578125 acc: 0.9882387518882751  val: loss: 712053.3125 acc: 0.8870469927787781\n",
      "step: 26375\n",
      "train: loss: 78968.8515625 acc: 0.987438976764679  val: loss: 1638481.75 acc: 0.29922986030578613\n",
      "step: 26380\n",
      "train: loss: 469595.40625 acc: 0.9720126986503601  val: loss: 1030584.9375 acc: 0.8417014479637146\n",
      "step: 26385\n",
      "train: loss: 631279.6875 acc: 0.9634281992912292  val: loss: 168200.203125 acc: 0.9437328577041626\n",
      "step: 26390\n",
      "train: loss: 156318.890625 acc: 0.9740321636199951  val: loss: 1661698.25 acc: 0.2087554931640625\n",
      "step: 26395\n",
      "train: loss: 2253868.25 acc: 0.9314466714859009  val: loss: 2601151.75 acc: 0.6547993421554565\n",
      "step: 26400\n",
      "train: loss: 2864087.0 acc: 0.9306213855743408  val: loss: 514196.5 acc: 0.8114370107650757\n",
      "step: 26405\n",
      "train: loss: 1004945.4375 acc: 0.9591648578643799  val: loss: 513109.8125 acc: 0.8158674836158752\n",
      "step: 26410\n",
      "train: loss: 740138.0 acc: 0.9536113739013672  val: loss: 2073958.75 acc: 0.41375356912612915\n",
      "step: 26415\n",
      "train: loss: 311446.75 acc: 0.9777108430862427  val: loss: 115309.65625 acc: 0.9142166972160339\n",
      "step: 26420\n",
      "train: loss: 1745392.5 acc: 0.8892943859100342  val: loss: 220370.6875 acc: 0.8979671001434326\n",
      "step: 26425\n",
      "train: loss: 171630.296875 acc: 0.9044511318206787  val: loss: 829304.1875 acc: 0.8493786454200745\n",
      "step: 26430\n",
      "train: loss: 1337240.875 acc: 0.7848943471908569  val: loss: 964346.5625 acc: 0.8706035017967224\n",
      "step: 26435\n",
      "train: loss: 852338.375 acc: 0.6469244360923767  val: loss: 804136.75 acc: 0.8419051766395569\n",
      "step: 26440\n",
      "train: loss: 837750.6875 acc: 0.749603271484375  val: loss: 1049733.0 acc: 0.7742410898208618\n",
      "step: 26445\n",
      "train: loss: 438290.96875 acc: 0.7756738066673279  val: loss: 681797.25 acc: 0.8267204761505127\n",
      "step: 26450\n",
      "train: loss: 601286.4375 acc: 0.7999719977378845  val: loss: 1401936.875 acc: 0.7568817734718323\n",
      "step: 26455\n",
      "train: loss: 1080567.5 acc: 0.5821160078048706  val: loss: 812441.875 acc: 0.7947585582733154\n",
      "step: 26460\n",
      "train: loss: 667343.9375 acc: 0.7393200397491455  val: loss: 3562426.75 acc: 0.5416867733001709\n",
      "step: 26465\n",
      "train: loss: 176345.0625 acc: 0.8711300492286682  val: loss: 3689232.75 acc: 0.5421228408813477\n",
      "step: 26470\n",
      "train: loss: 139844.109375 acc: 0.8845118284225464  val: loss: 6152993.0 acc: 0.5624512434005737\n",
      "step: 26475\n",
      "train: loss: 90800.7265625 acc: 0.9315730333328247  val: loss: 6570570.0 acc: 0.5481418371200562\n",
      "step: 26480\n",
      "train: loss: 46262.1953125 acc: 0.9658598899841309  val: loss: 4462721.0 acc: 0.5393352508544922\n",
      "step: 26485\n",
      "train: loss: 127050.0546875 acc: 0.9221771359443665  val: loss: 3775983.25 acc: 0.6215412616729736\n",
      "step: 26490\n",
      "train: loss: 114818.640625 acc: 0.9230397939682007  val: loss: 3124020.25 acc: 0.5226678252220154\n",
      "step: 26495\n",
      "train: loss: 78577.796875 acc: 0.9409420490264893  val: loss: 7927879.0 acc: 0.4798572063446045\n",
      "step: 26500\n",
      "train: loss: 71740.4609375 acc: 0.9436133503913879  val: loss: 1606493.625 acc: 0.6165157556533813\n",
      "step: 26505\n",
      "train: loss: 17550.701171875 acc: 0.9810686111450195  val: loss: 2297719.25 acc: 0.5924659371376038\n",
      "step: 26510\n",
      "train: loss: 427700.21875 acc: 0.758300244808197  val: loss: 489373.03125 acc: 0.818149983882904\n",
      "step: 26515\n",
      "train: loss: 228012.375 acc: 0.8636187314987183  val: loss: 1237921.5 acc: 0.6596980690956116\n",
      "step: 26520\n",
      "train: loss: 73417.109375 acc: 0.9336307644844055  val: loss: 1312627.875 acc: 0.6719674468040466\n",
      "step: 26525\n",
      "train: loss: 636542.4375 acc: 0.7183307409286499  val: loss: 2061992.25 acc: 0.6593071222305298\n",
      "step: 26530\n",
      "train: loss: 1091514.25 acc: 0.6828924417495728  val: loss: 2440465.5 acc: 0.6092726588249207\n",
      "step: 26535\n",
      "train: loss: 321696.96875 acc: 0.8138384819030762  val: loss: 3628621.0 acc: 0.569199800491333\n",
      "step: 26540\n",
      "train: loss: 526688.0 acc: 0.7866417169570923  val: loss: 2119583.75 acc: 0.6878539323806763\n",
      "step: 26545\n",
      "train: loss: 1466726.25 acc: 0.7982425093650818  val: loss: 471331.5625 acc: 0.8675624132156372\n",
      "step: 26550\n",
      "train: loss: 919513.5 acc: 0.8721442222595215  val: loss: 1337375.625 acc: 0.8150979280471802\n",
      "step: 26555\n",
      "train: loss: 429350.625 acc: 0.9726907014846802  val: loss: 803457.125 acc: 0.8529263734817505\n",
      "step: 26560\n",
      "train: loss: 244925.96875 acc: 0.973458468914032  val: loss: 445371.78125 acc: 0.8320680260658264\n",
      "step: 26565\n",
      "train: loss: 124605.671875 acc: 0.9726577401161194  val: loss: 1304084.0 acc: 0.46911948919296265\n",
      "step: 26570\n",
      "train: loss: 197070.09375 acc: 0.976291298866272  val: loss: 1061194.125 acc: 0.8385807275772095\n",
      "step: 26575\n",
      "train: loss: 85459.9375 acc: 0.9919264316558838  val: loss: 1018375.1875 acc: 0.5977401733398438\n",
      "step: 26580\n",
      "train: loss: 41142.51171875 acc: 0.9972305297851562  val: loss: 2345404.5 acc: 0.38088375329971313\n",
      "step: 26585\n",
      "train: loss: 70507.6015625 acc: 0.993249237537384  val: loss: 1023332.125 acc: 0.5708823204040527\n",
      "step: 26590\n",
      "train: loss: 55935.1484375 acc: 0.993005633354187  val: loss: 1494844.0 acc: 0.7647265791893005\n",
      "step: 26595\n",
      "train: loss: 14441.2841796875 acc: 0.9962172508239746  val: loss: 1272039.75 acc: 0.6264288425445557\n",
      "step: 26600\n",
      "train: loss: 12957.166015625 acc: 0.9656698703765869  val: loss: 1835559.75 acc: 0.3444802165031433\n",
      "step: 26605\n",
      "train: loss: 4503.18408203125 acc: 0.9821493625640869  val: loss: 1498209.5 acc: 0.6014865040779114\n",
      "step: 26610\n",
      "train: loss: 13369.248046875 acc: 0.9919095039367676  val: loss: 2018272.75 acc: 0.535871148109436\n",
      "step: 26615\n",
      "train: loss: 25739.1171875 acc: 0.9773833751678467  val: loss: 2197469.25 acc: 0.6410152912139893\n",
      "step: 26620\n",
      "train: loss: 26411.416015625 acc: 0.965757429599762  val: loss: 1367425.0 acc: 0.7234967947006226\n",
      "step: 26625\n",
      "train: loss: 10182.8427734375 acc: 0.9821179509162903  val: loss: 1956897.5 acc: 0.7512718439102173\n",
      "step: 26630\n",
      "train: loss: 12690.6787109375 acc: 0.9757920503616333  val: loss: 1127590.25 acc: 0.7797000408172607\n",
      "step: 26635\n",
      "train: loss: 23201.783203125 acc: 0.958044171333313  val: loss: 995110.5625 acc: 0.8894355893135071\n",
      "step: 26640\n",
      "train: loss: 11778.279296875 acc: 0.972480297088623  val: loss: 3225804.75 acc: -0.14596056938171387\n",
      "step: 26645\n",
      "train: loss: 14736.841796875 acc: 0.9885041117668152  val: loss: 331217.8125 acc: 0.9298232197761536\n",
      "step: 26650\n",
      "train: loss: 34229.125 acc: 0.983642041683197  val: loss: 664822.75 acc: 0.48675966262817383\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 26655\n",
      "train: loss: 5990.66943359375 acc: 0.9964132308959961  val: loss: 1788982.0 acc: 0.03419482707977295\n",
      "step: 26660\n",
      "train: loss: 7547.8095703125 acc: 0.9950160980224609  val: loss: 3430625.75 acc: 0.31209754943847656\n",
      "step: 26665\n",
      "train: loss: 14890.5126953125 acc: 0.9910404086112976  val: loss: 1413856.375 acc: 0.580913245677948\n",
      "step: 26670\n",
      "train: loss: 14775.1943359375 acc: 0.9894208312034607  val: loss: 472111.15625 acc: 0.8919193744659424\n",
      "step: 26675\n",
      "train: loss: 16768.21875 acc: 0.9906639456748962  val: loss: 1837647.0 acc: 0.5590678453445435\n",
      "step: 26680\n",
      "train: loss: 26399.490234375 acc: 0.9852553009986877  val: loss: 1083967.375 acc: 0.8213244676589966\n",
      "step: 26685\n",
      "train: loss: 49770.5859375 acc: 0.9846999049186707  val: loss: 292786.28125 acc: 0.939244270324707\n",
      "step: 26690\n",
      "train: loss: 14378.6845703125 acc: 0.9970031976699829  val: loss: 1571835.75 acc: 0.57429438829422\n",
      "step: 26695\n",
      "train: loss: 30285.78125 acc: 0.9707225561141968  val: loss: 2122790.75 acc: -0.459043025970459\n",
      "step: 26700\n",
      "train: loss: 53734.640625 acc: 0.9785240888595581  val: loss: 1239686.5 acc: 0.8089722990989685\n",
      "step: 26705\n",
      "train: loss: 336786.8125 acc: 0.9165471196174622  val: loss: 475257.09375 acc: 0.702838659286499\n",
      "step: 26710\n",
      "train: loss: 118620.6484375 acc: 0.9456206560134888  val: loss: 225797.859375 acc: 0.9421163201332092\n",
      "step: 26715\n",
      "train: loss: 644265.125 acc: 0.8319057822227478  val: loss: 280633.75 acc: 0.8853906393051147\n",
      "step: 26720\n",
      "train: loss: 597376.75 acc: 0.8926283121109009  val: loss: 719111.625 acc: 0.46441471576690674\n",
      "step: 26725\n",
      "train: loss: 83468.171875 acc: 0.9905669093132019  val: loss: 250045.46875 acc: 0.9147339463233948\n",
      "step: 26730\n",
      "train: loss: 96634.6796875 acc: 0.9895549416542053  val: loss: 220283.71875 acc: 0.9693489074707031\n",
      "step: 26735\n",
      "train: loss: 37202.0703125 acc: 0.9939568042755127  val: loss: 742415.0 acc: 0.7815818786621094\n",
      "step: 26740\n",
      "train: loss: 143121.578125 acc: 0.9826602339744568  val: loss: 299520.125 acc: 0.9600954651832581\n",
      "step: 26745\n",
      "train: loss: 681298.125 acc: 0.9697067737579346  val: loss: 597368.3125 acc: 0.5809646844863892\n",
      "step: 26750\n",
      "train: loss: 430300.53125 acc: 0.9664125442504883  val: loss: 712235.9375 acc: 0.8815923929214478\n",
      "step: 26755\n",
      "train: loss: 120022.7421875 acc: 0.9187867641448975  val: loss: 1170018.75 acc: 0.8655228614807129\n",
      "step: 26760\n",
      "train: loss: 1113391.75 acc: 0.9617224931716919  val: loss: 428162.46875 acc: 0.8535640239715576\n",
      "step: 26765\n",
      "train: loss: 1178360.0 acc: 0.9558977484703064  val: loss: 1966405.0 acc: 0.6289217472076416\n",
      "step: 26770\n",
      "train: loss: 686367.25 acc: 0.9780030846595764  val: loss: 148851.796875 acc: 0.9508763551712036\n",
      "step: 26775\n",
      "train: loss: 1564803.25 acc: 0.9388883113861084  val: loss: 1140580.875 acc: 0.8720972537994385\n",
      "step: 26780\n",
      "train: loss: 957193.5 acc: 0.8952063322067261  val: loss: 811922.5625 acc: 0.8092625141143799\n",
      "step: 26785\n",
      "train: loss: 142720.296875 acc: 0.9616693258285522  val: loss: 384668.40625 acc: 0.8742287158966064\n",
      "step: 26790\n",
      "train: loss: 643119.0 acc: 0.8641068339347839  val: loss: 993886.875 acc: 0.5487724542617798\n",
      "step: 26795\n",
      "train: loss: 422753.875 acc: 0.8930258750915527  val: loss: 688490.6875 acc: 0.9009356498718262\n",
      "step: 26800\n",
      "train: loss: 847041.875 acc: 0.7354974746704102  val: loss: 628457.5 acc: 0.82914137840271\n",
      "step: 26805\n",
      "train: loss: 633472.125 acc: 0.7624051570892334  val: loss: 731026.125 acc: 0.7616517543792725\n",
      "step: 26810\n",
      "train: loss: 167146.390625 acc: 0.8855763673782349  val: loss: 1371681.5 acc: 0.759797215461731\n",
      "step: 26815\n",
      "train: loss: 614684.75 acc: 0.865566611289978  val: loss: 491600.90625 acc: 0.8226608037948608\n",
      "step: 26820\n",
      "train: loss: 988005.125 acc: 0.5052512884140015  val: loss: 2621644.5 acc: 0.7391637563705444\n",
      "step: 26825\n",
      "train: loss: 510313.78125 acc: 0.7622639536857605  val: loss: 1643110.0 acc: 0.756962239742279\n",
      "step: 26830\n",
      "train: loss: 146384.859375 acc: 0.8867233991622925  val: loss: 1381809.75 acc: 0.6557509899139404\n",
      "step: 26835\n",
      "train: loss: 342945.8125 acc: 0.8060261011123657  val: loss: 3240053.25 acc: 0.5817939043045044\n",
      "step: 26840\n",
      "train: loss: 59728.04296875 acc: 0.9458313584327698  val: loss: 6628824.0 acc: 0.5154951810836792\n",
      "step: 26845\n",
      "train: loss: 32502.775390625 acc: 0.9730250835418701  val: loss: 1342715.625 acc: 0.6582561731338501\n",
      "step: 26850\n",
      "train: loss: 12608.826171875 acc: 0.9901642799377441  val: loss: 2501163.75 acc: 0.5707277059555054\n",
      "step: 26855\n",
      "train: loss: 133061.609375 acc: 0.8990102410316467  val: loss: 1691814.5 acc: 0.618910551071167\n",
      "step: 26860\n",
      "train: loss: 424138.125 acc: 0.777347981929779  val: loss: 2818693.25 acc: 0.5370889902114868\n",
      "step: 26865\n",
      "train: loss: 27825.810546875 acc: 0.9743912220001221  val: loss: 271492.5 acc: 0.8491102457046509\n",
      "step: 26870\n",
      "train: loss: 526630.8125 acc: 0.7626875638961792  val: loss: 2510654.0 acc: 0.5943933129310608\n",
      "step: 26875\n",
      "train: loss: 45746.62890625 acc: 0.9563916325569153  val: loss: 3160021.25 acc: 0.6253787279129028\n",
      "step: 26880\n",
      "train: loss: 388608.8125 acc: 0.8000869750976562  val: loss: 484001.96875 acc: 0.7256832122802734\n",
      "step: 26885\n",
      "train: loss: 260478.328125 acc: 0.8330106139183044  val: loss: 655542.8125 acc: 0.6879401206970215\n",
      "step: 26890\n",
      "train: loss: 92118.1015625 acc: 0.9096612930297852  val: loss: 578885.5 acc: 0.781007707118988\n",
      "step: 26895\n",
      "train: loss: 346357.46875 acc: 0.76881343126297  val: loss: 707168.375 acc: 0.7230987548828125\n",
      "step: 26900\n",
      "train: loss: 500218.96875 acc: 0.7819918394088745  val: loss: 1565910.5 acc: 0.6477562189102173\n",
      "step: 26905\n",
      "train: loss: 1009784.875 acc: 0.7137399911880493  val: loss: 1563905.5 acc: 0.6631641387939453\n",
      "step: 26910\n",
      "train: loss: 2109798.75 acc: 0.7785306572914124  val: loss: 1690198.875 acc: 0.7878614068031311\n",
      "step: 26915\n",
      "train: loss: 690538.5 acc: 0.713227391242981  val: loss: 877332.1875 acc: 0.6714081168174744\n",
      "step: 26920\n",
      "train: loss: 116106.90625 acc: 0.992485523223877  val: loss: 1223606.75 acc: 0.70222008228302\n",
      "step: 26925\n",
      "train: loss: 704887.75 acc: 0.9304197430610657  val: loss: 948438.0 acc: 0.7981246709823608\n",
      "step: 26930\n",
      "train: loss: 730464.125 acc: 0.8933156728744507  val: loss: 1318723.25 acc: 0.6840017437934875\n",
      "step: 26935\n",
      "train: loss: 59344.859375 acc: 0.9884048700332642  val: loss: 1528211.25 acc: 0.7453867197036743\n",
      "step: 26940\n",
      "train: loss: 35207.234375 acc: 0.997798502445221  val: loss: 926523.375 acc: 0.802222728729248\n",
      "step: 26945\n",
      "train: loss: 32758.185546875 acc: 0.9976667761802673  val: loss: 741018.5 acc: 0.783183753490448\n",
      "step: 26950\n",
      "train: loss: 364805.84375 acc: 0.9641503095626831  val: loss: 174703.171875 acc: 0.9844973683357239\n",
      "step: 26955\n",
      "train: loss: 40526.35546875 acc: 0.9925262331962585  val: loss: 1861023.75 acc: 0.6855443120002747\n",
      "step: 26960\n",
      "train: loss: 27431.720703125 acc: 0.9930016398429871  val: loss: 1183458.25 acc: 0.7630698680877686\n",
      "step: 26965\n",
      "train: loss: 20548.173828125 acc: 0.9943963885307312  val: loss: 1534265.5 acc: -0.4207500219345093\n",
      "step: 26970\n",
      "train: loss: 16175.4228515625 acc: 0.9963026642799377  val: loss: 1448678.875 acc: 0.603081464767456\n",
      "step: 26975\n",
      "train: loss: 11199.935546875 acc: 0.9704891443252563  val: loss: 1105638.875 acc: 0.692878782749176\n",
      "step: 26980\n",
      "train: loss: 25814.583984375 acc: 0.9913303256034851  val: loss: 1386451.0 acc: 0.8684902191162109\n",
      "step: 26985\n",
      "train: loss: 38587.05078125 acc: 0.9557778835296631  val: loss: 883795.3125 acc: 0.8848814964294434\n",
      "step: 26990\n",
      "train: loss: 10786.2890625 acc: 0.974885880947113  val: loss: 1285824.75 acc: 0.8076912760734558\n",
      "step: 26995\n",
      "train: loss: 9172.5380859375 acc: 0.9742287993431091  val: loss: 867396.75 acc: 0.8496676087379456\n",
      "step: 27000\n",
      "train: loss: 9193.091796875 acc: 0.9620618224143982  val: loss: 2291101.25 acc: 0.2934568524360657\n",
      "step: 27005\n",
      "train: loss: 12687.9423828125 acc: 0.9641753435134888  val: loss: 705405.5 acc: 0.8606134653091431\n",
      "step: 27010\n",
      "train: loss: 22169.59375 acc: 0.9832562804222107  val: loss: 721570.0 acc: 0.734704852104187\n",
      "step: 27015\n",
      "train: loss: 27933.16796875 acc: 0.9856358170509338  val: loss: 1509311.125 acc: 0.5934435129165649\n",
      "step: 27020\n",
      "train: loss: 20050.177734375 acc: 0.98824143409729  val: loss: 234650.234375 acc: 0.9637841582298279\n",
      "step: 27025\n",
      "train: loss: 29036.529296875 acc: 0.9802695512771606  val: loss: 1242034.5 acc: 0.48745590448379517\n",
      "step: 27030\n",
      "train: loss: 7069.44384765625 acc: 0.9928730130195618  val: loss: 89905.8828125 acc: 0.9617047309875488\n",
      "step: 27035\n",
      "train: loss: 9846.4697265625 acc: 0.9898302555084229  val: loss: 2341855.75 acc: 0.5906866788864136\n",
      "step: 27040\n",
      "train: loss: 14426.1220703125 acc: 0.9813721179962158  val: loss: 1654593.25 acc: 0.44079166650772095\n",
      "step: 27045\n",
      "train: loss: 13286.8359375 acc: 0.9943525791168213  val: loss: 365004.15625 acc: 0.8758985996246338\n",
      "step: 27050\n",
      "train: loss: 18756.0546875 acc: 0.9956090450286865  val: loss: 1174587.25 acc: 0.7996593713760376\n",
      "step: 27055\n",
      "train: loss: 46567.95703125 acc: 0.988318920135498  val: loss: 131750.34375 acc: 0.9675463438034058\n",
      "step: 27060\n",
      "train: loss: 43536.265625 acc: 0.9856043457984924  val: loss: 900923.1875 acc: 0.8090873956680298\n",
      "step: 27065\n",
      "train: loss: 38495.12890625 acc: 0.9856691360473633  val: loss: 608636.5625 acc: 0.8541862368583679\n",
      "step: 27070\n",
      "train: loss: 68954.1875 acc: 0.9792450070381165  val: loss: 177839.53125 acc: 0.9365088939666748\n",
      "step: 27075\n",
      "train: loss: 58768.96875 acc: 0.9793331623077393  val: loss: 115094.6875 acc: 0.9019085168838501\n",
      "step: 27080\n",
      "train: loss: 431901.625 acc: 0.8617135882377625  val: loss: 2262754.25 acc: 0.14055800437927246\n",
      "step: 27085\n",
      "train: loss: 802870.625 acc: 0.8775780200958252  val: loss: 606278.8125 acc: 0.9349678158760071\n",
      "step: 27090\n",
      "train: loss: 67195.359375 acc: 0.993986189365387  val: loss: 462341.90625 acc: 0.9576248526573181\n",
      "step: 27095\n",
      "train: loss: 152335.59375 acc: 0.9817290902137756  val: loss: 619273.375 acc: 0.8811644315719604\n",
      "step: 27100\n",
      "train: loss: 57429.359375 acc: 0.9933648109436035  val: loss: 2768641.75 acc: 0.33582693338394165\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 27105\n",
      "train: loss: 444212.25 acc: 0.9631006121635437  val: loss: 292131.5625 acc: 0.9689069390296936\n",
      "step: 27110\n",
      "train: loss: 2470094.0 acc: 0.9159931540489197  val: loss: 501179.125 acc: 0.9004377722740173\n",
      "step: 27115\n",
      "train: loss: 457636.90625 acc: 0.9623968601226807  val: loss: 1169954.75 acc: 0.8366814255714417\n",
      "step: 27120\n",
      "train: loss: 220969.5 acc: 0.980813205242157  val: loss: 641685.25 acc: 0.900347113609314\n",
      "step: 27125\n",
      "train: loss: 1505879.75 acc: 0.9060602188110352  val: loss: 649021.0 acc: 0.8365554213523865\n",
      "step: 27130\n",
      "train: loss: 316078.6875 acc: 0.9810906052589417  val: loss: 1188303.75 acc: 0.7965415716171265\n",
      "step: 27135\n",
      "train: loss: 1260046.625 acc: 0.9627758264541626  val: loss: 471285.0625 acc: 0.9205779433250427\n",
      "step: 27140\n",
      "train: loss: 1377497.25 acc: 0.9365928173065186  val: loss: 1701336.5 acc: 0.7617313861846924\n",
      "step: 27145\n",
      "train: loss: 616947.125 acc: 0.953080952167511  val: loss: 2490287.25 acc: 0.441697359085083\n",
      "step: 27150\n",
      "train: loss: 573542.125 acc: 0.9141155481338501  val: loss: 803961.25 acc: 0.860873281955719\n",
      "step: 27155\n",
      "train: loss: 290480.75 acc: 0.9216049909591675  val: loss: 493760.875 acc: 0.8997669219970703\n",
      "step: 27160\n",
      "train: loss: 311702.34375 acc: 0.9367462396621704  val: loss: 545552.1875 acc: 0.9526679515838623\n",
      "step: 27165\n",
      "train: loss: 1946064.25 acc: 0.710351824760437  val: loss: 1207593.125 acc: 0.7407154440879822\n",
      "step: 27170\n",
      "train: loss: 1546786.5 acc: 0.7314821481704712  val: loss: 948192.875 acc: 0.7151932716369629\n",
      "step: 27175\n",
      "train: loss: 354343.0 acc: 0.8875736594200134  val: loss: 864905.25 acc: 0.7388367652893066\n",
      "step: 27180\n",
      "train: loss: 361005.625 acc: 0.8749890923500061  val: loss: 920128.5 acc: 0.844653308391571\n",
      "step: 27185\n",
      "train: loss: 833174.625 acc: 0.6160078644752502  val: loss: 1886454.5 acc: 0.7100324034690857\n",
      "step: 27190\n",
      "train: loss: 566602.75 acc: 0.7456427812576294  val: loss: 1527565.875 acc: 0.6401727199554443\n",
      "step: 27195\n",
      "train: loss: 103804.3046875 acc: 0.9253000617027283  val: loss: 1977756.0 acc: 0.5928301811218262\n",
      "step: 27200\n",
      "train: loss: 150951.203125 acc: 0.909072995185852  val: loss: 314164.71875 acc: 0.8222617506980896\n",
      "step: 27205\n",
      "train: loss: 186866.21875 acc: 0.8765949606895447  val: loss: 688289.375 acc: 0.7429178953170776\n",
      "step: 27210\n",
      "train: loss: 178413.859375 acc: 0.8468852043151855  val: loss: 2265478.5 acc: 0.7254082560539246\n",
      "step: 27215\n",
      "train: loss: 87604.390625 acc: 0.9351930022239685  val: loss: 382677.65625 acc: 0.8032622337341309\n",
      "step: 27220\n",
      "train: loss: 9465.5791015625 acc: 0.992152988910675  val: loss: 3096870.25 acc: 0.5727920532226562\n",
      "step: 27225\n",
      "train: loss: 50646.37890625 acc: 0.96254563331604  val: loss: 5215745.5 acc: 0.5782679915428162\n",
      "step: 27230\n",
      "train: loss: 354810.84375 acc: 0.8026272058486938  val: loss: 1530667.5 acc: 0.6902536749839783\n",
      "step: 27235\n",
      "train: loss: 429535.84375 acc: 0.798162579536438  val: loss: 416455.6875 acc: 0.8271428346633911\n",
      "step: 27240\n",
      "train: loss: 68775.6953125 acc: 0.9430698752403259  val: loss: 4764654.0 acc: 0.5566737651824951\n",
      "step: 27245\n",
      "train: loss: 696135.1875 acc: 0.7691774964332581  val: loss: 373719.5 acc: 0.7517644762992859\n",
      "step: 27250\n",
      "train: loss: 632187.6875 acc: 0.7032196521759033  val: loss: 2350245.5 acc: 0.5911047458648682\n",
      "step: 27255\n",
      "train: loss: 248611.21875 acc: 0.8083091378211975  val: loss: 781069.1875 acc: 0.7186312675476074\n",
      "step: 27260\n",
      "train: loss: 115446.8984375 acc: 0.8774284720420837  val: loss: 1852974.5 acc: 0.6661927700042725\n",
      "step: 27265\n",
      "train: loss: 468488.875 acc: 0.7582595348358154  val: loss: 2780575.0 acc: 0.6792675256729126\n",
      "step: 27270\n",
      "train: loss: 315356.09375 acc: 0.7849106192588806  val: loss: 2527064.25 acc: 0.6110882759094238\n",
      "step: 27275\n",
      "train: loss: 2160910.0 acc: 0.7170634269714355  val: loss: 1267641.0 acc: 0.7817323207855225\n",
      "step: 27280\n",
      "train: loss: 793372.375 acc: 0.8130208849906921  val: loss: 623354.875 acc: 0.842610239982605\n",
      "step: 27285\n",
      "train: loss: 770382.0625 acc: 0.9205561876296997  val: loss: 1340468.5 acc: 0.8400296568870544\n",
      "step: 27290\n",
      "train: loss: 991067.0 acc: 0.9159618020057678  val: loss: 411149.6875 acc: 0.9134653210639954\n",
      "step: 27295\n",
      "train: loss: 122369.125 acc: 0.9844356775283813  val: loss: 726645.9375 acc: 0.9484453797340393\n",
      "step: 27300\n",
      "train: loss: 70283.609375 acc: 0.9875404834747314  val: loss: 882875.25 acc: 0.44317537546157837\n",
      "step: 27305\n",
      "train: loss: 426985.21875 acc: 0.9589334726333618  val: loss: 1544454.5 acc: 0.6849614381790161\n",
      "step: 27310\n",
      "train: loss: 98349.328125 acc: 0.9933333992958069  val: loss: 365801.25 acc: 0.8749042749404907\n",
      "step: 27315\n",
      "train: loss: 92454.78125 acc: 0.9915863275527954  val: loss: 1072402.125 acc: 0.8910862803459167\n",
      "step: 27320\n",
      "train: loss: 27130.16015625 acc: 0.996639609336853  val: loss: 1140191.25 acc: 0.6098176836967468\n",
      "step: 27325\n",
      "train: loss: 40551.78515625 acc: 0.9920916557312012  val: loss: 330982.46875 acc: 0.7911210060119629\n",
      "step: 27330\n",
      "train: loss: 25626.958984375 acc: 0.9931996464729309  val: loss: 100049.234375 acc: 0.9681088924407959\n",
      "step: 27335\n",
      "train: loss: 17143.259765625 acc: 0.9947986006736755  val: loss: 131664.484375 acc: 0.9487217664718628\n",
      "step: 27340\n",
      "train: loss: 3438.188720703125 acc: 0.9984354972839355  val: loss: 249506.859375 acc: 0.962634265422821\n",
      "step: 27345\n",
      "train: loss: 6114.87109375 acc: 0.9978623390197754  val: loss: 859823.1875 acc: 0.09271520376205444\n",
      "step: 27350\n",
      "train: loss: 17006.462890625 acc: 0.9928835034370422  val: loss: 1075029.25 acc: 0.8755455017089844\n",
      "step: 27355\n",
      "train: loss: 6462.40087890625 acc: 0.9741518497467041  val: loss: 784449.75 acc: 0.8458791971206665\n",
      "step: 27360\n",
      "train: loss: 19839.294921875 acc: 0.9864166975021362  val: loss: 1814397.75 acc: 0.6892184019088745\n",
      "step: 27365\n",
      "train: loss: 7949.2373046875 acc: 0.9792184829711914  val: loss: 1070359.125 acc: 0.6884929537773132\n",
      "step: 27370\n",
      "train: loss: 5397.43701171875 acc: 0.9910218715667725  val: loss: 183765.265625 acc: 0.9612535834312439\n",
      "step: 27375\n",
      "train: loss: 32694.26953125 acc: 0.9767749309539795  val: loss: 305936.375 acc: 0.8368433713912964\n",
      "step: 27380\n",
      "train: loss: 21656.947265625 acc: 0.9789942502975464  val: loss: 1370516.25 acc: 0.6938687562942505\n",
      "step: 27385\n",
      "train: loss: 14504.703125 acc: 0.9852888584136963  val: loss: 410641.4375 acc: 0.9575604796409607\n",
      "step: 27390\n",
      "train: loss: 94173.640625 acc: 0.954296350479126  val: loss: 635870.6875 acc: 0.8066809177398682\n",
      "step: 27395\n",
      "train: loss: 100887.671875 acc: 0.9550963044166565  val: loss: 342253.71875 acc: 0.933543860912323\n",
      "step: 27400\n",
      "train: loss: 5270.5927734375 acc: 0.9942404627799988  val: loss: 1088147.625 acc: 0.7051602005958557\n",
      "step: 27405\n",
      "train: loss: 8246.142578125 acc: 0.9832369685173035  val: loss: 1036371.375 acc: 0.8830244541168213\n",
      "step: 27410\n",
      "train: loss: 13461.0302734375 acc: 0.9942265748977661  val: loss: 1290926.125 acc: 0.8244438171386719\n",
      "step: 27415\n",
      "train: loss: 17296.400390625 acc: 0.9953945875167847  val: loss: 675101.5625 acc: 0.9042182564735413\n",
      "step: 27420\n",
      "train: loss: 21405.994140625 acc: 0.9946908354759216  val: loss: 856784.5625 acc: 0.8207815885543823\n",
      "step: 27425\n",
      "train: loss: 6589.19873046875 acc: 0.9982187747955322  val: loss: 583133.375 acc: 0.8966220021247864\n",
      "step: 27430\n",
      "train: loss: 69981.3125 acc: 0.9779024720191956  val: loss: 648075.5 acc: 0.9188076257705688\n",
      "step: 27435\n",
      "train: loss: 148607.984375 acc: 0.9614954590797424  val: loss: 177994.484375 acc: 0.9826701283454895\n",
      "step: 27440\n",
      "train: loss: 335108.1875 acc: 0.8729157447814941  val: loss: 1515930.625 acc: 0.8356602191925049\n",
      "step: 27445\n",
      "train: loss: 278996.25 acc: 0.7830277681350708  val: loss: 1034527.25 acc: 0.49086594581604004\n",
      "step: 27450\n",
      "train: loss: 660613.5 acc: 0.8681142926216125  val: loss: 2938510.25 acc: 0.7369031310081482\n",
      "step: 27455\n",
      "train: loss: 187288.953125 acc: 0.9772850275039673  val: loss: 2252380.0 acc: 0.31604641675949097\n",
      "step: 27460\n",
      "train: loss: 556170.0 acc: 0.9296687245368958  val: loss: 669407.375 acc: 0.9251458644866943\n",
      "step: 27465\n",
      "train: loss: 344038.8125 acc: 0.9661175012588501  val: loss: 1982103.0 acc: 0.7527782320976257\n",
      "step: 27470\n",
      "train: loss: 595447.0625 acc: 0.9427976608276367  val: loss: 1609399.625 acc: 0.20546835660934448\n",
      "step: 27475\n",
      "train: loss: 338684.28125 acc: 0.9823336601257324  val: loss: 1121618.0 acc: 0.6220353841781616\n",
      "step: 27480\n",
      "train: loss: 410074.75 acc: 0.9754092693328857  val: loss: 335387.3125 acc: 0.9326654076576233\n",
      "step: 27485\n",
      "train: loss: 136499.328125 acc: 0.9674723148345947  val: loss: 1557008.375 acc: 0.35671770572662354\n",
      "step: 27490\n",
      "train: loss: 726715.875 acc: 0.9769901037216187  val: loss: 513851.40625 acc: 0.8988925218582153\n",
      "step: 27495\n",
      "train: loss: 598691.875 acc: 0.9780797958374023  val: loss: 157694.640625 acc: 0.8840468525886536\n",
      "step: 27500\n",
      "train: loss: 519043.0625 acc: 0.9775398373603821  val: loss: 329008.375 acc: 0.9079228043556213\n",
      "step: 27505\n",
      "train: loss: 1997611.125 acc: 0.9276795983314514  val: loss: 233511.828125 acc: 0.9507185220718384\n",
      "step: 27510\n",
      "train: loss: 982851.5 acc: 0.9589720368385315  val: loss: 1045063.0 acc: 0.6368216872215271\n",
      "step: 27515\n",
      "train: loss: 944905.5625 acc: 0.8946269750595093  val: loss: 514704.1875 acc: 0.9496415257453918\n",
      "step: 27520\n",
      "train: loss: 563575.8125 acc: 0.9221971035003662  val: loss: 844218.4375 acc: 0.8895100355148315\n",
      "step: 27525\n",
      "train: loss: 847230.9375 acc: 0.9139536619186401  val: loss: 602279.375 acc: 0.9453744292259216\n",
      "step: 27530\n",
      "train: loss: 3388222.75 acc: -0.10096371173858643  val: loss: 1092601.125 acc: 0.7406879663467407\n",
      "step: 27535\n",
      "train: loss: 861088.1875 acc: 0.5858795642852783  val: loss: 566481.625 acc: 0.6631469130516052\n",
      "step: 27540\n",
      "train: loss: 398444.65625 acc: 0.8261696100234985  val: loss: 2574701.5 acc: 0.7201906442642212\n",
      "step: 27545\n",
      "train: loss: 477062.65625 acc: 0.8505055904388428  val: loss: 1265284.875 acc: 0.765324056148529\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 27550\n",
      "train: loss: 838307.25 acc: 0.6842420101165771  val: loss: 1837885.75 acc: 0.7810003161430359\n",
      "step: 27555\n",
      "train: loss: 941809.5 acc: 0.6027970314025879  val: loss: 3030918.75 acc: 0.6260806322097778\n",
      "step: 27560\n",
      "train: loss: 113974.8515625 acc: 0.9106190204620361  val: loss: 1540303.125 acc: 0.6486581563949585\n",
      "step: 27565\n",
      "train: loss: 447758.46875 acc: 0.8143823146820068  val: loss: 1681189.125 acc: 0.6240005493164062\n",
      "step: 27570\n",
      "train: loss: 52659.18359375 acc: 0.9473256468772888  val: loss: 1400670.125 acc: 0.6620538234710693\n",
      "step: 27575\n",
      "train: loss: 25244.4453125 acc: 0.9782199263572693  val: loss: 4147477.75 acc: 0.5438311100006104\n",
      "step: 27580\n",
      "train: loss: 21690.236328125 acc: 0.9829844236373901  val: loss: 619405.9375 acc: 0.7124015092849731\n",
      "step: 27585\n",
      "train: loss: 218439.078125 acc: 0.8745245933532715  val: loss: 2142751.5 acc: 0.6315491199493408\n",
      "step: 27590\n",
      "train: loss: 275749.9375 acc: 0.8478416800498962  val: loss: 691226.6875 acc: 0.727586567401886\n",
      "step: 27595\n",
      "train: loss: 142773.328125 acc: 0.8576762080192566  val: loss: 406897.25 acc: 0.7996720671653748\n",
      "step: 27600\n",
      "train: loss: 168940.34375 acc: 0.8576064109802246  val: loss: 1200267.5 acc: 0.6420236825942993\n",
      "step: 27605\n",
      "train: loss: 22092.283203125 acc: 0.9726845026016235  val: loss: 3124058.0 acc: 0.5698137283325195\n",
      "step: 27610\n",
      "train: loss: 69755.8671875 acc: 0.9381028413772583  val: loss: 1060796.25 acc: 0.6675212383270264\n",
      "step: 27615\n",
      "train: loss: 636322.5 acc: 0.7624362111091614  val: loss: 2595907.0 acc: 0.6302756071090698\n",
      "step: 27620\n",
      "train: loss: 131516.828125 acc: 0.9158494472503662  val: loss: 617414.75 acc: 0.6996405124664307\n",
      "step: 27625\n",
      "train: loss: 566763.875 acc: 0.7445487976074219  val: loss: 2546855.5 acc: 0.6519715189933777\n",
      "step: 27630\n",
      "train: loss: 180802.875 acc: 0.800255537033081  val: loss: 3279167.5 acc: 0.5915703773498535\n",
      "step: 27635\n",
      "train: loss: 295901.71875 acc: 0.8555087447166443  val: loss: 105562.09375 acc: 0.9025092720985413\n",
      "step: 27640\n",
      "train: loss: 2244717.0 acc: 0.7236838340759277  val: loss: 592555.125 acc: 0.8198481202125549\n",
      "step: 27645\n",
      "train: loss: 865990.875 acc: 0.8399776220321655  val: loss: 239925.125 acc: 0.8917547464370728\n",
      "step: 27650\n",
      "train: loss: 892817.125 acc: 0.86912602186203  val: loss: 786249.375 acc: 0.8385986685752869\n",
      "step: 27655\n",
      "train: loss: 217034.578125 acc: 0.9775476455688477  val: loss: 235116.28125 acc: 0.8473594188690186\n",
      "step: 27660\n",
      "train: loss: 249109.578125 acc: 0.9683908820152283  val: loss: 686347.75 acc: 0.6576570272445679\n",
      "step: 27665\n",
      "train: loss: 163561.6875 acc: 0.9746084809303284  val: loss: 958432.5 acc: 0.7480579614639282\n",
      "step: 27670\n",
      "train: loss: 47676.83203125 acc: 0.9949747920036316  val: loss: 900509.375 acc: 0.6659703254699707\n",
      "step: 27675\n",
      "train: loss: 48786.640625 acc: 0.9962032437324524  val: loss: 943656.5 acc: 0.7031134366989136\n",
      "step: 27680\n",
      "train: loss: 43626.796875 acc: 0.9967803955078125  val: loss: 1049656.5 acc: 0.5944972038269043\n",
      "step: 27685\n",
      "train: loss: 35824.3203125 acc: 0.9960441589355469  val: loss: 866068.0625 acc: 0.6502742767333984\n",
      "step: 27690\n",
      "train: loss: 46130.046875 acc: 0.994037926197052  val: loss: 921481.375 acc: 0.7722496390342712\n",
      "step: 27695\n",
      "train: loss: 16980.89453125 acc: 0.9978712201118469  val: loss: 701257.5 acc: 0.8515695333480835\n",
      "step: 27700\n",
      "train: loss: 22004.857421875 acc: 0.9917992353439331  val: loss: 1214197.5 acc: 0.7327038049697876\n",
      "step: 27705\n",
      "train: loss: 18111.30078125 acc: 0.9917021989822388  val: loss: 1917419.0 acc: 0.6346151828765869\n",
      "step: 27710\n",
      "train: loss: 8714.6279296875 acc: 0.9877685308456421  val: loss: 157874.234375 acc: 0.9623591303825378\n",
      "step: 27715\n",
      "train: loss: 14869.8935546875 acc: 0.9928448796272278  val: loss: 1120133.875 acc: 0.682694673538208\n",
      "step: 27720\n",
      "train: loss: 14667.884765625 acc: 0.9912601709365845  val: loss: 712395.5 acc: 0.9000346660614014\n",
      "step: 27725\n",
      "train: loss: 18941.58984375 acc: 0.9877555966377258  val: loss: 689658.8125 acc: 0.8969663977622986\n",
      "step: 27730\n",
      "train: loss: 19410.33203125 acc: 0.9697373509407043  val: loss: 555922.5 acc: 0.9146744012832642\n",
      "step: 27735\n",
      "train: loss: 10146.7607421875 acc: 0.9847630858421326  val: loss: 1082903.75 acc: 0.8999308347702026\n",
      "step: 27740\n",
      "train: loss: 9160.62109375 acc: 0.9684363007545471  val: loss: 701711.375 acc: 0.723779022693634\n",
      "step: 27745\n",
      "train: loss: 27096.5625 acc: 0.9905806183815002  val: loss: 535247.875 acc: 0.932847261428833\n",
      "step: 27750\n",
      "train: loss: 19180.734375 acc: 0.9942219257354736  val: loss: 467518.8125 acc: 0.9283648729324341\n",
      "step: 27755\n",
      "train: loss: 14414.4296875 acc: 0.9915834069252014  val: loss: 118298.1640625 acc: 0.9752349257469177\n",
      "step: 27760\n",
      "train: loss: 7429.84375 acc: 0.9944482445716858  val: loss: 1059787.375 acc: 0.8840429782867432\n",
      "step: 27765\n",
      "train: loss: 8594.669921875 acc: 0.9928183555603027  val: loss: 1564340.25 acc: 0.6298567652702332\n",
      "step: 27770\n",
      "train: loss: 15690.80859375 acc: 0.9871869683265686  val: loss: 2364822.0 acc: 0.35635870695114136\n",
      "step: 27775\n",
      "train: loss: 22445.88671875 acc: 0.9865055680274963  val: loss: 1381871.375 acc: 0.7117486596107483\n",
      "step: 27780\n",
      "train: loss: 27280.64453125 acc: 0.9905536770820618  val: loss: 2259772.5 acc: 0.7529276609420776\n",
      "step: 27785\n",
      "train: loss: 18179.279296875 acc: 0.9946237802505493  val: loss: 1526972.75 acc: 0.6513075828552246\n",
      "step: 27790\n",
      "train: loss: 51689.21484375 acc: 0.9905709028244019  val: loss: 2905111.5 acc: 0.4632725715637207\n",
      "step: 27795\n",
      "train: loss: 13214.6181640625 acc: 0.9952523708343506  val: loss: 839432.625 acc: 0.6461546421051025\n",
      "step: 27800\n",
      "train: loss: 370687.5 acc: 0.9298057556152344  val: loss: 3034808.25 acc: -0.8716936111450195\n",
      "step: 27805\n",
      "train: loss: 54847.73046875 acc: 0.9839913249015808  val: loss: 2807825.5 acc: 0.37304389476776123\n",
      "step: 27810\n",
      "train: loss: 54464.37109375 acc: 0.9363507032394409  val: loss: 544058.3125 acc: 0.660554051399231\n",
      "step: 27815\n",
      "train: loss: 105426.7734375 acc: 0.9821734428405762  val: loss: 432848.6875 acc: 0.9404171109199524\n",
      "step: 27820\n",
      "train: loss: 74960.796875 acc: 0.9919008016586304  val: loss: 2251052.25 acc: 0.24732273817062378\n",
      "step: 27825\n",
      "train: loss: 679971.125 acc: 0.9001663327217102  val: loss: 1239007.25 acc: -0.27635693550109863\n",
      "step: 27830\n",
      "train: loss: 99132.6328125 acc: 0.9885103702545166  val: loss: 1327599.375 acc: 0.548348605632782\n",
      "step: 27835\n",
      "train: loss: 88539.4296875 acc: 0.9849684834480286  val: loss: 313584.84375 acc: 0.940276563167572\n",
      "step: 27840\n",
      "train: loss: 181665.78125 acc: 0.9876409769058228  val: loss: 2034881.625 acc: 0.47447389364242554\n",
      "step: 27845\n",
      "train: loss: 408862.625 acc: 0.9795732498168945  val: loss: 3586492.25 acc: -0.9995336532592773\n",
      "step: 27850\n",
      "train: loss: 272372.8125 acc: 0.954014778137207  val: loss: 529799.375 acc: 0.9324344992637634\n",
      "step: 27855\n",
      "train: loss: 173595.96875 acc: 0.9789375066757202  val: loss: 638520.25 acc: 0.8972616791725159\n",
      "step: 27860\n",
      "train: loss: 643125.375 acc: 0.9754563570022583  val: loss: 620553.75 acc: 0.8607786893844604\n",
      "step: 27865\n",
      "train: loss: 700711.0 acc: 0.9728667736053467  val: loss: 473077.75 acc: 0.9312536120414734\n",
      "step: 27870\n",
      "train: loss: 730413.5 acc: 0.9613308906555176  val: loss: 482505.21875 acc: 0.8519706726074219\n",
      "step: 27875\n",
      "train: loss: 402732.3125 acc: 0.9395185708999634  val: loss: 1186782.75 acc: 0.8556958436965942\n",
      "step: 27880\n",
      "train: loss: 152754.96875 acc: 0.9757367968559265  val: loss: 1473785.625 acc: 0.6394010782241821\n",
      "step: 27885\n",
      "train: loss: 279885.4375 acc: 0.9651341438293457  val: loss: 1104740.75 acc: 0.6799765825271606\n",
      "step: 27890\n",
      "train: loss: 299643.0 acc: 0.9598724246025085  val: loss: 421528.09375 acc: 0.9166416525840759\n",
      "step: 27895\n",
      "train: loss: 1751226.0 acc: 0.6006476879119873  val: loss: 1469183.875 acc: 0.8229614496231079\n",
      "step: 27900\n",
      "train: loss: 458100.53125 acc: 0.8376754522323608  val: loss: 1139542.5 acc: 0.7745436429977417\n",
      "step: 27905\n",
      "train: loss: 697568.25 acc: 0.7538608312606812  val: loss: 810478.75 acc: 0.7796422243118286\n",
      "step: 27910\n",
      "train: loss: 241092.09375 acc: 0.8797956705093384  val: loss: 743278.8125 acc: 0.7431336045265198\n",
      "step: 27915\n",
      "train: loss: 551797.375 acc: 0.7828614115715027  val: loss: 2576589.75 acc: 0.7302587032318115\n",
      "step: 27920\n",
      "train: loss: 746054.1875 acc: 0.7700570821762085  val: loss: 859593.75 acc: 0.7612051963806152\n",
      "step: 27925\n",
      "train: loss: 312390.59375 acc: 0.8489412665367126  val: loss: 1514108.5 acc: 0.6062607765197754\n",
      "step: 27930\n",
      "train: loss: 473331.75 acc: 0.7790035605430603  val: loss: 789105.3125 acc: 0.7486617565155029\n",
      "step: 27935\n",
      "train: loss: 100331.0625 acc: 0.8902742266654968  val: loss: 3573160.75 acc: 0.5501080751419067\n",
      "step: 27940\n",
      "train: loss: 29378.0625 acc: 0.9726932048797607  val: loss: 3820233.25 acc: 0.5586133003234863\n",
      "step: 27945\n",
      "train: loss: 28264.736328125 acc: 0.976294755935669  val: loss: 2728712.75 acc: 0.5485343933105469\n",
      "step: 27950\n",
      "train: loss: 32938.140625 acc: 0.9725050926208496  val: loss: 1124697.0 acc: 0.660109281539917\n",
      "step: 27955\n",
      "train: loss: 86867.6953125 acc: 0.9234850406646729  val: loss: 4333991.5 acc: 0.49226945638656616\n",
      "step: 27960\n",
      "train: loss: 268913.3125 acc: 0.8162931203842163  val: loss: 2969527.25 acc: 0.5752016305923462\n",
      "step: 27965\n",
      "train: loss: 72894.640625 acc: 0.9432207942008972  val: loss: 753517.25 acc: 0.7277400493621826\n",
      "step: 27970\n",
      "train: loss: 401157.46875 acc: 0.7688863277435303  val: loss: 1066169.375 acc: 0.7237887978553772\n",
      "step: 27975\n",
      "train: loss: 159173.265625 acc: 0.8817712664604187  val: loss: 1598784.0 acc: 0.6712838411331177\n",
      "step: 27980\n",
      "train: loss: 826722.875 acc: 0.7130634188652039  val: loss: 313864.21875 acc: 0.8122400045394897\n",
      "step: 27985\n",
      "train: loss: 84574.0546875 acc: 0.9189422130584717  val: loss: 2271556.75 acc: 0.6084098219871521\n",
      "step: 27990\n",
      "train: loss: 137422.9375 acc: 0.8833719491958618  val: loss: 1052055.0 acc: 0.6310299634933472\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 27995\n",
      "train: loss: 521831.46875 acc: 0.7257318496704102  val: loss: 494968.53125 acc: 0.7712070941925049\n",
      "step: 28000\n",
      "train: loss: 243449.234375 acc: 0.693961501121521  val: loss: 3163496.75 acc: 0.5144863128662109\n",
      "step: 28005\n",
      "train: loss: 1845612.875 acc: 0.65082848072052  val: loss: 3356173.0 acc: 0.7034907341003418\n",
      "step: 28010\n",
      "train: loss: 663688.25 acc: 0.8914044499397278  val: loss: 351053.5625 acc: 0.8816384673118591\n",
      "step: 28015\n",
      "train: loss: 822793.25 acc: 0.9356269240379333  val: loss: 815415.375 acc: 0.8758062720298767\n",
      "step: 28020\n",
      "train: loss: 357750.78125 acc: 0.9636856317520142  val: loss: 446893.1875 acc: 0.9260410666465759\n",
      "step: 28025\n",
      "train: loss: 149454.90625 acc: 0.9777510166168213  val: loss: 454811.53125 acc: 0.9632476568222046\n",
      "step: 28030\n",
      "train: loss: 550615.4375 acc: 0.9055455923080444  val: loss: 556978.625 acc: 0.9313217401504517\n",
      "step: 28035\n",
      "train: loss: 89227.96875 acc: 0.9887665510177612  val: loss: 378142.40625 acc: 0.8770784139633179\n",
      "step: 28040\n",
      "train: loss: 76891.875 acc: 0.9916201233863831  val: loss: 227604.21875 acc: 0.9731321930885315\n",
      "step: 28045\n",
      "train: loss: 36596.2109375 acc: 0.9971323609352112  val: loss: 1929893.375 acc: 0.668670654296875\n",
      "step: 28050\n",
      "train: loss: 45150.859375 acc: 0.992020845413208  val: loss: 1169097.5 acc: 0.8381707668304443\n",
      "step: 28055\n",
      "train: loss: 46639.23046875 acc: 0.9942927956581116  val: loss: 432214.1875 acc: 0.9606513381004333\n",
      "step: 28060\n",
      "train: loss: 19095.38671875 acc: 0.9941224455833435  val: loss: 838949.3125 acc: 0.8879995346069336\n",
      "step: 28065\n",
      "train: loss: 7969.0068359375 acc: 0.9977704286575317  val: loss: 283622.90625 acc: 0.9558645486831665\n",
      "step: 28070\n",
      "train: loss: 7600.06298828125 acc: 0.9763948917388916  val: loss: 1073219.375 acc: 0.7818595170974731\n",
      "step: 28075\n",
      "train: loss: 12278.212890625 acc: 0.981927752494812  val: loss: 1974104.75 acc: 0.8119456768035889\n",
      "step: 28080\n",
      "train: loss: 248256.953125 acc: 0.9413331747055054  val: loss: 1444936.25 acc: 0.8799474239349365\n",
      "step: 28085\n",
      "train: loss: 23876.3046875 acc: 0.9856505393981934  val: loss: 1066687.0 acc: 0.5003857612609863\n",
      "step: 28090\n",
      "train: loss: 14096.1962890625 acc: 0.9402585625648499  val: loss: 885289.5 acc: 0.8810818791389465\n",
      "step: 28095\n",
      "train: loss: 8224.3466796875 acc: 0.9834158420562744  val: loss: 431111.03125 acc: 0.9252366423606873\n",
      "step: 28100\n",
      "train: loss: 9473.4267578125 acc: 0.9548965692520142  val: loss: 1326039.125 acc: 0.8392249941825867\n",
      "step: 28105\n",
      "train: loss: 7606.947265625 acc: 0.9847511649131775  val: loss: 290419.96875 acc: 0.9540266990661621\n",
      "step: 28110\n",
      "train: loss: 21332.77734375 acc: 0.9902451634407043  val: loss: 2949657.75 acc: 0.36833006143569946\n",
      "step: 28115\n",
      "train: loss: 12646.3837890625 acc: 0.9912572503089905  val: loss: 975299.3125 acc: 0.9007642865180969\n",
      "step: 28120\n",
      "train: loss: 38233.77734375 acc: 0.9678877592086792  val: loss: 1973317.125 acc: 0.08102291822433472\n",
      "step: 28125\n",
      "train: loss: 60381.3984375 acc: 0.9697979688644409  val: loss: 540376.75 acc: 0.8938847780227661\n",
      "step: 28130\n",
      "train: loss: 7911.4306640625 acc: 0.9874488115310669  val: loss: 2039889.625 acc: 0.7194348573684692\n",
      "step: 28135\n",
      "train: loss: 4472.46728515625 acc: 0.9939906001091003  val: loss: 1207180.875 acc: 0.8097938299179077\n",
      "step: 28140\n",
      "train: loss: 9064.2705078125 acc: 0.9952957630157471  val: loss: 2494526.75 acc: -0.10331392288208008\n",
      "step: 28145\n",
      "train: loss: 19880.310546875 acc: 0.9941415190696716  val: loss: 2653601.25 acc: 0.4487200379371643\n",
      "step: 28150\n",
      "train: loss: 33525.67578125 acc: 0.9810299873352051  val: loss: 4162509.25 acc: -0.6507567167282104\n",
      "step: 28155\n",
      "train: loss: 37384.328125 acc: 0.9903382658958435  val: loss: 1956823.75 acc: -0.07277286052703857\n",
      "step: 28160\n",
      "train: loss: 44417.85546875 acc: 0.9864579439163208  val: loss: 1673933.25 acc: 0.5239825248718262\n",
      "step: 28165\n",
      "train: loss: 89490.765625 acc: 0.9648317694664001  val: loss: 2398974.75 acc: 0.7191216945648193\n",
      "step: 28170\n",
      "train: loss: 82206.3515625 acc: 0.9790197610855103  val: loss: 1670443.5 acc: 0.5177381634712219\n",
      "step: 28175\n",
      "train: loss: 141389.53125 acc: 0.9559555649757385  val: loss: 1546170.125 acc: 0.7754675149917603\n",
      "step: 28180\n",
      "train: loss: 260424.59375 acc: 0.9363763332366943  val: loss: 424105.84375 acc: 0.9026683568954468\n",
      "step: 28185\n",
      "train: loss: 49789.0 acc: 0.9948059916496277  val: loss: 1519391.75 acc: 0.8060328960418701\n",
      "step: 28190\n",
      "train: loss: 92852.90625 acc: 0.9932723641395569  val: loss: 613668.5 acc: 0.6962676048278809\n",
      "step: 28195\n",
      "train: loss: 95273.9765625 acc: 0.9900089502334595  val: loss: 1609604.0 acc: 0.8623189330101013\n",
      "step: 28200\n",
      "train: loss: 40902.65234375 acc: 0.9888196587562561  val: loss: 263715.4375 acc: 0.9282422661781311\n",
      "step: 28205\n",
      "train: loss: 115535.8984375 acc: 0.9864193201065063  val: loss: 1203790.0 acc: 0.357336163520813\n",
      "step: 28210\n",
      "train: loss: 3585774.25 acc: 0.8191699981689453  val: loss: 1197015.25 acc: 0.7732159495353699\n",
      "step: 28215\n",
      "train: loss: 267957.375 acc: 0.9577727317810059  val: loss: 2133764.0 acc: 0.23245394229888916\n",
      "step: 28220\n",
      "train: loss: 61397.796875 acc: 0.9886006712913513  val: loss: 615010.625 acc: 0.9240325093269348\n",
      "step: 28225\n",
      "train: loss: 694992.5625 acc: 0.9669843316078186  val: loss: 665961.625 acc: 0.8273828029632568\n",
      "step: 28230\n",
      "train: loss: 908936.0 acc: 0.9708057045936584  val: loss: 402354.3125 acc: 0.9240691065788269\n",
      "step: 28235\n",
      "train: loss: 2148737.0 acc: 0.9410254955291748  val: loss: 377022.65625 acc: 0.8743247985839844\n",
      "step: 28240\n",
      "train: loss: 1686179.25 acc: 0.8513215780258179  val: loss: 310711.09375 acc: 0.8952286839485168\n",
      "step: 28245\n",
      "train: loss: 492170.90625 acc: 0.9675700068473816  val: loss: 1637517.25 acc: 0.35722851753234863\n",
      "step: 28250\n",
      "train: loss: 725869.625 acc: 0.9398736953735352  val: loss: 346815.96875 acc: 0.8276140689849854\n",
      "step: 28255\n",
      "train: loss: 306737.90625 acc: 0.9063708186149597  val: loss: 393152.78125 acc: 0.9031923413276672\n",
      "step: 28260\n",
      "train: loss: 2234532.75 acc: 0.08936023712158203  val: loss: 247265.0625 acc: 0.8626639246940613\n",
      "step: 28265\n",
      "train: loss: 722259.1875 acc: 0.6451849937438965  val: loss: 818055.5 acc: 0.7756929993629456\n",
      "step: 28270\n",
      "train: loss: 601579.0 acc: 0.8031289577484131  val: loss: 1248716.125 acc: 0.7312306761741638\n",
      "step: 28275\n",
      "train: loss: 1948698.0 acc: 0.7096349000930786  val: loss: 696934.5 acc: 0.7185417413711548\n",
      "step: 28280\n",
      "train: loss: 494502.1875 acc: 0.8860956430435181  val: loss: 449700.8125 acc: 0.808139979839325\n",
      "step: 28285\n",
      "train: loss: 1043184.0625 acc: 0.3530096411705017  val: loss: 606583.1875 acc: 0.8110096454620361\n",
      "step: 28290\n",
      "train: loss: 661349.125 acc: 0.7679070830345154  val: loss: 1533557.625 acc: 0.682400107383728\n",
      "step: 28295\n",
      "train: loss: 208516.859375 acc: 0.8891696333885193  val: loss: 1310262.5 acc: 0.6321834325790405\n",
      "step: 28300\n",
      "train: loss: 120921.8046875 acc: 0.900402307510376  val: loss: 1577946.25 acc: 0.673365592956543\n",
      "step: 28305\n",
      "train: loss: 22849.115234375 acc: 0.979598879814148  val: loss: 817378.9375 acc: 0.7360155582427979\n",
      "step: 28310\n",
      "train: loss: 30028.421875 acc: 0.9751686453819275  val: loss: 1916544.0 acc: 0.6773772835731506\n",
      "step: 28315\n",
      "train: loss: 39517.7421875 acc: 0.9666867256164551  val: loss: 3648723.0 acc: 0.5696953535079956\n",
      "step: 28320\n",
      "train: loss: 226081.28125 acc: 0.8848045468330383  val: loss: 1453341.25 acc: 0.63362717628479\n",
      "step: 28325\n",
      "train: loss: 390802.96875 acc: 0.8132869601249695  val: loss: 1763754.125 acc: 0.6745301485061646\n",
      "step: 28330\n",
      "train: loss: 288766.34375 acc: 0.8290100693702698  val: loss: 2228230.0 acc: 0.6189767122268677\n",
      "step: 28335\n",
      "train: loss: 33302.97265625 acc: 0.959662139415741  val: loss: 3293041.5 acc: 0.5308997631072998\n",
      "step: 28340\n",
      "train: loss: 81908.3984375 acc: 0.9128753542900085  val: loss: 1790644.5 acc: 0.6385117173194885\n",
      "step: 28345\n",
      "train: loss: 519552.0 acc: 0.7013644576072693  val: loss: 776173.25 acc: 0.683086633682251\n",
      "step: 28350\n",
      "train: loss: 415759.1875 acc: 0.7825571298599243  val: loss: 1846459.625 acc: 0.5893831849098206\n",
      "step: 28355\n",
      "train: loss: 633476.8125 acc: 0.7088390588760376  val: loss: 1650934.5 acc: 0.6370369791984558\n",
      "step: 28360\n",
      "train: loss: 302230.40625 acc: 0.8126659989356995  val: loss: 4994792.5 acc: 0.5350474119186401\n",
      "step: 28365\n",
      "train: loss: 826450.5625 acc: 0.6744276285171509  val: loss: 4975478.5 acc: 0.542728066444397\n",
      "step: 28370\n",
      "train: loss: 1854987.75 acc: 0.6994796991348267  val: loss: 589389.3125 acc: 0.8577307462692261\n",
      "step: 28375\n",
      "train: loss: 1286076.75 acc: 0.7551137208938599  val: loss: 718826.5 acc: 0.8807140588760376\n",
      "step: 28380\n",
      "train: loss: 1487511.25 acc: 0.7950242757797241  val: loss: 1043441.1875 acc: 0.788601815700531\n",
      "step: 28385\n",
      "train: loss: 236832.328125 acc: 0.9794023633003235  val: loss: 2034737.0 acc: 0.4786888360977173\n",
      "step: 28390\n",
      "train: loss: 155820.453125 acc: 0.9805012345314026  val: loss: 1289730.0 acc: 0.8800638318061829\n",
      "step: 28395\n",
      "train: loss: 93382.3828125 acc: 0.9866244792938232  val: loss: 299808.0 acc: 0.8897775411605835\n",
      "step: 28400\n",
      "train: loss: 272254.59375 acc: 0.9695137143135071  val: loss: 1004705.0625 acc: 0.8892017006874084\n",
      "step: 28405\n",
      "train: loss: 77466.15625 acc: 0.9932141304016113  val: loss: 1217949.125 acc: 0.8920205235481262\n",
      "step: 28410\n",
      "train: loss: 78091.6796875 acc: 0.9938605427742004  val: loss: 1590641.5 acc: 0.4998781681060791\n",
      "step: 28415\n",
      "train: loss: 25514.666015625 acc: 0.997486412525177  val: loss: 1971393.375 acc: 0.8501861095428467\n",
      "step: 28420\n",
      "train: loss: 35248.47265625 acc: 0.9949747323989868  val: loss: 638672.375 acc: 0.8528311252593994\n",
      "step: 28425\n",
      "train: loss: 68814.546875 acc: 0.9849602580070496  val: loss: 2075979.5 acc: 0.23634368181228638\n",
      "step: 28430\n",
      "train: loss: 13716.0712890625 acc: 0.9961959719657898  val: loss: 1021124.6875 acc: 0.5199593305587769\n",
      "step: 28435\n",
      "train: loss: 9535.9296875 acc: 0.9887970685958862  val: loss: 2947736.5 acc: 0.7587517499923706\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 28440\n",
      "train: loss: 6578.94140625 acc: 0.97898930311203  val: loss: 2028616.5 acc: 0.7069678902626038\n",
      "step: 28445\n",
      "train: loss: 14317.7685546875 acc: 0.983249306678772  val: loss: 1206796.25 acc: 0.4100760817527771\n",
      "step: 28450\n",
      "train: loss: 25287.01953125 acc: 0.9826835989952087  val: loss: 2162924.25 acc: -0.02708578109741211\n",
      "step: 28455\n",
      "train: loss: 13822.884765625 acc: 0.9730677008628845  val: loss: 2057551.25 acc: 0.6176860928535461\n",
      "step: 28460\n",
      "train: loss: 250498.765625 acc: 0.5319597721099854  val: loss: 1573904.875 acc: 0.8131793737411499\n",
      "step: 28465\n",
      "train: loss: 2676.572265625 acc: 0.9895690679550171  val: loss: 1687939.25 acc: 0.6192566156387329\n",
      "step: 28470\n",
      "train: loss: 18783.736328125 acc: 0.9828811883926392  val: loss: 2814931.25 acc: 0.695152759552002\n",
      "step: 28475\n",
      "train: loss: 30508.939453125 acc: 0.9090330004692078  val: loss: 2120510.0 acc: 0.6706618666648865\n",
      "step: 28480\n",
      "train: loss: 36772.8359375 acc: 0.9814725518226624  val: loss: 2075482.5 acc: 0.41899287700653076\n",
      "step: 28485\n",
      "train: loss: 11492.5009765625 acc: 0.9936630129814148  val: loss: 1622935.375 acc: 0.81373131275177\n",
      "step: 28490\n",
      "train: loss: 11526.2822265625 acc: 0.9942716360092163  val: loss: 1319286.25 acc: 0.7824980616569519\n",
      "step: 28495\n",
      "train: loss: 7849.8701171875 acc: 0.9935831427574158  val: loss: 1373059.125 acc: 0.4741330146789551\n",
      "step: 28500\n",
      "train: loss: 12306.1455078125 acc: 0.9906542897224426  val: loss: 3321839.25 acc: 0.701473593711853\n",
      "step: 28505\n",
      "train: loss: 12871.189453125 acc: 0.993776798248291  val: loss: 1546757.5 acc: 0.372394859790802\n",
      "step: 28510\n",
      "train: loss: 16108.099609375 acc: 0.9917322993278503  val: loss: 2253876.75 acc: 0.3798249363899231\n",
      "step: 28515\n",
      "train: loss: 18643.794921875 acc: 0.9948607683181763  val: loss: 1883618.0 acc: -0.4159764051437378\n",
      "step: 28520\n",
      "train: loss: 26955.87109375 acc: 0.9949698448181152  val: loss: 1853970.25 acc: 0.7546193599700928\n",
      "step: 28525\n",
      "train: loss: 44362.3828125 acc: 0.9826732873916626  val: loss: 2057859.75 acc: 0.582255482673645\n",
      "step: 28530\n",
      "train: loss: 138207.296875 acc: 0.950905442237854  val: loss: 796510.9375 acc: 0.7671539783477783\n",
      "step: 28535\n",
      "train: loss: 199110.78125 acc: 0.9417991638183594  val: loss: 1450855.75 acc: 0.6185462474822998\n",
      "step: 28540\n",
      "train: loss: 38744.6796875 acc: 0.9797720909118652  val: loss: 378684.625 acc: 0.9013246297836304\n",
      "step: 28545\n",
      "train: loss: 368056.71875 acc: 0.8780081272125244  val: loss: 1537327.375 acc: 0.3619845509529114\n",
      "step: 28550\n",
      "train: loss: 127760.3828125 acc: 0.9816572666168213  val: loss: 2048938.75 acc: 0.6729989647865295\n",
      "step: 28555\n",
      "train: loss: 167645.484375 acc: 0.982793390750885  val: loss: 1194729.375 acc: 0.44327789545059204\n",
      "step: 28560\n",
      "train: loss: 59931.70703125 acc: 0.9924485087394714  val: loss: 1063706.125 acc: -0.7682950496673584\n",
      "step: 28565\n",
      "train: loss: 90367.5625 acc: 0.976931095123291  val: loss: 128459.984375 acc: 0.9733501672744751\n",
      "step: 28570\n",
      "train: loss: 52281.98828125 acc: 0.9955765604972839  val: loss: 472687.84375 acc: 0.8683754205703735\n",
      "step: 28575\n",
      "train: loss: 278894.6875 acc: 0.9788223505020142  val: loss: 955454.8125 acc: 0.7671183943748474\n",
      "step: 28580\n",
      "train: loss: 264553.0 acc: 0.9800350666046143  val: loss: 1278333.75 acc: 0.7940016388893127\n",
      "step: 28585\n",
      "train: loss: 85280.6875 acc: 0.8684829473495483  val: loss: 1318477.375 acc: 0.649165153503418\n",
      "step: 28590\n",
      "train: loss: 1198078.375 acc: 0.9255581498146057  val: loss: 876117.8125 acc: 0.683114767074585\n",
      "step: 28595\n",
      "train: loss: 503678.09375 acc: 0.9898198246955872  val: loss: 642415.0625 acc: 0.6608353853225708\n",
      "step: 28600\n",
      "train: loss: 1684776.75 acc: 0.9253802299499512  val: loss: 630510.75 acc: 0.7799863815307617\n",
      "step: 28605\n",
      "train: loss: 555304.0625 acc: 0.9736518859863281  val: loss: 909046.9375 acc: 0.8488415479660034\n",
      "step: 28610\n",
      "train: loss: 1069728.75 acc: 0.9127700328826904  val: loss: 307433.0625 acc: 0.9343867301940918\n",
      "step: 28615\n",
      "train: loss: 555519.25 acc: 0.9206535220146179  val: loss: 410979.3125 acc: 0.8851533532142639\n",
      "step: 28620\n",
      "train: loss: 648407.375 acc: 0.9489390850067139  val: loss: 686045.6875 acc: 0.8615895509719849\n",
      "step: 28625\n",
      "train: loss: 1338208.375 acc: 0.5365213751792908  val: loss: 900241.6875 acc: 0.8920350074768066\n",
      "step: 28630\n",
      "train: loss: 874401.6875 acc: 0.7623080015182495  val: loss: 608608.1875 acc: 0.8127226829528809\n",
      "step: 28635\n",
      "train: loss: 439804.25 acc: 0.8628841638565063  val: loss: 397481.4375 acc: 0.8131075501441956\n",
      "step: 28640\n",
      "train: loss: 234468.234375 acc: 0.9130603075027466  val: loss: 354968.875 acc: 0.856895387172699\n",
      "step: 28645\n",
      "train: loss: 398025.6875 acc: 0.8911415934562683  val: loss: 669372.4375 acc: 0.8844596147537231\n",
      "step: 28650\n",
      "train: loss: 705178.9375 acc: 0.631237268447876  val: loss: 304964.96875 acc: 0.8026049137115479\n",
      "step: 28655\n",
      "train: loss: 357084.03125 acc: 0.7765834331512451  val: loss: 4215393.5 acc: 0.5933258533477783\n",
      "step: 28660\n",
      "train: loss: 1677361.625 acc: 0.6233839988708496  val: loss: 1347638.25 acc: 0.6042225360870361\n",
      "step: 28665\n",
      "train: loss: 130627.6484375 acc: 0.8909352421760559  val: loss: 1583653.0 acc: 0.6561670303344727\n",
      "step: 28670\n",
      "train: loss: 404167.03125 acc: 0.7815342545509338  val: loss: 1525690.0 acc: 0.6517807841300964\n",
      "step: 28675\n",
      "train: loss: 28963.853515625 acc: 0.9741340279579163  val: loss: 4080007.25 acc: 0.532656192779541\n",
      "step: 28680\n",
      "train: loss: 198734.78125 acc: 0.8968695998191833  val: loss: 3460071.0 acc: 0.576969563961029\n",
      "step: 28685\n",
      "train: loss: 9395.904296875 acc: 0.9919219017028809  val: loss: 3769344.25 acc: 0.49418044090270996\n",
      "step: 28690\n",
      "train: loss: 70302.9296875 acc: 0.946274995803833  val: loss: 2675662.25 acc: 0.6308395266532898\n",
      "step: 28695\n",
      "train: loss: 198360.78125 acc: 0.8802988529205322  val: loss: 3535214.75 acc: 0.5155961513519287\n",
      "step: 28700\n",
      "train: loss: 62236.7734375 acc: 0.9356909394264221  val: loss: 1104878.875 acc: 0.6630116701126099\n",
      "step: 28705\n",
      "train: loss: 44599.97265625 acc: 0.9542519450187683  val: loss: 1611312.375 acc: 0.660019040107727\n",
      "step: 28710\n",
      "train: loss: 41941.6015625 acc: 0.962857186794281  val: loss: 4832011.5 acc: 0.4456392526626587\n",
      "step: 28715\n",
      "train: loss: 350067.9375 acc: 0.8043144941329956  val: loss: 695871.625 acc: 0.7107295989990234\n",
      "step: 28720\n",
      "train: loss: 463666.0625 acc: 0.754891574382782  val: loss: 652611.8125 acc: 0.7824357748031616\n",
      "step: 28725\n",
      "train: loss: 533926.5625 acc: 0.7432546615600586  val: loss: 2769073.25 acc: 0.6251943111419678\n",
      "step: 28730\n",
      "train: loss: 251211.625 acc: 0.821708083152771  val: loss: 1032791.9375 acc: 0.7006930112838745\n",
      "step: 28735\n",
      "train: loss: 399273.375 acc: 0.7839733362197876  val: loss: 2512815.5 acc: 0.5995083451271057\n",
      "step: 28740\n",
      "train: loss: 1535145.5 acc: 0.7634106278419495  val: loss: 3089260.25 acc: 0.774655818939209\n",
      "step: 28745\n",
      "train: loss: 680227.5625 acc: 0.9374206066131592  val: loss: 1336037.125 acc: 0.8960886597633362\n",
      "step: 28750\n",
      "train: loss: 908072.1875 acc: 0.9194627404212952  val: loss: 1728203.25 acc: 0.5330897569656372\n",
      "step: 28755\n",
      "train: loss: 566607.3125 acc: 0.907188355922699  val: loss: 1021160.4375 acc: 0.7063388824462891\n",
      "step: 28760\n",
      "train: loss: 62133.125 acc: 0.990566611289978  val: loss: 1115001.375 acc: 0.6831772327423096\n",
      "step: 28765\n",
      "train: loss: 248310.84375 acc: 0.9576184153556824  val: loss: 544780.1875 acc: 0.7306965589523315\n",
      "step: 28770\n",
      "train: loss: 84667.359375 acc: 0.9909878373146057  val: loss: 3238634.25 acc: -1.8259472846984863\n",
      "step: 28775\n",
      "train: loss: 28800.59765625 acc: 0.9974672794342041  val: loss: 855959.875 acc: 0.7409906387329102\n",
      "step: 28780\n",
      "train: loss: 33120.50390625 acc: 0.9956255555152893  val: loss: 2160753.75 acc: -0.896257758140564\n",
      "step: 28785\n",
      "train: loss: 50748.1484375 acc: 0.9926642179489136  val: loss: 2566110.25 acc: 0.03754770755767822\n",
      "step: 28790\n",
      "train: loss: 22216.0546875 acc: 0.9952466487884521  val: loss: 680628.3125 acc: 0.9238547682762146\n",
      "step: 28795\n",
      "train: loss: 22592.16796875 acc: 0.9904939532279968  val: loss: 331027.875 acc: 0.8970016241073608\n",
      "step: 28800\n",
      "train: loss: 4366.0 acc: 0.9842696189880371  val: loss: 1115960.25 acc: 0.4145891070365906\n",
      "step: 28805\n",
      "train: loss: 7684.296875 acc: 0.9958714842796326  val: loss: 1923072.875 acc: 0.32411861419677734\n",
      "step: 28810\n",
      "train: loss: 25311.0390625 acc: 0.9857257604598999  val: loss: 1098120.375 acc: 0.8572835326194763\n",
      "step: 28815\n",
      "train: loss: 48015.25390625 acc: 0.9402838349342346  val: loss: 1718526.25 acc: 0.5395998954772949\n",
      "step: 28820\n",
      "train: loss: 23144.443359375 acc: 0.9712435007095337  val: loss: 1683839.125 acc: 0.35398077964782715\n",
      "step: 28825\n",
      "train: loss: 6754.521484375 acc: 0.9882588982582092  val: loss: 536570.75 acc: 0.9206919074058533\n",
      "step: 28830\n",
      "train: loss: 5628.63232421875 acc: 0.9832346439361572  val: loss: 605359.6875 acc: 0.8481698036193848\n",
      "step: 28835\n",
      "train: loss: 19839.46875 acc: 0.9426848292350769  val: loss: 679464.3125 acc: 0.9229469299316406\n",
      "step: 28840\n",
      "train: loss: 10134.501953125 acc: 0.9882888197898865  val: loss: 1624235.75 acc: 0.8088369369506836\n",
      "step: 28845\n",
      "train: loss: 21963.55859375 acc: 0.9847021698951721  val: loss: 827032.375 acc: 0.8699144124984741\n",
      "step: 28850\n",
      "train: loss: 10493.0224609375 acc: 0.9928200244903564  val: loss: 661622.0625 acc: 0.3679732084274292\n",
      "step: 28855\n",
      "train: loss: 17861.9765625 acc: 0.9906852841377258  val: loss: 1828897.25 acc: 0.40772563219070435\n",
      "step: 28860\n",
      "train: loss: 27553.771484375 acc: 0.9684261083602905  val: loss: 1195259.875 acc: 0.7877143025398254\n",
      "step: 28865\n",
      "train: loss: 4335.302734375 acc: 0.9929131269454956  val: loss: 1002450.75 acc: 0.8355178833007812\n",
      "step: 28870\n",
      "train: loss: 5718.97802734375 acc: 0.9973376989364624  val: loss: 455639.84375 acc: 0.9307751059532166\n",
      "step: 28875\n",
      "train: loss: 20371.521484375 acc: 0.9875265955924988  val: loss: 1382806.875 acc: 0.3277985453605652\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 28880\n",
      "train: loss: 25853.095703125 acc: 0.9926208853721619  val: loss: 1301057.375 acc: 0.7723122835159302\n",
      "step: 28885\n",
      "train: loss: 44872.3203125 acc: 0.9811803102493286  val: loss: 300771.03125 acc: 0.9460102915763855\n",
      "step: 28890\n",
      "train: loss: 12635.4765625 acc: 0.9923027157783508  val: loss: 714942.25 acc: 0.6594581604003906\n",
      "step: 28895\n",
      "train: loss: 76565.890625 acc: 0.9712671637535095  val: loss: 316994.9375 acc: 0.8572426438331604\n",
      "step: 28900\n",
      "train: loss: 363137.34375 acc: 0.9229417443275452  val: loss: 284825.75 acc: 0.9132983684539795\n",
      "step: 28905\n",
      "train: loss: 41507.3203125 acc: 0.9479303359985352  val: loss: 336240.8125 acc: 0.9637930393218994\n",
      "step: 28910\n",
      "train: loss: 830321.4375 acc: 0.597032368183136  val: loss: 666756.4375 acc: 0.9114184975624084\n",
      "step: 28915\n",
      "train: loss: 585691.3125 acc: 0.8786780834197998  val: loss: 400085.6875 acc: 0.953773558139801\n",
      "step: 28920\n",
      "train: loss: 54607.93359375 acc: 0.9955756068229675  val: loss: 1568437.875 acc: 0.5749582052230835\n",
      "step: 28925\n",
      "train: loss: 70996.8203125 acc: 0.9926124811172485  val: loss: 899790.125 acc: 0.5359163284301758\n",
      "step: 28930\n",
      "train: loss: 197041.90625 acc: 0.9789182543754578  val: loss: 820035.375 acc: 0.8942835330963135\n",
      "step: 28935\n",
      "train: loss: 180550.75 acc: 0.9827335476875305  val: loss: 746064.4375 acc: 0.8128692507743835\n",
      "step: 28940\n",
      "train: loss: 478077.875 acc: 0.9602456092834473  val: loss: 290476.125 acc: 0.9189392924308777\n",
      "step: 28945\n",
      "train: loss: 220207.015625 acc: 0.9771116375923157  val: loss: 894859.3125 acc: 0.8450260758399963\n",
      "step: 28950\n",
      "train: loss: 107433.3515625 acc: 0.9714116454124451  val: loss: 646585.8125 acc: 0.8874128460884094\n",
      "step: 28955\n",
      "train: loss: 499688.40625 acc: 0.9748037457466125  val: loss: 791750.1875 acc: 0.8948413133621216\n",
      "step: 28960\n",
      "train: loss: 2137547.0 acc: 0.9134464263916016  val: loss: 314341.59375 acc: 0.9292019605636597\n",
      "step: 28965\n",
      "train: loss: 4421696.5 acc: 0.8675678968429565  val: loss: 423995.8125 acc: 0.8300447463989258\n",
      "step: 28970\n",
      "train: loss: 1038010.8125 acc: 0.9276203513145447  val: loss: 322767.125 acc: 0.9641027450561523\n",
      "step: 28975\n",
      "train: loss: 632462.8125 acc: 0.9636151790618896  val: loss: 1749074.75 acc: 0.7268381118774414\n",
      "step: 28980\n",
      "train: loss: 363707.625 acc: 0.9796159863471985  val: loss: 551024.5625 acc: 0.9583903551101685\n",
      "step: 28985\n",
      "train: loss: 487487.0625 acc: 0.9069764614105225  val: loss: 721836.3125 acc: 0.9358735680580139\n",
      "step: 28990\n",
      "train: loss: 681090.3125 acc: 0.7188810110092163  val: loss: 274479.125 acc: 0.9151486158370972\n",
      "step: 28995\n",
      "train: loss: 854126.375 acc: 0.6432809829711914  val: loss: 1692395.5 acc: 0.7464176416397095\n",
      "step: 29000\n",
      "train: loss: 740271.5625 acc: 0.7742019891738892  val: loss: 2425104.5 acc: 0.7126325964927673\n",
      "step: 29005\n",
      "train: loss: 268438.59375 acc: 0.8185731768608093  val: loss: 2319476.5 acc: 0.7594913840293884\n",
      "step: 29010\n",
      "train: loss: 368318.1875 acc: 0.8782522082328796  val: loss: 2280508.5 acc: 0.7668778896331787\n",
      "step: 29015\n",
      "train: loss: 739787.375 acc: 0.5677391290664673  val: loss: 481850.6875 acc: 0.8531992435455322\n",
      "step: 29020\n",
      "train: loss: 403010.34375 acc: 0.8118407726287842  val: loss: 1383288.625 acc: 0.6735700368881226\n",
      "step: 29025\n",
      "train: loss: 105506.3203125 acc: 0.9203345775604248  val: loss: 2118287.5 acc: 0.6079100370407104\n",
      "step: 29030\n",
      "train: loss: 56044.04296875 acc: 0.9527921676635742  val: loss: 4290711.5 acc: 0.5399242043495178\n",
      "step: 29035\n",
      "train: loss: 91405.1171875 acc: 0.9167349338531494  val: loss: 1461305.25 acc: 0.6690433621406555\n",
      "step: 29040\n",
      "train: loss: 156787.265625 acc: 0.9072237610816956  val: loss: 1530076.75 acc: 0.6905962228775024\n",
      "step: 29045\n",
      "train: loss: 18684.548828125 acc: 0.9837865233421326  val: loss: 1069764.25 acc: 0.7037687301635742\n",
      "step: 29050\n",
      "train: loss: 9027.876953125 acc: 0.9928866028785706  val: loss: 339044.125 acc: 0.7826346158981323\n",
      "step: 29055\n",
      "train: loss: 96126.2890625 acc: 0.9284480810165405  val: loss: 841318.4375 acc: 0.7340549230575562\n",
      "step: 29060\n",
      "train: loss: 90459.53125 acc: 0.9354344010353088  val: loss: 2667239.0 acc: 0.5409175157546997\n",
      "step: 29065\n",
      "train: loss: 72395.6796875 acc: 0.946120023727417  val: loss: 2452576.5 acc: 0.4846741557121277\n",
      "step: 29070\n",
      "train: loss: 85126.953125 acc: 0.8963565230369568  val: loss: 1857168.5 acc: 0.588485836982727\n",
      "step: 29075\n",
      "train: loss: 104094.765625 acc: 0.9172540903091431  val: loss: 2864919.5 acc: 0.5107553601264954\n",
      "step: 29080\n",
      "train: loss: 52021.390625 acc: 0.9529951810836792  val: loss: 586206.4375 acc: 0.7527438998222351\n",
      "step: 29085\n",
      "train: loss: 446016.25 acc: 0.7876079082489014  val: loss: 177878.40625 acc: 0.8824893832206726\n",
      "step: 29090\n",
      "train: loss: 159623.71875 acc: 0.8455623984336853  val: loss: 2534149.0 acc: 0.4999525547027588\n",
      "step: 29095\n",
      "train: loss: 179450.796875 acc: 0.8700809478759766  val: loss: 504282.5625 acc: 0.7805659770965576\n",
      "step: 29100\n",
      "train: loss: 652062.6875 acc: 0.7536779046058655  val: loss: 440447.0625 acc: 0.8058260679244995\n",
      "step: 29105\n",
      "train: loss: 864118.3125 acc: 0.8138164281845093  val: loss: 1520836.0 acc: 0.48505616188049316\n",
      "step: 29110\n",
      "train: loss: 1168920.125 acc: 0.8674445152282715  val: loss: 975480.75 acc: 0.5170851945877075\n",
      "step: 29115\n",
      "train: loss: 428234.0625 acc: 0.9668363928794861  val: loss: 657496.25 acc: 0.665229856967926\n",
      "step: 29120\n",
      "train: loss: 337198.1875 acc: 0.9623655676841736  val: loss: 882667.375 acc: 0.7759425640106201\n",
      "step: 29125\n",
      "train: loss: 198505.390625 acc: 0.9683793187141418  val: loss: 1037047.625 acc: 0.8964120149612427\n",
      "step: 29130\n",
      "train: loss: 67387.2421875 acc: 0.9893221259117126  val: loss: 1152793.5 acc: 0.8173942565917969\n",
      "step: 29135\n",
      "train: loss: 55696.15625 acc: 0.9927163124084473  val: loss: 729003.1875 acc: 0.8346278667449951\n",
      "step: 29140\n",
      "train: loss: 34292.06640625 acc: 0.9974237680435181  val: loss: 1598317.875 acc: 0.14274048805236816\n",
      "step: 29145\n",
      "train: loss: 55188.359375 acc: 0.9926621913909912  val: loss: 1145112.875 acc: 0.6784974336624146\n",
      "step: 29150\n",
      "train: loss: 47442.0078125 acc: 0.9898958206176758  val: loss: 528439.6875 acc: 0.9236314296722412\n",
      "step: 29155\n",
      "train: loss: 21972.24609375 acc: 0.9909476041793823  val: loss: 256338.0 acc: 0.9520449638366699\n",
      "step: 29160\n",
      "train: loss: 15695.62109375 acc: 0.9909434914588928  val: loss: 626278.1875 acc: 0.8997005820274353\n",
      "step: 29165\n",
      "train: loss: 7159.8642578125 acc: 0.9746378064155579  val: loss: 850377.0625 acc: 0.766285240650177\n",
      "step: 29170\n",
      "train: loss: 15999.4267578125 acc: 0.9892569780349731  val: loss: 565395.3125 acc: 0.9102095365524292\n",
      "step: 29175\n",
      "train: loss: 8533.4013671875 acc: 0.979010283946991  val: loss: 1796655.625 acc: 0.7456854581832886\n",
      "step: 29180\n",
      "train: loss: 6188.34716796875 acc: 0.99583500623703  val: loss: 1081751.875 acc: 0.7879365086555481\n",
      "step: 29185\n",
      "train: loss: 22235.75 acc: 0.9584932327270508  val: loss: 782530.75 acc: 0.8344306945800781\n",
      "step: 29190\n",
      "train: loss: 25822.8515625 acc: 0.9823409914970398  val: loss: 449092.8125 acc: 0.8725857734680176\n",
      "step: 29195\n",
      "train: loss: 12406.9599609375 acc: 0.9842305183410645  val: loss: 1508312.875 acc: 0.6394625902175903\n",
      "step: 29200\n",
      "train: loss: 22588.056640625 acc: 0.9701992869377136  val: loss: 922432.375 acc: 0.9056941866874695\n",
      "step: 29205\n",
      "train: loss: 14537.326171875 acc: 0.9836545586585999  val: loss: 1146870.0 acc: 0.5202271342277527\n",
      "step: 29210\n",
      "train: loss: 16818.67578125 acc: 0.9873789548873901  val: loss: 813818.8125 acc: 0.8922666907310486\n",
      "step: 29215\n",
      "train: loss: 16070.83984375 acc: 0.9930270910263062  val: loss: 368343.6875 acc: 0.8207079172134399\n",
      "step: 29220\n",
      "train: loss: 5895.271484375 acc: 0.9969476461410522  val: loss: 1457218.625 acc: 0.42760175466537476\n",
      "step: 29225\n",
      "train: loss: 111041.6796875 acc: 0.9107657074928284  val: loss: 233062.625 acc: 0.930605411529541\n",
      "step: 29230\n",
      "train: loss: 7843.8671875 acc: 0.9843971133232117  val: loss: 1430534.0 acc: 0.7216933369636536\n",
      "step: 29235\n",
      "train: loss: 29871.97265625 acc: 0.9805393815040588  val: loss: 1724954.75 acc: 0.6268632411956787\n",
      "step: 29240\n",
      "train: loss: 16449.466796875 acc: 0.988957941532135  val: loss: 868618.3125 acc: 0.7515139579772949\n",
      "step: 29245\n",
      "train: loss: 30592.5625 acc: 0.9890317320823669  val: loss: 225432.0 acc: 0.7636955380439758\n",
      "step: 29250\n",
      "train: loss: 25987.53125 acc: 0.9932808876037598  val: loss: 499225.21875 acc: 0.8831719756126404\n",
      "step: 29255\n",
      "train: loss: 36598.1953125 acc: 0.9901174306869507  val: loss: 454635.4375 acc: 0.9367554187774658\n",
      "step: 29260\n",
      "train: loss: 10116.99609375 acc: 0.9952025413513184  val: loss: 199750.453125 acc: 0.9703311324119568\n",
      "step: 29265\n",
      "train: loss: 96021.9140625 acc: 0.9733317494392395  val: loss: 988201.375 acc: 0.855779230594635\n",
      "step: 29270\n",
      "train: loss: 53044.28515625 acc: 0.9770007729530334  val: loss: 1864054.625 acc: 0.3120080232620239\n",
      "step: 29275\n",
      "train: loss: 35155.0625 acc: 0.9876764416694641  val: loss: 687458.75 acc: 0.9308620691299438\n",
      "step: 29280\n",
      "train: loss: 373764.4375 acc: 0.9387301206588745  val: loss: 454913.15625 acc: 0.9569972157478333\n",
      "step: 29285\n",
      "train: loss: 36201.609375 acc: 0.9956939220428467  val: loss: 219923.21875 acc: 0.9279372096061707\n",
      "step: 29290\n",
      "train: loss: 56884.078125 acc: 0.9939481019973755  val: loss: 574580.1875 acc: 0.950044572353363\n",
      "step: 29295\n",
      "train: loss: 246631.65625 acc: 0.9576320648193359  val: loss: 942118.5625 acc: 0.8599151372909546\n",
      "step: 29300\n",
      "train: loss: 114732.3984375 acc: 0.9841115474700928  val: loss: 1497573.875 acc: 0.5650392174720764\n",
      "step: 29305\n",
      "train: loss: 1340725.125 acc: 0.8332757949829102  val: loss: 579152.9375 acc: 0.918687105178833\n",
      "step: 29310\n",
      "train: loss: 514181.375 acc: 0.9725533723831177  val: loss: 1020136.0 acc: 0.6468594074249268\n",
      "step: 29315\n",
      "train: loss: 140986.0625 acc: 0.9815873503684998  val: loss: 697821.9375 acc: 0.8664461970329285\n",
      "step: 29320\n",
      "train: loss: 1086828.5 acc: 0.9261811375617981  val: loss: 2448587.25 acc: 0.5103925466537476\n",
      "step: 29325\n",
      "train: loss: 494113.5625 acc: 0.9846272468566895  val: loss: 1377159.875 acc: 0.8365426659584045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 29330\n",
      "train: loss: 990122.75 acc: 0.9553264379501343  val: loss: 488761.28125 acc: 0.8210883140563965\n",
      "step: 29335\n",
      "train: loss: 710508.1875 acc: 0.968391478061676  val: loss: 1847992.875 acc: 0.5483450293540955\n",
      "step: 29340\n",
      "train: loss: 288956.21875 acc: 0.9642165899276733  val: loss: 1263718.125 acc: 0.7578380703926086\n",
      "step: 29345\n",
      "train: loss: 747657.5625 acc: 0.9483334422111511  val: loss: 2093474.875 acc: 0.8036069273948669\n",
      "step: 29350\n",
      "train: loss: 879931.5625 acc: 0.8874670267105103  val: loss: 2181355.75 acc: 0.7155342102050781\n",
      "step: 29355\n",
      "train: loss: 456450.4375 acc: 0.8580077886581421  val: loss: 598241.1875 acc: 0.8688674569129944\n",
      "step: 29360\n",
      "train: loss: 925288.5625 acc: 0.8051580190658569  val: loss: 686709.5 acc: 0.805716335773468\n",
      "step: 29365\n",
      "train: loss: 230172.84375 acc: 0.9020305871963501  val: loss: 3472348.75 acc: 0.7194557189941406\n",
      "step: 29370\n",
      "train: loss: 1145187.75 acc: 0.7950116991996765  val: loss: 1295347.25 acc: 0.7345091104507446\n",
      "step: 29375\n",
      "train: loss: 588788.3125 acc: 0.8235295414924622  val: loss: 809949.5 acc: 0.719609260559082\n",
      "step: 29380\n",
      "train: loss: 660134.3125 acc: 0.6258701086044312  val: loss: 1270877.875 acc: 0.8229499459266663\n",
      "step: 29385\n",
      "train: loss: 474312.78125 acc: 0.7476773262023926  val: loss: 661886.8125 acc: 0.7496311068534851\n",
      "step: 29390\n",
      "train: loss: 520161.84375 acc: 0.7860578298568726  val: loss: 614137.25 acc: 0.7270236611366272\n",
      "step: 29395\n",
      "train: loss: 64547.515625 acc: 0.940351128578186  val: loss: 1080482.875 acc: 0.6947279572486877\n",
      "step: 29400\n",
      "train: loss: 146956.78125 acc: 0.9014496207237244  val: loss: 129777.078125 acc: 0.896361768245697\n",
      "step: 29405\n",
      "train: loss: 27123.11328125 acc: 0.9768656492233276  val: loss: 1295607.625 acc: 0.702528715133667\n",
      "step: 29410\n",
      "train: loss: 507573.125 acc: 0.7788341045379639  val: loss: 1207817.125 acc: 0.7212771773338318\n",
      "step: 29415\n",
      "train: loss: 23450.33203125 acc: 0.9813345670700073  val: loss: 355421.65625 acc: 0.7786950469017029\n",
      "step: 29420\n",
      "train: loss: 79233.09375 acc: 0.9268510937690735  val: loss: 950034.9375 acc: 0.6938978433609009\n",
      "step: 29425\n",
      "train: loss: 491629.53125 acc: 0.7863967418670654  val: loss: 844748.25 acc: 0.7269152402877808\n",
      "step: 29430\n",
      "train: loss: 51936.98828125 acc: 0.9330489039421082  val: loss: 795629.4375 acc: 0.7249739170074463\n",
      "step: 29435\n",
      "train: loss: 193941.25 acc: 0.8581633567810059  val: loss: 2246249.5 acc: 0.5855578780174255\n",
      "step: 29440\n",
      "train: loss: 150850.890625 acc: 0.861740231513977  val: loss: 2234813.25 acc: 0.6718881130218506\n",
      "step: 29445\n",
      "train: loss: 96523.1015625 acc: 0.9116451144218445  val: loss: 1205538.875 acc: 0.6833614110946655\n",
      "step: 29450\n",
      "train: loss: 273304.75 acc: 0.8423786163330078  val: loss: 5722157.5 acc: 0.5503714680671692\n",
      "step: 29455\n",
      "train: loss: 297887.59375 acc: 0.8200459480285645  val: loss: 1426209.375 acc: 0.673015832901001\n",
      "step: 29460\n",
      "train: loss: 246460.484375 acc: 0.8009428977966309  val: loss: 2950446.25 acc: 0.6178436279296875\n",
      "step: 29465\n",
      "train: loss: 483766.5625 acc: 0.7882126569747925  val: loss: 2468170.75 acc: 0.5385290384292603\n",
      "step: 29470\n",
      "train: loss: 2269940.5 acc: 0.74437016248703  val: loss: 764319.6875 acc: 0.8147582411766052\n",
      "step: 29475\n",
      "train: loss: 1136561.5 acc: 0.8461673259735107  val: loss: 563908.0625 acc: 0.8311392068862915\n",
      "step: 29480\n",
      "train: loss: 1062330.625 acc: 0.9094020128250122  val: loss: 687684.75 acc: 0.7221959829330444\n",
      "step: 29485\n",
      "train: loss: 252027.859375 acc: 0.9714756011962891  val: loss: 1264217.875 acc: 0.6648540496826172\n",
      "step: 29490\n",
      "train: loss: 121633.6875 acc: 0.9822362661361694  val: loss: 1088842.0 acc: 0.8470479249954224\n",
      "step: 29495\n",
      "train: loss: 74726.2734375 acc: 0.9893966317176819  val: loss: 604555.9375 acc: 0.33752304315567017\n",
      "step: 29500\n",
      "train: loss: 46933.4453125 acc: 0.9955905675888062  val: loss: 948493.25 acc: 0.683254599571228\n",
      "step: 29505\n",
      "train: loss: 64857.1015625 acc: 0.995313286781311  val: loss: 577324.625 acc: 0.8877755403518677\n",
      "step: 29510\n",
      "train: loss: 58260.16015625 acc: 0.9952813982963562  val: loss: 426430.28125 acc: 0.7371779680252075\n",
      "step: 29515\n",
      "train: loss: 84221.90625 acc: 0.9853482842445374  val: loss: 1029025.375 acc: 0.8466353416442871\n",
      "step: 29520\n",
      "train: loss: 69820.1875 acc: 0.9857469201087952  val: loss: 1009901.625 acc: 0.8252882957458496\n",
      "step: 29525\n",
      "train: loss: 11701.890625 acc: 0.9958004355430603  val: loss: 112916.2421875 acc: 0.9794674515724182\n",
      "step: 29530\n",
      "train: loss: 8079.75732421875 acc: 0.9976366758346558  val: loss: 854530.5625 acc: 0.5750822424888611\n",
      "step: 29535\n",
      "train: loss: 4277.1435546875 acc: 0.9931219816207886  val: loss: 2004998.875 acc: 0.30363261699676514\n",
      "step: 29540\n",
      "train: loss: 4352.79296875 acc: 0.9884644746780396  val: loss: 989306.375 acc: 0.6883177757263184\n",
      "step: 29545\n",
      "train: loss: 26296.740234375 acc: 0.9838448762893677  val: loss: 607272.625 acc: 0.7861437797546387\n",
      "step: 29550\n",
      "train: loss: 9141.88671875 acc: 0.9665879011154175  val: loss: 635975.1875 acc: 0.24516499042510986\n",
      "step: 29555\n",
      "train: loss: 7439.7451171875 acc: 0.9854353070259094  val: loss: 1145057.0 acc: 0.8379995226860046\n",
      "step: 29560\n",
      "train: loss: 6013.7216796875 acc: 0.9789592027664185  val: loss: 229060.78125 acc: 0.9782782196998596\n",
      "step: 29565\n",
      "train: loss: 24183.76953125 acc: 0.9495857954025269  val: loss: 344527.125 acc: 0.9604430794715881\n",
      "step: 29570\n",
      "train: loss: 17885.474609375 acc: 0.9883914589881897  val: loss: 405254.03125 acc: 0.9221745133399963\n",
      "step: 29575\n",
      "train: loss: 28034.123046875 acc: 0.9752271771430969  val: loss: 203964.796875 acc: 0.9600772261619568\n",
      "step: 29580\n",
      "train: loss: 19551.0234375 acc: 0.9844440221786499  val: loss: 1307927.625 acc: 0.6651282906532288\n",
      "step: 29585\n",
      "train: loss: 8175.935546875 acc: 0.9962137937545776  val: loss: 1561806.75 acc: 0.7991983890533447\n",
      "step: 29590\n",
      "train: loss: 13557.9443359375 acc: 0.9922174215316772  val: loss: 200657.9375 acc: 0.9225821495056152\n",
      "step: 29595\n",
      "train: loss: 17249.0390625 acc: 0.9847358465194702  val: loss: 3206298.5 acc: 0.6764975786209106\n",
      "step: 29600\n",
      "train: loss: 9424.22265625 acc: 0.9699602127075195  val: loss: 881830.5625 acc: 0.8664553761482239\n",
      "step: 29605\n",
      "train: loss: 20014.955078125 acc: 0.99248868227005  val: loss: 995141.0 acc: 0.6493276357650757\n",
      "step: 29610\n",
      "train: loss: 16777.6796875 acc: 0.9952293038368225  val: loss: 127536.125 acc: 0.9691594839096069\n",
      "step: 29615\n",
      "train: loss: 34048.890625 acc: 0.9915630221366882  val: loss: 1314572.125 acc: 0.44850677251815796\n",
      "step: 29620\n",
      "train: loss: 35306.890625 acc: 0.9745123982429504  val: loss: 2273337.75 acc: 0.8161205053329468\n",
      "step: 29625\n",
      "train: loss: 38907.7734375 acc: 0.9895642995834351  val: loss: 1182171.5 acc: 0.5644111633300781\n",
      "step: 29630\n",
      "train: loss: 135402.671875 acc: 0.9624180793762207  val: loss: 1126990.0 acc: 0.9002219438552856\n",
      "step: 29635\n",
      "train: loss: 109087.4921875 acc: 0.9410886764526367  val: loss: 1127320.25 acc: 0.6494290828704834\n",
      "step: 29640\n",
      "train: loss: 86580.453125 acc: 0.9636203646659851  val: loss: 2088761.375 acc: 0.8009056448936462\n",
      "step: 29645\n",
      "train: loss: 784480.25 acc: 0.8846452832221985  val: loss: 1546118.75 acc: 0.8725184202194214\n",
      "step: 29650\n",
      "train: loss: 741539.375 acc: 0.8982935547828674  val: loss: 1521897.875 acc: 0.8531233072280884\n",
      "step: 29655\n",
      "train: loss: 181985.34375 acc: 0.9828333854675293  val: loss: 1524631.625 acc: 0.8014630675315857\n",
      "step: 29660\n",
      "train: loss: 43420.53515625 acc: 0.991811990737915  val: loss: 1024082.5625 acc: 0.7571024894714355\n",
      "step: 29665\n",
      "train: loss: 195883.5625 acc: 0.9743154644966125  val: loss: 475824.71875 acc: 0.8628360629081726\n",
      "step: 29670\n",
      "train: loss: 605132.3125 acc: 0.9621877074241638  val: loss: 3504203.75 acc: -0.4619150161743164\n",
      "step: 29675\n",
      "train: loss: 403791.8125 acc: 0.9515552520751953  val: loss: 389032.15625 acc: 0.9080016016960144\n",
      "step: 29680\n",
      "train: loss: 156199.0 acc: 0.9903224110603333  val: loss: 1303433.0 acc: 0.0339779257774353\n",
      "step: 29685\n",
      "train: loss: 2220420.75 acc: 0.9097578525543213  val: loss: 1502834.625 acc: 0.8143346309661865\n",
      "step: 29690\n",
      "train: loss: 672105.0 acc: 0.9691348671913147  val: loss: 969661.5 acc: 0.735903799533844\n",
      "step: 29695\n",
      "train: loss: 1471683.25 acc: 0.951947033405304  val: loss: 1126489.625 acc: 0.691547155380249\n",
      "step: 29700\n",
      "train: loss: 3436303.75 acc: 0.7693741321563721  val: loss: 695874.125 acc: 0.9488875269889832\n",
      "step: 29705\n",
      "train: loss: 619950.4375 acc: 0.9727080464363098  val: loss: 399732.0625 acc: 0.89995276927948\n",
      "step: 29710\n",
      "train: loss: 952716.125 acc: 0.8881073594093323  val: loss: 1369967.625 acc: 0.7437273859977722\n",
      "step: 29715\n",
      "train: loss: 548397.1875 acc: 0.9270796179771423  val: loss: 2541375.75 acc: -0.9217398166656494\n",
      "step: 29720\n",
      "train: loss: 342733.78125 acc: 0.8475025296211243  val: loss: 1225553.375 acc: 0.7684140205383301\n",
      "step: 29725\n",
      "train: loss: 2350580.5 acc: 0.6990605592727661  val: loss: 707399.8125 acc: 0.8081598281860352\n",
      "step: 29730\n",
      "train: loss: 362952.40625 acc: 0.7847109436988831  val: loss: 315893.84375 acc: 0.7949104309082031\n",
      "step: 29735\n",
      "train: loss: 549050.4375 acc: 0.791810154914856  val: loss: 1768355.625 acc: 0.7568922638893127\n",
      "step: 29740\n",
      "train: loss: 340749.03125 acc: 0.8020232915878296  val: loss: 477074.9375 acc: 0.8350951671600342\n",
      "step: 29745\n",
      "train: loss: 869324.1875 acc: 0.7726704478263855  val: loss: 3695266.25 acc: 0.7227727174758911\n",
      "step: 29750\n",
      "train: loss: 517066.78125 acc: 0.7413324117660522  val: loss: 2510297.25 acc: 0.6323931217193604\n",
      "step: 29755\n",
      "train: loss: 210112.15625 acc: 0.8905715942382812  val: loss: 1687526.375 acc: 0.6235795021057129\n",
      "step: 29760\n",
      "train: loss: 378653.71875 acc: 0.7929276823997498  val: loss: 1242634.125 acc: 0.7234094738960266\n",
      "step: 29765\n",
      "train: loss: 40552.83203125 acc: 0.9704368114471436  val: loss: 1123973.625 acc: 0.6521967649459839\n",
      "step: 29770\n",
      "train: loss: 165809.75 acc: 0.9009912610054016  val: loss: 3403243.25 acc: 0.6049633026123047\n",
      "step: 29775\n",
      "train: loss: 96605.359375 acc: 0.9339041709899902  val: loss: 728391.375 acc: 0.7334173917770386\n",
      "step: 29780\n",
      "train: loss: 56505.484375 acc: 0.9572076201438904  val: loss: 882068.25 acc: 0.6916027069091797\n",
      "step: 29785\n",
      "train: loss: 114360.1953125 acc: 0.9298482537269592  val: loss: 1563553.875 acc: 0.6513365507125854\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 29790\n",
      "train: loss: 326246.5625 acc: 0.7805171012878418  val: loss: 1597717.5 acc: 0.6537212133407593\n",
      "step: 29795\n",
      "train: loss: 48516.1640625 acc: 0.956579327583313  val: loss: 2303010.75 acc: 0.5991165041923523\n",
      "step: 29800\n",
      "train: loss: 188647.25 acc: 0.8588403463363647  val: loss: 1627478.375 acc: 0.6236560940742493\n",
      "step: 29805\n",
      "train: loss: 26360.458984375 acc: 0.9554139375686646  val: loss: 1315793.875 acc: 0.6589785814285278\n",
      "step: 29810\n",
      "train: loss: 439914.6875 acc: 0.7753946781158447  val: loss: 1116851.5 acc: 0.7059884071350098\n",
      "step: 29815\n",
      "train: loss: 229033.15625 acc: 0.8641367554664612  val: loss: 2821065.25 acc: 0.6042585968971252\n",
      "step: 29820\n",
      "train: loss: 113389.203125 acc: 0.8661006093025208  val: loss: 1270032.125 acc: 0.6845495700836182\n",
      "step: 29825\n",
      "train: loss: 616856.5625 acc: 0.7070962190628052  val: loss: 1992117.5 acc: 0.6217886209487915\n",
      "step: 29830\n",
      "train: loss: 495069.34375 acc: 0.762516975402832  val: loss: 616668.0 acc: 0.7636715769767761\n",
      "step: 29835\n",
      "train: loss: 1296116.625 acc: 0.7091416120529175  val: loss: 630710.1875 acc: 0.8172199726104736\n",
      "step: 29840\n",
      "train: loss: 881657.375 acc: 0.7682747840881348  val: loss: 782981.875 acc: 0.8251792788505554\n",
      "step: 29845\n",
      "train: loss: 737725.5625 acc: 0.9423049688339233  val: loss: 316144.6875 acc: 0.8945443630218506\n",
      "step: 29850\n",
      "train: loss: 204200.171875 acc: 0.9703345894813538  val: loss: 480417.09375 acc: 0.916387140750885\n",
      "step: 29855\n",
      "train: loss: 357019.125 acc: 0.934378981590271  val: loss: 306988.3125 acc: 0.9316429495811462\n",
      "step: 29860\n",
      "train: loss: 111987.265625 acc: 0.9780759215354919  val: loss: 313482.28125 acc: 0.9373107552528381\n",
      "step: 29865\n",
      "train: loss: 269283.21875 acc: 0.978209376335144  val: loss: 1232110.75 acc: 0.8011437654495239\n",
      "step: 29870\n",
      "train: loss: 33755.8515625 acc: 0.9974889755249023  val: loss: 2005178.125 acc: -0.018972039222717285\n",
      "step: 29875\n",
      "train: loss: 55229.890625 acc: 0.9949312210083008  val: loss: 571843.4375 acc: 0.8555872440338135\n",
      "step: 29880\n",
      "train: loss: 47267.58203125 acc: 0.9942920207977295  val: loss: 859978.4375 acc: 0.6311624050140381\n",
      "step: 29885\n",
      "train: loss: 52090.04296875 acc: 0.9925875663757324  val: loss: 90465.5859375 acc: 0.9769682288169861\n",
      "step: 29890\n",
      "train: loss: 14147.25390625 acc: 0.9803290367126465  val: loss: 1323367.5 acc: 0.7480118870735168\n",
      "step: 29895\n",
      "train: loss: 15493.8056640625 acc: 0.9923244714736938  val: loss: 713908.125 acc: 0.9032279253005981\n",
      "step: 29900\n",
      "train: loss: 13030.8369140625 acc: 0.9649704098701477  val: loss: 34400.0546875 acc: 0.9790585041046143\n",
      "step: 29905\n",
      "train: loss: 6513.64111328125 acc: 0.9858291745185852  val: loss: 258254.46875 acc: 0.9647668600082397\n",
      "step: 29910\n",
      "train: loss: 16039.8935546875 acc: 0.9871184825897217  val: loss: 800446.5625 acc: 0.683355987071991\n",
      "step: 29915\n",
      "train: loss: 264590.59375 acc: 0.4945594072341919  val: loss: 160310.203125 acc: 0.9614102244377136\n",
      "step: 29920\n",
      "train: loss: 8916.6328125 acc: 0.9712945222854614  val: loss: 948903.9375 acc: 0.9077657461166382\n",
      "step: 29925\n",
      "train: loss: 4980.97265625 acc: 0.9903237819671631  val: loss: 1282034.875 acc: 0.7973357439041138\n",
      "step: 29930\n",
      "train: loss: 17732.89453125 acc: 0.9733936190605164  val: loss: 364076.65625 acc: 0.9038122296333313\n",
      "step: 29935\n",
      "train: loss: 13190.3603515625 acc: 0.9698861241340637  val: loss: 398829.3125 acc: 0.9615258574485779\n",
      "step: 29940\n",
      "train: loss: 26762.103515625 acc: 0.9799360632896423  val: loss: 892851.4375 acc: 0.874234676361084\n",
      "step: 29945\n",
      "train: loss: 12727.4765625 acc: 0.9881518483161926  val: loss: 3067587.25 acc: 0.5868947505950928\n",
      "step: 29950\n",
      "train: loss: 18103.70703125 acc: 0.9887754321098328  val: loss: 2074258.75 acc: 0.7780308723449707\n",
      "step: 29955\n",
      "train: loss: 15404.2578125 acc: 0.9915561079978943  val: loss: 1912206.625 acc: 0.6425169706344604\n",
      "step: 29960\n",
      "train: loss: 18108.884765625 acc: 0.9912084341049194  val: loss: 1627044.875 acc: 0.7478981614112854\n",
      "step: 29965\n",
      "train: loss: 6554.96875 acc: 0.9944203495979309  val: loss: 1506213.25 acc: 0.8110215067863464\n",
      "step: 29970\n",
      "train: loss: 20121.078125 acc: 0.9838613271713257  val: loss: 437355.09375 acc: 0.8982738852500916\n",
      "step: 29975\n",
      "train: loss: 25749.37890625 acc: 0.9945536255836487  val: loss: 1810121.0 acc: 0.4020003080368042\n",
      "step: 29980\n",
      "train: loss: 10876.1201171875 acc: 0.9974227547645569  val: loss: 1016017.25 acc: 0.7803996801376343\n",
      "step: 29985\n",
      "train: loss: 30631.88671875 acc: 0.9871322512626648  val: loss: 840446.6875 acc: 0.8443056344985962\n",
      "step: 29990\n",
      "train: loss: 11178.2353515625 acc: 0.994848906993866  val: loss: 3194514.75 acc: 0.3510819673538208\n",
      "step: 29995\n",
      "train: loss: 383605.59375 acc: 0.8959007263183594  val: loss: 289747.3125 acc: 0.7993693351745605\n",
      "step: 30000\n",
      "train: loss: 293219.71875 acc: 0.9253447651863098  val: loss: 964143.125 acc: 0.796962559223175\n",
      "step: 30005\n",
      "train: loss: 65021.78125 acc: 0.983165442943573  val: loss: 1252931.625 acc: 0.6260854601860046\n",
      "step: 30010\n",
      "train: loss: 356667.71875 acc: 0.9358099699020386  val: loss: 1669626.625 acc: 0.6490404009819031\n",
      "step: 30015\n",
      "train: loss: 46199.546875 acc: 0.9958653450012207  val: loss: 1811556.125 acc: 0.28835535049438477\n",
      "step: 30020\n",
      "train: loss: 1107088.375 acc: 0.8844533562660217  val: loss: 620855.0625 acc: 0.8564167022705078\n",
      "step: 30025\n",
      "train: loss: 35818.12890625 acc: 0.9957536458969116  val: loss: 1645147.5 acc: 0.8212864398956299\n",
      "step: 30030\n",
      "train: loss: 330911.375 acc: 0.9677499532699585  val: loss: 577314.625 acc: 0.7860937118530273\n",
      "step: 30035\n",
      "train: loss: 199269.0625 acc: 0.9818727374076843  val: loss: 1888997.5 acc: 0.12430840730667114\n",
      "step: 30040\n",
      "train: loss: 422245.21875 acc: 0.982059121131897  val: loss: 720952.0 acc: 0.7589412927627563\n",
      "step: 30045\n",
      "train: loss: 316805.5 acc: 0.9722278118133545  val: loss: 617478.0 acc: 0.9241092801094055\n",
      "step: 30050\n",
      "train: loss: 848556.625 acc: 0.9162180423736572  val: loss: 683187.25 acc: 0.8378231525421143\n",
      "step: 30055\n",
      "train: loss: 243843.90625 acc: 0.9894275665283203  val: loss: 633519.75 acc: 0.9001445770263672\n",
      "step: 30060\n",
      "train: loss: 986944.875 acc: 0.965833842754364  val: loss: 940535.75 acc: 0.8946026563644409\n",
      "step: 30065\n",
      "train: loss: 785949.125 acc: 0.9606167078018188  val: loss: 751592.25 acc: 0.8405282497406006\n",
      "step: 30070\n",
      "train: loss: 772480.5625 acc: 0.9579111337661743  val: loss: 1863606.125 acc: 0.6064022779464722\n",
      "step: 30075\n",
      "train: loss: 240139.65625 acc: 0.9605812430381775  val: loss: 330491.09375 acc: 0.9337666034698486\n",
      "step: 30080\n",
      "train: loss: 367052.09375 acc: 0.9615924954414368  val: loss: 341078.09375 acc: 0.9414417743682861\n",
      "step: 30085\n",
      "train: loss: 288296.28125 acc: 0.9158644676208496  val: loss: 1560750.875 acc: 0.36116886138916016\n",
      "step: 30090\n",
      "train: loss: 2716890.75 acc: 0.7457919120788574  val: loss: 1128675.75 acc: 0.5966614484786987\n",
      "step: 30095\n",
      "train: loss: 539964.5 acc: 0.7646159529685974  val: loss: 787258.625 acc: 0.7530133724212646\n",
      "step: 30100\n",
      "train: loss: 422858.125 acc: 0.845218300819397  val: loss: 2614535.0 acc: 0.766197681427002\n",
      "step: 30105\n",
      "train: loss: 532775.625 acc: 0.7597306370735168  val: loss: 837236.25 acc: 0.6847324371337891\n",
      "step: 30110\n",
      "train: loss: 628474.625 acc: 0.7603424787521362  val: loss: 980044.6875 acc: 0.8512773513793945\n",
      "step: 30115\n",
      "train: loss: 478662.25 acc: 0.7637099027633667  val: loss: 922672.375 acc: 0.6633292436599731\n",
      "step: 30120\n",
      "train: loss: 73999.0234375 acc: 0.9392275214195251  val: loss: 967465.875 acc: 0.6459568738937378\n",
      "step: 30125\n",
      "train: loss: 43926.984375 acc: 0.9675331115722656  val: loss: 901244.875 acc: 0.657092809677124\n",
      "step: 30130\n",
      "train: loss: 151396.375 acc: 0.8603569865226746  val: loss: 1756542.875 acc: 0.6765939593315125\n",
      "step: 30135\n",
      "train: loss: 35205.453125 acc: 0.9691505432128906  val: loss: 3724077.5 acc: 0.5118317604064941\n",
      "step: 30140\n",
      "train: loss: 84298.1953125 acc: 0.9319159984588623  val: loss: 2750285.5 acc: 0.5703584551811218\n",
      "step: 30145\n",
      "train: loss: 74343.890625 acc: 0.9332618713378906  val: loss: 465486.4375 acc: 0.7689617872238159\n",
      "step: 30150\n",
      "train: loss: 68237.0703125 acc: 0.9448562860488892  val: loss: 1213007.125 acc: 0.7151778936386108\n",
      "step: 30155\n",
      "train: loss: 96069.359375 acc: 0.9201821088790894  val: loss: 929800.5625 acc: 0.7301813364028931\n",
      "step: 30160\n",
      "train: loss: 326269.34375 acc: 0.8248128294944763  val: loss: 1223096.0 acc: 0.7438679933547974\n",
      "step: 30165\n",
      "train: loss: 58546.890625 acc: 0.9383240342140198  val: loss: 750841.6875 acc: 0.7258414030075073\n",
      "step: 30170\n",
      "train: loss: 67928.3984375 acc: 0.9134776592254639  val: loss: 2524419.25 acc: 0.603548526763916\n",
      "step: 30175\n",
      "train: loss: 88478.0390625 acc: 0.939014732837677  val: loss: 1138537.5 acc: 0.6821575164794922\n",
      "step: 30180\n",
      "train: loss: 126291.703125 acc: 0.8941992521286011  val: loss: 3459724.75 acc: 0.5912536382675171\n",
      "step: 30185\n",
      "train: loss: 835371.9375 acc: 0.6999391913414001  val: loss: 930007.6875 acc: 0.6478134989738464\n",
      "step: 30190\n",
      "train: loss: 452365.0 acc: 0.7552583813667297  val: loss: 2741646.0 acc: 0.6382006406784058\n",
      "step: 30195\n",
      "train: loss: 347693.34375 acc: 0.7879768013954163  val: loss: 414275.40625 acc: 0.8010503053665161\n",
      "step: 30200\n",
      "train: loss: 1592811.625 acc: 0.7246440649032593  val: loss: 1597208.125 acc: 0.7435688972473145\n",
      "step: 30205\n",
      "train: loss: 962191.1875 acc: 0.7979429960250854  val: loss: 540908.0 acc: 0.7567411661148071\n",
      "step: 30210\n",
      "train: loss: 1038059.875 acc: 0.8584505915641785  val: loss: 1200117.125 acc: 0.8772596716880798\n",
      "step: 30215\n",
      "train: loss: 350637.375 acc: 0.95997554063797  val: loss: 345455.4375 acc: 0.88675856590271\n",
      "step: 30220\n",
      "train: loss: 152101.03125 acc: 0.9822160601615906  val: loss: 1217129.75 acc: 0.7475253343582153\n",
      "step: 30225\n",
      "train: loss: 136269.078125 acc: 0.9826860427856445  val: loss: 710107.25 acc: 0.9188511967658997\n",
      "step: 30230\n",
      "train: loss: 74327.46875 acc: 0.9900112748146057  val: loss: 585553.125 acc: 0.7799949645996094\n",
      "step: 30235\n",
      "train: loss: 240702.421875 acc: 0.9733824729919434  val: loss: 1108446.75 acc: 0.8361364603042603\n",
      "step: 30240\n",
      "train: loss: 27201.77734375 acc: 0.9975375533103943  val: loss: 579610.875 acc: 0.9215582013130188\n",
      "step: 30245\n",
      "train: loss: 25528.98046875 acc: 0.9970728754997253  val: loss: 383351.03125 acc: 0.9347874522209167\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 30250\n",
      "train: loss: 20659.794921875 acc: 0.992283046245575  val: loss: 895444.375 acc: 0.8701008558273315\n",
      "step: 30255\n",
      "train: loss: 11359.6484375 acc: 0.9894801378250122  val: loss: 560580.1875 acc: 0.9075818061828613\n",
      "step: 30260\n",
      "train: loss: 11462.1396484375 acc: 0.9955332279205322  val: loss: 929096.8125 acc: 0.8317549228668213\n",
      "step: 30265\n",
      "train: loss: 10679.3271484375 acc: 0.9955706596374512  val: loss: 415286.3125 acc: 0.8940227031707764\n",
      "step: 30270\n",
      "train: loss: 4939.51953125 acc: 0.9970105886459351  val: loss: 792704.9375 acc: 0.849544882774353\n",
      "step: 30275\n",
      "train: loss: 17786.9296875 acc: 0.9931803941726685  val: loss: 971717.0 acc: 0.9135696887969971\n",
      "step: 30280\n",
      "train: loss: 288321.9375 acc: 0.4176298975944519  val: loss: 2882212.0 acc: 0.6517962217330933\n",
      "step: 30285\n",
      "train: loss: 6544.2197265625 acc: 0.9872632622718811  val: loss: 1380326.125 acc: 0.51690673828125\n",
      "step: 30290\n",
      "train: loss: 16146.14453125 acc: 0.9737817049026489  val: loss: 1842866.375 acc: 0.7958974242210388\n",
      "step: 30295\n",
      "train: loss: 14840.6708984375 acc: 0.9775828719139099  val: loss: 956910.25 acc: 0.7240936160087585\n",
      "step: 30300\n",
      "train: loss: 18330.63671875 acc: 0.9768819212913513  val: loss: 1694417.625 acc: 0.8331349492073059\n",
      "step: 30305\n",
      "train: loss: 14511.2578125 acc: 0.9912633895874023  val: loss: 736334.4375 acc: 0.8098200559616089\n",
      "step: 30310\n",
      "train: loss: 11561.7802734375 acc: 0.9933797121047974  val: loss: 1920594.875 acc: 0.6732196807861328\n",
      "step: 30315\n",
      "train: loss: 5827.49951171875 acc: 0.9970896244049072  val: loss: 2587448.0 acc: 0.4337296485900879\n",
      "step: 30320\n",
      "train: loss: 15359.689453125 acc: 0.9921066164970398  val: loss: 1292784.75 acc: 0.37003153562545776\n",
      "step: 30325\n",
      "train: loss: 11252.392578125 acc: 0.9845600724220276  val: loss: 1351741.625 acc: 0.6258413195610046\n",
      "step: 30330\n",
      "train: loss: 9372.0625 acc: 0.98601895570755  val: loss: 2814655.0 acc: 0.5209177732467651\n",
      "step: 30335\n",
      "train: loss: 18390.5546875 acc: 0.986543595790863  val: loss: 2193981.25 acc: 0.15327054262161255\n",
      "step: 30340\n",
      "train: loss: 9494.525390625 acc: 0.9970017075538635  val: loss: 2371755.0 acc: 0.4075075387954712\n",
      "step: 30345\n",
      "train: loss: 13920.0546875 acc: 0.9959941506385803  val: loss: 983678.25 acc: 0.8193339705467224\n",
      "step: 30350\n",
      "train: loss: 15581.2177734375 acc: 0.9920910596847534  val: loss: 1808255.375 acc: 0.33850526809692383\n",
      "step: 30355\n",
      "train: loss: 37255.2109375 acc: 0.9804158806800842  val: loss: 2373571.75 acc: -0.6611554622650146\n",
      "step: 30360\n",
      "train: loss: 253864.6875 acc: 0.9078150987625122  val: loss: 447929.03125 acc: 0.9378403425216675\n",
      "step: 30365\n",
      "train: loss: 87162.6484375 acc: 0.9846122860908508  val: loss: 1603067.875 acc: 0.8032702207565308\n",
      "step: 30370\n",
      "train: loss: 70161.1328125 acc: 0.9339912533760071  val: loss: 688800.875 acc: 0.8770877122879028\n",
      "step: 30375\n",
      "train: loss: 519723.15625 acc: 0.8301932215690613  val: loss: 1245084.625 acc: 0.7195343375205994\n",
      "step: 30380\n",
      "train: loss: 39996.50390625 acc: 0.9955517649650574  val: loss: 1112033.375 acc: 0.9112451076507568\n",
      "step: 30385\n",
      "train: loss: 55011.5703125 acc: 0.9927421808242798  val: loss: 581934.75 acc: 0.9214877486228943\n",
      "step: 30390\n",
      "train: loss: 24817.193359375 acc: 0.997315526008606  val: loss: 702952.0 acc: 0.8376587629318237\n",
      "step: 30395\n",
      "train: loss: 475785.59375 acc: 0.9475482106208801  val: loss: 2518907.5 acc: 0.5195274949073792\n",
      "step: 30400\n",
      "train: loss: 202711.640625 acc: 0.9644120931625366  val: loss: 1446801.125 acc: 0.513505220413208\n",
      "step: 30405\n",
      "train: loss: 584899.5 acc: 0.9779633283615112  val: loss: 1079637.75 acc: 0.8087766170501709\n",
      "step: 30410\n",
      "train: loss: 276836.3125 acc: 0.9765080809593201  val: loss: 1346627.625 acc: 0.7038761377334595\n",
      "step: 30415\n",
      "train: loss: 303098.9375 acc: 0.975260317325592  val: loss: 1437224.125 acc: 0.7453750371932983\n",
      "step: 30420\n",
      "train: loss: 430254.5625 acc: 0.9781122803688049  val: loss: 232418.71875 acc: 0.8680239915847778\n",
      "step: 30425\n",
      "train: loss: 1019544.6875 acc: 0.9767494201660156  val: loss: 1119056.75 acc: 0.8188426494598389\n",
      "step: 30430\n",
      "train: loss: 1170835.375 acc: 0.945417046546936  val: loss: 1499001.625 acc: 0.47664445638656616\n",
      "step: 30435\n",
      "train: loss: 1013719.375 acc: 0.9389763474464417  val: loss: 1362855.125 acc: 0.8988996148109436\n",
      "step: 30440\n",
      "train: loss: 503691.6875 acc: 0.9535067677497864  val: loss: 633708.8125 acc: 0.8503449559211731\n",
      "step: 30445\n",
      "train: loss: 840381.375 acc: 0.9329373836517334  val: loss: 661630.0 acc: 0.9308634996414185\n",
      "step: 30450\n",
      "train: loss: 302225.875 acc: 0.9498769640922546  val: loss: 395909.09375 acc: 0.8221624493598938\n",
      "step: 30455\n",
      "train: loss: 930323.1875 acc: 0.7524547576904297  val: loss: 415137.875 acc: 0.8153003454208374\n",
      "step: 30460\n",
      "train: loss: 817129.0 acc: 0.6961337327957153  val: loss: 847015.125 acc: 0.7627483606338501\n",
      "step: 30465\n",
      "train: loss: 736169.3125 acc: 0.8319486975669861  val: loss: 1181470.375 acc: 0.7865866422653198\n",
      "step: 30470\n",
      "train: loss: 225901.671875 acc: 0.8870988488197327  val: loss: 240110.875 acc: 0.8315619230270386\n",
      "step: 30475\n",
      "train: loss: 496900.53125 acc: 0.865509569644928  val: loss: 409749.78125 acc: 0.8467857837677002\n",
      "step: 30480\n",
      "train: loss: 722263.375 acc: 0.6775071024894714  val: loss: 470876.1875 acc: 0.7866711616516113\n",
      "step: 30485\n",
      "train: loss: 203724.90625 acc: 0.8896733522415161  val: loss: 2603862.75 acc: 0.5580782890319824\n",
      "step: 30490\n",
      "train: loss: 245290.5 acc: 0.8457796573638916  val: loss: 1235848.375 acc: 0.6865061521530151\n",
      "step: 30495\n",
      "train: loss: 31221.279296875 acc: 0.9704209566116333  val: loss: 2556935.25 acc: 0.5542740225791931\n",
      "step: 30500\n",
      "train: loss: 17362.068359375 acc: 0.9836470484733582  val: loss: 2131732.0 acc: 0.6012958288192749\n",
      "step: 30505\n",
      "train: loss: 22929.193359375 acc: 0.9794373512268066  val: loss: 852713.625 acc: 0.6750352382659912\n",
      "step: 30510\n",
      "train: loss: 29285.669921875 acc: 0.9764427542686462  val: loss: 2465952.75 acc: 0.6160618662834167\n",
      "step: 30515\n",
      "train: loss: 66146.328125 acc: 0.9528009295463562  val: loss: 608267.375 acc: 0.7252506017684937\n",
      "step: 30520\n",
      "train: loss: 39559.11328125 acc: 0.9694299697875977  val: loss: 1822167.375 acc: 0.656219482421875\n",
      "step: 30525\n",
      "train: loss: 180501.234375 acc: 0.8087543845176697  val: loss: 3676078.5 acc: 0.5907720923423767\n",
      "step: 30530\n",
      "train: loss: 32255.798828125 acc: 0.9557856321334839  val: loss: 2846291.25 acc: 0.6074219942092896\n",
      "step: 30535\n",
      "train: loss: 104199.28125 acc: 0.9146283268928528  val: loss: 7228119.5 acc: 0.5424574017524719\n",
      "step: 30540\n",
      "train: loss: 438618.75 acc: 0.8343995809555054  val: loss: 1373072.5 acc: 0.7130287289619446\n",
      "step: 30545\n",
      "train: loss: 563917.125 acc: 0.7491471767425537  val: loss: 7138070.5 acc: 0.5207847356796265\n",
      "step: 30550\n",
      "train: loss: 155685.03125 acc: 0.8460745215415955  val: loss: 839925.4375 acc: 0.7021112442016602\n",
      "step: 30555\n",
      "train: loss: 947526.25 acc: 0.6656919121742249  val: loss: 2971802.5 acc: 0.5835998058319092\n",
      "step: 30560\n",
      "train: loss: 100960.8125 acc: 0.8934724926948547  val: loss: 2784972.5 acc: 0.5571289658546448\n",
      "step: 30565\n",
      "train: loss: 1818117.625 acc: 0.6703011989593506  val: loss: 1795397.125 acc: 0.747785747051239\n",
      "step: 30570\n",
      "train: loss: 1255929.625 acc: 0.7629250288009644  val: loss: 2414477.25 acc: 0.7190837264060974\n",
      "step: 30575\n",
      "train: loss: 1209223.625 acc: 0.8734031915664673  val: loss: 364880.21875 acc: 0.9328114986419678\n",
      "step: 30580\n",
      "train: loss: 137235.109375 acc: 0.9906395077705383  val: loss: 2772444.75 acc: 0.6964032649993896\n",
      "step: 30585\n",
      "train: loss: 110831.6796875 acc: 0.9848765730857849  val: loss: 1323701.625 acc: 0.8122898936271667\n",
      "step: 30590\n",
      "train: loss: 52906.85546875 acc: 0.9884089827537537  val: loss: 1381062.0 acc: 0.04962944984436035\n",
      "step: 30595\n",
      "train: loss: 71946.84375 acc: 0.9928778409957886  val: loss: 1648228.625 acc: 0.6062716841697693\n",
      "step: 30600\n",
      "train: loss: 467083.65625 acc: 0.962158739566803  val: loss: 973443.0625 acc: 0.7692596912384033\n",
      "step: 30605\n",
      "train: loss: 59415.71875 acc: 0.9951421022415161  val: loss: 722536.0 acc: 0.812515377998352\n",
      "step: 30610\n",
      "train: loss: 30861.5625 acc: 0.9962459802627563  val: loss: 725215.0625 acc: 0.7866092324256897\n",
      "step: 30615\n",
      "train: loss: 59201.84765625 acc: 0.9893544912338257  val: loss: 1200810.375 acc: 0.894217848777771\n",
      "step: 30620\n",
      "train: loss: 10483.9423828125 acc: 0.9925739169120789  val: loss: 494269.40625 acc: 0.7905789017677307\n",
      "step: 30625\n",
      "train: loss: 11673.6162109375 acc: 0.9926133155822754  val: loss: 1620325.875 acc: 0.7656958699226379\n",
      "step: 30630\n",
      "train: loss: 7823.98828125 acc: 0.9942882061004639  val: loss: 2055829.625 acc: 0.5752847194671631\n",
      "step: 30635\n",
      "train: loss: 14236.4375 acc: 0.9945903420448303  val: loss: 1855452.375 acc: 0.7463520169258118\n",
      "step: 30640\n",
      "train: loss: 19189.904296875 acc: 0.9787431955337524  val: loss: 2358577.0 acc: 0.12136805057525635\n",
      "step: 30645\n",
      "train: loss: 18241.3359375 acc: 0.9691100120544434  val: loss: 2073490.0 acc: 0.8076505661010742\n",
      "step: 30650\n",
      "train: loss: 23897.33203125 acc: 0.9834575653076172  val: loss: 782088.3125 acc: 0.9011150598526001\n",
      "step: 30655\n",
      "train: loss: 4968.03515625 acc: 0.9918321371078491  val: loss: 1047487.625 acc: 0.7662911415100098\n",
      "step: 30660\n",
      "train: loss: 7721.6767578125 acc: 0.9697567820549011  val: loss: 1434198.5 acc: 0.5688268542289734\n",
      "step: 30665\n",
      "train: loss: 10770.8349609375 acc: 0.9813866019248962  val: loss: 3022292.0 acc: -0.4941157102584839\n",
      "step: 30670\n",
      "train: loss: 23409.970703125 acc: 0.9820247888565063  val: loss: 1947827.125 acc: 0.5746029615402222\n",
      "step: 30675\n",
      "train: loss: 13256.9794921875 acc: 0.990581214427948  val: loss: 1565552.125 acc: 0.7163234949111938\n",
      "step: 30680\n",
      "train: loss: 100629.5546875 acc: 0.9381924271583557  val: loss: 468147.28125 acc: 0.8247376680374146\n",
      "step: 30685\n",
      "train: loss: 28862.466796875 acc: 0.987788200378418  val: loss: 983207.5 acc: 0.8751574754714966\n",
      "step: 30690\n",
      "train: loss: 10955.7314453125 acc: 0.9893742203712463  val: loss: 2916217.5 acc: 0.11700046062469482\n",
      "step: 30695\n",
      "train: loss: 6564.50634765625 acc: 0.9889624118804932  val: loss: 2496813.0 acc: 0.10021233558654785\n",
      "step: 30700\n",
      "train: loss: 6813.2431640625 acc: 0.9942587614059448  val: loss: 415027.96875 acc: 0.9248440861701965\n",
      "step: 30705\n",
      "train: loss: 32684.416015625 acc: 0.9905567765235901  val: loss: 1093157.125 acc: 0.842829704284668\n",
      "step: 30710\n",
      "train: loss: 45283.6640625 acc: 0.9865853786468506  val: loss: 1552212.125 acc: 0.700074315071106\n",
      "step: 30715\n",
      "train: loss: 26892.623046875 acc: 0.9892527461051941  val: loss: 1161157.375 acc: 0.7366583347320557\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 30720\n",
      "train: loss: 16711.72265625 acc: 0.9930697679519653  val: loss: 807394.1875 acc: 0.7142634391784668\n",
      "step: 30725\n",
      "train: loss: 190147.484375 acc: 0.9582404494285583  val: loss: 2384333.0 acc: -0.4582585096359253\n",
      "step: 30730\n",
      "train: loss: 80340.6875 acc: 0.967917799949646  val: loss: 1923173.5 acc: -0.24477577209472656\n",
      "step: 30735\n",
      "train: loss: 73258.5625 acc: 0.9810837507247925  val: loss: 1651019.125 acc: 0.6101694107055664\n",
      "step: 30740\n",
      "train: loss: 111831.734375 acc: 0.9833101034164429  val: loss: 3140265.25 acc: -1.0046164989471436\n",
      "step: 30745\n",
      "train: loss: 370667.0 acc: 0.9524304866790771  val: loss: 384603.6875 acc: 0.7488272786140442\n",
      "step: 30750\n",
      "train: loss: 610448.0625 acc: 0.9333723783493042  val: loss: 858496.6875 acc: 0.6717219352722168\n",
      "step: 30755\n",
      "train: loss: 40316.3515625 acc: 0.9958380460739136  val: loss: 316595.15625 acc: 0.9176819324493408\n",
      "step: 30760\n",
      "train: loss: 425526.84375 acc: 0.9438877701759338  val: loss: 563853.9375 acc: 0.8991504907608032\n",
      "step: 30765\n",
      "train: loss: 180215.6875 acc: 0.9825472235679626  val: loss: 634749.8125 acc: 0.8204846978187561\n",
      "step: 30770\n",
      "train: loss: 747649.75 acc: 0.9572886824607849  val: loss: 1152069.375 acc: 0.8259065747261047\n",
      "step: 30775\n",
      "train: loss: 1892958.625 acc: 0.7829128503799438  val: loss: 1269740.625 acc: 0.17015427350997925\n",
      "step: 30780\n",
      "train: loss: 195018.1875 acc: 0.9388272762298584  val: loss: 675471.625 acc: 0.8090225458145142\n",
      "step: 30785\n",
      "train: loss: 690829.875 acc: 0.9582605361938477  val: loss: 494822.34375 acc: 0.8677203059196472\n",
      "step: 30790\n",
      "train: loss: 1257164.625 acc: 0.9673563838005066  val: loss: 636037.375 acc: 0.8400992155075073\n",
      "step: 30795\n",
      "train: loss: 781747.375 acc: 0.9702295064926147  val: loss: 1476515.5 acc: 0.5310420989990234\n",
      "step: 30800\n",
      "train: loss: 873314.125 acc: 0.9520232081413269  val: loss: 477960.40625 acc: 0.9540354609489441\n",
      "step: 30805\n",
      "train: loss: 788223.875 acc: 0.8937779068946838  val: loss: 1257886.25 acc: 0.5008520483970642\n",
      "step: 30810\n",
      "train: loss: 411800.46875 acc: 0.9567622542381287  val: loss: 219885.109375 acc: 0.9290463924407959\n",
      "step: 30815\n",
      "train: loss: 196725.671875 acc: 0.9520369172096252  val: loss: 396221.125 acc: 0.8065217733383179\n",
      "step: 30820\n",
      "train: loss: 1173053.375 acc: 0.6071987152099609  val: loss: 333402.71875 acc: 0.9441516995429993\n",
      "step: 30825\n",
      "train: loss: 1491469.375 acc: -0.36224472522735596  val: loss: 1058904.625 acc: 0.7801831960678101\n",
      "step: 30830\n",
      "train: loss: 1068053.875 acc: 0.7032594084739685  val: loss: 792979.875 acc: 0.763161301612854\n",
      "step: 30835\n",
      "train: loss: 437032.34375 acc: 0.8298689126968384  val: loss: 1476564.375 acc: 0.7178664803504944\n",
      "step: 30840\n",
      "train: loss: 738057.3125 acc: 0.8147023320198059  val: loss: 1039840.625 acc: 0.7613039016723633\n",
      "step: 30845\n",
      "train: loss: 735085.75 acc: 0.6755905151367188  val: loss: 2458053.0 acc: 0.6765637397766113\n",
      "step: 30850\n",
      "train: loss: 959174.375 acc: 0.7058011293411255  val: loss: 3766126.5 acc: 0.5406545400619507\n",
      "step: 30855\n",
      "train: loss: 65764.5390625 acc: 0.9514808058738708  val: loss: 5316473.0 acc: 0.523943305015564\n",
      "step: 30860\n",
      "train: loss: 76263.890625 acc: 0.931796133518219  val: loss: 629660.375 acc: 0.7368900775909424\n",
      "step: 30865\n",
      "train: loss: 15456.033203125 acc: 0.9867510199546814  val: loss: 1096878.375 acc: 0.7209615707397461\n",
      "step: 30870\n",
      "train: loss: 14567.5849609375 acc: 0.9884403944015503  val: loss: 868858.4375 acc: 0.7136791944503784\n",
      "step: 30875\n",
      "train: loss: 91966.796875 acc: 0.9398027658462524  val: loss: 3005835.5 acc: 0.5948506593704224\n",
      "step: 30880\n",
      "train: loss: 23214.935546875 acc: 0.9828026294708252  val: loss: 4083757.5 acc: 0.45261889696121216\n",
      "step: 30885\n",
      "train: loss: 176158.984375 acc: 0.8852975368499756  val: loss: 222497.09375 acc: 0.8641382455825806\n",
      "step: 30890\n",
      "train: loss: 119116.140625 acc: 0.8969113826751709  val: loss: 1271710.125 acc: 0.6543478965759277\n",
      "step: 30895\n",
      "train: loss: 6338.80810546875 acc: 0.9931373596191406  val: loss: 2564402.5 acc: 0.5825784206390381\n",
      "step: 30900\n",
      "train: loss: 39478.3203125 acc: 0.9543701410293579  val: loss: 3535685.5 acc: 0.5900272727012634\n",
      "step: 30905\n",
      "train: loss: 150471.546875 acc: 0.9079335331916809  val: loss: 1821805.375 acc: 0.6458286046981812\n",
      "step: 30910\n",
      "train: loss: 190233.203125 acc: 0.8798051476478577  val: loss: 1610991.125 acc: 0.6513268947601318\n",
      "step: 30915\n",
      "train: loss: 236273.09375 acc: 0.8357774019241333  val: loss: 4319785.5 acc: 0.5201638340950012\n",
      "step: 30920\n",
      "train: loss: 84866.65625 acc: 0.9251008033752441  val: loss: 657819.625 acc: 0.7492659687995911\n",
      "step: 30925\n",
      "train: loss: 247529.84375 acc: 0.7886630296707153  val: loss: 689096.5 acc: 0.7629064321517944\n",
      "step: 30930\n",
      "train: loss: 955007.75 acc: 0.6207792162895203  val: loss: 860364.625 acc: 0.739119827747345\n",
      "step: 30935\n",
      "train: loss: 954401.5 acc: 0.8047007322311401  val: loss: 911087.4375 acc: 0.785125195980072\n",
      "step: 30940\n",
      "train: loss: 816855.0 acc: 0.931460440158844  val: loss: 936939.0625 acc: 0.4314786195755005\n",
      "step: 30945\n",
      "train: loss: 187045.3125 acc: 0.9794677495956421  val: loss: 1353769.25 acc: 0.7181276082992554\n",
      "step: 30950\n",
      "train: loss: 153129.71875 acc: 0.9835121035575867  val: loss: 420613.28125 acc: 0.8176435232162476\n",
      "step: 30955\n",
      "train: loss: 201858.46875 acc: 0.9634695053100586  val: loss: 953410.0625 acc: 0.6743170022964478\n",
      "step: 30960\n",
      "train: loss: 87114.6171875 acc: 0.9911513924598694  val: loss: 2424064.0 acc: 0.14535754919052124\n",
      "step: 30965\n",
      "train: loss: 119936.46875 acc: 0.9892187714576721  val: loss: 1152491.875 acc: 0.8050715923309326\n",
      "step: 30970\n",
      "train: loss: 72542.3515625 acc: 0.9945020079612732  val: loss: 1395318.625 acc: 0.8442324995994568\n",
      "step: 30975\n",
      "train: loss: 41656.28515625 acc: 0.9964337348937988  val: loss: 1927736.875 acc: 0.7215549945831299\n",
      "step: 30980\n",
      "train: loss: 45455.36328125 acc: 0.9921296238899231  val: loss: 1106717.375 acc: 0.8829805850982666\n",
      "step: 30985\n",
      "train: loss: 10482.8115234375 acc: 0.9972440004348755  val: loss: 1321825.5 acc: 0.6905120015144348\n",
      "step: 30990\n",
      "train: loss: 16476.013671875 acc: 0.9958385229110718  val: loss: 1085443.375 acc: 0.6835521459579468\n",
      "step: 30995\n",
      "train: loss: 8267.666015625 acc: 0.9949939250946045  val: loss: 1785295.375 acc: 0.732532799243927\n",
      "step: 31000\n",
      "train: loss: 17577.51953125 acc: 0.9790451526641846  val: loss: 1951945.875 acc: 0.7033626437187195\n",
      "step: 31005\n",
      "train: loss: 257589.375 acc: 0.7606643438339233  val: loss: 1775199.375 acc: 0.632386326789856\n",
      "step: 31010\n",
      "train: loss: 23846.66015625 acc: 0.9584109783172607  val: loss: 2074463.875 acc: 0.7105156183242798\n",
      "step: 31015\n",
      "train: loss: 7625.9462890625 acc: 0.9947891235351562  val: loss: 849646.5625 acc: 0.8291717767715454\n",
      "step: 31020\n",
      "train: loss: 16456.5234375 acc: 0.9791938662528992  val: loss: 892827.0625 acc: 0.9154890179634094\n",
      "step: 31025\n",
      "train: loss: 10468.1064453125 acc: 0.9732389450073242  val: loss: 2037894.375 acc: 0.26886385679244995\n",
      "step: 31030\n",
      "train: loss: 17213.083984375 acc: 0.9692923426628113  val: loss: 1141598.875 acc: 0.8327920436859131\n",
      "step: 31035\n",
      "train: loss: 32932.95703125 acc: 0.9193857312202454  val: loss: 1214526.375 acc: 0.64934903383255\n",
      "step: 31040\n",
      "train: loss: 19255.935546875 acc: 0.9912379384040833  val: loss: 681724.5 acc: 0.8677587509155273\n",
      "step: 31045\n",
      "train: loss: 21208.337890625 acc: 0.9909446835517883  val: loss: 992519.3125 acc: 0.7998225688934326\n",
      "step: 31050\n",
      "train: loss: 19394.451171875 acc: 0.9926329851150513  val: loss: 1652329.75 acc: 0.7547274231910706\n",
      "step: 31055\n",
      "train: loss: 2173.314208984375 acc: 0.9949061870574951  val: loss: 1718527.75 acc: 0.5864897966384888\n",
      "step: 31060\n",
      "train: loss: 7629.45556640625 acc: 0.988174319267273  val: loss: 1700739.375 acc: 0.46143585443496704\n",
      "step: 31065\n",
      "train: loss: 16060.1669921875 acc: 0.9922875165939331  val: loss: 2227245.25 acc: -0.5244468450546265\n",
      "step: 31070\n",
      "train: loss: 25749.552734375 acc: 0.9888619184494019  val: loss: 328457.6875 acc: 0.9444335699081421\n",
      "step: 31075\n",
      "train: loss: 21220.65625 acc: 0.9959716796875  val: loss: 2237802.0 acc: 0.6586390137672424\n",
      "step: 31080\n",
      "train: loss: 8419.92578125 acc: 0.9960650205612183  val: loss: 204180.765625 acc: 0.9683414101600647\n",
      "step: 31085\n",
      "train: loss: 66961.2734375 acc: 0.9796198606491089  val: loss: 2240464.0 acc: 0.5468973517417908\n",
      "step: 31090\n",
      "train: loss: 166677.328125 acc: 0.9642271995544434  val: loss: 1352067.625 acc: 0.758723258972168\n",
      "step: 31095\n",
      "train: loss: 302742.71875 acc: 0.8869012594223022  val: loss: 914152.0625 acc: 0.6537082195281982\n",
      "step: 31100\n",
      "train: loss: 82483.9921875 acc: 0.9630266427993774  val: loss: 266830.9375 acc: 0.9496592879295349\n",
      "step: 31105\n",
      "train: loss: 1186356.875 acc: 0.7319574952125549  val: loss: 451789.65625 acc: 0.9043447375297546\n",
      "step: 31110\n",
      "train: loss: 29065.853515625 acc: 0.996177613735199  val: loss: 773669.0 acc: 0.8770174980163574\n",
      "step: 31115\n",
      "train: loss: 146877.421875 acc: 0.9871556758880615  val: loss: 179133.671875 acc: 0.9602826237678528\n",
      "step: 31120\n",
      "train: loss: 41809.64453125 acc: 0.9941020011901855  val: loss: 243809.265625 acc: 0.9420242309570312\n",
      "step: 31125\n",
      "train: loss: 39389.16796875 acc: 0.9956632256507874  val: loss: 280350.875 acc: 0.8582675457000732\n",
      "step: 31130\n",
      "train: loss: 672407.3125 acc: 0.8287258148193359  val: loss: 321710.75 acc: 0.8829624056816101\n",
      "step: 31135\n",
      "train: loss: 881799.5625 acc: 0.9556617140769958  val: loss: 906367.25 acc: 0.8849855661392212\n",
      "step: 31140\n",
      "train: loss: 342789.625 acc: 0.9676913619041443  val: loss: 399897.75 acc: 0.9082574844360352\n",
      "step: 31145\n",
      "train: loss: 160855.421875 acc: 0.9805046916007996  val: loss: 742937.875 acc: 0.8298876881599426\n",
      "step: 31150\n",
      "train: loss: 314268.5 acc: 0.984444260597229  val: loss: 190982.546875 acc: 0.8945956230163574\n",
      "step: 31155\n",
      "train: loss: 2200323.0 acc: 0.9385345578193665  val: loss: 529765.125 acc: 0.8800071477890015\n",
      "step: 31160\n",
      "train: loss: 1611472.75 acc: 0.9272443652153015  val: loss: 1092725.375 acc: 0.9282728433609009\n",
      "step: 31165\n",
      "train: loss: 475900.8125 acc: 0.9760848879814148  val: loss: 264277.15625 acc: 0.9533373713493347\n",
      "step: 31170\n",
      "train: loss: 376035.5 acc: 0.9735146164894104  val: loss: 621076.1875 acc: 0.949219822883606\n",
      "step: 31175\n",
      "train: loss: 296342.03125 acc: 0.9693487882614136  val: loss: 295372.09375 acc: 0.9627580046653748\n",
      "step: 31180\n",
      "train: loss: 315402.96875 acc: 0.9585656523704529  val: loss: 770564.6875 acc: 0.8768332004547119\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 31185\n",
      "train: loss: 730051.9375 acc: 0.885903000831604  val: loss: 1167186.625 acc: 0.9199669361114502\n",
      "step: 31190\n",
      "train: loss: 1425412.625 acc: 0.446616530418396  val: loss: 404524.46875 acc: 0.7459901571273804\n",
      "step: 31195\n",
      "train: loss: 616969.875 acc: 0.733873724937439  val: loss: 1861075.25 acc: 0.7607962489128113\n",
      "step: 31200\n",
      "train: loss: 490134.875 acc: 0.7958479523658752  val: loss: 1877461.875 acc: 0.7698911428451538\n",
      "step: 31205\n",
      "train: loss: 1022433.5625 acc: 0.8112647533416748  val: loss: 2252555.0 acc: 0.6524475812911987\n",
      "step: 31210\n",
      "train: loss: 1260557.25 acc: 0.18441754579544067  val: loss: 1968513.625 acc: 0.7662554979324341\n",
      "step: 31215\n",
      "train: loss: 367348.1875 acc: 0.7538074254989624  val: loss: 2684725.0 acc: 0.6289747953414917\n",
      "step: 31220\n",
      "train: loss: 372764.1875 acc: 0.8137315511703491  val: loss: 1727765.375 acc: 0.6358734369277954\n",
      "step: 31225\n",
      "train: loss: 133009.125 acc: 0.890652060508728  val: loss: 1133412.875 acc: 0.679484486579895\n",
      "step: 31230\n",
      "train: loss: 222082.765625 acc: 0.8596464395523071  val: loss: 2527837.75 acc: 0.5153540968894958\n",
      "step: 31235\n",
      "train: loss: 285753.03125 acc: 0.8686053156852722  val: loss: 1744374.375 acc: 0.6470794677734375\n",
      "step: 31240\n",
      "train: loss: 8562.455078125 acc: 0.9933081865310669  val: loss: 2008442.875 acc: 0.5629341006278992\n",
      "step: 31245\n",
      "train: loss: 106590.0234375 acc: 0.9290498495101929  val: loss: 1370104.625 acc: 0.6739696264266968\n",
      "step: 31250\n",
      "train: loss: 334683.96875 acc: 0.7860437631607056  val: loss: 1811172.875 acc: 0.5854605436325073\n",
      "step: 31255\n",
      "train: loss: 209494.46875 acc: 0.8669267296791077  val: loss: 1901172.875 acc: 0.593505859375\n",
      "step: 31260\n",
      "train: loss: 40307.71484375 acc: 0.9450971484184265  val: loss: 5757007.5 acc: 0.40266549587249756\n",
      "step: 31265\n",
      "train: loss: 20671.306640625 acc: 0.9712303876876831  val: loss: 1684414.0 acc: 0.5750176310539246\n",
      "step: 31270\n",
      "train: loss: 100146.7421875 acc: 0.914931058883667  val: loss: 978339.9375 acc: 0.7122057676315308\n",
      "step: 31275\n",
      "train: loss: 930640.125 acc: 0.7202434539794922  val: loss: 1440991.75 acc: 0.6340458393096924\n",
      "step: 31280\n",
      "train: loss: 697820.3125 acc: 0.6545165181159973  val: loss: 534575.875 acc: 0.7906436324119568\n",
      "step: 31285\n",
      "train: loss: 857516.125 acc: 0.6894168853759766  val: loss: 5164811.5 acc: 0.48839008808135986\n",
      "step: 31290\n",
      "train: loss: 86200.1328125 acc: 0.9257718324661255  val: loss: 2435359.5 acc: 0.48198866844177246\n",
      "step: 31295\n",
      "train: loss: 477954.8125 acc: 0.7917812466621399  val: loss: 1490991.875 acc: 0.6909341812133789\n",
      "step: 31300\n",
      "train: loss: 1242406.625 acc: 0.8066477179527283  val: loss: 939811.8125 acc: 0.7912498116493225\n",
      "step: 31305\n",
      "train: loss: 1047545.25 acc: 0.8800979256629944  val: loss: 741282.5 acc: 0.8887121081352234\n",
      "step: 31310\n",
      "train: loss: 266586.6875 acc: 0.9778650999069214  val: loss: 1267448.0 acc: 0.831562340259552\n",
      "step: 31315\n",
      "train: loss: 233812.484375 acc: 0.9747054576873779  val: loss: 851425.3125 acc: 0.8047559261322021\n",
      "step: 31320\n",
      "train: loss: 1291361.75 acc: 0.7228821516036987  val: loss: 388013.78125 acc: 0.932790219783783\n",
      "step: 31325\n",
      "train: loss: 530371.1875 acc: 0.8987420201301575  val: loss: 1642008.625 acc: 0.6378529071807861\n",
      "step: 31330\n",
      "train: loss: 128213.046875 acc: 0.9878354668617249  val: loss: 841476.3125 acc: 0.8015445470809937\n",
      "step: 31335\n",
      "train: loss: 252184.734375 acc: 0.9826353192329407  val: loss: 1440729.75 acc: 0.7516913414001465\n",
      "step: 31340\n",
      "train: loss: 301937.9375 acc: 0.9550546407699585  val: loss: 1491877.25 acc: 0.6535851955413818\n",
      "step: 31345\n",
      "train: loss: 309161.625 acc: 0.9608860015869141  val: loss: 950589.5625 acc: 0.7930746078491211\n",
      "step: 31350\n",
      "train: loss: 62348.25 acc: 0.984498918056488  val: loss: 552566.6875 acc: 0.8809394836425781\n",
      "step: 31355\n",
      "train: loss: 98786.7578125 acc: 0.9838273525238037  val: loss: 1065472.5 acc: 0.7484722137451172\n",
      "step: 31360\n",
      "train: loss: 53573.953125 acc: 0.9673365950584412  val: loss: 378140.84375 acc: 0.9350829720497131\n",
      "step: 31365\n",
      "train: loss: 43119.45703125 acc: 0.9732164144515991  val: loss: 663617.25 acc: 0.8719410300254822\n",
      "step: 31370\n",
      "train: loss: 9969.5185546875 acc: 0.984851598739624  val: loss: 1160904.25 acc: 0.5654802322387695\n",
      "step: 31375\n",
      "train: loss: 4929.89404296875 acc: 0.9902967810630798  val: loss: 807308.0 acc: 0.7345858216285706\n",
      "step: 31380\n",
      "train: loss: 35666.6171875 acc: 0.9276201725006104  val: loss: 875283.625 acc: -0.15439951419830322\n",
      "step: 31385\n",
      "train: loss: 57427.74609375 acc: 0.9524518251419067  val: loss: 245914.8125 acc: 0.9104413390159607\n",
      "step: 31390\n",
      "train: loss: 15555.8515625 acc: 0.9671736359596252  val: loss: 978042.875 acc: 0.27411770820617676\n",
      "step: 31395\n",
      "train: loss: 103941.453125 acc: 0.8659420609474182  val: loss: 210583.984375 acc: 0.9040719270706177\n",
      "step: 31400\n",
      "train: loss: 14151.279296875 acc: 0.9875137209892273  val: loss: 348993.125 acc: 0.904335618019104\n",
      "step: 31405\n",
      "train: loss: 97341.609375 acc: 0.9523090124130249  val: loss: 223794.90625 acc: 0.9252628684043884\n",
      "step: 31410\n",
      "train: loss: 79008.953125 acc: 0.9779712557792664  val: loss: 1953220.5 acc: 0.6725353002548218\n",
      "step: 31415\n",
      "train: loss: 21644.47265625 acc: 0.9890681505203247  val: loss: 965345.9375 acc: 0.5505601167678833\n",
      "step: 31420\n",
      "train: loss: 48790.96484375 acc: 0.9670681953430176  val: loss: 544082.9375 acc: 0.8622161149978638\n",
      "step: 31425\n",
      "train: loss: 25775.029296875 acc: 0.9393240809440613  val: loss: 552238.75 acc: 0.8056149482727051\n",
      "step: 31430\n",
      "train: loss: 13332.556640625 acc: 0.9879224300384521  val: loss: 529989.3125 acc: 0.9266614317893982\n",
      "step: 31435\n",
      "train: loss: 42609.83203125 acc: 0.9848672747612  val: loss: 832239.125 acc: 0.7663110494613647\n",
      "step: 31440\n",
      "train: loss: 79885.703125 acc: 0.9835476875305176  val: loss: 916745.5 acc: 0.7514944076538086\n",
      "step: 31445\n",
      "train: loss: 98449.40625 acc: 0.9800230264663696  val: loss: 642269.8125 acc: 0.8444867730140686\n",
      "step: 31450\n",
      "train: loss: 57657.89453125 acc: 0.978053867816925  val: loss: 965302.375 acc: 0.7720084190368652\n",
      "step: 31455\n",
      "train: loss: 121359.2421875 acc: 0.9733129143714905  val: loss: 665739.0625 acc: 0.594902753829956\n",
      "step: 31460\n",
      "train: loss: 155461.65625 acc: 0.948529064655304  val: loss: 929774.0625 acc: 0.9165610074996948\n",
      "step: 31465\n",
      "train: loss: 116891.7421875 acc: 0.9394416809082031  val: loss: 1004823.8125 acc: 0.9174560904502869\n",
      "step: 31470\n",
      "train: loss: 629608.125 acc: 0.890873372554779  val: loss: 119130.4609375 acc: 0.9564484357833862\n",
      "step: 31475\n",
      "train: loss: 197548.0625 acc: 0.9669333100318909  val: loss: 1106861.25 acc: 0.7029674053192139\n",
      "step: 31480\n",
      "train: loss: 90771.7890625 acc: 0.9882953763008118  val: loss: 1078139.625 acc: 0.8717960119247437\n",
      "step: 31485\n",
      "train: loss: 72043.46875 acc: 0.9876641631126404  val: loss: 363930.40625 acc: 0.9621922373771667\n",
      "step: 31490\n",
      "train: loss: 237049.125 acc: 0.9671040177345276  val: loss: 234050.5 acc: 0.901528000831604\n",
      "step: 31495\n",
      "train: loss: 244338.734375 acc: 0.940500795841217  val: loss: 348559.96875 acc: 0.9468374848365784\n",
      "step: 31500\n",
      "train: loss: 792315.1875 acc: 0.9699150919914246  val: loss: 2326253.25 acc: 0.5074551701545715\n",
      "step: 31505\n",
      "train: loss: 830268.5625 acc: 0.9435229301452637  val: loss: 930761.3125 acc: 0.8866265416145325\n",
      "step: 31510\n",
      "train: loss: 238317.234375 acc: 0.9805421829223633  val: loss: 398432.09375 acc: 0.8905236124992371\n",
      "step: 31515\n",
      "train: loss: 551032.9375 acc: 0.9437963366508484  val: loss: 1878430.875 acc: 0.7225915193557739\n",
      "step: 31520\n",
      "train: loss: 2195788.25 acc: 0.9396145343780518  val: loss: 2063147.25 acc: 0.3909044861793518\n",
      "step: 31525\n",
      "train: loss: 1128815.25 acc: 0.960489809513092  val: loss: 536187.9375 acc: 0.8946582078933716\n",
      "step: 31530\n",
      "train: loss: 1093694.75 acc: 0.9363700151443481  val: loss: 1231658.25 acc: 0.8806504607200623\n",
      "step: 31535\n",
      "train: loss: 862524.9375 acc: 0.9498401284217834  val: loss: 973578.1875 acc: 0.49880385398864746\n",
      "step: 31540\n",
      "train: loss: 1390422.625 acc: 0.9301719665527344  val: loss: 469643.28125 acc: 0.9292014241218567\n",
      "step: 31545\n",
      "train: loss: 534985.8125 acc: 0.958234965801239  val: loss: 2518450.0 acc: 0.7909843921661377\n",
      "step: 31550\n",
      "train: loss: 811964.75 acc: 0.7610105276107788  val: loss: 1640183.375 acc: 0.6497780680656433\n",
      "step: 31555\n",
      "train: loss: 1162991.0 acc: 0.3682570457458496  val: loss: 891649.4375 acc: 0.8092175722122192\n",
      "step: 31560\n",
      "train: loss: 562663.9375 acc: 0.8535271883010864  val: loss: 954973.4375 acc: 0.4402536153793335\n",
      "step: 31565\n",
      "train: loss: 342801.0 acc: 0.8692455291748047  val: loss: 1013593.8125 acc: 0.7668747305870056\n",
      "step: 31570\n",
      "train: loss: 587828.4375 acc: 0.8601682186126709  val: loss: 1301437.25 acc: 0.8125783205032349\n",
      "step: 31575\n",
      "train: loss: 882188.0 acc: 0.6879172325134277  val: loss: 826382.6875 acc: 0.7973556518554688\n",
      "step: 31580\n",
      "train: loss: 217213.046875 acc: 0.8434767723083496  val: loss: 2890223.75 acc: 0.46910470724105835\n",
      "step: 31585\n",
      "train: loss: 603501.6875 acc: 0.7572590112686157  val: loss: 801604.4375 acc: 0.7913681268692017\n",
      "step: 31590\n",
      "train: loss: 151047.5625 acc: 0.8808391094207764  val: loss: 318150.65625 acc: 0.7937263250350952\n",
      "step: 31595\n",
      "train: loss: 137883.625 acc: 0.8902396559715271  val: loss: 1158047.875 acc: 0.6649333238601685\n",
      "step: 31600\n",
      "train: loss: 635267.0 acc: 0.7666716575622559  val: loss: 2886574.0 acc: 0.6442078351974487\n",
      "step: 31605\n",
      "train: loss: 70322.125 acc: 0.9484513998031616  val: loss: 2687552.0 acc: 0.5997968316078186\n",
      "step: 31610\n",
      "train: loss: 36845.20703125 acc: 0.9720767140388489  val: loss: 2233976.0 acc: 0.5407999753952026\n",
      "step: 31615\n",
      "train: loss: 97471.5 acc: 0.9283263683319092  val: loss: 698972.5 acc: 0.7294809818267822\n",
      "step: 31620\n",
      "train: loss: 106286.7734375 acc: 0.9105477929115295  val: loss: 476489.1875 acc: 0.7768353223800659\n",
      "step: 31625\n",
      "train: loss: 278724.28125 acc: 0.827799916267395  val: loss: 508712.15625 acc: 0.7677291035652161\n",
      "step: 31630\n",
      "train: loss: 17182.275390625 acc: 0.9773393869400024  val: loss: 3102801.0 acc: 0.5642290115356445\n",
      "step: 31635\n",
      "train: loss: 172026.90625 acc: 0.8638255000114441  val: loss: 6423735.0 acc: 0.5714420080184937\n",
      "step: 31640\n",
      "train: loss: 103381.9140625 acc: 0.9257912039756775  val: loss: 1343279.5 acc: 0.7063091397285461\n",
      "step: 31645\n",
      "train: loss: 1128099.5 acc: 0.6940312385559082  val: loss: 857098.0 acc: 0.7615389823913574\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 31650\n",
      "train: loss: 141806.625 acc: 0.8526468276977539  val: loss: 2606999.25 acc: 0.6247590780258179\n",
      "step: 31655\n",
      "train: loss: 587371.125 acc: 0.7403478622436523  val: loss: 740411.4375 acc: 0.7821780443191528\n",
      "step: 31660\n",
      "train: loss: 580005.625 acc: 0.780038058757782  val: loss: 1081718.5 acc: 0.7536972165107727\n",
      "step: 31665\n",
      "train: loss: 804996.375 acc: 0.8417613506317139  val: loss: 795968.0 acc: 0.7092770934104919\n",
      "step: 31670\n",
      "train: loss: 1120746.25 acc: 0.8669244050979614  val: loss: 754670.4375 acc: 0.783242404460907\n",
      "step: 31675\n",
      "train: loss: 422813.25 acc: 0.9678139090538025  val: loss: 1145787.75 acc: 0.8497354984283447\n",
      "step: 31680\n",
      "train: loss: 160846.953125 acc: 0.9864261746406555  val: loss: 1662278.0 acc: 0.743121862411499\n",
      "step: 31685\n",
      "train: loss: 124069.46875 acc: 0.9848667979240417  val: loss: 1817090.75 acc: 0.1379026174545288\n",
      "step: 31690\n",
      "train: loss: 175502.953125 acc: 0.9713904857635498  val: loss: 456329.8125 acc: 0.9285783767700195\n",
      "step: 31695\n",
      "train: loss: 77235.375 acc: 0.991236686706543  val: loss: 329422.96875 acc: 0.942855954170227\n",
      "step: 31700\n",
      "train: loss: 48993.78515625 acc: 0.9963899254798889  val: loss: 641325.125 acc: 0.8796723484992981\n",
      "step: 31705\n",
      "train: loss: 116802.359375 acc: 0.9915953278541565  val: loss: 853598.375 acc: 0.7225435376167297\n",
      "step: 31710\n",
      "train: loss: 102659.5703125 acc: 0.9873946905136108  val: loss: 954648.9375 acc: 0.8727311491966248\n",
      "step: 31715\n",
      "train: loss: 25524.107421875 acc: 0.9955018758773804  val: loss: 520261.0625 acc: 0.8483505845069885\n",
      "step: 31720\n",
      "train: loss: 43115.23828125 acc: 0.9847400188446045  val: loss: 137121.71875 acc: 0.947584331035614\n",
      "step: 31725\n",
      "train: loss: 23853.451171875 acc: 0.9822169542312622  val: loss: 215686.203125 acc: 0.9669346213340759\n",
      "step: 31730\n",
      "train: loss: 16915.861328125 acc: 0.9888454675674438  val: loss: 561617.625 acc: 0.8051044344902039\n",
      "step: 31735\n",
      "train: loss: 16021.498046875 acc: 0.9871634244918823  val: loss: 449729.625 acc: 0.901393473148346\n",
      "step: 31740\n",
      "train: loss: 16554.1171875 acc: 0.971204400062561  val: loss: 902234.625 acc: 0.7934436202049255\n",
      "step: 31745\n",
      "train: loss: 14686.3115234375 acc: 0.9703678488731384  val: loss: 80746.6015625 acc: 0.9792208075523376\n",
      "step: 31750\n",
      "train: loss: 15314.1142578125 acc: 0.9528225064277649  val: loss: 936961.0625 acc: 0.5868711471557617\n",
      "step: 31755\n",
      "train: loss: 9481.89453125 acc: 0.9519014358520508  val: loss: 288822.5625 acc: 0.938256561756134\n",
      "step: 31760\n",
      "train: loss: 9313.697265625 acc: 0.9808323383331299  val: loss: 176220.71875 acc: 0.9585144519805908\n",
      "step: 31765\n",
      "train: loss: 42254.05078125 acc: 0.9775792360305786  val: loss: 841461.875 acc: 0.8104079961776733\n",
      "step: 31770\n",
      "train: loss: 10004.8134765625 acc: 0.9907412528991699  val: loss: 823203.4375 acc: 0.8878324627876282\n",
      "step: 31775\n",
      "train: loss: 17648.806640625 acc: 0.9876468777656555  val: loss: 229957.6875 acc: 0.9663822650909424\n",
      "step: 31780\n",
      "train: loss: 113894.703125 acc: 0.9232577085494995  val: loss: 839195.0625 acc: 0.9301095604896545\n",
      "step: 31785\n",
      "train: loss: 15069.87890625 acc: 0.9846479892730713  val: loss: 954576.8125 acc: 0.7849649786949158\n",
      "step: 31790\n",
      "train: loss: 29683.310546875 acc: 0.953850507736206  val: loss: 393093.25 acc: 0.9366625547409058\n",
      "step: 31795\n",
      "train: loss: 29967.685546875 acc: 0.9824385643005371  val: loss: 1355901.375 acc: 0.6607837677001953\n",
      "step: 31800\n",
      "train: loss: 34627.31640625 acc: 0.9911853075027466  val: loss: 249382.96875 acc: 0.7963104248046875\n",
      "step: 31805\n",
      "train: loss: 53590.13671875 acc: 0.9855832457542419  val: loss: 966385.875 acc: 0.7106789350509644\n",
      "step: 31810\n",
      "train: loss: 42345.91015625 acc: 0.9849418997764587  val: loss: 404189.5625 acc: 0.9373450875282288\n",
      "step: 31815\n",
      "train: loss: 58726.75 acc: 0.9709944725036621  val: loss: 382037.625 acc: 0.9264196753501892\n",
      "step: 31820\n",
      "train: loss: 65906.1015625 acc: 0.9828911423683167  val: loss: 132559.265625 acc: 0.9799593091011047\n",
      "step: 31825\n",
      "train: loss: 98393.671875 acc: 0.9793348908424377  val: loss: 315081.4375 acc: 0.941683828830719\n",
      "step: 31830\n",
      "train: loss: 177786.171875 acc: 0.9286835193634033  val: loss: 1067253.0 acc: 0.7802290916442871\n",
      "step: 31835\n",
      "train: loss: 89455.421875 acc: 0.9696023464202881  val: loss: 1201752.5 acc: 0.8562381267547607\n",
      "step: 31840\n",
      "train: loss: 56673.20703125 acc: 0.9824160933494568  val: loss: 1233525.875 acc: 0.8084157705307007\n",
      "step: 31845\n",
      "train: loss: 101582.421875 acc: 0.9921181201934814  val: loss: 1457581.625 acc: -0.08221995830535889\n",
      "step: 31850\n",
      "train: loss: 69459.078125 acc: 0.9929382801055908  val: loss: 2316501.5 acc: 0.7856845259666443\n",
      "step: 31855\n",
      "train: loss: 31878.513671875 acc: 0.9954351782798767  val: loss: 137333.796875 acc: 0.9502037763595581\n",
      "step: 31860\n",
      "train: loss: 229576.484375 acc: 0.9838592410087585  val: loss: 380051.90625 acc: 0.9262263774871826\n",
      "step: 31865\n",
      "train: loss: 741473.125 acc: 0.9336135983467102  val: loss: 2926004.0 acc: 0.25117045640945435\n",
      "step: 31870\n",
      "train: loss: 298596.0 acc: 0.9830799102783203  val: loss: 752554.6875 acc: 0.8177331686019897\n",
      "step: 31875\n",
      "train: loss: 1971079.375 acc: 0.6520985960960388  val: loss: 1395528.75 acc: 0.4256365895271301\n",
      "step: 31880\n",
      "train: loss: 1288033.375 acc: 0.9522429704666138  val: loss: 565197.375 acc: 0.9474791884422302\n",
      "step: 31885\n",
      "train: loss: 1101179.75 acc: 0.9625065326690674  val: loss: 271381.84375 acc: 0.9126306772232056\n",
      "step: 31890\n",
      "train: loss: 1372425.625 acc: 0.9616990089416504  val: loss: 1186115.375 acc: 0.7274951934814453\n",
      "step: 31895\n",
      "train: loss: 1971160.5 acc: 0.9146897792816162  val: loss: 2485068.0 acc: -0.158791184425354\n",
      "step: 31900\n",
      "train: loss: 298580.125 acc: 0.9594990611076355  val: loss: 946954.5 acc: 0.8491719961166382\n",
      "step: 31905\n",
      "train: loss: 364410.09375 acc: 0.9778796434402466  val: loss: 2984143.0 acc: 0.6080796718597412\n",
      "step: 31910\n",
      "train: loss: 220895.34375 acc: 0.9605928659439087  val: loss: 1327661.0 acc: 0.8808199763298035\n",
      "step: 31915\n",
      "train: loss: 705131.25 acc: 0.8675258159637451  val: loss: 1615547.75 acc: 0.664813756942749\n",
      "step: 31920\n",
      "train: loss: 1973452.5 acc: 0.7901430130004883  val: loss: 1057840.75 acc: 0.6965798139572144\n",
      "step: 31925\n",
      "train: loss: 538283.3125 acc: 0.8255639672279358  val: loss: 3972657.5 acc: 0.7042219638824463\n",
      "step: 31930\n",
      "train: loss: 391638.34375 acc: 0.8303825259208679  val: loss: 484126.34375 acc: 0.8222439885139465\n",
      "step: 31935\n",
      "train: loss: 511780.09375 acc: 0.8467918634414673  val: loss: 2037432.0 acc: 0.7467348575592041\n",
      "step: 31940\n",
      "train: loss: 528138.5625 acc: 0.7725939750671387  val: loss: 1511444.25 acc: 0.7251420021057129\n",
      "step: 31945\n",
      "train: loss: 276943.5625 acc: 0.8407710194587708  val: loss: 983293.125 acc: 0.6802771687507629\n",
      "step: 31950\n",
      "train: loss: 178596.1875 acc: 0.8887676000595093  val: loss: 1672266.0 acc: 0.5617276430130005\n",
      "step: 31955\n",
      "train: loss: 75351.9765625 acc: 0.9382615089416504  val: loss: 629713.1875 acc: 0.7310341596603394\n",
      "step: 31960\n",
      "train: loss: 367796.4375 acc: 0.8175601363182068  val: loss: 1403540.875 acc: 0.6389672756195068\n",
      "step: 31965\n",
      "train: loss: 60339.80859375 acc: 0.9547607898712158  val: loss: 4422725.5 acc: 0.5051442384719849\n",
      "step: 31970\n",
      "train: loss: 29642.4609375 acc: 0.9788987040519714  val: loss: 1057380.375 acc: 0.72403484582901\n",
      "step: 31975\n",
      "train: loss: 27206.3984375 acc: 0.9805873036384583  val: loss: 633560.5625 acc: 0.7247154712677002\n",
      "step: 31980\n",
      "train: loss: 544055.25 acc: 0.7887411117553711  val: loss: 1467560.625 acc: 0.7355470657348633\n",
      "step: 31985\n",
      "train: loss: 122886.796875 acc: 0.9123738408088684  val: loss: 1065768.5 acc: 0.6712942123413086\n",
      "step: 31990\n",
      "train: loss: 90277.6015625 acc: 0.942911684513092  val: loss: 3760857.5 acc: 0.6037920713424683\n",
      "step: 31995\n",
      "train: loss: 36222.80078125 acc: 0.9567464590072632  val: loss: 685299.5 acc: 0.7162151336669922\n",
      "step: 32000\n",
      "train: loss: 210357.265625 acc: 0.861107587814331  val: loss: 718140.125 acc: 0.7748477458953857\n",
      "step: 32005\n",
      "train: loss: 575533.125 acc: 0.7692759037017822  val: loss: 1590555.5 acc: 0.7246226072311401\n",
      "step: 32010\n",
      "train: loss: 218173.3125 acc: 0.8152101039886475  val: loss: 623708.1875 acc: 0.7104607820510864\n",
      "step: 32015\n",
      "train: loss: 433360.53125 acc: 0.7662637233734131  val: loss: 855824.3125 acc: 0.6821155548095703\n",
      "step: 32020\n",
      "train: loss: 510527.625 acc: 0.7279878854751587  val: loss: 2374625.25 acc: 0.47504866123199463\n",
      "step: 32025\n",
      "train: loss: 206214.46875 acc: 0.8939798474311829  val: loss: 1201202.0 acc: 0.7275656461715698\n",
      "step: 32030\n",
      "train: loss: 1211353.0 acc: 0.7748826742172241  val: loss: 314511.15625 acc: 0.545462429523468\n",
      "step: 32035\n",
      "train: loss: 805932.5 acc: 0.8811811208724976  val: loss: 1065504.875 acc: 0.6447985172271729\n",
      "step: 32040\n",
      "train: loss: 752388.125 acc: 0.8813714385032654  val: loss: 93306.5234375 acc: 0.9810640215873718\n",
      "step: 32045\n",
      "train: loss: 103493.796875 acc: 0.9836652278900146  val: loss: 1359212.375 acc: 0.4105050563812256\n",
      "step: 32050\n",
      "train: loss: 276866.96875 acc: 0.964049756526947  val: loss: 821527.0 acc: 0.8468122482299805\n",
      "step: 32055\n",
      "train: loss: 85458.6640625 acc: 0.9824948906898499  val: loss: 172335.75 acc: 0.9622935652732849\n",
      "step: 32060\n",
      "train: loss: 155715.625 acc: 0.9795377850532532  val: loss: 798411.375 acc: 0.796846330165863\n",
      "step: 32065\n",
      "train: loss: 123639.578125 acc: 0.9916099309921265  val: loss: 564117.5 acc: 0.8278900384902954\n",
      "step: 32070\n",
      "train: loss: 82547.90625 acc: 0.993659496307373  val: loss: 456117.34375 acc: 0.7823947668075562\n",
      "step: 32075\n",
      "train: loss: 107763.4375 acc: 0.9820380806922913  val: loss: 251321.71875 acc: 0.8828926086425781\n",
      "step: 32080\n",
      "train: loss: 24502.685546875 acc: 0.9946470260620117  val: loss: 856715.125 acc: 0.8066585063934326\n",
      "step: 32085\n",
      "train: loss: 40778.3984375 acc: 0.9909473061561584  val: loss: 579240.125 acc: 0.9154781103134155\n",
      "step: 32090\n",
      "train: loss: 17426.515625 acc: 0.986870527267456  val: loss: 386341.09375 acc: 0.9193106889724731\n",
      "step: 32095\n",
      "train: loss: 14661.9931640625 acc: 0.9772297739982605  val: loss: 1756974.5 acc: 0.6325510740280151\n",
      "step: 32100\n",
      "train: loss: 17766.16796875 acc: 0.9934526085853577  val: loss: 472223.4375 acc: 0.9506754875183105\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 32105\n",
      "train: loss: 9885.064453125 acc: 0.9783909916877747  val: loss: 441161.25 acc: 0.8996052145957947\n",
      "step: 32110\n",
      "train: loss: 15080.7861328125 acc: 0.9725390672683716  val: loss: 957766.375 acc: 0.9213622212409973\n",
      "step: 32115\n",
      "train: loss: 10975.6904296875 acc: 0.9617059826850891  val: loss: 833515.8125 acc: 0.7810977101325989\n",
      "step: 32120\n",
      "train: loss: 15357.173828125 acc: 0.9697628021240234  val: loss: 497386.21875 acc: 0.8606243133544922\n",
      "step: 32125\n",
      "train: loss: 12220.5966796875 acc: 0.9844639301300049  val: loss: 871267.5 acc: 0.9226308465003967\n",
      "step: 32130\n",
      "train: loss: 57882.94140625 acc: 0.9640079140663147  val: loss: 853759.5625 acc: 0.8289682269096375\n",
      "step: 32135\n",
      "train: loss: 23549.6171875 acc: 0.9864395260810852  val: loss: 1278036.25 acc: 0.8978258967399597\n",
      "step: 32140\n",
      "train: loss: 21856.830078125 acc: 0.9848120212554932  val: loss: 1086988.125 acc: 0.8993682861328125\n",
      "step: 32145\n",
      "train: loss: 14893.7451171875 acc: 0.9901127219200134  val: loss: 753681.6875 acc: 0.5661001801490784\n",
      "step: 32150\n",
      "train: loss: 16469.5546875 acc: 0.9896751642227173  val: loss: 2465301.75 acc: 0.7113305330276489\n",
      "step: 32155\n",
      "train: loss: 27621.8203125 acc: 0.9765012860298157  val: loss: 1056061.25 acc: 0.5886281132698059\n",
      "step: 32160\n",
      "train: loss: 9796.4921875 acc: 0.9846288561820984  val: loss: 3161409.0 acc: -0.5263341665267944\n",
      "step: 32165\n",
      "train: loss: 29227.908203125 acc: 0.9866083860397339  val: loss: 1280823.5 acc: 0.5581937432289124\n",
      "step: 32170\n",
      "train: loss: 69646.109375 acc: 0.9855280518531799  val: loss: 1402191.75 acc: 0.8540053367614746\n",
      "step: 32175\n",
      "train: loss: 51923.35546875 acc: 0.9842723608016968  val: loss: 1351435.875 acc: 0.8441770672798157\n",
      "step: 32180\n",
      "train: loss: 45545.58203125 acc: 0.9787601828575134  val: loss: 3139712.25 acc: 0.6680254936218262\n",
      "step: 32185\n",
      "train: loss: 52496.7734375 acc: 0.9843432307243347  val: loss: 858327.125 acc: 0.4205564856529236\n",
      "step: 32190\n",
      "train: loss: 102740.25 acc: 0.9726187586784363  val: loss: 2167627.5 acc: 0.06476032733917236\n",
      "step: 32195\n",
      "train: loss: 90544.1796875 acc: 0.9789642095565796  val: loss: 852298.5625 acc: 0.91411292552948\n",
      "step: 32200\n",
      "train: loss: 405431.75 acc: 0.6430820226669312  val: loss: 248959.6875 acc: 0.9129904508590698\n",
      "step: 32205\n",
      "train: loss: 482759.5 acc: 0.9363673329353333  val: loss: 491384.0 acc: 0.8689813017845154\n",
      "step: 32210\n",
      "train: loss: 90181.34375 acc: 0.9909000992774963  val: loss: 1559027.625 acc: 0.5478295683860779\n",
      "step: 32215\n",
      "train: loss: 43779.9609375 acc: 0.9960662126541138  val: loss: 1414803.375 acc: 0.8471866250038147\n",
      "step: 32220\n",
      "train: loss: 38355.83984375 acc: 0.9917692542076111  val: loss: 902238.3125 acc: 0.877798855304718\n",
      "step: 32225\n",
      "train: loss: 661970.75 acc: 0.9562922120094299  val: loss: 671867.125 acc: 0.6845976114273071\n",
      "step: 32230\n",
      "train: loss: 202603.28125 acc: 0.9705215096473694  val: loss: 3818707.5 acc: 0.3072870969772339\n",
      "step: 32235\n",
      "train: loss: 452807.9375 acc: 0.9344002604484558  val: loss: 1763088.125 acc: 0.696392297744751\n",
      "step: 32240\n",
      "train: loss: 154508.671875 acc: 0.9739744663238525  val: loss: 1188809.75 acc: 0.13864845037460327\n",
      "step: 32245\n",
      "train: loss: 1128339.875 acc: 0.9486851096153259  val: loss: 484511.375 acc: 0.8988915681838989\n",
      "step: 32250\n",
      "train: loss: 904504.875 acc: 0.9817978143692017  val: loss: 1041978.6875 acc: 0.8143301010131836\n",
      "step: 32255\n",
      "train: loss: 4273464.5 acc: 0.7980943322181702  val: loss: 2110175.25 acc: 0.7179255485534668\n",
      "step: 32260\n",
      "train: loss: 733428.0 acc: 0.9582238793373108  val: loss: 676514.4375 acc: 0.8733357191085815\n",
      "step: 32265\n",
      "train: loss: 389533.65625 acc: 0.9772519469261169  val: loss: 438794.03125 acc: 0.9581652879714966\n",
      "step: 32270\n",
      "train: loss: 971040.9375 acc: 0.9471227526664734  val: loss: 1489404.125 acc: 0.5718528628349304\n",
      "step: 32275\n",
      "train: loss: 569059.75 acc: 0.9375011920928955  val: loss: 889597.625 acc: 0.7778377532958984\n",
      "step: 32280\n",
      "train: loss: 343103.125 acc: 0.8879801034927368  val: loss: 292178.84375 acc: 0.9156946539878845\n",
      "step: 32285\n",
      "train: loss: 1381910.125 acc: 0.8096386194229126  val: loss: 1195336.875 acc: 0.846548318862915\n",
      "step: 32290\n",
      "train: loss: 217309.515625 acc: 0.7274777293205261  val: loss: 810091.375 acc: 0.5905935764312744\n",
      "step: 32295\n",
      "train: loss: 438096.0 acc: 0.7667908668518066  val: loss: 1147675.625 acc: 0.6914083957672119\n",
      "step: 32300\n",
      "train: loss: 114140.71875 acc: 0.9318165183067322  val: loss: 482534.53125 acc: 0.8073872327804565\n",
      "step: 32305\n",
      "train: loss: 748494.3125 acc: 0.7978802919387817  val: loss: 237184.359375 acc: 0.7824879884719849\n",
      "step: 32310\n",
      "train: loss: 417436.8125 acc: 0.7169926166534424  val: loss: 1439312.875 acc: 0.6809852123260498\n",
      "step: 32315\n",
      "train: loss: 79445.9921875 acc: 0.9453291296958923  val: loss: 2424233.5 acc: 0.5922267436981201\n",
      "step: 32320\n",
      "train: loss: 346336.96875 acc: 0.8064789175987244  val: loss: 1684329.875 acc: 0.6477274894714355\n",
      "step: 32325\n",
      "train: loss: 164224.015625 acc: 0.8947742581367493  val: loss: 3203839.5 acc: 0.4597916603088379\n",
      "step: 32330\n",
      "train: loss: 94305.6640625 acc: 0.9319062232971191  val: loss: 1708818.375 acc: 0.6569232940673828\n",
      "step: 32335\n",
      "train: loss: 45597.5234375 acc: 0.9542545080184937  val: loss: 1919300.375 acc: 0.5900322794914246\n",
      "step: 32340\n",
      "train: loss: 249301.0 acc: 0.8716011643409729  val: loss: 2784609.5 acc: 0.6080572605133057\n",
      "step: 32345\n",
      "train: loss: 293406.40625 acc: 0.8311411142349243  val: loss: 1267010.625 acc: 0.6727012395858765\n",
      "step: 32350\n",
      "train: loss: 95768.9296875 acc: 0.928282618522644  val: loss: 4777337.5 acc: 0.5292853713035583\n",
      "step: 32355\n",
      "train: loss: 319900.84375 acc: 0.8130890130996704  val: loss: 648914.875 acc: 0.7343752384185791\n",
      "step: 32360\n",
      "train: loss: 222422.390625 acc: 0.8432580828666687  val: loss: 911598.9375 acc: 0.7148994207382202\n",
      "step: 32365\n",
      "train: loss: 35095.5078125 acc: 0.9648058414459229  val: loss: 1282254.75 acc: 0.6774978637695312\n",
      "step: 32370\n",
      "train: loss: 132842.4375 acc: 0.8966432809829712  val: loss: 745628.25 acc: 0.6714544296264648\n",
      "step: 32375\n",
      "train: loss: 568881.3125 acc: 0.6302847862243652  val: loss: 290989.03125 acc: 0.893408477306366\n",
      "step: 32380\n",
      "train: loss: 53699.1796875 acc: 0.9523106813430786  val: loss: 1662804.625 acc: 0.6409664154052734\n",
      "step: 32385\n",
      "train: loss: 91677.40625 acc: 0.9033089876174927  val: loss: 1981619.75 acc: 0.6045181751251221\n",
      "step: 32390\n",
      "train: loss: 518254.78125 acc: 0.7793344259262085  val: loss: 3647433.25 acc: 0.56749027967453\n",
      "step: 32395\n",
      "train: loss: 1200209.5 acc: 0.8036571741104126  val: loss: 1146013.75 acc: 0.7485772967338562\n",
      "step: 32400\n",
      "train: loss: 1058755.5 acc: 0.822508692741394  val: loss: 1460563.0 acc: 0.7921912670135498\n",
      "step: 32405\n",
      "train: loss: 925407.8125 acc: 0.8974835276603699  val: loss: 1060576.875 acc: 0.8910974860191345\n",
      "step: 32410\n",
      "train: loss: 110949.359375 acc: 0.9920476078987122  val: loss: 142522.328125 acc: 0.9627024531364441\n",
      "step: 32415\n",
      "train: loss: 548406.3125 acc: 0.8030737042427063  val: loss: 1454247.25 acc: 0.6798203587532043\n",
      "step: 32420\n",
      "train: loss: 108840.046875 acc: 0.9831027388572693  val: loss: 1148485.0 acc: 0.85588139295578\n",
      "step: 32425\n",
      "train: loss: 109733.625 acc: 0.9886809587478638  val: loss: 385996.5 acc: 0.9523584842681885\n",
      "step: 32430\n",
      "train: loss: 66274.390625 acc: 0.9945246577262878  val: loss: 1109777.125 acc: 0.6722474098205566\n",
      "step: 32435\n",
      "train: loss: 124669.4140625 acc: 0.9910933375358582  val: loss: 1015369.0 acc: 0.7270146608352661\n",
      "step: 32440\n",
      "train: loss: 71411.6796875 acc: 0.993208646774292  val: loss: 1451553.5 acc: 0.6615786552429199\n",
      "step: 32445\n",
      "train: loss: 56330.7109375 acc: 0.9912937879562378  val: loss: 756137.5 acc: 0.8569828867912292\n",
      "step: 32450\n",
      "train: loss: 9793.451171875 acc: 0.9939719438552856  val: loss: 759845.625 acc: 0.874241292476654\n",
      "step: 32455\n",
      "train: loss: 16598.03125 acc: 0.990359902381897  val: loss: 716972.9375 acc: 0.8655259013175964\n",
      "step: 32460\n",
      "train: loss: 13640.9267578125 acc: 0.9615226984024048  val: loss: 1919044.0 acc: -0.5022901296615601\n",
      "step: 32465\n",
      "train: loss: 14712.986328125 acc: 0.9901040196418762  val: loss: 1057377.625 acc: 0.6801843643188477\n",
      "step: 32470\n",
      "train: loss: 27906.142578125 acc: 0.9675668478012085  val: loss: 1355423.125 acc: 0.8205171823501587\n",
      "step: 32475\n",
      "train: loss: 20299.802734375 acc: 0.9871024489402771  val: loss: 1449606.125 acc: 0.8750904202461243\n",
      "step: 32480\n",
      "train: loss: 6451.88916015625 acc: 0.9791101813316345  val: loss: 2120006.75 acc: 0.7430906295776367\n",
      "step: 32485\n",
      "train: loss: 12303.705078125 acc: 0.9823830723762512  val: loss: 1083967.75 acc: 0.8437777757644653\n",
      "step: 32490\n",
      "train: loss: 11732.564453125 acc: 0.9855837225914001  val: loss: 1984340.625 acc: 0.7791314125061035\n",
      "step: 32495\n",
      "train: loss: 25070.49609375 acc: 0.9813674092292786  val: loss: 518128.1875 acc: 0.843151330947876\n",
      "step: 32500\n",
      "train: loss: 22013.76953125 acc: 0.9761521220207214  val: loss: 536872.0 acc: 0.7572550773620605\n",
      "step: 32505\n",
      "train: loss: 33842.6796875 acc: 0.9742324948310852  val: loss: 1445417.5 acc: 0.6147851943969727\n",
      "step: 32510\n",
      "train: loss: 24217.248046875 acc: 0.9889922142028809  val: loss: 1819038.125 acc: 0.3939790725708008\n",
      "step: 32515\n",
      "train: loss: 19951.751953125 acc: 0.9898014068603516  val: loss: 1916468.0 acc: 0.5195282101631165\n",
      "step: 32520\n",
      "train: loss: 14889.5283203125 acc: 0.9840896129608154  val: loss: 1882913.25 acc: 0.21557331085205078\n",
      "step: 32525\n",
      "train: loss: 10343.759765625 acc: 0.9869184494018555  val: loss: 4151318.25 acc: -1.5369946956634521\n",
      "step: 32530\n",
      "train: loss: 22468.2421875 acc: 0.9855239391326904  val: loss: 1217010.0 acc: 0.8121989965438843\n",
      "step: 32535\n",
      "train: loss: 43073.484375 acc: 0.9914716482162476  val: loss: 1012809.9375 acc: 0.8357706665992737\n",
      "step: 32540\n",
      "train: loss: 19106.974609375 acc: 0.9919371604919434  val: loss: 951985.625 acc: 0.8244560956954956\n",
      "step: 32545\n",
      "train: loss: 38798.42578125 acc: 0.9792436957359314  val: loss: 2692863.75 acc: -0.20174598693847656\n",
      "step: 32550\n",
      "train: loss: 40764.5390625 acc: 0.9853247404098511  val: loss: 702209.6875 acc: 0.8092971444129944\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 32555\n",
      "train: loss: 161424.609375 acc: 0.9778551459312439  val: loss: 1661661.375 acc: 0.7156316041946411\n",
      "step: 32560\n",
      "train: loss: 162582.6875 acc: 0.9276864528656006  val: loss: 310443.59375 acc: 0.9408122301101685\n",
      "step: 32565\n",
      "train: loss: 42590.84765625 acc: 0.9739869832992554  val: loss: 1016089.625 acc: 0.9011399745941162\n",
      "step: 32570\n",
      "train: loss: 311360.53125 acc: 0.9230747222900391  val: loss: 1385322.625 acc: 0.7900720834732056\n",
      "step: 32575\n",
      "train: loss: 42156.0703125 acc: 0.9962005615234375  val: loss: 2290263.25 acc: 0.28870153427124023\n",
      "step: 32580\n",
      "train: loss: 49363.72265625 acc: 0.9957752227783203  val: loss: 378140.34375 acc: 0.9436990022659302\n",
      "step: 32585\n",
      "train: loss: 21445.6875 acc: 0.9974901080131531  val: loss: 1239547.375 acc: 0.7670061588287354\n",
      "step: 32590\n",
      "train: loss: 524694.375 acc: 0.9542810916900635  val: loss: 957394.8125 acc: 0.8482745885848999\n",
      "step: 32595\n",
      "train: loss: 239175.484375 acc: 0.9786535501480103  val: loss: 512710.9375 acc: 0.8922104835510254\n",
      "step: 32600\n",
      "train: loss: 400328.59375 acc: 0.977112889289856  val: loss: 1051538.75 acc: 0.8209595680236816\n",
      "step: 32605\n",
      "train: loss: 367648.46875 acc: 0.9623549580574036  val: loss: 1918641.25 acc: 0.8235062956809998\n",
      "step: 32610\n",
      "train: loss: 241618.390625 acc: 0.9629145264625549  val: loss: 1762640.625 acc: 0.21206659078598022\n",
      "step: 32615\n",
      "train: loss: 616163.375 acc: 0.9799946546554565  val: loss: 620680.5 acc: 0.8538382053375244\n",
      "step: 32620\n",
      "train: loss: 3339021.75 acc: 0.8581914305686951  val: loss: 529354.5 acc: 0.8661359548568726\n",
      "step: 32625\n",
      "train: loss: 950761.875 acc: 0.9696663618087769  val: loss: 507839.0625 acc: 0.927383542060852\n",
      "step: 32630\n",
      "train: loss: 636224.0 acc: 0.940711259841919  val: loss: 756141.0 acc: 0.8308753371238708\n",
      "step: 32635\n",
      "train: loss: 72881.703125 acc: 0.9905023574829102  val: loss: 258917.21875 acc: 0.8969696760177612\n",
      "step: 32640\n",
      "train: loss: 413198.03125 acc: 0.9017593860626221  val: loss: 1654864.75 acc: 0.4436625838279724\n",
      "step: 32645\n",
      "train: loss: 364125.6875 acc: 0.9200081825256348  val: loss: 497602.125 acc: 0.6295092105865479\n",
      "step: 32650\n",
      "train: loss: 2057924.25 acc: 0.09672611951828003  val: loss: 488324.34375 acc: 0.8976755142211914\n",
      "step: 32655\n",
      "train: loss: 776312.9375 acc: 0.7117183208465576  val: loss: 928447.75 acc: 0.8004709482192993\n",
      "step: 32660\n",
      "train: loss: 627200.375 acc: 0.822414755821228  val: loss: 698953.875 acc: 0.7403461933135986\n",
      "step: 32665\n",
      "train: loss: 711475.9375 acc: 0.6912585496902466  val: loss: 2250677.0 acc: 0.7256977558135986\n",
      "step: 32670\n",
      "train: loss: 722028.125 acc: 0.5698795318603516  val: loss: 1031337.0625 acc: 0.7489892244338989\n",
      "step: 32675\n",
      "train: loss: 584648.3125 acc: 0.7458811402320862  val: loss: 2826454.25 acc: 0.5889706611633301\n",
      "step: 32680\n",
      "train: loss: 132071.25 acc: 0.9168860912322998  val: loss: 1518136.125 acc: 0.6162244081497192\n",
      "step: 32685\n",
      "train: loss: 204083.78125 acc: 0.8455602526664734  val: loss: 789617.75 acc: 0.665056586265564\n",
      "step: 32690\n",
      "train: loss: 155721.296875 acc: 0.889002799987793  val: loss: 331118.96875 acc: 0.7311758995056152\n",
      "step: 32695\n",
      "train: loss: 377278.90625 acc: 0.8161766529083252  val: loss: 3064941.25 acc: 0.5257120132446289\n",
      "step: 32700\n",
      "train: loss: 50623.58203125 acc: 0.9611579179763794  val: loss: 6364182.5 acc: 0.4538837671279907\n",
      "step: 32705\n",
      "train: loss: 57853.359375 acc: 0.9485023021697998  val: loss: 1432200.5 acc: 0.6601500511169434\n",
      "step: 32710\n",
      "train: loss: 49037.51953125 acc: 0.9544274806976318  val: loss: 4849057.5 acc: 0.5190683007240295\n",
      "step: 32715\n",
      "train: loss: 234141.78125 acc: 0.84957355260849  val: loss: 5881646.5 acc: 0.4669053554534912\n",
      "step: 32720\n",
      "train: loss: 116727.375 acc: 0.9151501059532166  val: loss: 2417658.25 acc: 0.5680676698684692\n",
      "step: 32725\n",
      "train: loss: 120651.140625 acc: 0.8576571345329285  val: loss: 7574154.0 acc: 0.38070881366729736\n",
      "step: 32730\n",
      "train: loss: 290830.71875 acc: 0.8356962203979492  val: loss: 2245738.25 acc: 0.6480300426483154\n",
      "step: 32735\n",
      "train: loss: 525220.6875 acc: 0.755885660648346  val: loss: 3257242.25 acc: 0.5087395906448364\n",
      "step: 32740\n",
      "train: loss: 359918.875 acc: 0.7677196860313416  val: loss: 348314.78125 acc: 0.8305745720863342\n",
      "step: 32745\n",
      "train: loss: 398814.8125 acc: 0.8093088865280151  val: loss: 2187303.25 acc: 0.6552186012268066\n",
      "step: 32750\n",
      "train: loss: 990451.0625 acc: 0.7373532056808472  val: loss: 167637.84375 acc: 0.9211894273757935\n",
      "step: 32755\n",
      "train: loss: 57788.5390625 acc: 0.948375940322876  val: loss: 487756.03125 acc: 0.799519419670105\n",
      "step: 32760\n",
      "train: loss: 967642.375 acc: 0.8017579317092896  val: loss: 547519.4375 acc: 0.7996902465820312\n",
      "step: 32765\n",
      "train: loss: 875400.375 acc: 0.6438318490982056  val: loss: 3054173.25 acc: 0.7678631544113159\n",
      "step: 32770\n",
      "train: loss: 550261.5625 acc: 0.9391136765480042  val: loss: 767311.1875 acc: 0.7212849855422974\n",
      "step: 32775\n",
      "train: loss: 532326.6875 acc: 0.8804150819778442  val: loss: 1562078.125 acc: 0.3260921835899353\n",
      "step: 32780\n",
      "train: loss: 87230.1015625 acc: 0.9873889684677124  val: loss: 724347.625 acc: 0.8310009241104126\n",
      "step: 32785\n",
      "train: loss: 269621.09375 acc: 0.9509150981903076  val: loss: 1146312.375 acc: 0.8519824147224426\n",
      "step: 32790\n",
      "train: loss: 287171.71875 acc: 0.9627783298492432  val: loss: 1367324.125 acc: 0.6836669445037842\n",
      "step: 32795\n",
      "train: loss: 100329.875 acc: 0.9895961284637451  val: loss: 1793684.75 acc: 0.09884703159332275\n",
      "step: 32800\n",
      "train: loss: 61514.5703125 acc: 0.995573878288269  val: loss: 1533627.75 acc: 0.49141061305999756\n",
      "step: 32805\n",
      "train: loss: 56643.9921875 acc: 0.9913466572761536  val: loss: 1268337.75 acc: 0.365894615650177\n",
      "step: 32810\n",
      "train: loss: 55883.04296875 acc: 0.9876525402069092  val: loss: 345536.59375 acc: 0.9370152354240417\n",
      "step: 32815\n",
      "train: loss: 32305.126953125 acc: 0.9864978194236755  val: loss: 1480337.375 acc: 0.8035410642623901\n",
      "step: 32820\n",
      "train: loss: 24063.58984375 acc: 0.9874029159545898  val: loss: 1728293.375 acc: 0.729158341884613\n",
      "step: 32825\n",
      "train: loss: 24963.5078125 acc: 0.9824725389480591  val: loss: 1558590.875 acc: 0.8189501166343689\n",
      "step: 32830\n",
      "train: loss: 14166.501953125 acc: 0.983574628829956  val: loss: 793011.875 acc: 0.6422012448310852\n",
      "step: 32835\n",
      "train: loss: 16880.5625 acc: 0.9666938185691833  val: loss: 577001.4375 acc: 0.9235137701034546\n",
      "step: 32840\n",
      "train: loss: 176214.453125 acc: 0.47118037939071655  val: loss: 403901.34375 acc: 0.8724204301834106\n",
      "step: 32845\n",
      "train: loss: 19530.6484375 acc: 0.9800992012023926  val: loss: 1978562.75 acc: 0.6779473423957825\n",
      "step: 32850\n",
      "train: loss: 12344.748046875 acc: 0.9736660718917847  val: loss: 1248631.25 acc: 0.6779921054840088\n",
      "step: 32855\n",
      "train: loss: 26330.099609375 acc: 0.9489052891731262  val: loss: 1720439.25 acc: 0.6347863674163818\n",
      "step: 32860\n",
      "train: loss: 38104.4453125 acc: 0.9201819896697998  val: loss: 1575974.625 acc: 0.42426741123199463\n",
      "step: 32865\n",
      "train: loss: 33671.328125 acc: 0.9634354114532471  val: loss: 890991.8125 acc: 0.8058245778083801\n",
      "step: 32870\n",
      "train: loss: 41740.6875 acc: 0.9785817861557007  val: loss: 1851450.5 acc: 0.4663460850715637\n",
      "step: 32875\n",
      "train: loss: 7342.1669921875 acc: 0.9925859570503235  val: loss: 1632569.125 acc: 0.262534499168396\n",
      "step: 32880\n",
      "train: loss: 23988.1484375 acc: 0.9916160106658936  val: loss: 2746947.75 acc: 0.5152057409286499\n",
      "step: 32885\n",
      "train: loss: 19548.697265625 acc: 0.9859477281570435  val: loss: 3011024.25 acc: 0.4828900098800659\n",
      "step: 32890\n",
      "train: loss: 90382.25 acc: 0.6704858541488647  val: loss: 1298844.875 acc: 0.8549224138259888\n",
      "step: 32895\n",
      "train: loss: 19381.431640625 acc: 0.988999605178833  val: loss: 1711083.75 acc: 0.6117867231369019\n",
      "step: 32900\n",
      "train: loss: 26247.865234375 acc: 0.9865859746932983  val: loss: 2637347.75 acc: 0.7837058305740356\n",
      "step: 32905\n",
      "train: loss: 39230.84375 acc: 0.9847263097763062  val: loss: 1288085.75 acc: 0.7854017019271851\n",
      "step: 32910\n",
      "train: loss: 39508.16015625 acc: 0.9776855111122131  val: loss: 415613.875 acc: 0.9111326336860657\n",
      "step: 32915\n",
      "train: loss: 51151.85546875 acc: 0.9841599464416504  val: loss: 1351006.0 acc: 0.5845955610275269\n",
      "step: 32920\n",
      "train: loss: 297809.84375 acc: 0.8855275511741638  val: loss: 399813.78125 acc: 0.8610702753067017\n",
      "step: 32925\n",
      "train: loss: 52105.1640625 acc: 0.9814335703849792  val: loss: 1513755.0 acc: 0.7369205951690674\n",
      "step: 32930\n",
      "train: loss: 100431.640625 acc: 0.975278913974762  val: loss: 232335.046875 acc: 0.9435238242149353\n",
      "step: 32935\n",
      "train: loss: 1141909.0 acc: 0.7374289035797119  val: loss: 735751.375 acc: 0.8080587387084961\n",
      "step: 32940\n",
      "train: loss: 794523.4375 acc: 0.862318754196167  val: loss: 876367.25 acc: 0.36062324047088623\n",
      "step: 32945\n",
      "train: loss: 108857.3984375 acc: 0.9885951280593872  val: loss: 512733.84375 acc: 0.8145431280136108\n",
      "step: 32950\n",
      "train: loss: 69660.7109375 acc: 0.9909523129463196  val: loss: 2041781.25 acc: 0.007728517055511475\n",
      "step: 32955\n",
      "train: loss: 26875.517578125 acc: 0.9933300614356995  val: loss: 1702313.625 acc: 0.3625933527946472\n",
      "step: 32960\n",
      "train: loss: 383685.84375 acc: 0.9679965376853943  val: loss: 283894.4375 acc: 0.8541864156723022\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 32965\n",
      "train: loss: 503724.71875 acc: 0.9645379781723022  val: loss: 2430008.25 acc: 0.5217627286911011\n",
      "step: 32970\n",
      "train: loss: 302125.65625 acc: 0.9713237285614014  val: loss: 731710.9375 acc: 0.7366687059402466\n",
      "step: 32975\n",
      "train: loss: 2131975.25 acc: 0.6980279684066772  val: loss: 846349.125 acc: 0.8467085957527161\n",
      "step: 32980\n",
      "train: loss: 410896.40625 acc: 0.9858181476593018  val: loss: 1206374.0 acc: 0.6396324634552002\n",
      "step: 32985\n",
      "train: loss: 992868.875 acc: 0.9636116623878479  val: loss: 765648.6875 acc: 0.8813924193382263\n",
      "step: 32990\n",
      "train: loss: 994006.9375 acc: 0.9662792682647705  val: loss: 158751.34375 acc: 0.9309896230697632\n",
      "step: 32995\n",
      "train: loss: 851420.8125 acc: 0.9610259532928467  val: loss: 1076315.875 acc: 0.40205085277557373\n",
      "step: 33000\n",
      "train: loss: 429407.96875 acc: 0.9576190114021301  val: loss: 806479.0625 acc: 0.8865888714790344\n",
      "step: 33005\n",
      "train: loss: 499368.5 acc: 0.9656961560249329  val: loss: 264812.53125 acc: 0.9444996118545532\n",
      "step: 33010\n",
      "train: loss: 819388.1875 acc: 0.9027119278907776  val: loss: 246154.546875 acc: 0.9672893285751343\n",
      "step: 33015\n",
      "train: loss: 3835459.25 acc: 0.25247758626937866  val: loss: 377430.15625 acc: 0.8330574035644531\n",
      "step: 33020\n",
      "train: loss: 984110.8125 acc: 0.8368462920188904  val: loss: 2008531.5 acc: 0.7125037312507629\n",
      "step: 33025\n",
      "train: loss: 787029.5625 acc: 0.7703005075454712  val: loss: 1314856.0 acc: 0.8339611291885376\n",
      "step: 33030\n",
      "train: loss: 858491.5625 acc: 0.6591391563415527  val: loss: 1962228.25 acc: 0.7874506711959839\n",
      "step: 33035\n",
      "train: loss: 479057.375 acc: 0.7781352400779724  val: loss: 1527077.25 acc: 0.7763752341270447\n",
      "step: 33040\n",
      "train: loss: 606832.75 acc: 0.7337667942047119  val: loss: 2465742.0 acc: 0.5401365756988525\n",
      "step: 33045\n",
      "train: loss: 210281.65625 acc: 0.8791652917861938  val: loss: 4276799.0 acc: 0.44591981172561646\n",
      "step: 33050\n",
      "train: loss: 531531.125 acc: 0.7632632851600647  val: loss: 619109.0625 acc: 0.719724178314209\n",
      "step: 33055\n",
      "train: loss: 129912.453125 acc: 0.8849626183509827  val: loss: 2001925.375 acc: 0.6527621746063232\n",
      "step: 33060\n",
      "train: loss: 178458.921875 acc: 0.8768777847290039  val: loss: 526286.25 acc: 0.6627302169799805\n",
      "step: 33065\n",
      "train: loss: 21827.5 acc: 0.9828000068664551  val: loss: 6896135.0 acc: 0.3805472254753113\n",
      "step: 33070\n",
      "train: loss: 26631.96484375 acc: 0.979221761226654  val: loss: 588562.25 acc: 0.7194409370422363\n",
      "step: 33075\n",
      "train: loss: 105084.859375 acc: 0.9234170317649841  val: loss: 6280324.5 acc: 0.4326229691505432\n",
      "step: 33080\n",
      "train: loss: 524542.3125 acc: 0.782840371131897  val: loss: 3526473.25 acc: 0.5612273216247559\n",
      "step: 33085\n",
      "train: loss: 321473.46875 acc: 0.7820000648498535  val: loss: 3824826.0 acc: 0.5833802819252014\n",
      "step: 33090\n",
      "train: loss: 48590.10546875 acc: 0.9566570520401001  val: loss: 2176808.0 acc: 0.622263491153717\n",
      "step: 33095\n",
      "train: loss: 308828.1875 acc: 0.8203467726707458  val: loss: 674820.5625 acc: 0.7644880414009094\n",
      "step: 33100\n",
      "train: loss: 181521.4375 acc: 0.8550641536712646  val: loss: 2537777.0 acc: 0.5478177070617676\n",
      "step: 33105\n",
      "train: loss: 398285.3125 acc: 0.8005721569061279  val: loss: 4696544.0 acc: 0.5320578217506409\n",
      "step: 33110\n",
      "train: loss: 723334.75 acc: 0.7290369868278503  val: loss: 1200583.875 acc: 0.7018232345581055\n",
      "step: 33115\n",
      "train: loss: 193384.8125 acc: 0.8436023592948914  val: loss: 2690812.5 acc: 0.5133005380630493\n",
      "step: 33120\n",
      "train: loss: 398050.21875 acc: 0.7790538668632507  val: loss: 4057992.75 acc: 0.4811757206916809\n",
      "step: 33125\n",
      "train: loss: 1047440.0625 acc: 0.7202951908111572  val: loss: 1302607.125 acc: 0.7368860840797424\n",
      "step: 33130\n",
      "train: loss: 673821.4375 acc: 0.900833010673523  val: loss: 996649.3125 acc: 0.6094735860824585\n",
      "step: 33135\n",
      "train: loss: 360667.84375 acc: 0.9493560791015625  val: loss: 1608348.75 acc: 0.7354270219802856\n",
      "step: 33140\n",
      "train: loss: 468425.78125 acc: 0.9667999148368835  val: loss: 2562252.25 acc: 0.16315478086471558\n",
      "step: 33145\n",
      "train: loss: 490251.1875 acc: 0.9311777353286743  val: loss: 1406457.5 acc: 0.7462747693061829\n",
      "step: 33150\n",
      "train: loss: 77144.078125 acc: 0.9886012077331543  val: loss: 1109330.875 acc: 0.8752409219741821\n",
      "step: 33155\n",
      "train: loss: 84370.2578125 acc: 0.9892225861549377  val: loss: 2083332.375 acc: 0.36067628860473633\n",
      "step: 33160\n",
      "train: loss: 61384.99609375 acc: 0.9937167167663574  val: loss: 1763714.375 acc: 0.5712629556655884\n",
      "step: 33165\n",
      "train: loss: 100086.21875 acc: 0.9919630289077759  val: loss: 2017794.125 acc: 0.3704366683959961\n",
      "step: 33170\n",
      "train: loss: 37068.25 acc: 0.9966325759887695  val: loss: 882468.5 acc: 0.597459077835083\n",
      "step: 33175\n",
      "train: loss: 24496.88671875 acc: 0.995725154876709  val: loss: 1121259.75 acc: 0.7923108339309692\n",
      "step: 33180\n",
      "train: loss: 46225.48046875 acc: 0.9888063073158264  val: loss: 1946131.375 acc: 0.2958104610443115\n",
      "step: 33185\n",
      "train: loss: 58100.515625 acc: 0.9698978662490845  val: loss: 1609611.125 acc: 0.23969650268554688\n",
      "step: 33190\n",
      "train: loss: 14254.12109375 acc: 0.9940904974937439  val: loss: 1101357.75 acc: 0.8619847297668457\n",
      "step: 33195\n",
      "train: loss: 28848.982421875 acc: 0.9824222326278687  val: loss: 253768.453125 acc: 0.9410338401794434\n",
      "step: 33200\n",
      "train: loss: 7947.08203125 acc: 0.9895036220550537  val: loss: 1654265.625 acc: 0.3707010746002197\n",
      "step: 33205\n",
      "train: loss: 227058.640625 acc: 0.49173682928085327  val: loss: 364011.15625 acc: 0.9614486694335938\n",
      "step: 33210\n",
      "train: loss: 17387.041015625 acc: 0.977170467376709  val: loss: 1949124.375 acc: 0.6203855276107788\n",
      "step: 33215\n",
      "train: loss: 9790.205078125 acc: 0.9835825562477112  val: loss: 1204527.125 acc: 0.7459429502487183\n",
      "step: 33220\n",
      "train: loss: 14269.4453125 acc: 0.9652066826820374  val: loss: 1430896.375 acc: 0.8045588135719299\n",
      "step: 33225\n",
      "train: loss: 12653.251953125 acc: 0.9664587378501892  val: loss: 892814.625 acc: 0.5414311289787292\n",
      "step: 33230\n",
      "train: loss: 26832.65234375 acc: 0.9861962795257568  val: loss: 1502088.5 acc: 0.7173246145248413\n",
      "step: 33235\n",
      "train: loss: 27596.173828125 acc: 0.9812914133071899  val: loss: 844451.5625 acc: 0.8749208450317383\n",
      "step: 33240\n",
      "train: loss: 24517.095703125 acc: 0.9809339046478271  val: loss: 855231.375 acc: 0.7507886290550232\n",
      "step: 33245\n",
      "train: loss: 19958.994140625 acc: 0.9937664866447449  val: loss: 1215340.125 acc: 0.576903223991394\n",
      "step: 33250\n",
      "train: loss: 14779.677734375 acc: 0.9867638349533081  val: loss: 915898.4375 acc: 0.605554461479187\n",
      "step: 33255\n",
      "train: loss: 12780.2783203125 acc: 0.9605022668838501  val: loss: 2484529.75 acc: -1.1671466827392578\n",
      "step: 33260\n",
      "train: loss: 7740.37060546875 acc: 0.9966437220573425  val: loss: 330654.40625 acc: 0.9410338997840881\n",
      "step: 33265\n",
      "train: loss: 51704.6875 acc: 0.9806243777275085  val: loss: 210209.5625 acc: 0.9750308990478516\n",
      "step: 33270\n",
      "train: loss: 34808.8984375 acc: 0.9833898544311523  val: loss: 1308575.625 acc: 0.822790265083313\n",
      "step: 33275\n",
      "train: loss: 30137.341796875 acc: 0.9846616983413696  val: loss: 903797.25 acc: 0.6302770972251892\n",
      "step: 33280\n",
      "train: loss: 33802.3203125 acc: 0.9804844260215759  val: loss: 527104.875 acc: 0.9334238171577454\n",
      "step: 33285\n",
      "train: loss: 42120.58984375 acc: 0.9870357513427734  val: loss: 549808.0 acc: 0.8804099559783936\n",
      "step: 33290\n",
      "train: loss: 176468.3125 acc: 0.889941394329071  val: loss: 716652.875 acc: 0.7283912897109985\n",
      "step: 33295\n",
      "train: loss: 91307.234375 acc: 0.9564300775527954  val: loss: 118096.2578125 acc: 0.977081835269928\n",
      "step: 33300\n",
      "train: loss: 34162.70703125 acc: 0.9928096532821655  val: loss: 583458.625 acc: 0.3645515441894531\n",
      "step: 33305\n",
      "train: loss: 640353.1875 acc: 0.9013773202896118  val: loss: 582258.1875 acc: 0.9266492128372192\n",
      "step: 33310\n",
      "train: loss: 669921.25 acc: 0.9306738972663879  val: loss: 445116.40625 acc: 0.9282562732696533\n",
      "step: 33315\n",
      "train: loss: 119904.046875 acc: 0.9848625063896179  val: loss: 300023.5 acc: 0.9037579894065857\n",
      "step: 33320\n",
      "train: loss: 262919.09375 acc: 0.9679214358329773  val: loss: 202386.25 acc: 0.9640500545501709\n",
      "step: 33325\n",
      "train: loss: 235272.09375 acc: 0.9567758440971375  val: loss: 1391710.875 acc: 0.5621535778045654\n",
      "step: 33330\n",
      "train: loss: 583274.4375 acc: 0.9496489763259888  val: loss: 1072961.75 acc: 0.5863216519355774\n",
      "step: 33335\n",
      "train: loss: 264602.375 acc: 0.9495002627372742  val: loss: 263759.96875 acc: 0.8369566202163696\n",
      "step: 33340\n",
      "train: loss: 315120.4375 acc: 0.9572499394416809  val: loss: 516827.34375 acc: 0.9466621279716492\n",
      "step: 33345\n",
      "train: loss: 1229962.125 acc: 0.9524815678596497  val: loss: 988056.375 acc: 0.9057750105857849\n",
      "step: 33350\n",
      "train: loss: 731419.125 acc: 0.9789960384368896  val: loss: 1123048.625 acc: 0.8141082525253296\n",
      "step: 33355\n",
      "train: loss: 850923.0 acc: 0.9643208980560303  val: loss: 372512.1875 acc: 0.8275762796401978\n",
      "step: 33360\n",
      "train: loss: 1840248.875 acc: 0.9108140468597412  val: loss: 420839.1875 acc: 0.8392378091812134\n",
      "step: 33365\n",
      "train: loss: 708820.75 acc: 0.9538971781730652  val: loss: 354741.15625 acc: 0.9475780725479126\n",
      "step: 33370\n",
      "train: loss: 1613722.0 acc: 0.8377401828765869  val: loss: 1246601.375 acc: 0.8981322646141052\n",
      "step: 33375\n",
      "train: loss: 158614.515625 acc: 0.9822033047676086  val: loss: 1903703.375 acc: 0.7441850900650024\n",
      "step: 33380\n",
      "train: loss: 1253146.75 acc: 0.8065395355224609  val: loss: 566438.1875 acc: 0.9170070290565491\n",
      "step: 33385\n",
      "train: loss: 578032.875 acc: 0.8528744578361511  val: loss: 2564917.5 acc: 0.7627174854278564\n",
      "step: 33390\n",
      "train: loss: 474520.96875 acc: 0.792871356010437  val: loss: 3798138.75 acc: 0.666038990020752\n",
      "step: 33395\n",
      "train: loss: 693144.125 acc: 0.3652324676513672  val: loss: 890344.125 acc: 0.7598220705986023\n",
      "step: 33400\n",
      "train: loss: 436713.5 acc: 0.8023855686187744  val: loss: 752749.875 acc: 0.8205565214157104\n",
      "step: 33405\n",
      "train: loss: 1030112.8125 acc: 0.5551810264587402  val: loss: 1584344.625 acc: 0.7324777841567993\n",
      "step: 33410\n",
      "train: loss: 178163.546875 acc: 0.9024776220321655  val: loss: 299398.59375 acc: 0.8216164112091064\n",
      "step: 33415\n",
      "train: loss: 65641.7890625 acc: 0.9466478824615479  val: loss: 1675672.125 acc: 0.6168736815452576\n",
      "step: 33420\n",
      "train: loss: 371157.78125 acc: 0.8079388737678528  val: loss: 2695618.75 acc: 0.4996340274810791\n",
      "step: 33425\n",
      "train: loss: 68116.53125 acc: 0.9410197138786316  val: loss: 1386983.625 acc: 0.6688038110733032\n",
      "step: 33430\n",
      "train: loss: 113565.3359375 acc: 0.9169622659683228  val: loss: 1119446.125 acc: 0.6736219525337219\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 33435\n",
      "train: loss: 82880.4375 acc: 0.9418536424636841  val: loss: 773103.375 acc: 0.730881929397583\n",
      "step: 33440\n",
      "train: loss: 210951.5625 acc: 0.8750613927841187  val: loss: 1162451.125 acc: 0.6493067145347595\n",
      "step: 33445\n",
      "train: loss: 349487.3125 acc: 0.8241651654243469  val: loss: 4817437.0 acc: 0.4724119305610657\n",
      "step: 33450\n",
      "train: loss: 227089.734375 acc: 0.8465774059295654  val: loss: 352973.9375 acc: 0.8104884028434753\n",
      "step: 33455\n",
      "train: loss: 202412.734375 acc: 0.8609718084335327  val: loss: 464180.25 acc: 0.7634710073471069\n",
      "step: 33460\n",
      "train: loss: 53648.609375 acc: 0.9533922076225281  val: loss: 2360168.25 acc: 0.5233272314071655\n",
      "step: 33465\n",
      "train: loss: 578484.0 acc: 0.6672598123550415  val: loss: 2248703.0 acc: 0.5238698124885559\n",
      "step: 33470\n",
      "train: loss: 67811.96875 acc: 0.9476688504219055  val: loss: 2387212.25 acc: 0.6502654552459717\n",
      "step: 33475\n",
      "train: loss: 222752.46875 acc: 0.8215548992156982  val: loss: 3479029.5 acc: 0.4350894093513489\n",
      "step: 33480\n",
      "train: loss: 564837.625 acc: 0.7541834115982056  val: loss: 3822878.75 acc: 0.521504282951355\n",
      "step: 33485\n",
      "train: loss: 637785.875 acc: 0.7415966391563416  val: loss: 3078877.5 acc: 0.68050217628479\n",
      "step: 33490\n",
      "train: loss: 378361.0625 acc: 0.8424073457717896  val: loss: 201048.59375 acc: 0.8699132204055786\n",
      "step: 33495\n",
      "train: loss: 667062.25 acc: 0.8724836111068726  val: loss: 958946.5625 acc: 0.8297081589698792\n",
      "step: 33500\n",
      "train: loss: 656662.75 acc: 0.9156186580657959  val: loss: 1036607.1875 acc: 0.8509879112243652\n",
      "step: 33505\n",
      "train: loss: 127056.40625 acc: 0.9902678728103638  val: loss: 418829.90625 acc: 0.9624433517456055\n",
      "step: 33510\n",
      "train: loss: 500183.59375 acc: 0.8701586723327637  val: loss: 1600083.125 acc: 0.8215347528457642\n",
      "step: 33515\n",
      "train: loss: 263722.46875 acc: 0.9712135791778564  val: loss: 1924057.625 acc: 0.6750442981719971\n",
      "step: 33520\n",
      "train: loss: 98127.203125 acc: 0.9870563745498657  val: loss: 1135056.625 acc: 0.8629554510116577\n",
      "step: 33525\n",
      "train: loss: 48424.09765625 acc: 0.9960042834281921  val: loss: 345301.6875 acc: 0.9140983819961548\n",
      "step: 33530\n",
      "train: loss: 54699.83203125 acc: 0.9961251020431519  val: loss: 1421431.625 acc: -0.3574788570404053\n",
      "step: 33535\n",
      "train: loss: 62291.2109375 acc: 0.9948309063911438  val: loss: 906292.1875 acc: 0.8399654626846313\n",
      "step: 33540\n",
      "train: loss: 54989.2421875 acc: 0.9919630289077759  val: loss: 941201.1875 acc: 0.8164792060852051\n",
      "step: 33545\n",
      "train: loss: 29730.26171875 acc: 0.9894763827323914  val: loss: 592920.75 acc: 0.46725231409072876\n",
      "step: 33550\n",
      "train: loss: 14691.474609375 acc: 0.9835405349731445  val: loss: 56286.40625 acc: 0.9776610136032104\n",
      "step: 33555\n",
      "train: loss: 13029.8291015625 acc: 0.9605417847633362  val: loss: 369114.40625 acc: 0.7880560755729675\n",
      "step: 33560\n",
      "train: loss: 22747.515625 acc: 0.9590958952903748  val: loss: 1668433.125 acc: 0.4717743396759033\n",
      "step: 33565\n",
      "train: loss: 7306.7236328125 acc: 0.9887148141860962  val: loss: 592276.4375 acc: 0.9226634502410889\n",
      "step: 33570\n",
      "train: loss: 14094.44140625 acc: 0.9711266756057739  val: loss: 507481.75 acc: 0.9489721059799194\n",
      "step: 33575\n",
      "train: loss: 12265.85546875 acc: 0.9661351442337036  val: loss: 344949.0625 acc: 0.8543587923049927\n",
      "step: 33580\n",
      "train: loss: 13657.8525390625 acc: 0.9771721363067627  val: loss: 841925.9375 acc: 0.8754577040672302\n",
      "step: 33585\n",
      "train: loss: 11892.9599609375 acc: 0.9767957329750061  val: loss: 213717.09375 acc: 0.9679592847824097\n",
      "step: 33590\n",
      "train: loss: 21383.140625 acc: 0.9690573215484619  val: loss: 433632.5625 acc: 0.9513523578643799\n",
      "step: 33595\n",
      "train: loss: 37405.73046875 acc: 0.9808294177055359  val: loss: 228414.6875 acc: 0.8781238794326782\n",
      "step: 33600\n",
      "train: loss: 27128.2265625 acc: 0.9828975200653076  val: loss: 889654.3125 acc: 0.6085109114646912\n",
      "step: 33605\n",
      "train: loss: 39697.23046875 acc: 0.9715192914009094  val: loss: 1246015.875 acc: 0.4157294034957886\n",
      "step: 33610\n",
      "train: loss: 17148.33984375 acc: 0.9923392534255981  val: loss: 275037.6875 acc: 0.6191360950469971\n",
      "step: 33615\n",
      "train: loss: 26839.15234375 acc: 0.980814516544342  val: loss: 575426.125 acc: 0.868574857711792\n",
      "step: 33620\n",
      "train: loss: 12656.748046875 acc: 0.9713504314422607  val: loss: 1033327.125 acc: 0.7232553958892822\n",
      "step: 33625\n",
      "train: loss: 16275.958984375 acc: 0.9906755685806274  val: loss: 1702066.375 acc: 0.30125850439071655\n",
      "step: 33630\n",
      "train: loss: 20492.22265625 acc: 0.9911446571350098  val: loss: 1234179.25 acc: 0.6507449150085449\n",
      "step: 33635\n",
      "train: loss: 36059.25 acc: 0.9891490936279297  val: loss: 924008.0 acc: 0.7387337684631348\n",
      "step: 33640\n",
      "train: loss: 38942.3203125 acc: 0.9858359694480896  val: loss: 226757.640625 acc: 0.9100790619850159\n",
      "step: 33645\n",
      "train: loss: 15139.1494140625 acc: 0.9947940707206726  val: loss: 2197990.75 acc: -0.0759270191192627\n",
      "step: 33650\n",
      "train: loss: 60354.8203125 acc: 0.9833421111106873  val: loss: 497278.15625 acc: 0.8572808504104614\n",
      "step: 33655\n",
      "train: loss: 291022.9375 acc: 0.8991190195083618  val: loss: 400669.84375 acc: 0.967735230922699\n",
      "step: 33660\n",
      "train: loss: 91274.109375 acc: 0.9470056295394897  val: loss: 204943.265625 acc: 0.9455487728118896\n",
      "step: 33665\n",
      "train: loss: 491672.3125 acc: 0.9089919328689575  val: loss: 264672.875 acc: 0.9783049821853638\n",
      "step: 33670\n",
      "train: loss: 86502.796875 acc: 0.9878579378128052  val: loss: 337351.875 acc: 0.9605312347412109\n",
      "step: 33675\n",
      "train: loss: 40545.51171875 acc: 0.9963146448135376  val: loss: 482446.71875 acc: 0.947022557258606\n",
      "step: 33680\n",
      "train: loss: 26501.666015625 acc: 0.9962469339370728  val: loss: 1343548.625 acc: 0.7227504849433899\n",
      "step: 33685\n",
      "train: loss: 97210.1875 acc: 0.9857006669044495  val: loss: 801305.375 acc: 0.8754068613052368\n",
      "step: 33690\n",
      "train: loss: 387078.53125 acc: 0.9553457498550415  val: loss: 253054.84375 acc: 0.9234793782234192\n",
      "step: 33695\n",
      "train: loss: 536088.0 acc: 0.9754664301872253  val: loss: 2543312.5 acc: 0.7354730367660522\n",
      "step: 33700\n",
      "train: loss: 477367.1875 acc: 0.9718770980834961  val: loss: 744676.8125 acc: 0.6999210715293884\n",
      "step: 33705\n",
      "train: loss: 374302.25 acc: 0.9638861417770386  val: loss: 711774.0625 acc: 0.9116268157958984\n",
      "step: 33710\n",
      "train: loss: 960901.75 acc: 0.9555796384811401  val: loss: 1497350.25 acc: 0.7827112078666687\n",
      "step: 33715\n",
      "train: loss: 2621611.75 acc: 0.9376780986785889  val: loss: 685430.5 acc: 0.9039211273193359\n",
      "step: 33720\n",
      "train: loss: 803242.5625 acc: 0.9739525318145752  val: loss: 718739.875 acc: 0.9257500171661377\n",
      "step: 33725\n",
      "train: loss: 1300650.75 acc: 0.9351488351821899  val: loss: 1175457.5 acc: 0.4751057028770447\n",
      "step: 33730\n",
      "train: loss: 363064.15625 acc: 0.951271116733551  val: loss: 892674.25 acc: 0.8399989604949951\n",
      "step: 33735\n",
      "train: loss: 404053.125 acc: 0.9591936469078064  val: loss: 1477495.375 acc: 0.7684580683708191\n",
      "step: 33740\n",
      "train: loss: 223290.84375 acc: 0.9565536975860596  val: loss: 1747550.125 acc: 0.14713728427886963\n",
      "step: 33745\n",
      "train: loss: 588699.125 acc: 0.9363977313041687  val: loss: 1319665.875 acc: 0.34995996952056885\n",
      "step: 33750\n",
      "train: loss: 671785.25 acc: 0.8314158916473389  val: loss: 1235998.25 acc: 0.7932213544845581\n",
      "step: 33755\n",
      "train: loss: 2239458.25 acc: 0.6613091230392456  val: loss: 542882.4375 acc: 0.8618776798248291\n",
      "step: 33760\n",
      "train: loss: 254379.171875 acc: 0.8280635476112366  val: loss: 300667.0 acc: 0.8647159934043884\n",
      "step: 33765\n",
      "train: loss: 1169007.0 acc: 0.7900392413139343  val: loss: 851266.1875 acc: 0.7868154048919678\n",
      "step: 33770\n",
      "train: loss: 382094.96875 acc: 0.8224422931671143  val: loss: 2136724.0 acc: 0.6502155661582947\n",
      "step: 33775\n",
      "train: loss: 222354.765625 acc: 0.8280876874923706  val: loss: 149223.375 acc: 0.8365403413772583\n",
      "step: 33780\n",
      "train: loss: 116089.40625 acc: 0.9161291718482971  val: loss: 2046250.375 acc: 0.5826375484466553\n",
      "step: 33785\n",
      "train: loss: 99515.4140625 acc: 0.9160097241401672  val: loss: 4995733.0 acc: 0.3866230249404907\n",
      "step: 33790\n",
      "train: loss: 41934.2109375 acc: 0.9650039076805115  val: loss: 1488803.125 acc: 0.6501684188842773\n",
      "step: 33795\n",
      "train: loss: 37817.7109375 acc: 0.9664005041122437  val: loss: 94058.7421875 acc: 0.9027389883995056\n",
      "step: 33800\n",
      "train: loss: 27736.76171875 acc: 0.9778225421905518  val: loss: 764006.6875 acc: 0.6851544380187988\n",
      "step: 33805\n",
      "train: loss: 44554.4375 acc: 0.9615241289138794  val: loss: 1390170.125 acc: 0.545548677444458\n",
      "step: 33810\n",
      "train: loss: 168421.8125 acc: 0.9013903141021729  val: loss: 3010651.25 acc: 0.5690909624099731\n",
      "step: 33815\n",
      "train: loss: 81933.1796875 acc: 0.9340359568595886  val: loss: 1601684.625 acc: 0.6241579055786133\n",
      "step: 33820\n",
      "train: loss: 139492.671875 acc: 0.8534416556358337  val: loss: 3890735.0 acc: 0.5301185846328735\n",
      "step: 33825\n",
      "train: loss: 145830.21875 acc: 0.9002915620803833  val: loss: 1074262.375 acc: 0.6608713865280151\n",
      "step: 33830\n",
      "train: loss: 353751.625 acc: 0.8201464414596558  val: loss: 1355606.375 acc: 0.705464243888855\n",
      "step: 33835\n",
      "train: loss: 323997.28125 acc: 0.6581616997718811  val: loss: 652112.375 acc: 0.7908104658126831\n",
      "step: 33840\n",
      "train: loss: 66556.9296875 acc: 0.9458006620407104  val: loss: 1762495.875 acc: 0.6049894690513611\n",
      "step: 33845\n",
      "train: loss: 237759.515625 acc: 0.8460928797721863  val: loss: 1410652.0 acc: 0.678710401058197\n",
      "step: 33850\n",
      "train: loss: 288400.21875 acc: 0.8357343673706055  val: loss: 4302037.5 acc: 0.49501121044158936\n",
      "step: 33855\n",
      "train: loss: 320606.78125 acc: 0.8660035133361816  val: loss: 537571.25 acc: 0.7792845964431763\n",
      "step: 33860\n",
      "train: loss: 1428100.375 acc: 0.8368858098983765  val: loss: 1027158.3125 acc: 0.8221390247344971\n",
      "step: 33865\n",
      "train: loss: 1323002.25 acc: 0.865634024143219  val: loss: 831324.5 acc: 0.738210141658783\n",
      "step: 33870\n",
      "train: loss: 547957.9375 acc: 0.9571118950843811  val: loss: 1487150.375 acc: 0.26389092206954956\n",
      "step: 33875\n",
      "train: loss: 117950.5390625 acc: 0.9851837754249573  val: loss: 1114794.5 acc: 0.45610588788986206\n",
      "step: 33880\n",
      "train: loss: 380307.53125 acc: 0.9256606101989746  val: loss: 260632.46875 acc: 0.9479292631149292\n",
      "step: 33885\n",
      "train: loss: 368613.78125 acc: 0.9434460997581482  val: loss: 484641.90625 acc: 0.9385597109794617\n",
      "step: 33890\n",
      "train: loss: 140313.53125 acc: 0.9910519123077393  val: loss: 525642.0 acc: 0.8765932321548462\n",
      "step: 33895\n",
      "train: loss: 82314.75 acc: 0.9930552840232849  val: loss: 313648.84375 acc: 0.9268806576728821\n",
      "step: 33900\n",
      "train: loss: 52604.30078125 acc: 0.9951839447021484  val: loss: 535942.5625 acc: 0.824296236038208\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 33905\n",
      "train: loss: 26405.5703125 acc: 0.9948068261146545  val: loss: 471719.84375 acc: 0.8393923044204712\n",
      "step: 33910\n",
      "train: loss: 40481.16015625 acc: 0.9950425624847412  val: loss: 67466.09375 acc: 0.9815040826797485\n",
      "step: 33915\n",
      "train: loss: 26862.13671875 acc: 0.9886355996131897  val: loss: 1191715.625 acc: 0.6661022901535034\n",
      "step: 33920\n",
      "train: loss: 8627.3115234375 acc: 0.9956890344619751  val: loss: 1076712.375 acc: 0.7606005668640137\n",
      "step: 33925\n",
      "train: loss: 26646.265625 acc: 0.9672622084617615  val: loss: 806501.125 acc: 0.8302996158599854\n",
      "step: 33930\n",
      "train: loss: 30968.873046875 acc: 0.9868251085281372  val: loss: 1445852.625 acc: 0.6397932767868042\n",
      "step: 33935\n",
      "train: loss: 8925.94140625 acc: 0.975841760635376  val: loss: 1520883.875 acc: 0.7393186092376709\n",
      "step: 33940\n",
      "train: loss: 21197.595703125 acc: 0.9482139348983765  val: loss: 82205.09375 acc: 0.9766994118690491\n",
      "step: 33945\n",
      "train: loss: 18012.45703125 acc: 0.9879392385482788  val: loss: 154324.625 acc: 0.9458746910095215\n",
      "step: 33950\n",
      "train: loss: 16175.22265625 acc: 0.979079008102417  val: loss: 485079.53125 acc: 0.9085398316383362\n",
      "step: 33955\n",
      "train: loss: 24721.8671875 acc: 0.9868097901344299  val: loss: 695155.75 acc: 0.7800866365432739\n",
      "step: 33960\n",
      "train: loss: 30721.904296875 acc: 0.9824504852294922  val: loss: 1174438.625 acc: 0.7981086373329163\n",
      "step: 33965\n",
      "train: loss: 17358.47265625 acc: 0.9778968095779419  val: loss: 271154.21875 acc: 0.9459189772605896\n",
      "step: 33970\n",
      "train: loss: 21467.724609375 acc: 0.9870008826255798  val: loss: 1165498.625 acc: 0.6489500999450684\n",
      "step: 33975\n",
      "train: loss: 18723.275390625 acc: 0.991313099861145  val: loss: 285779.78125 acc: 0.9778568148612976\n",
      "step: 33980\n",
      "train: loss: 12193.5908203125 acc: 0.9920743107795715  val: loss: 857382.375 acc: 0.9440556168556213\n",
      "step: 33985\n",
      "train: loss: 14786.728515625 acc: 0.9880568981170654  val: loss: 1392028.75 acc: 0.7745826244354248\n",
      "step: 33990\n",
      "train: loss: 19106.59765625 acc: 0.9851694703102112  val: loss: 1436548.125 acc: 0.5208851099014282\n",
      "step: 33995\n",
      "train: loss: 30716.07421875 acc: 0.9875330328941345  val: loss: 1862631.625 acc: 0.7351422309875488\n",
      "step: 34000\n",
      "train: loss: 27918.337890625 acc: 0.9929981231689453  val: loss: 416313.59375 acc: 0.9433508515357971\n",
      "step: 34005\n",
      "train: loss: 21969.75390625 acc: 0.9915135502815247  val: loss: 646381.3125 acc: 0.9366928935050964\n",
      "step: 34010\n",
      "train: loss: 44096.81640625 acc: 0.9747310876846313  val: loss: 1315681.875 acc: 0.81251060962677\n",
      "step: 34015\n",
      "train: loss: 60353.71875 acc: 0.9851788282394409  val: loss: 760968.25 acc: 0.9251842498779297\n",
      "step: 34020\n",
      "train: loss: 329053.78125 acc: 0.936350405216217  val: loss: 2719505.25 acc: -1.0265851020812988\n",
      "step: 34025\n",
      "train: loss: 75083.203125 acc: 0.9792891144752502  val: loss: 1599321.625 acc: 0.8359435200691223\n",
      "step: 34030\n",
      "train: loss: 261155.15625 acc: 0.8691917657852173  val: loss: 904941.9375 acc: 0.5841679573059082\n",
      "step: 34035\n",
      "train: loss: 730501.5 acc: 0.8723940253257751  val: loss: 4159011.75 acc: 0.4615824818611145\n",
      "step: 34040\n",
      "train: loss: 68297.5 acc: 0.9933657050132751  val: loss: 3558576.75 acc: 0.31109416484832764\n",
      "step: 34045\n",
      "train: loss: 47446.1796875 acc: 0.9931573867797852  val: loss: 226252.90625 acc: 0.9125109910964966\n",
      "step: 34050\n",
      "train: loss: 87389.5 acc: 0.984230101108551  val: loss: 1324319.875 acc: 0.9098271727561951\n",
      "step: 34055\n",
      "train: loss: 263402.28125 acc: 0.9794913530349731  val: loss: 1578844.0 acc: 0.763066828250885\n",
      "step: 34060\n",
      "train: loss: 815770.5 acc: 0.9240506887435913  val: loss: 1139180.875 acc: 0.81708824634552\n",
      "step: 34065\n",
      "train: loss: 244591.671875 acc: 0.9717501997947693  val: loss: 389431.53125 acc: 0.8137950301170349\n",
      "step: 34070\n",
      "train: loss: 200764.734375 acc: 0.969750702381134  val: loss: 483408.53125 acc: 0.8039562702178955\n",
      "step: 34075\n",
      "train: loss: 674563.3125 acc: 0.9703218936920166  val: loss: 893215.375 acc: 0.8957659602165222\n",
      "step: 34080\n",
      "train: loss: 651571.5625 acc: 0.9719064235687256  val: loss: 4001269.75 acc: 0.5083546042442322\n",
      "step: 34085\n",
      "train: loss: 1533746.125 acc: 0.9443410634994507  val: loss: 684530.9375 acc: 0.9371363520622253\n",
      "step: 34090\n",
      "train: loss: 1220012.75 acc: 0.9421489238739014  val: loss: 2355506.0 acc: 0.15273147821426392\n",
      "step: 34095\n",
      "train: loss: 414103.46875 acc: 0.967982828617096  val: loss: 441738.9375 acc: 0.545954167842865\n",
      "step: 34100\n",
      "train: loss: 134571.515625 acc: 0.9806768894195557  val: loss: 645399.3125 acc: 0.8650941848754883\n",
      "step: 34105\n",
      "train: loss: 380661.78125 acc: 0.9463895559310913  val: loss: 301614.375 acc: 0.9266684055328369\n",
      "step: 34110\n",
      "train: loss: 422671.78125 acc: 0.9466615319252014  val: loss: 1208696.625 acc: 0.6425785422325134\n",
      "step: 34115\n",
      "train: loss: 1455034.625 acc: 0.37513071298599243  val: loss: 2137146.75 acc: 0.7764700651168823\n",
      "step: 34120\n",
      "train: loss: 562916.4375 acc: 0.7102961540222168  val: loss: 632385.4375 acc: 0.7655255794525146\n",
      "step: 34125\n",
      "train: loss: 2855427.75 acc: 0.678078293800354  val: loss: 704607.3125 acc: 0.8353161811828613\n",
      "step: 34130\n",
      "train: loss: 743729.875 acc: 0.834599494934082  val: loss: 504289.5 acc: 0.8602978587150574\n",
      "step: 34135\n",
      "train: loss: 617621.8125 acc: 0.8088135719299316  val: loss: 712001.8125 acc: 0.7575331330299377\n",
      "step: 34140\n",
      "train: loss: 188431.796875 acc: 0.8809998035430908  val: loss: 4155031.75 acc: 0.5034008026123047\n",
      "step: 34145\n",
      "train: loss: 643408.1875 acc: 0.7583748698234558  val: loss: 1645616.125 acc: 0.6219493746757507\n",
      "step: 34150\n",
      "train: loss: 195856.859375 acc: 0.8803130984306335  val: loss: 227791.34375 acc: 0.8454421758651733\n",
      "step: 34155\n",
      "train: loss: 59795.7109375 acc: 0.9525270462036133  val: loss: 2891351.5 acc: 0.4783625602722168\n",
      "step: 34160\n",
      "train: loss: 40941.23828125 acc: 0.9649378061294556  val: loss: 3229676.25 acc: 0.5686978101730347\n",
      "step: 34165\n",
      "train: loss: 64609.7890625 acc: 0.9508897066116333  val: loss: 853040.9375 acc: 0.677919864654541\n",
      "step: 34170\n",
      "train: loss: 134674.859375 acc: 0.9115779399871826  val: loss: 1672309.625 acc: 0.6013785004615784\n",
      "step: 34175\n",
      "train: loss: 93538.1171875 acc: 0.935727596282959  val: loss: 5312896.5 acc: 0.4960551857948303\n",
      "step: 34180\n",
      "train: loss: 511119.0625 acc: 0.7775257229804993  val: loss: 1316886.875 acc: 0.6213477849960327\n",
      "step: 34185\n",
      "train: loss: 33755.22265625 acc: 0.9621406197547913  val: loss: 4358691.0 acc: 0.34134596586227417\n",
      "step: 34190\n",
      "train: loss: 72878.3203125 acc: 0.9362316131591797  val: loss: 1026439.3125 acc: 0.6553152799606323\n",
      "step: 34195\n",
      "train: loss: 301900.9375 acc: 0.83290696144104  val: loss: 2017289.125 acc: 0.6604510545730591\n",
      "step: 34200\n",
      "train: loss: 80574.1484375 acc: 0.918459951877594  val: loss: 1593699.75 acc: 0.5782577991485596\n",
      "step: 34205\n",
      "train: loss: 165475.3125 acc: 0.8649332523345947  val: loss: 2287177.25 acc: 0.5650890469551086\n",
      "step: 34210\n",
      "train: loss: 843377.4375 acc: 0.6972894668579102  val: loss: 495294.625 acc: 0.7566317915916443\n",
      "step: 34215\n",
      "train: loss: 75980.171875 acc: 0.9149914383888245  val: loss: 3357722.0 acc: 0.5286213755607605\n",
      "step: 34220\n",
      "train: loss: 487724.0625 acc: 0.7909579277038574  val: loss: 498180.125 acc: 0.7876782417297363\n",
      "step: 34225\n",
      "train: loss: 731326.0625 acc: 0.8763949275016785  val: loss: 354168.59375 acc: 0.8312062621116638\n",
      "step: 34230\n",
      "train: loss: 614752.5625 acc: 0.9074217677116394  val: loss: 562181.8125 acc: 0.8954979777336121\n",
      "step: 34235\n",
      "train: loss: 398576.96875 acc: 0.9623446464538574  val: loss: 850927.6875 acc: 0.6994009017944336\n",
      "step: 34240\n",
      "train: loss: 415543.09375 acc: 0.9515323042869568  val: loss: 889068.0 acc: 0.8341001272201538\n",
      "step: 34245\n",
      "train: loss: 68528.34375 acc: 0.9919692873954773  val: loss: 107053.28125 acc: 0.9768714904785156\n",
      "step: 34250\n",
      "train: loss: 79731.421875 acc: 0.9889107346534729  val: loss: 1035923.4375 acc: 0.8188493251800537\n",
      "step: 34255\n",
      "train: loss: 55626.1875 acc: 0.9954051375389099  val: loss: 492366.75 acc: 0.8289700746536255\n",
      "step: 34260\n",
      "train: loss: 25359.150390625 acc: 0.9966180920600891  val: loss: 258027.96875 acc: 0.9254934787750244\n",
      "step: 34265\n",
      "train: loss: 218562.671875 acc: 0.9835072755813599  val: loss: 337575.46875 acc: 0.9262810945510864\n",
      "step: 34270\n",
      "train: loss: 36742.17578125 acc: 0.9958412647247314  val: loss: 381909.09375 acc: 0.9474022388458252\n",
      "step: 34275\n",
      "train: loss: 41998.9296875 acc: 0.9893530607223511  val: loss: 401276.0625 acc: 0.9241976737976074\n",
      "step: 34280\n",
      "train: loss: 62669.23828125 acc: 0.9767244458198547  val: loss: 599153.0 acc: 0.9411128163337708\n",
      "step: 34285\n",
      "train: loss: 15007.01171875 acc: 0.9896757006645203  val: loss: 510152.84375 acc: 0.8959812521934509\n",
      "step: 34290\n",
      "train: loss: 17225.888671875 acc: 0.9893279671669006  val: loss: 463247.8125 acc: 0.926642656326294\n",
      "step: 34295\n",
      "train: loss: 9286.560546875 acc: 0.988325834274292  val: loss: 445111.3125 acc: 0.9384654760360718\n",
      "step: 34300\n",
      "train: loss: 17614.51171875 acc: 0.9637168049812317  val: loss: 680029.1875 acc: 0.9311034679412842\n",
      "step: 34305\n",
      "train: loss: 18005.955078125 acc: 0.9714731574058533  val: loss: 1444986.875 acc: 0.7672409415245056\n",
      "step: 34310\n",
      "train: loss: 8342.3603515625 acc: 0.9943745732307434  val: loss: 543933.9375 acc: 0.7822969555854797\n",
      "step: 34315\n",
      "train: loss: 14648.271484375 acc: 0.9341148138046265  val: loss: 1187796.75 acc: 0.2271512746810913\n",
      "step: 34320\n",
      "train: loss: 11156.974609375 acc: 0.9824699759483337  val: loss: 721190.75 acc: 0.9189797043800354\n",
      "step: 34325\n",
      "train: loss: 88520.703125 acc: 0.9729470610618591  val: loss: 2475807.75 acc: 0.6212570071220398\n",
      "step: 34330\n",
      "train: loss: 21204.044921875 acc: 0.9867460131645203  val: loss: 1011991.4375 acc: 0.75091153383255\n",
      "step: 34335\n",
      "train: loss: 21157.45703125 acc: 0.990820050239563  val: loss: 433225.125 acc: 0.9310264587402344\n",
      "step: 34340\n",
      "train: loss: 17510.548828125 acc: 0.9898844957351685  val: loss: 1587128.875 acc: 0.15482079982757568\n",
      "step: 34345\n",
      "train: loss: 6359.8505859375 acc: 0.9931833744049072  val: loss: 3029578.5 acc: -0.20416617393493652\n",
      "step: 34350\n",
      "train: loss: 13341.62109375 acc: 0.9855358600616455  val: loss: 831616.625 acc: 0.660450279712677\n",
      "step: 34355\n",
      "train: loss: 9669.173828125 acc: 0.9904927015304565  val: loss: 1911249.375 acc: 0.6372814178466797\n",
      "step: 34360\n",
      "train: loss: 25474.42578125 acc: 0.9817795753479004  val: loss: 2494639.75 acc: 0.6000988483428955\n",
      "step: 34365\n",
      "train: loss: 23727.810546875 acc: 0.9946064352989197  val: loss: 2550591.0 acc: -0.07910168170928955\n",
      "step: 34370\n",
      "train: loss: 35897.4609375 acc: 0.9927690029144287  val: loss: 1422898.5 acc: 0.6826452612876892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 34375\n",
      "train: loss: 12739.197265625 acc: 0.9929059147834778  val: loss: 1254821.875 acc: 0.6164412498474121\n",
      "step: 34380\n",
      "train: loss: 42843.96484375 acc: 0.97775799036026  val: loss: 729670.6875 acc: 0.8991172313690186\n",
      "step: 34385\n",
      "train: loss: 177645.65625 acc: 0.9574763178825378  val: loss: 1661976.375 acc: 0.5464358329772949\n",
      "step: 34390\n",
      "train: loss: 76619.6640625 acc: 0.9672626852989197  val: loss: 3047760.5 acc: 0.45968466997146606\n",
      "step: 34395\n",
      "train: loss: 99410.1953125 acc: 0.9624038934707642  val: loss: 2484692.25 acc: 0.6066838502883911\n",
      "step: 34400\n",
      "train: loss: 72566.21875 acc: 0.9838237762451172  val: loss: 1928638.375 acc: 0.4882088303565979\n",
      "step: 34405\n",
      "train: loss: 680089.125 acc: 0.9125436544418335  val: loss: 482377.9375 acc: 0.747551441192627\n",
      "step: 34410\n",
      "train: loss: 48520.33984375 acc: 0.9920323491096497  val: loss: 916826.875 acc: 0.377302348613739\n",
      "step: 34415\n",
      "train: loss: 602504.25 acc: 0.9482588171958923  val: loss: 1097320.75 acc: 0.050476133823394775\n",
      "step: 34420\n",
      "train: loss: 391722.0625 acc: 0.9489316940307617  val: loss: 499552.125 acc: 0.9245465397834778\n",
      "step: 34425\n",
      "train: loss: 544666.1875 acc: 0.9518486857414246  val: loss: 809793.0625 acc: 0.7755900025367737\n",
      "step: 34430\n",
      "train: loss: 699183.5 acc: 0.9096735715866089  val: loss: 1877095.625 acc: 0.640045166015625\n",
      "step: 34435\n",
      "train: loss: 236923.71875 acc: 0.9667322635650635  val: loss: 446997.3125 acc: 0.9151583909988403\n",
      "step: 34440\n",
      "train: loss: 963688.4375 acc: 0.9550909996032715  val: loss: 1337142.5 acc: 0.7995579838752747\n",
      "step: 34445\n",
      "train: loss: 664335.625 acc: 0.9802832007408142  val: loss: 1217472.0 acc: 0.8981382250785828\n",
      "step: 34450\n",
      "train: loss: 1075700.0 acc: 0.9665088057518005  val: loss: 1889817.75 acc: 0.4060126543045044\n",
      "step: 34455\n",
      "train: loss: 807143.375 acc: 0.9536361694335938  val: loss: 334673.5625 acc: 0.7756277322769165\n",
      "step: 34460\n",
      "train: loss: 201961.484375 acc: 0.9782184362411499  val: loss: 1562632.875 acc: 0.49376213550567627\n",
      "step: 34465\n",
      "train: loss: 317241.6875 acc: 0.9652189016342163  val: loss: 306360.5625 acc: 0.7429604530334473\n",
      "step: 34470\n",
      "train: loss: 604311.0 acc: 0.9212757349014282  val: loss: 1824945.875 acc: 0.6441663503646851\n",
      "step: 34475\n",
      "train: loss: 278669.34375 acc: 0.9561840295791626  val: loss: 275398.75 acc: 0.9534079432487488\n",
      "step: 34480\n",
      "train: loss: 1027253.1875 acc: 0.6867037415504456  val: loss: 3172020.75 acc: 0.692863941192627\n",
      "step: 34485\n",
      "train: loss: 527446.75 acc: 0.7915774583816528  val: loss: 864001.125 acc: 0.6638917922973633\n",
      "step: 34490\n",
      "train: loss: 328666.75 acc: 0.8240490555763245  val: loss: 800253.0625 acc: 0.7895898818969727\n",
      "step: 34495\n",
      "train: loss: 288047.0625 acc: 0.8231914043426514  val: loss: 1034853.5625 acc: 0.8053678274154663\n",
      "step: 34500\n",
      "train: loss: 824119.8125 acc: 0.8111674189567566  val: loss: 2111316.75 acc: 0.5955504179000854\n",
      "step: 34505\n",
      "train: loss: 264564.9375 acc: 0.8077774047851562  val: loss: 2032100.875 acc: 0.6669754981994629\n",
      "step: 34510\n",
      "train: loss: 2390967.25 acc: 0.5803313255310059  val: loss: 1262468.375 acc: 0.7601485848426819\n",
      "step: 34515\n",
      "train: loss: 71243.515625 acc: 0.9487900137901306  val: loss: 1734771.125 acc: 0.5701687335968018\n",
      "step: 34520\n",
      "train: loss: 96246.265625 acc: 0.9230280518531799  val: loss: 1999370.125 acc: 0.6099573373794556\n",
      "step: 34525\n",
      "train: loss: 245595.9375 acc: 0.8525341749191284  val: loss: 2174959.0 acc: 0.5288733243942261\n",
      "step: 34530\n",
      "train: loss: 292331.34375 acc: 0.8507888913154602  val: loss: 715976.5 acc: 0.667285680770874\n",
      "step: 34535\n",
      "train: loss: 54308.71875 acc: 0.9549375772476196  val: loss: 3851982.25 acc: 0.4269200563430786\n",
      "step: 34540\n",
      "train: loss: 307307.46875 acc: 0.8395090699195862  val: loss: 2115308.75 acc: 0.5165137052536011\n",
      "step: 34545\n",
      "train: loss: 453934.75 acc: 0.781407356262207  val: loss: 1036404.1875 acc: 0.7083748579025269\n",
      "step: 34550\n",
      "train: loss: 99356.953125 acc: 0.9206945896148682  val: loss: 646096.25 acc: 0.7251513600349426\n",
      "step: 34555\n",
      "train: loss: 139146.21875 acc: 0.8110834360122681  val: loss: 4143801.75 acc: 0.474765419960022\n",
      "step: 34560\n",
      "train: loss: 99416.3515625 acc: 0.9245892763137817  val: loss: 5115844.0 acc: 0.5176761746406555\n",
      "step: 34565\n",
      "train: loss: 564210.75 acc: 0.7362860441207886  val: loss: 3091326.25 acc: 0.6744862794876099\n",
      "step: 34570\n",
      "train: loss: 168888.0625 acc: 0.897962749004364  val: loss: 76330.796875 acc: 0.9158802032470703\n",
      "step: 34575\n",
      "train: loss: 735007.25 acc: 0.6710784435272217  val: loss: 1102944.375 acc: 0.7306545972824097\n",
      "step: 34580\n",
      "train: loss: 394818.3125 acc: 0.7823330760002136  val: loss: 617759.1875 acc: 0.8159492015838623\n",
      "step: 34585\n",
      "train: loss: 435780.0 acc: 0.7916198968887329  val: loss: 5427861.5 acc: 0.4522480368614197\n",
      "step: 34590\n",
      "train: loss: 960270.5 acc: 0.8322381973266602  val: loss: 334567.8125 acc: 0.8990832567214966\n",
      "step: 34595\n",
      "train: loss: 636924.0625 acc: 0.9166959524154663  val: loss: 659920.25 acc: 0.908701479434967\n",
      "step: 34600\n",
      "train: loss: 120692.2890625 acc: 0.987775981426239  val: loss: 1324860.375 acc: 0.8950217962265015\n",
      "step: 34605\n",
      "train: loss: 33573.83203125 acc: 0.9936061501502991  val: loss: 739188.625 acc: 0.8815374374389648\n",
      "step: 34610\n",
      "train: loss: 84475.3828125 acc: 0.9887009859085083  val: loss: 1451222.625 acc: 0.6519229412078857\n",
      "step: 34615\n",
      "train: loss: 110086.9921875 acc: 0.9734717011451721  val: loss: 557005.9375 acc: 0.913391649723053\n",
      "step: 34620\n",
      "train: loss: 37738.2890625 acc: 0.9962728023529053  val: loss: 1373896.375 acc: 0.874321699142456\n",
      "step: 34625\n",
      "train: loss: 83085.6015625 acc: 0.9947189688682556  val: loss: 966401.125 acc: 0.8699129819869995\n",
      "step: 34630\n",
      "train: loss: 67894.1328125 acc: 0.9946077466011047  val: loss: 1200411.625 acc: 0.8105136156082153\n",
      "step: 34635\n",
      "train: loss: 82099.1171875 acc: 0.9895932674407959  val: loss: 1216090.0 acc: 0.5825263857841492\n",
      "step: 34640\n",
      "train: loss: 7349.630859375 acc: 0.9984811544418335  val: loss: 2029621.875 acc: 0.42990344762802124\n",
      "step: 34645\n",
      "train: loss: 8650.056640625 acc: 0.98921138048172  val: loss: 381261.71875 acc: 0.9531888365745544\n",
      "step: 34650\n",
      "train: loss: 6848.10546875 acc: 0.9814280271530151  val: loss: 2070964.125 acc: 0.7507908940315247\n",
      "step: 34655\n",
      "train: loss: 16610.67578125 acc: 0.9696928262710571  val: loss: 1772522.125 acc: 0.1674218773841858\n",
      "step: 34660\n",
      "train: loss: 18654.8125 acc: 0.9684096574783325  val: loss: 1820993.75 acc: 0.7053077220916748\n",
      "step: 34665\n",
      "train: loss: 14783.90234375 acc: 0.9439615607261658  val: loss: 1077386.5 acc: 0.7538673281669617\n",
      "step: 34670\n",
      "train: loss: 15792.109375 acc: 0.9647477269172668  val: loss: 465924.96875 acc: 0.8857368230819702\n",
      "step: 34675\n",
      "train: loss: 18240.1484375 acc: 0.9679837226867676  val: loss: 1195742.5 acc: 0.8306400775909424\n",
      "step: 34680\n",
      "train: loss: 25506.5078125 acc: 0.9491155743598938  val: loss: 1650087.5 acc: 0.7362000942230225\n",
      "step: 34685\n",
      "train: loss: 6603.9208984375 acc: 0.9896712303161621  val: loss: 1019615.6875 acc: 0.6114825010299683\n",
      "step: 34690\n",
      "train: loss: 35790.16796875 acc: 0.9774662256240845  val: loss: 890307.5625 acc: 0.8884662389755249\n",
      "step: 34695\n",
      "train: loss: 24191.294921875 acc: 0.979292631149292  val: loss: 1522435.875 acc: 0.3656589388847351\n",
      "step: 34700\n",
      "train: loss: 25995.955078125 acc: 0.9861629605293274  val: loss: 1011104.4375 acc: 0.7520902752876282\n",
      "step: 34705\n",
      "train: loss: 16095.8662109375 acc: 0.9887786507606506  val: loss: 2146453.5 acc: 0.713144063949585\n",
      "step: 34710\n",
      "train: loss: 7626.82958984375 acc: 0.9967730641365051  val: loss: 1155096.625 acc: 0.7009697556495667\n",
      "step: 34715\n",
      "train: loss: 11701.6630859375 acc: 0.9776097536087036  val: loss: 2367471.25 acc: 0.15877538919448853\n",
      "step: 34720\n",
      "train: loss: 17025.314453125 acc: 0.9764798283576965  val: loss: 565058.8125 acc: 0.8724914789199829\n",
      "step: 34725\n",
      "train: loss: 17145.306640625 acc: 0.9799258708953857  val: loss: 815053.0 acc: 0.8238122463226318\n",
      "step: 34730\n",
      "train: loss: 32507.412109375 acc: 0.984872043132782  val: loss: 1973888.625 acc: 0.35756391286849976\n",
      "step: 34735\n",
      "train: loss: 37800.71484375 acc: 0.9909442067146301  val: loss: 1849686.125 acc: 0.7070878744125366\n",
      "step: 34740\n",
      "train: loss: 57937.078125 acc: 0.9834074378013611  val: loss: 3744016.0 acc: 0.42878901958465576\n",
      "step: 34745\n",
      "train: loss: 14049.431640625 acc: 0.9946478605270386  val: loss: 584936.0625 acc: 0.9108025431632996\n",
      "step: 34750\n",
      "train: loss: 88877.8515625 acc: 0.9792748689651489  val: loss: 3935836.5 acc: -0.9033111333847046\n",
      "step: 34755\n",
      "train: loss: 147730.15625 acc: 0.9731984734535217  val: loss: 152595.03125 acc: 0.9764173626899719\n",
      "step: 34760\n",
      "train: loss: 290235.0 acc: 0.9179714918136597  val: loss: 739246.6875 acc: 0.6595050096511841\n",
      "step: 34765\n",
      "train: loss: 619985.75 acc: 0.8907099962234497  val: loss: 268031.65625 acc: 0.9506977796554565\n",
      "step: 34770\n",
      "train: loss: 50457.828125 acc: 0.9954583048820496  val: loss: 842500.4375 acc: 0.7878570556640625\n",
      "step: 34775\n",
      "train: loss: 50595.1484375 acc: 0.9948304891586304  val: loss: 1581378.5 acc: 0.6297820210456848\n",
      "step: 34780\n",
      "train: loss: 42263.1640625 acc: 0.9957824349403381  val: loss: 1562513.5 acc: 0.6357676386833191\n",
      "step: 34785\n",
      "train: loss: 220563.03125 acc: 0.9759635925292969  val: loss: 448493.1875 acc: 0.9363793134689331\n",
      "step: 34790\n",
      "train: loss: 412491.59375 acc: 0.9702234268188477  val: loss: 788814.3125 acc: 0.7314247488975525\n",
      "step: 34795\n",
      "train: loss: 2033135.5 acc: 0.8297797441482544  val: loss: 844304.75 acc: 0.758713960647583\n",
      "step: 34800\n",
      "train: loss: 174741.796875 acc: 0.9741308093070984  val: loss: 390673.53125 acc: 0.8612412810325623\n",
      "step: 34805\n",
      "train: loss: 510388.4375 acc: 0.9635546207427979  val: loss: 798860.75 acc: 0.716770350933075\n",
      "step: 34810\n",
      "train: loss: 922445.5625 acc: 0.974443793296814  val: loss: 2710278.5 acc: 0.6506494283676147\n",
      "step: 34815\n",
      "train: loss: 1479393.25 acc: 0.957991361618042  val: loss: 556210.1875 acc: 0.9216238260269165\n",
      "step: 34820\n",
      "train: loss: 901612.0 acc: 0.96589595079422  val: loss: 327042.9375 acc: 0.9076517820358276\n",
      "step: 34825\n",
      "train: loss: 323257.71875 acc: 0.961112916469574  val: loss: 268654.0 acc: 0.9631460905075073\n",
      "step: 34830\n",
      "train: loss: 616453.0 acc: 0.9438366889953613  val: loss: 1347895.875 acc: 0.4920864701271057\n",
      "step: 34835\n",
      "train: loss: 734344.0625 acc: 0.878996729850769  val: loss: 295264.15625 acc: 0.944778561592102\n",
      "step: 34840\n",
      "train: loss: 264597.875 acc: 0.9594528079032898  val: loss: 202977.734375 acc: 0.9471887350082397\n",
      "step: 34845\n",
      "train: loss: 2296299.25 acc: 0.7613115310668945  val: loss: 453512.0625 acc: 0.8909165859222412\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 34850\n",
      "train: loss: 473571.5 acc: 0.8331570625305176  val: loss: 893728.3125 acc: 0.7303429841995239\n",
      "step: 34855\n",
      "train: loss: 406500.90625 acc: 0.843835711479187  val: loss: 3102456.75 acc: 0.5794421434402466\n",
      "step: 34860\n",
      "train: loss: 301649.09375 acc: 0.8304697275161743  val: loss: 828741.3125 acc: 0.6212892532348633\n",
      "step: 34865\n",
      "train: loss: 202336.75 acc: 0.8966886401176453  val: loss: 798864.0625 acc: 0.7968090772628784\n",
      "step: 34870\n",
      "train: loss: 141451.109375 acc: 0.8973182439804077  val: loss: 1851693.25 acc: 0.5839632749557495\n",
      "step: 34875\n",
      "train: loss: 154927.671875 acc: 0.8615983724594116  val: loss: 2979018.75 acc: 0.5679774284362793\n",
      "step: 34880\n",
      "train: loss: 2509144.25 acc: 0.41863375902175903  val: loss: 3767852.75 acc: 0.5687615871429443\n",
      "step: 34885\n",
      "train: loss: 242696.640625 acc: 0.8590713143348694  val: loss: 3313144.0 acc: 0.48785585165023804\n",
      "step: 34890\n",
      "train: loss: 481586.875 acc: 0.7668229937553406  val: loss: 280349.375 acc: 0.8090847730636597\n",
      "step: 34895\n",
      "train: loss: 25362.435546875 acc: 0.9791415929794312  val: loss: 3390101.25 acc: 0.5131438374519348\n",
      "step: 34900\n",
      "train: loss: 64568.0 acc: 0.9535741806030273  val: loss: 2079446.5 acc: 0.6071630120277405\n",
      "step: 34905\n",
      "train: loss: 349162.5625 acc: 0.8295944929122925  val: loss: 5325177.0 acc: 0.47908151149749756\n",
      "step: 34910\n",
      "train: loss: 84800.0234375 acc: 0.9240322709083557  val: loss: 2475270.75 acc: 0.5267353057861328\n",
      "step: 34915\n",
      "train: loss: 271708.46875 acc: 0.8238319158554077  val: loss: 1325489.625 acc: 0.7230584621429443\n",
      "step: 34920\n",
      "train: loss: 54238.109375 acc: 0.9220354557037354  val: loss: 4715175.0 acc: 0.3333582878112793\n",
      "step: 34925\n",
      "train: loss: 165880.09375 acc: 0.9149312376976013  val: loss: 1627593.0 acc: 0.7339696884155273\n",
      "step: 34930\n",
      "train: loss: 16920.169921875 acc: 0.9780817627906799  val: loss: 7252673.0 acc: 0.34385067224502563\n",
      "step: 34935\n",
      "train: loss: 493941.21875 acc: 0.7838683128356934  val: loss: 863473.3125 acc: 0.7278896570205688\n",
      "step: 34940\n",
      "train: loss: 266891.5 acc: 0.8540973663330078  val: loss: 2355593.0 acc: 0.6194536685943604\n",
      "step: 34945\n",
      "train: loss: 926084.5625 acc: 0.7126230001449585  val: loss: 2585173.25 acc: 0.6589305996894836\n",
      "step: 34950\n",
      "train: loss: 357460.59375 acc: 0.7963000535964966  val: loss: 547386.0625 acc: 0.7362469434738159\n",
      "step: 34955\n",
      "train: loss: 1174919.25 acc: 0.780576229095459  val: loss: 1405494.125 acc: 0.7913736701011658\n",
      "step: 34960\n",
      "train: loss: 788682.4375 acc: 0.8121095895767212  val: loss: 1396975.0 acc: 0.724493682384491\n",
      "step: 34965\n",
      "train: loss: 404594.1875 acc: 0.9696101546287537  val: loss: 867763.5 acc: 0.9182947278022766\n",
      "step: 34970\n",
      "train: loss: 99395.6328125 acc: 0.9910236597061157  val: loss: 572553.125 acc: 0.9086578488349915\n",
      "step: 34975\n",
      "train: loss: 51017.4453125 acc: 0.9918562769889832  val: loss: 1894518.0 acc: 0.7524324655532837\n",
      "step: 34980\n",
      "train: loss: 62419.48046875 acc: 0.989899218082428  val: loss: 1584388.25 acc: 0.8067833185195923\n",
      "step: 34985\n",
      "train: loss: 56545.55859375 acc: 0.9931915402412415  val: loss: 1125426.875 acc: 0.8637539148330688\n",
      "step: 34990\n",
      "train: loss: 64796.91015625 acc: 0.9940678477287292  val: loss: 841506.9375 acc: 0.8472751379013062\n",
      "step: 34995\n",
      "train: loss: 61446.67578125 acc: 0.9960252046585083  val: loss: 573959.4375 acc: 0.8531349301338196\n",
      "step: 35000\n",
      "train: loss: 31813.0703125 acc: 0.995499849319458  val: loss: 545322.3125 acc: 0.7116057276725769\n",
      "step: 35005\n",
      "train: loss: 19747.3515625 acc: 0.9961631298065186  val: loss: 1521950.75 acc: 0.8633063435554504\n",
      "step: 35010\n",
      "train: loss: 33718.12109375 acc: 0.9863404631614685  val: loss: 1467491.375 acc: 0.7910422682762146\n",
      "step: 35015\n",
      "train: loss: 14823.9970703125 acc: 0.9923223853111267  val: loss: 1478298.75 acc: 0.8413754105567932\n",
      "step: 35020\n",
      "train: loss: 10276.4951171875 acc: 0.9871509671211243  val: loss: 975251.4375 acc: 0.7893679141998291\n",
      "step: 35025\n",
      "train: loss: 16843.689453125 acc: 0.9880630970001221  val: loss: 2263944.75 acc: 0.45917701721191406\n",
      "step: 35030\n",
      "train: loss: 18494.591796875 acc: 0.968654215335846  val: loss: 615894.4375 acc: 0.8969559669494629\n",
      "step: 35035\n",
      "train: loss: 8375.3720703125 acc: 0.9818198084831238  val: loss: 912278.625 acc: 0.7180780172348022\n",
      "step: 35040\n",
      "train: loss: 15661.388671875 acc: 0.949704110622406  val: loss: 718537.5 acc: 0.7532762289047241\n",
      "step: 35045\n",
      "train: loss: 12111.9736328125 acc: 0.9735530018806458  val: loss: 4511144.0 acc: 0.1599690318107605\n",
      "step: 35050\n",
      "train: loss: 11127.89453125 acc: 0.989088773727417  val: loss: 1872086.125 acc: 0.7411584854125977\n",
      "step: 35055\n",
      "train: loss: 5617.7001953125 acc: 0.9872299432754517  val: loss: 575625.9375 acc: 0.8341190814971924\n",
      "step: 35060\n",
      "train: loss: 20657.732421875 acc: 0.9816596508026123  val: loss: 1170940.625 acc: 0.7906308770179749\n",
      "step: 35065\n",
      "train: loss: 18599.65625 acc: 0.9898475408554077  val: loss: 509187.59375 acc: 0.9200447201728821\n",
      "step: 35070\n",
      "train: loss: 28611.2265625 acc: 0.9791868925094604  val: loss: 3003381.75 acc: 0.2941204309463501\n",
      "step: 35075\n",
      "train: loss: 6862.35546875 acc: 0.9946320056915283  val: loss: 2154787.25 acc: 0.27508288621902466\n",
      "step: 35080\n",
      "train: loss: 18263.080078125 acc: 0.9876472353935242  val: loss: 2133153.0 acc: -0.4961341619491577\n",
      "step: 35085\n",
      "train: loss: 15742.373046875 acc: 0.9748746156692505  val: loss: 2358054.5 acc: 0.7111138105392456\n",
      "step: 35090\n",
      "train: loss: 12550.216796875 acc: 0.9852628707885742  val: loss: 964757.4375 acc: 0.6020070314407349\n",
      "step: 35095\n",
      "train: loss: 26015.421875 acc: 0.9943503737449646  val: loss: 993257.375 acc: 0.8806626200675964\n",
      "step: 35100\n",
      "train: loss: 39206.68359375 acc: 0.9904298186302185  val: loss: 1519431.875 acc: 0.7587227821350098\n",
      "step: 35105\n",
      "train: loss: 12917.3056640625 acc: 0.9952234625816345  val: loss: 2404817.75 acc: 0.165904700756073\n",
      "step: 35110\n",
      "train: loss: 43910.76171875 acc: 0.9838196039199829  val: loss: 1114978.75 acc: 0.4545673727989197\n",
      "step: 35115\n",
      "train: loss: 75076.1875 acc: 0.9872866272926331  val: loss: 1875718.75 acc: 0.857306957244873\n",
      "step: 35120\n",
      "train: loss: 90475.859375 acc: 0.974373996257782  val: loss: 209012.515625 acc: 0.9755065441131592\n",
      "step: 35125\n",
      "train: loss: 49354.64453125 acc: 0.9747742414474487  val: loss: 525776.5625 acc: 0.8780399560928345\n",
      "step: 35130\n",
      "train: loss: 355178.9375 acc: 0.9371935725212097  val: loss: 486283.5625 acc: 0.8617507219314575\n",
      "step: 35135\n",
      "train: loss: 102404.1640625 acc: 0.9840404391288757  val: loss: 614112.5 acc: 0.6976077556610107\n",
      "step: 35140\n",
      "train: loss: 68687.6328125 acc: 0.9937081336975098  val: loss: 2145387.75 acc: 0.3568462133407593\n",
      "step: 35145\n",
      "train: loss: 33314.3359375 acc: 0.9957482814788818  val: loss: 1129666.25 acc: 0.5977902412414551\n",
      "step: 35150\n",
      "train: loss: 378864.59375 acc: 0.9263254404067993  val: loss: 1378373.75 acc: 0.7883304357528687\n",
      "step: 35155\n",
      "train: loss: 256303.734375 acc: 0.9768386483192444  val: loss: 965820.8125 acc: 0.7176209092140198\n",
      "step: 35160\n",
      "train: loss: 518253.34375 acc: 0.9721315503120422  val: loss: 1535714.625 acc: 0.7233153581619263\n",
      "step: 35165\n",
      "train: loss: 212115.734375 acc: 0.9655504822731018  val: loss: 1645795.625 acc: 0.07366114854812622\n",
      "step: 35170\n",
      "train: loss: 279366.34375 acc: 0.9599912166595459  val: loss: 370523.96875 acc: 0.6274691820144653\n",
      "step: 35175\n",
      "train: loss: 551403.4375 acc: 0.9756813049316406  val: loss: 514890.84375 acc: 0.8949039578437805\n",
      "step: 35180\n",
      "train: loss: 2928872.75 acc: 0.906808614730835  val: loss: 388549.65625 acc: 0.9443692564964294\n",
      "step: 35185\n",
      "train: loss: 4257581.0 acc: 0.8742977976799011  val: loss: 189370.515625 acc: 0.9500297904014587\n",
      "step: 35190\n",
      "train: loss: 1001704.6875 acc: 0.9280390739440918  val: loss: 161128.8125 acc: 0.9683784246444702\n",
      "step: 35195\n",
      "train: loss: 783471.75 acc: 0.9467424154281616  val: loss: 462457.59375 acc: 0.9168208837509155\n",
      "step: 35200\n",
      "train: loss: 376971.46875 acc: 0.9492034316062927  val: loss: 582248.4375 acc: 0.8989071846008301\n",
      "step: 35205\n",
      "train: loss: 576750.3125 acc: 0.8691883683204651  val: loss: 491735.46875 acc: 0.8585916757583618\n",
      "step: 35210\n",
      "train: loss: 559068.0625 acc: 0.884206235408783  val: loss: 408278.90625 acc: 0.91876620054245\n",
      "step: 35215\n",
      "train: loss: 626982.75 acc: 0.6838568449020386  val: loss: 2338756.0 acc: 0.6775290369987488\n",
      "step: 35220\n",
      "train: loss: 644108.8125 acc: 0.7456521987915039  val: loss: 6291578.0 acc: 0.5298030376434326\n",
      "step: 35225\n",
      "train: loss: 284692.03125 acc: 0.8684206008911133  val: loss: 2344105.0 acc: 0.640372633934021\n",
      "step: 35230\n",
      "train: loss: 848965.125 acc: 0.721973180770874  val: loss: 610136.75 acc: 0.8007389307022095\n",
      "step: 35235\n",
      "train: loss: 197860.40625 acc: 0.8749502897262573  val: loss: 3646800.25 acc: 0.37106800079345703\n",
      "step: 35240\n",
      "train: loss: 183329.65625 acc: 0.9051196575164795  val: loss: 9926915.0 acc: 0.3810690641403198\n",
      "step: 35245\n",
      "train: loss: 137043.890625 acc: 0.9033359885215759  val: loss: 885430.5 acc: 0.6415545344352722\n",
      "step: 35250\n",
      "train: loss: 3455522.5 acc: 0.3861234784126282  val: loss: 2474766.75 acc: 0.6679439544677734\n",
      "step: 35255\n",
      "train: loss: 124082.15625 acc: 0.9239296913146973  val: loss: 3581496.25 acc: 0.5905959606170654\n",
      "step: 35260\n",
      "train: loss: 22904.2265625 acc: 0.9813205003738403  val: loss: 1982297.0 acc: 0.5409478545188904\n",
      "step: 35265\n",
      "train: loss: 256337.796875 acc: 0.8660052418708801  val: loss: 1409242.25 acc: 0.6726864576339722\n",
      "step: 35270\n",
      "train: loss: 102869.0546875 acc: 0.9216368198394775  val: loss: 4656886.0 acc: 0.4008575677871704\n",
      "step: 35275\n",
      "train: loss: 134243.921875 acc: 0.9092744588851929  val: loss: 3129970.75 acc: 0.551459789276123\n",
      "step: 35280\n",
      "train: loss: 118700.96875 acc: 0.9057284593582153  val: loss: 3725779.5 acc: 0.41130226850509644\n",
      "step: 35285\n",
      "train: loss: 51262.48828125 acc: 0.9504870176315308  val: loss: 2053267.375 acc: 0.48054343461990356\n",
      "step: 35290\n",
      "train: loss: 295388.125 acc: 0.8441641926765442  val: loss: 2492536.25 acc: 0.6056814193725586\n",
      "step: 35295\n",
      "train: loss: 223676.328125 acc: 0.8401918411254883  val: loss: 2379393.0 acc: 0.4934545159339905\n",
      "step: 35300\n",
      "train: loss: 174666.59375 acc: 0.8738143444061279  val: loss: 641137.0625 acc: 0.7884464263916016\n",
      "step: 35305\n",
      "train: loss: 545535.25 acc: 0.7553490996360779  val: loss: 2138054.5 acc: 0.5295089483261108\n",
      "step: 35310\n",
      "train: loss: 121593.453125 acc: 0.8708210587501526  val: loss: 3326957.5 acc: 0.43970638513565063\n",
      "step: 35315\n",
      "train: loss: 499863.875 acc: 0.7577555179595947  val: loss: 316386.9375 acc: 0.8606729507446289\n",
      "step: 35320\n",
      "train: loss: 980947.3125 acc: 0.8372799158096313  val: loss: 1425910.5 acc: 0.661522626876831\n",
      "step: 35325\n",
      "train: loss: 580825.9375 acc: 0.8040795922279358  val: loss: 1909360.5 acc: -0.5075546503067017\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 35330\n",
      "train: loss: 373047.90625 acc: 0.9636637568473816  val: loss: 1762593.25 acc: 0.523242175579071\n",
      "step: 35335\n",
      "train: loss: 83566.3671875 acc: 0.9898978471755981  val: loss: 2135343.25 acc: -0.2486274242401123\n",
      "step: 35340\n",
      "train: loss: 70440.125 acc: 0.989840567111969  val: loss: 1530241.375 acc: 0.516831636428833\n",
      "step: 35345\n",
      "train: loss: 88869.03125 acc: 0.9885126352310181  val: loss: 684336.5 acc: 0.8419302105903625\n",
      "step: 35350\n",
      "train: loss: 53268.63671875 acc: 0.9917201399803162  val: loss: 2048489.0 acc: 0.8165476322174072\n",
      "step: 35355\n",
      "train: loss: 84625.9140625 acc: 0.9917264580726624  val: loss: 1615266.0 acc: 0.5941531658172607\n",
      "step: 35360\n",
      "train: loss: 35794.79296875 acc: 0.9973363280296326  val: loss: 1136912.125 acc: 0.6740603446960449\n",
      "step: 35365\n",
      "train: loss: 36377.41796875 acc: 0.9917015433311462  val: loss: 998869.75 acc: -0.00031638145446777344\n",
      "step: 35370\n",
      "train: loss: 44156.0625 acc: 0.9732904434204102  val: loss: 1047454.1875 acc: 0.8335870504379272\n",
      "step: 35375\n",
      "train: loss: 568813.9375 acc: 0.8166807889938354  val: loss: 779859.3125 acc: 0.8349537253379822\n",
      "step: 35380\n",
      "train: loss: 13453.9892578125 acc: 0.9966909289360046  val: loss: 150115.28125 acc: 0.9506918787956238\n",
      "step: 35385\n",
      "train: loss: 7475.31298828125 acc: 0.9778994917869568  val: loss: 283401.75 acc: 0.9212160110473633\n",
      "step: 35390\n",
      "train: loss: 19552.75390625 acc: 0.9741113781929016  val: loss: 486467.09375 acc: 0.9149640202522278\n",
      "step: 35395\n",
      "train: loss: 16414.896484375 acc: 0.9824431538581848  val: loss: 459963.40625 acc: 0.8886760473251343\n",
      "step: 35400\n",
      "train: loss: 13548.408203125 acc: 0.9731506705284119  val: loss: 1169351.75 acc: 0.8826864957809448\n",
      "step: 35405\n",
      "train: loss: 15172.24609375 acc: 0.9442999362945557  val: loss: 665936.1875 acc: 0.7938323616981506\n",
      "step: 35410\n",
      "train: loss: 9222.515625 acc: 0.9830852746963501  val: loss: 2440842.75 acc: 0.2283608317375183\n",
      "step: 35415\n",
      "train: loss: 11025.7001953125 acc: 0.9788175821304321  val: loss: 1116282.125 acc: 0.8095031380653381\n",
      "step: 35420\n",
      "train: loss: 22746.34375 acc: 0.956585705280304  val: loss: 926986.75 acc: 0.7722876071929932\n",
      "step: 35425\n",
      "train: loss: 21663.962890625 acc: 0.9910809397697449  val: loss: 1925495.5 acc: 0.3814217448234558\n",
      "step: 35430\n",
      "train: loss: 31717.267578125 acc: 0.9839743971824646  val: loss: 870556.4375 acc: 0.20771527290344238\n",
      "step: 35435\n",
      "train: loss: 81264.5234375 acc: 0.930047869682312  val: loss: 1274593.0 acc: 0.8538405895233154\n",
      "step: 35440\n",
      "train: loss: 18746.62109375 acc: 0.9924306273460388  val: loss: 1925034.75 acc: 0.13042014837265015\n",
      "step: 35445\n",
      "train: loss: 17331.859375 acc: 0.9798623323440552  val: loss: 1433677.75 acc: -0.04395473003387451\n",
      "step: 35450\n",
      "train: loss: 88190.9296875 acc: 0.8625070452690125  val: loss: 852475.3125 acc: 0.8379056453704834\n",
      "step: 35455\n",
      "train: loss: 14353.318359375 acc: 0.9955298900604248  val: loss: 1609086.5 acc: 0.5160105228424072\n",
      "step: 35460\n",
      "train: loss: 45473.23046875 acc: 0.9906461834907532  val: loss: 1547658.5 acc: 0.5199700593948364\n",
      "step: 35465\n",
      "train: loss: 31169.091796875 acc: 0.9868398904800415  val: loss: 926487.75 acc: 0.5934827327728271\n",
      "step: 35470\n",
      "train: loss: 32345.814453125 acc: 0.9896390438079834  val: loss: 2142511.5 acc: -1.5010786056518555\n",
      "step: 35475\n",
      "train: loss: 59109.7734375 acc: 0.9805063605308533  val: loss: 1945058.25 acc: 0.7617650628089905\n",
      "step: 35480\n",
      "train: loss: 178512.21875 acc: 0.9291896820068359  val: loss: 172484.484375 acc: 0.9106972217559814\n",
      "step: 35485\n",
      "train: loss: 145635.5625 acc: 0.9341439008712769  val: loss: 285002.125 acc: 0.9424171447753906\n",
      "step: 35490\n",
      "train: loss: 183254.46875 acc: 0.8389474153518677  val: loss: 932393.5 acc: 0.7015037536621094\n",
      "step: 35495\n",
      "train: loss: 64838.359375 acc: 0.9854981899261475  val: loss: 62393.7421875 acc: 0.9728999137878418\n",
      "step: 35500\n",
      "train: loss: 153264.734375 acc: 0.9761698842048645  val: loss: 407278.71875 acc: 0.8141192197799683\n",
      "step: 35505\n",
      "train: loss: 104961.2734375 acc: 0.991378664970398  val: loss: 867331.9375 acc: 0.735487163066864\n",
      "step: 35510\n",
      "train: loss: 69172.34375 acc: 0.9917325973510742  val: loss: 343952.375 acc: 0.9368680119514465\n",
      "step: 35515\n",
      "train: loss: 216431.875 acc: 0.9779022932052612  val: loss: 944965.9375 acc: 0.8401743769645691\n",
      "step: 35520\n",
      "train: loss: 378123.96875 acc: 0.9799872040748596  val: loss: 204658.171875 acc: 0.8461591005325317\n",
      "step: 35525\n",
      "train: loss: 446526.21875 acc: 0.9741085767745972  val: loss: 2846705.25 acc: 0.5822491645812988\n",
      "step: 35530\n",
      "train: loss: 211521.859375 acc: 0.9816445708274841  val: loss: 357314.71875 acc: 0.9700741171836853\n",
      "step: 35535\n",
      "train: loss: 126577.7265625 acc: 0.9572651386260986  val: loss: 820970.0 acc: 0.8961833119392395\n",
      "step: 35540\n",
      "train: loss: 1632535.25 acc: 0.9331961870193481  val: loss: 416901.59375 acc: 0.9577155113220215\n",
      "step: 35545\n",
      "train: loss: 680965.9375 acc: 0.9761435985565186  val: loss: 1188059.5 acc: 0.8521376848220825\n",
      "step: 35550\n",
      "train: loss: 1155420.25 acc: 0.9579834938049316  val: loss: 257162.59375 acc: 0.9490343332290649\n",
      "step: 35555\n",
      "train: loss: 552889.1875 acc: 0.9567590355873108  val: loss: 861769.9375 acc: 0.8383762836456299\n",
      "step: 35560\n",
      "train: loss: 442312.71875 acc: 0.9443531036376953  val: loss: 1014339.375 acc: 0.8657698631286621\n",
      "step: 35565\n",
      "train: loss: 509730.96875 acc: 0.9315198063850403  val: loss: 735064.8125 acc: 0.8315837383270264\n",
      "step: 35570\n",
      "train: loss: 200249.734375 acc: 0.9452725648880005  val: loss: 638307.9375 acc: 0.9379296898841858\n",
      "step: 35575\n",
      "train: loss: 1745710.75 acc: 0.7090867757797241  val: loss: 2215984.5 acc: 0.8203707337379456\n",
      "step: 35580\n",
      "train: loss: 486581.15625 acc: 0.821469247341156  val: loss: 2576790.25 acc: 0.641384482383728\n",
      "step: 35585\n",
      "train: loss: 301958.78125 acc: 0.8308783769607544  val: loss: 1605547.0 acc: 0.7663328647613525\n",
      "step: 35590\n",
      "train: loss: 292237.71875 acc: 0.8319189548492432  val: loss: 742909.875 acc: 0.60555100440979\n",
      "step: 35595\n",
      "train: loss: 219313.890625 acc: 0.8466747403144836  val: loss: 380380.75 acc: 0.8251076340675354\n",
      "step: 35600\n",
      "train: loss: 305067.15625 acc: 0.7813290953636169  val: loss: 614351.6875 acc: 0.7715965509414673\n",
      "step: 35605\n",
      "train: loss: 121841.0234375 acc: 0.918442964553833  val: loss: 2430710.25 acc: 0.5742339491844177\n",
      "step: 35610\n",
      "train: loss: 55743.83984375 acc: 0.9576824307441711  val: loss: 3007523.75 acc: 0.4243165850639343\n",
      "step: 35615\n",
      "train: loss: 98391.0546875 acc: 0.9305516481399536  val: loss: 1206626.375 acc: 0.6221812963485718\n",
      "step: 35620\n",
      "train: loss: 346304.09375 acc: 0.8138718605041504  val: loss: 3935098.25 acc: 0.48589175939559937\n",
      "step: 35625\n",
      "train: loss: 28094.4765625 acc: 0.977787971496582  val: loss: 3429667.5 acc: 0.3410492539405823\n",
      "step: 35630\n",
      "train: loss: 26345.3984375 acc: 0.9768686294555664  val: loss: 3317144.75 acc: 0.46811485290527344\n",
      "step: 35635\n",
      "train: loss: 73304.3828125 acc: 0.9384560585021973  val: loss: 292685.46875 acc: 0.8477168083190918\n",
      "step: 35640\n",
      "train: loss: 284324.84375 acc: 0.8563980460166931  val: loss: 974110.1875 acc: 0.7147461175918579\n",
      "step: 35645\n",
      "train: loss: 147252.484375 acc: 0.8727476596832275  val: loss: 1682865.25 acc: 0.7014551162719727\n",
      "step: 35650\n",
      "train: loss: 291287.96875 acc: 0.80138099193573  val: loss: 1949005.75 acc: 0.5592818856239319\n",
      "step: 35655\n",
      "train: loss: 115136.71875 acc: 0.9061604738235474  val: loss: 2432887.5 acc: 0.559546709060669\n",
      "step: 35660\n",
      "train: loss: 657269.125 acc: 0.7408804893493652  val: loss: 4324500.5 acc: 0.5640237331390381\n",
      "step: 35665\n",
      "train: loss: 481835.75 acc: 0.7651106715202332  val: loss: 5723705.5 acc: 0.4903401732444763\n",
      "step: 35670\n",
      "train: loss: 763600.0625 acc: 0.6500754356384277  val: loss: 2883860.25 acc: 0.49597179889678955\n",
      "step: 35675\n",
      "train: loss: 76919.15625 acc: 0.9411080479621887  val: loss: 567877.875 acc: 0.8073608875274658\n",
      "step: 35680\n",
      "train: loss: 265467.0 acc: 0.6742228269577026  val: loss: 3048269.25 acc: 0.4438655376434326\n",
      "step: 35685\n",
      "train: loss: 1061102.75 acc: 0.7328307628631592  val: loss: 1068766.75 acc: 0.7397013902664185\n",
      "step: 35690\n",
      "train: loss: 678642.5 acc: 0.888239860534668  val: loss: 1603288.75 acc: 0.7233426570892334\n",
      "step: 35695\n",
      "train: loss: 654780.5625 acc: 0.9360462427139282  val: loss: 1782312.625 acc: 0.6306504011154175\n",
      "step: 35700\n",
      "train: loss: 422052.25 acc: 0.9670124650001526  val: loss: 378355.5 acc: 0.9511227607727051\n",
      "step: 35705\n",
      "train: loss: 76126.8046875 acc: 0.9881220459938049  val: loss: 1888966.75 acc: 0.738075852394104\n",
      "step: 35710\n",
      "train: loss: 809398.3125 acc: 0.8254754543304443  val: loss: 709332.375 acc: 0.8512227535247803\n",
      "step: 35715\n",
      "train: loss: 150719.859375 acc: 0.9824234247207642  val: loss: 2054795.25 acc: 0.5992405414581299\n",
      "step: 35720\n",
      "train: loss: 51212.66796875 acc: 0.9951903820037842  val: loss: 1997799.5 acc: 0.7125575542449951\n",
      "step: 35725\n",
      "train: loss: 50605.87109375 acc: 0.9960122108459473  val: loss: 1570244.25 acc: 0.7871590256690979\n",
      "step: 35730\n",
      "train: loss: 34133.91015625 acc: 0.9947608113288879  val: loss: 694203.8125 acc: 0.8351252675056458\n",
      "step: 35735\n",
      "train: loss: 21169.234375 acc: 0.9943035840988159  val: loss: 1189081.25 acc: 0.5998403429985046\n",
      "step: 35740\n",
      "train: loss: 16329.560546875 acc: 0.9952045679092407  val: loss: 333419.90625 acc: 0.9413778185844421\n",
      "step: 35745\n",
      "train: loss: 13950.453125 acc: 0.9933555722236633  val: loss: 796348.6875 acc: 0.8650376796722412\n",
      "step: 35750\n",
      "train: loss: 10891.3232421875 acc: 0.9971780776977539  val: loss: 311439.53125 acc: 0.9278977513313293\n",
      "step: 35755\n",
      "train: loss: 15026.1171875 acc: 0.9886795282363892  val: loss: 449505.75 acc: 0.8801771998405457\n",
      "step: 35760\n",
      "train: loss: 12624.3095703125 acc: 0.97362220287323  val: loss: 570941.9375 acc: 0.8260329365730286\n",
      "step: 35765\n",
      "train: loss: 13848.5205078125 acc: 0.97503662109375  val: loss: 511694.21875 acc: 0.5588786005973816\n",
      "step: 35770\n",
      "train: loss: 8161.24072265625 acc: 0.9893215298652649  val: loss: 1631770.75 acc: 0.16693049669265747\n",
      "step: 35775\n",
      "train: loss: 10288.6171875 acc: 0.9775600433349609  val: loss: 597023.375 acc: 0.8801182508468628\n",
      "step: 35780\n",
      "train: loss: 8795.1201171875 acc: 0.9540334939956665  val: loss: 942821.125 acc: 0.647964596748352\n",
      "step: 35785\n",
      "train: loss: 9899.900390625 acc: 0.9720661640167236  val: loss: 490616.25 acc: 0.8600944876670837\n",
      "step: 35790\n",
      "train: loss: 26447.33203125 acc: 0.9845443964004517  val: loss: 810777.6875 acc: 0.8685699701309204\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 35795\n",
      "train: loss: 23913.892578125 acc: 0.9862158298492432  val: loss: 686988.3125 acc: 0.8556755781173706\n",
      "step: 35800\n",
      "train: loss: 24059.859375 acc: 0.9846194386482239  val: loss: 989524.0625 acc: 0.7102695107460022\n",
      "step: 35805\n",
      "train: loss: 21629.609375 acc: 0.9875676035881042  val: loss: 255544.90625 acc: 0.9497998356819153\n",
      "step: 35810\n",
      "train: loss: 10199.03125 acc: 0.9894999861717224  val: loss: 698798.0625 acc: 0.5355545282363892\n",
      "step: 35815\n",
      "train: loss: 18900.865234375 acc: 0.9814348220825195  val: loss: 180294.078125 acc: 0.9564478397369385\n",
      "step: 35820\n",
      "train: loss: 7713.8740234375 acc: 0.9974880814552307  val: loss: 1082482.0 acc: 0.6445610523223877\n",
      "step: 35825\n",
      "train: loss: 37665.3046875 acc: 0.9860749244689941  val: loss: 1274725.75 acc: 0.8483653664588928\n",
      "step: 35830\n",
      "train: loss: 26821.837890625 acc: 0.9926246404647827  val: loss: 1233261.75 acc: 0.47941386699676514\n",
      "step: 35835\n",
      "train: loss: 20629.681640625 acc: 0.9937886595726013  val: loss: 1305498.375 acc: 0.5368728637695312\n",
      "step: 35840\n",
      "train: loss: 42008.54296875 acc: 0.9707573652267456  val: loss: 1352712.75 acc: 0.9212085008621216\n",
      "step: 35845\n",
      "train: loss: 112514.640625 acc: 0.9599474668502808  val: loss: 776707.25 acc: 0.902734637260437\n",
      "step: 35850\n",
      "train: loss: 75850.5234375 acc: 0.9783843755722046  val: loss: 552707.4375 acc: 0.8838380575180054\n",
      "step: 35855\n",
      "train: loss: 43379.00390625 acc: 0.9761883616447449  val: loss: 1710662.5 acc: 0.7798072695732117\n",
      "step: 35860\n",
      "train: loss: 759887.375 acc: 0.8511677384376526  val: loss: 324394.03125 acc: 0.9650353193283081\n",
      "step: 35865\n",
      "train: loss: 667039.25 acc: 0.9272353053092957  val: loss: 266715.21875 acc: 0.9534212350845337\n",
      "step: 35870\n",
      "train: loss: 56905.95703125 acc: 0.9926589727401733  val: loss: 847088.75 acc: 0.8933851718902588\n",
      "step: 35875\n",
      "train: loss: 16391.95703125 acc: 0.9976901412010193  val: loss: 1342954.875 acc: 0.8965928554534912\n",
      "step: 35880\n",
      "train: loss: 317404.15625 acc: 0.9417430758476257  val: loss: 385964.75 acc: 0.8887558579444885\n",
      "step: 35885\n",
      "train: loss: 215102.65625 acc: 0.9760423302650452  val: loss: 1288141.5 acc: 0.8730266094207764\n",
      "step: 35890\n",
      "train: loss: 183209.484375 acc: 0.9843224883079529  val: loss: 2293680.25 acc: 0.4477827548980713\n",
      "step: 35895\n",
      "train: loss: 502827.0 acc: 0.969036877155304  val: loss: 584020.1875 acc: 0.7832556962966919\n",
      "step: 35900\n",
      "train: loss: 330982.59375 acc: 0.9702565670013428  val: loss: 742509.125 acc: 0.8958411812782288\n",
      "step: 35905\n",
      "train: loss: 395038.53125 acc: 0.9841740727424622  val: loss: 1010837.8125 acc: 0.7888156175613403\n",
      "step: 35910\n",
      "train: loss: 2338725.0 acc: 0.9183477163314819  val: loss: 907494.0625 acc: 0.8207583427429199\n",
      "step: 35915\n",
      "train: loss: 702275.875 acc: 0.9620659351348877  val: loss: 1596913.875 acc: 0.7204552292823792\n",
      "step: 35920\n",
      "train: loss: 1123712.375 acc: 0.9290761351585388  val: loss: 1113456.875 acc: 0.5093905925750732\n",
      "step: 35925\n",
      "train: loss: 721729.875 acc: 0.9474019408226013  val: loss: 1271896.375 acc: 0.8161752223968506\n",
      "step: 35930\n",
      "train: loss: 867206.0 acc: 0.9229080080986023  val: loss: 599731.375 acc: 0.9174161553382874\n",
      "step: 35935\n",
      "train: loss: 499367.90625 acc: 0.8612231016159058  val: loss: 962720.4375 acc: 0.4057503342628479\n",
      "step: 35940\n",
      "train: loss: 2213131.75 acc: 0.48963987827301025  val: loss: 275048.21875 acc: 0.9471809267997742\n",
      "step: 35945\n",
      "train: loss: 1033204.375 acc: 0.7049814462661743  val: loss: 291590.75 acc: 0.8594943881034851\n",
      "step: 35950\n",
      "train: loss: 635439.4375 acc: 0.7508585453033447  val: loss: 266010.40625 acc: 0.7165785431861877\n",
      "step: 35955\n",
      "train: loss: 423384.96875 acc: 0.7973654866218567  val: loss: 1067674.5 acc: 0.7437229156494141\n",
      "step: 35960\n",
      "train: loss: 301627.0625 acc: 0.8537000417709351  val: loss: 2125934.75 acc: 0.7358954548835754\n",
      "step: 35965\n",
      "train: loss: 217203.28125 acc: 0.8475416898727417  val: loss: 644047.9375 acc: 0.7449259757995605\n",
      "step: 35970\n",
      "train: loss: 157666.078125 acc: 0.8915392756462097  val: loss: 651804.4375 acc: 0.6822420954704285\n",
      "step: 35975\n",
      "train: loss: 83908.3671875 acc: 0.9476161599159241  val: loss: 2714879.0 acc: 0.5151699781417847\n",
      "step: 35980\n",
      "train: loss: 549175.6875 acc: 0.7226768732070923  val: loss: 867063.625 acc: 0.660636842250824\n",
      "step: 35985\n",
      "train: loss: 207790.75 acc: 0.8792438507080078  val: loss: 4669890.5 acc: 0.41463541984558105\n",
      "step: 35990\n",
      "train: loss: 29626.0546875 acc: 0.9746559262275696  val: loss: 2254765.75 acc: 0.6104556918144226\n",
      "step: 35995\n",
      "train: loss: 62794.18359375 acc: 0.9561925530433655  val: loss: 557926.625 acc: 0.7641611695289612\n",
      "step: 36000\n",
      "train: loss: 437907.9375 acc: 0.8079326152801514  val: loss: 1710064.875 acc: 0.6256723999977112\n",
      "step: 36005\n",
      "train: loss: 261352.640625 acc: 0.8446705937385559  val: loss: 754196.6875 acc: 0.7138370275497437\n",
      "step: 36010\n",
      "train: loss: 125662.3671875 acc: 0.9057613611221313  val: loss: 1070853.5 acc: 0.6967414617538452\n",
      "step: 36015\n",
      "train: loss: 285150.4375 acc: 0.7865064144134521  val: loss: 1070636.5 acc: 0.7203352451324463\n",
      "step: 36020\n",
      "train: loss: 36776.9609375 acc: 0.9581267833709717  val: loss: 1411209.0 acc: 0.6609461307525635\n",
      "step: 36025\n",
      "train: loss: 8913.1103515625 acc: 0.991303563117981  val: loss: 6291096.0 acc: 0.4047354459762573\n",
      "step: 36030\n",
      "train: loss: 76622.8671875 acc: 0.9428011775016785  val: loss: 5366671.5 acc: 0.47308212518692017\n",
      "step: 36035\n",
      "train: loss: 380742.625 acc: 0.7607155442237854  val: loss: 1819660.75 acc: 0.5696749687194824\n",
      "step: 36040\n",
      "train: loss: 821146.0625 acc: 0.7103607654571533  val: loss: 2099760.75 acc: 0.7216206789016724\n",
      "step: 36045\n",
      "train: loss: 304866.5625 acc: 0.8045727610588074  val: loss: 1172842.875 acc: 0.6855854988098145\n",
      "step: 36050\n",
      "train: loss: 986820.4375 acc: 0.7133046984672546  val: loss: 818907.625 acc: 0.785361647605896\n",
      "step: 36055\n",
      "train: loss: 899967.125 acc: 0.8539854884147644  val: loss: 558864.5625 acc: 0.9231691360473633\n",
      "step: 36060\n",
      "train: loss: 953702.1875 acc: 0.8988034725189209  val: loss: 427410.28125 acc: 0.8286271095275879\n",
      "step: 36065\n",
      "train: loss: 1272473.25 acc: 0.8712053298950195  val: loss: 223700.59375 acc: 0.9636660218238831\n",
      "step: 36070\n",
      "train: loss: 152462.578125 acc: 0.9876781105995178  val: loss: 990692.9375 acc: 0.6123754978179932\n",
      "step: 36075\n",
      "train: loss: 133887.328125 acc: 0.9812458157539368  val: loss: 2453409.25 acc: 0.09118127822875977\n",
      "step: 36080\n",
      "train: loss: 75652.3671875 acc: 0.9881776571273804  val: loss: 326152.4375 acc: 0.8411505222320557\n",
      "step: 36085\n",
      "train: loss: 80983.09375 acc: 0.9926990270614624  val: loss: 151237.453125 acc: 0.9524725079536438\n",
      "step: 36090\n",
      "train: loss: 81785.3515625 acc: 0.9946239590644836  val: loss: 547489.0625 acc: 0.897383451461792\n",
      "step: 36095\n",
      "train: loss: 32059.169921875 acc: 0.9962178468704224  val: loss: 956389.25 acc: 0.8570480346679688\n",
      "step: 36100\n",
      "train: loss: 26995.23828125 acc: 0.9955524802207947  val: loss: 313877.78125 acc: 0.8970353007316589\n",
      "step: 36105\n",
      "train: loss: 21160.732421875 acc: 0.9941052794456482  val: loss: 1020033.0 acc: -0.2870454788208008\n",
      "step: 36110\n",
      "train: loss: 11299.71875 acc: 0.9930925369262695  val: loss: 1137724.75 acc: 0.7145704030990601\n",
      "step: 36115\n",
      "train: loss: 7640.9931640625 acc: 0.9762711524963379  val: loss: 171294.515625 acc: 0.9548547267913818\n",
      "step: 36120\n",
      "train: loss: 5328.0205078125 acc: 0.9970718622207642  val: loss: 554716.5 acc: 0.8821779489517212\n",
      "step: 36125\n",
      "train: loss: 19181.19140625 acc: 0.9936126470565796  val: loss: 245607.578125 acc: 0.9042961597442627\n",
      "step: 36130\n",
      "train: loss: 32812.203125 acc: 0.9807065725326538  val: loss: 1385065.875 acc: 0.47891491651535034\n",
      "step: 36135\n",
      "train: loss: 16408.763671875 acc: 0.9868395328521729  val: loss: 120070.203125 acc: 0.9758291840553284\n",
      "step: 36140\n",
      "train: loss: 12052.3486328125 acc: 0.9768601059913635  val: loss: 563257.3125 acc: 0.8433454036712646\n",
      "step: 36145\n",
      "train: loss: 10189.8818359375 acc: 0.9759423136711121  val: loss: 634195.0 acc: 0.7373636960983276\n",
      "step: 36150\n",
      "train: loss: 203076.453125 acc: 0.7458984851837158  val: loss: 631254.875 acc: 0.906733512878418\n",
      "step: 36155\n",
      "train: loss: 91017.3359375 acc: 0.9723508358001709  val: loss: 402921.0625 acc: 0.8690579533576965\n",
      "step: 36160\n",
      "train: loss: 20159.515625 acc: 0.9811325073242188  val: loss: 1748268.125 acc: 0.8296942114830017\n",
      "step: 36165\n",
      "train: loss: 19730.45703125 acc: 0.9910074472427368  val: loss: 1083284.25 acc: 0.9040589332580566\n",
      "step: 36170\n",
      "train: loss: 92533.7265625 acc: 0.9540334939956665  val: loss: 560420.875 acc: 0.9537983536720276\n",
      "step: 36175\n",
      "train: loss: 21328.556640625 acc: 0.9901261925697327  val: loss: 525768.0625 acc: 0.9012622833251953\n",
      "step: 36180\n",
      "train: loss: 9834.0830078125 acc: 0.9926884770393372  val: loss: 1488848.5 acc: 0.7276716232299805\n",
      "step: 36185\n",
      "train: loss: 10153.2060546875 acc: 0.9925377368927002  val: loss: 418148.84375 acc: 0.8672837018966675\n",
      "step: 36190\n",
      "train: loss: 34738.0625 acc: 0.9917006492614746  val: loss: 1700751.5 acc: 0.8522149920463562\n",
      "step: 36195\n",
      "train: loss: 30731.97265625 acc: 0.990958571434021  val: loss: 673127.375 acc: 0.8090881109237671\n",
      "step: 36200\n",
      "train: loss: 39982.19140625 acc: 0.990176796913147  val: loss: 1025270.125 acc: 0.7819855213165283\n",
      "step: 36205\n",
      "train: loss: 17641.00390625 acc: 0.9935640692710876  val: loss: 861598.25 acc: 0.805325448513031\n",
      "step: 36210\n",
      "train: loss: 38034.34765625 acc: 0.9808831214904785  val: loss: 1759502.125 acc: 0.3630738854408264\n",
      "step: 36215\n",
      "train: loss: 262504.65625 acc: 0.9255314469337463  val: loss: 789456.25 acc: 0.9208646416664124\n",
      "step: 36220\n",
      "train: loss: 44286.07421875 acc: 0.9783507585525513  val: loss: 2463003.0 acc: 0.6456876993179321\n",
      "step: 36225\n",
      "train: loss: 964772.3125 acc: 0.5604857206344604  val: loss: 3656055.5 acc: 0.776626467704773\n",
      "step: 36230\n",
      "train: loss: 115293.7421875 acc: 0.9804564714431763  val: loss: 1965980.75 acc: 0.5302966237068176\n",
      "step: 36235\n",
      "train: loss: 41614.6796875 acc: 0.9948727488517761  val: loss: 580551.9375 acc: 0.8883823156356812\n",
      "step: 36240\n",
      "train: loss: 203080.953125 acc: 0.9669981598854065  val: loss: 654257.6875 acc: 0.9288559556007385\n",
      "step: 36245\n",
      "train: loss: 424539.78125 acc: 0.9253000617027283  val: loss: 926210.3125 acc: 0.6872836947441101\n",
      "step: 36250\n",
      "train: loss: 281303.40625 acc: 0.975885808467865  val: loss: 101098.984375 acc: 0.9556275010108948\n",
      "step: 36255\n",
      "train: loss: 624204.625 acc: 0.9517495632171631  val: loss: 1989857.125 acc: -0.24420666694641113\n",
      "step: 36260\n",
      "train: loss: 459162.09375 acc: 0.9627801775932312  val: loss: 467534.4375 acc: 0.8937009572982788\n",
      "step: 36265\n",
      "train: loss: 2033543.5 acc: 0.7708192467689514  val: loss: 707454.125 acc: 0.8207939267158508\n",
      "step: 36270\n",
      "train: loss: 805622.0 acc: 0.9586488604545593  val: loss: 431529.5625 acc: 0.7047953605651855\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 36275\n",
      "train: loss: 1263066.0 acc: 0.9645354747772217  val: loss: 2540304.25 acc: 0.5957821607589722\n",
      "step: 36280\n",
      "train: loss: 1101757.375 acc: 0.966195821762085  val: loss: 192123.921875 acc: 0.9739067554473877\n",
      "step: 36285\n",
      "train: loss: 1450648.0 acc: 0.9425724148750305  val: loss: 1403000.75 acc: 0.8815801739692688\n",
      "step: 36290\n",
      "train: loss: 433147.6875 acc: 0.9616959095001221  val: loss: 2208409.75 acc: 0.47739702463150024\n",
      "step: 36295\n",
      "train: loss: 1220553.75 acc: 0.9378256797790527  val: loss: 1051376.375 acc: 0.22668015956878662\n",
      "step: 36300\n",
      "train: loss: 146286.96875 acc: 0.9731034636497498  val: loss: 290701.65625 acc: 0.9073619246482849\n",
      "step: 36305\n",
      "train: loss: 317993.03125 acc: 0.9077668190002441  val: loss: 1335437.625 acc: 0.8107171058654785\n",
      "step: 36310\n",
      "train: loss: 1435953.5 acc: 0.5846021175384521  val: loss: 969360.1875 acc: 0.792040228843689\n",
      "step: 36315\n",
      "train: loss: 965073.3125 acc: 0.7362850904464722  val: loss: 3379341.5 acc: 0.7262157201766968\n",
      "step: 36320\n",
      "train: loss: 2044434.875 acc: 0.6588876247406006  val: loss: 789395.75 acc: 0.79756098985672\n",
      "step: 36325\n",
      "train: loss: 154246.3125 acc: 0.9045912027359009  val: loss: 835457.875 acc: 0.8732433915138245\n",
      "step: 36330\n",
      "train: loss: 1026419.3125 acc: 0.6304524540901184  val: loss: 965640.1875 acc: 0.7462391257286072\n",
      "step: 36335\n",
      "train: loss: 109617.625 acc: 0.9147364497184753  val: loss: 1994115.5 acc: 0.6058763265609741\n",
      "step: 36340\n",
      "train: loss: 45450.88671875 acc: 0.9675490856170654  val: loss: 2351330.25 acc: 0.4999544024467468\n",
      "step: 36345\n",
      "train: loss: 90084.1796875 acc: 0.9264625906944275  val: loss: 1887707.75 acc: 0.6189912557601929\n",
      "step: 36350\n",
      "train: loss: 65127.89453125 acc: 0.947390079498291  val: loss: 594462.5625 acc: 0.7289644479751587\n",
      "step: 36355\n",
      "train: loss: 27238.568359375 acc: 0.9781373739242554  val: loss: 2624277.25 acc: 0.5054980516433716\n",
      "step: 36360\n",
      "train: loss: 200673.0 acc: 0.8871196508407593  val: loss: 540856.3125 acc: 0.6825332045555115\n",
      "step: 36365\n",
      "train: loss: 255614.15625 acc: 0.8399478793144226  val: loss: 490853.28125 acc: 0.7398988604545593\n",
      "step: 36370\n",
      "train: loss: 43836.48828125 acc: 0.9704397916793823  val: loss: 2027541.75 acc: 0.6159768104553223\n",
      "step: 36375\n",
      "train: loss: 383705.0 acc: 0.7992122173309326  val: loss: 3272759.25 acc: 0.5413821935653687\n",
      "step: 36380\n",
      "train: loss: 34747.66015625 acc: 0.9721664786338806  val: loss: 1107565.25 acc: 0.6743051409721375\n",
      "step: 36385\n",
      "train: loss: 37079.57421875 acc: 0.9614683985710144  val: loss: 1105355.75 acc: 0.6663120985031128\n",
      "step: 36390\n",
      "train: loss: 282212.34375 acc: 0.8259682655334473  val: loss: 471053.71875 acc: 0.7540938258171082\n",
      "step: 36395\n",
      "train: loss: 160492.0 acc: 0.8339999914169312  val: loss: 3159526.5 acc: 0.48859137296676636\n",
      "step: 36400\n",
      "train: loss: 666526.0 acc: 0.7536102533340454  val: loss: 3390860.75 acc: 0.5329899787902832\n",
      "step: 36405\n",
      "train: loss: 332515.71875 acc: 0.8248312473297119  val: loss: 670200.0 acc: 0.7638273239135742\n",
      "step: 36410\n",
      "train: loss: 43665.296875 acc: 0.9578702449798584  val: loss: 581269.375 acc: 0.7141193747520447\n",
      "step: 36415\n",
      "train: loss: 378523.9375 acc: 0.8169159293174744  val: loss: 810285.6875 acc: 0.7712194919586182\n",
      "step: 36420\n",
      "train: loss: 423607.90625 acc: 0.8977230191230774  val: loss: 440919.59375 acc: 0.8618463277816772\n",
      "step: 36425\n",
      "train: loss: 108880.9765625 acc: 0.9838440418243408  val: loss: 63678.8125 acc: 0.9883944392204285\n",
      "step: 36430\n",
      "train: loss: 66588.9296875 acc: 0.9948675036430359  val: loss: 634099.3125 acc: 0.7231175899505615\n",
      "step: 36435\n",
      "train: loss: 585728.125 acc: 0.844043493270874  val: loss: 718476.0 acc: 0.7698568105697632\n",
      "step: 36440\n",
      "train: loss: 737897.8125 acc: 0.8977164030075073  val: loss: 121315.046875 acc: 0.978660523891449\n",
      "step: 36445\n",
      "train: loss: 125922.46875 acc: 0.9830589890480042  val: loss: 852989.3125 acc: 0.822219729423523\n",
      "step: 36450\n",
      "train: loss: 51383.6875 acc: 0.9959847927093506  val: loss: 779032.0625 acc: 0.9006422162055969\n",
      "step: 36455\n",
      "train: loss: 52519.50390625 acc: 0.9955942630767822  val: loss: 260241.46875 acc: 0.9666996002197266\n",
      "step: 36460\n",
      "train: loss: 42880.8046875 acc: 0.9964656233787537  val: loss: 846614.375 acc: 0.8932995796203613\n",
      "step: 36465\n",
      "train: loss: 37748.01171875 acc: 0.9936837553977966  val: loss: 1304392.875 acc: 0.84212327003479\n",
      "step: 36470\n",
      "train: loss: 62854.734375 acc: 0.9804221391677856  val: loss: 718667.3125 acc: 0.8132259845733643\n",
      "step: 36475\n",
      "train: loss: 11779.267578125 acc: 0.9958972930908203  val: loss: 1715067.0 acc: 0.7682739496231079\n",
      "step: 36480\n",
      "train: loss: 12777.41015625 acc: 0.9927816390991211  val: loss: 1207952.0 acc: 0.8143541216850281\n",
      "step: 36485\n",
      "train: loss: 18458.42578125 acc: 0.9814289212226868  val: loss: 896709.25 acc: 0.923059344291687\n",
      "step: 36490\n",
      "train: loss: 17236.28515625 acc: 0.9708352088928223  val: loss: 381717.40625 acc: 0.9159204363822937\n",
      "step: 36495\n",
      "train: loss: 13211.5107421875 acc: 0.9555399417877197  val: loss: 732797.0625 acc: 0.8786951303482056\n",
      "step: 36500\n",
      "train: loss: 4068.676513671875 acc: 0.9792141914367676  val: loss: 1101056.875 acc: 0.30483323335647583\n",
      "step: 36505\n",
      "train: loss: 8847.55078125 acc: 0.9820983409881592  val: loss: 614172.75 acc: 0.9297239780426025\n",
      "step: 36510\n",
      "train: loss: 14246.7265625 acc: 0.9867050647735596  val: loss: 1151627.125 acc: 0.9259539842605591\n",
      "step: 36515\n",
      "train: loss: 19684.89453125 acc: 0.9568961262702942  val: loss: 1381519.125 acc: 0.5571900606155396\n",
      "step: 36520\n",
      "train: loss: 12231.4833984375 acc: 0.9774209856987  val: loss: 922584.25 acc: 0.4872128367424011\n",
      "step: 36525\n",
      "train: loss: 15401.58984375 acc: 0.9911528825759888  val: loss: 1415733.125 acc: 0.7745233774185181\n",
      "step: 36530\n",
      "train: loss: 24955.51953125 acc: 0.9857426285743713  val: loss: 1291303.875 acc: 0.823908269405365\n",
      "step: 36535\n",
      "train: loss: 18079.46875 acc: 0.9882246851921082  val: loss: 1453602.375 acc: 0.42187368869781494\n",
      "step: 36540\n",
      "train: loss: 4590.197265625 acc: 0.9966874718666077  val: loss: 3916378.75 acc: 0.5539895296096802\n",
      "step: 36545\n",
      "train: loss: 10507.6181640625 acc: 0.9753038883209229  val: loss: 2755355.5 acc: 0.29256361722946167\n",
      "step: 36550\n",
      "train: loss: 14822.853515625 acc: 0.9911414384841919  val: loss: 2023204.0 acc: 0.5790703296661377\n",
      "step: 36555\n",
      "train: loss: 20763.357421875 acc: 0.9878861308097839  val: loss: 1143779.875 acc: 0.8222166895866394\n",
      "step: 36560\n",
      "train: loss: 42067.58984375 acc: 0.9884057641029358  val: loss: 1228601.75 acc: 0.8656345009803772\n",
      "step: 36565\n",
      "train: loss: 34938.6953125 acc: 0.9917458891868591  val: loss: 1278167.75 acc: 0.8717736601829529\n",
      "step: 36570\n",
      "train: loss: 39825.44140625 acc: 0.973318874835968  val: loss: 742924.4375 acc: 0.8657792806625366\n",
      "step: 36575\n",
      "train: loss: 35035.16015625 acc: 0.9887437224388123  val: loss: 1365384.0 acc: 0.6783623695373535\n",
      "step: 36580\n",
      "train: loss: 74894.8125 acc: 0.9815688133239746  val: loss: 1656824.125 acc: 0.7446044683456421\n",
      "step: 36585\n",
      "train: loss: 63014.0 acc: 0.9738641977310181  val: loss: 1467906.5 acc: 0.743484616279602\n",
      "step: 36590\n",
      "train: loss: 272755.9375 acc: 0.9093247652053833  val: loss: 2721789.25 acc: -0.7956645488739014\n",
      "step: 36595\n",
      "train: loss: 821346.875 acc: 0.8723376989364624  val: loss: 1035690.75 acc: 0.5572775602340698\n",
      "step: 36600\n",
      "train: loss: 702375.75 acc: 0.9243170619010925  val: loss: 566697.3125 acc: 0.8598217964172363\n",
      "step: 36605\n",
      "train: loss: 26405.712890625 acc: 0.9963899850845337  val: loss: 2604005.25 acc: -1.7361862659454346\n",
      "step: 36610\n",
      "train: loss: 55056.08984375 acc: 0.9904724955558777  val: loss: 866462.5 acc: 0.6226669549942017\n",
      "step: 36615\n",
      "train: loss: 328558.09375 acc: 0.9501426815986633  val: loss: 359402.9375 acc: 0.9230709671974182\n",
      "step: 36620\n",
      "train: loss: 420235.3125 acc: 0.9576493501663208  val: loss: 964995.25 acc: 0.8705869913101196\n",
      "step: 36625\n",
      "train: loss: 288763.53125 acc: 0.9731040000915527  val: loss: 587839.5 acc: 0.7653847932815552\n",
      "step: 36630\n",
      "train: loss: 265628.34375 acc: 0.9717267155647278  val: loss: 784947.0 acc: 0.858340859413147\n",
      "step: 36635\n",
      "train: loss: 531327.25 acc: 0.9728250503540039  val: loss: 1745389.125 acc: 0.7805901765823364\n",
      "step: 36640\n",
      "train: loss: 671777.5625 acc: 0.9779770970344543  val: loss: 450355.8125 acc: 0.9271877408027649\n",
      "step: 36645\n",
      "train: loss: 1758405.5 acc: 0.9402377605438232  val: loss: 570138.5 acc: 0.8890379071235657\n",
      "step: 36650\n",
      "train: loss: 1197600.5 acc: 0.9564764499664307  val: loss: 602247.625 acc: 0.8247706294059753\n",
      "step: 36655\n",
      "train: loss: 385059.125 acc: 0.9842483997344971  val: loss: 2332369.75 acc: 0.14416903257369995\n",
      "step: 36660\n",
      "train: loss: 191367.09375 acc: 0.9663625955581665  val: loss: 679421.9375 acc: 0.9254319071769714\n",
      "step: 36665\n",
      "train: loss: 336812.09375 acc: 0.9380938410758972  val: loss: 851491.875 acc: 0.8619731664657593\n",
      "step: 36670\n",
      "train: loss: 746770.75 acc: 0.7888122200965881  val: loss: 865585.75 acc: 0.8265563249588013\n",
      "step: 36675\n",
      "train: loss: 661429.625 acc: 0.7741773128509521  val: loss: 438531.375 acc: 0.7584577798843384\n",
      "step: 36680\n",
      "train: loss: 2722809.25 acc: 0.6866267919540405  val: loss: 284743.75 acc: 0.8664866089820862\n",
      "step: 36685\n",
      "train: loss: 852862.4375 acc: 0.7071985006332397  val: loss: 673636.1875 acc: 0.6534799337387085\n",
      "step: 36690\n",
      "train: loss: 577712.125 acc: 0.5593166351318359  val: loss: 524961.625 acc: 0.8038681745529175\n",
      "step: 36695\n",
      "train: loss: 482482.90625 acc: 0.7096725702285767  val: loss: 513256.125 acc: 0.7544435858726501\n",
      "step: 36700\n",
      "train: loss: 319658.1875 acc: 0.8655427694320679  val: loss: 1149710.875 acc: 0.6608024835586548\n",
      "step: 36705\n",
      "train: loss: 282041.78125 acc: 0.8605887293815613  val: loss: 3808455.5 acc: 0.5515095591545105\n",
      "step: 36710\n",
      "train: loss: 583833.875 acc: 0.7288894057273865  val: loss: 3660392.25 acc: 0.5070139169692993\n",
      "step: 36715\n",
      "train: loss: 382782.3125 acc: 0.8039238452911377  val: loss: 1053249.75 acc: 0.7254683971405029\n",
      "step: 36720\n",
      "train: loss: 192626.21875 acc: 0.890809953212738  val: loss: 1180270.25 acc: 0.6314327716827393\n",
      "step: 36725\n",
      "train: loss: 13295.9912109375 acc: 0.9891205430030823  val: loss: 582544.375 acc: 0.6785549521446228\n",
      "step: 36730\n",
      "train: loss: 109232.0546875 acc: 0.9100555181503296  val: loss: 2043577.875 acc: 0.6211215853691101\n",
      "step: 36735\n",
      "train: loss: 37948.5625 acc: 0.9698405861854553  val: loss: 995590.25 acc: 0.7057691812515259\n",
      "step: 36740\n",
      "train: loss: 388526.53125 acc: 0.817824125289917  val: loss: 865271.3125 acc: 0.7486008405685425\n",
      "step: 36745\n",
      "train: loss: 139848.75 acc: 0.8596045970916748  val: loss: 799277.0 acc: 0.7254089117050171\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 36750\n",
      "train: loss: 76180.0546875 acc: 0.9079892039299011  val: loss: 1210444.75 acc: 0.730976939201355\n",
      "step: 36755\n",
      "train: loss: 417302.25 acc: 0.8062475919723511  val: loss: 704570.75 acc: 0.7686581611633301\n",
      "step: 36760\n",
      "train: loss: 161392.78125 acc: 0.8868281245231628  val: loss: 1822727.75 acc: 0.6002589464187622\n",
      "step: 36765\n",
      "train: loss: 599912.1875 acc: 0.7226446270942688  val: loss: 637874.3125 acc: 0.7444949150085449\n",
      "step: 36770\n",
      "train: loss: 217681.265625 acc: 0.8354548215866089  val: loss: 1000399.5625 acc: 0.7730708122253418\n",
      "step: 36775\n",
      "train: loss: 307387.25 acc: 0.8365639448165894  val: loss: 5998517.0 acc: 0.5253669023513794\n",
      "step: 36780\n",
      "train: loss: 437803.75 acc: 0.7802333831787109  val: loss: 1302792.25 acc: 0.6959114670753479\n",
      "step: 36785\n",
      "train: loss: 1407015.125 acc: 0.8397070169448853  val: loss: 759673.0 acc: 0.8885892629623413\n",
      "step: 36790\n",
      "train: loss: 897196.0625 acc: 0.8429241180419922  val: loss: 166471.34375 acc: 0.9227979183197021\n",
      "step: 36795\n",
      "train: loss: 454889.46875 acc: 0.9645868539810181  val: loss: 1367415.5 acc: 0.7243874073028564\n",
      "step: 36800\n",
      "train: loss: 594059.4375 acc: 0.9201506972312927  val: loss: 304194.875 acc: 0.9455339312553406\n",
      "step: 36805\n",
      "train: loss: 648085.625 acc: 0.9084271192550659  val: loss: 1471626.625 acc: 0.8338947892189026\n",
      "step: 36810\n",
      "train: loss: 89293.140625 acc: 0.9810059666633606  val: loss: 2061064.75 acc: 0.8049274682998657\n",
      "step: 36815\n",
      "train: loss: 45376.84375 acc: 0.9956581592559814  val: loss: 783375.875 acc: 0.7106688022613525\n",
      "step: 36820\n",
      "train: loss: 59864.62890625 acc: 0.9954038262367249  val: loss: 2057861.25 acc: 0.6780918836593628\n",
      "step: 36825\n",
      "train: loss: 44146.578125 acc: 0.9959203004837036  val: loss: 1893265.25 acc: 0.42480212450027466\n",
      "step: 36830\n",
      "train: loss: 31499.978515625 acc: 0.9965500831604004  val: loss: 698241.0 acc: 0.8892197012901306\n",
      "step: 36835\n",
      "train: loss: 28230.712890625 acc: 0.9878165125846863  val: loss: 376467.84375 acc: 0.9375741481781006\n",
      "step: 36840\n",
      "train: loss: 43269.625 acc: 0.9865613579750061  val: loss: 1074903.125 acc: 0.6018965840339661\n",
      "step: 36845\n",
      "train: loss: 33656.58203125 acc: 0.9920628070831299  val: loss: 1754160.625 acc: 0.4360560178756714\n",
      "step: 36850\n",
      "train: loss: 14128.0859375 acc: 0.9911416172981262  val: loss: 734059.4375 acc: 0.879254937171936\n",
      "step: 36855\n",
      "train: loss: 20511.107421875 acc: 0.96343994140625  val: loss: 2214133.5 acc: 0.8276600241661072\n",
      "step: 36860\n",
      "train: loss: 56743.19140625 acc: 0.9737201929092407  val: loss: 2121571.75 acc: 0.6183160543441772\n",
      "step: 36865\n",
      "train: loss: 17901.181640625 acc: 0.9489736557006836  val: loss: 1546677.625 acc: -0.46257686614990234\n",
      "step: 36870\n",
      "train: loss: 4988.4404296875 acc: 0.9907171726226807  val: loss: 1210995.75 acc: 0.5884126424789429\n",
      "step: 36875\n",
      "train: loss: 6565.1875 acc: 0.9748585224151611  val: loss: 1039046.75 acc: 0.7270508408546448\n",
      "step: 36880\n",
      "train: loss: 3799.331787109375 acc: 0.9926083087921143  val: loss: 818378.6875 acc: 0.8473067283630371\n",
      "step: 36885\n",
      "train: loss: 33824.9140625 acc: 0.9823560118675232  val: loss: 455019.71875 acc: 0.8642050623893738\n",
      "step: 36890\n",
      "train: loss: 22532.939453125 acc: 0.9884205460548401  val: loss: 1433782.375 acc: 0.6961878538131714\n",
      "step: 36895\n",
      "train: loss: 26274.814453125 acc: 0.9831169247627258  val: loss: 557064.3125 acc: 0.4767492413520813\n",
      "step: 36900\n",
      "train: loss: 14064.3623046875 acc: 0.9899317026138306  val: loss: 1879781.0 acc: 0.7473381757736206\n",
      "step: 36905\n",
      "train: loss: 97560.59375 acc: 0.8191360235214233  val: loss: 1622090.25 acc: 0.5092730522155762\n",
      "step: 36910\n",
      "train: loss: 10633.095703125 acc: 0.9892313480377197  val: loss: 1199162.0 acc: 0.47899943590164185\n",
      "step: 36915\n",
      "train: loss: 13228.5126953125 acc: 0.9863713383674622  val: loss: 2676607.25 acc: 0.4804556369781494\n",
      "step: 36920\n",
      "train: loss: 12570.19921875 acc: 0.9953004121780396  val: loss: 1126925.5 acc: 0.7975473403930664\n",
      "step: 36925\n",
      "train: loss: 16113.4990234375 acc: 0.9950924515724182  val: loss: 782424.5 acc: 0.7406070232391357\n",
      "step: 36930\n",
      "train: loss: 42113.62109375 acc: 0.9906284809112549  val: loss: 1723101.625 acc: 0.7644492983818054\n",
      "step: 36935\n",
      "train: loss: 32485.517578125 acc: 0.9802030324935913  val: loss: 674252.125 acc: 0.7865736484527588\n",
      "step: 36940\n",
      "train: loss: 25921.912109375 acc: 0.9940332174301147  val: loss: 1965915.25 acc: 0.5863150954246521\n",
      "step: 36945\n",
      "train: loss: 80580.640625 acc: 0.9846853613853455  val: loss: 946183.8125 acc: 0.6256668567657471\n",
      "step: 36950\n",
      "train: loss: 104618.265625 acc: 0.9345217943191528  val: loss: 774377.125 acc: 0.881966233253479\n",
      "step: 36955\n",
      "train: loss: 291631.0625 acc: 0.701941967010498  val: loss: 557658.6875 acc: 0.9389774203300476\n",
      "step: 36960\n",
      "train: loss: 47034.1796875 acc: 0.9905582666397095  val: loss: 579123.75 acc: 0.8595045208930969\n",
      "step: 36965\n",
      "train: loss: 148425.109375 acc: 0.9856690764427185  val: loss: 1755676.875 acc: 0.7026490569114685\n",
      "step: 36970\n",
      "train: loss: 36369.0625 acc: 0.9962885975837708  val: loss: 2363549.0 acc: 0.37737536430358887\n",
      "step: 36975\n",
      "train: loss: 115602.734375 acc: 0.9797652363777161  val: loss: 1609928.25 acc: 0.7504531145095825\n",
      "step: 36980\n",
      "train: loss: 683208.5 acc: 0.9075720906257629  val: loss: 2556393.5 acc: 0.7158836126327515\n",
      "step: 36985\n",
      "train: loss: 403037.25 acc: 0.9803872108459473  val: loss: 630191.0 acc: 0.8493034839630127\n",
      "step: 36990\n",
      "train: loss: 336079.25 acc: 0.9658682346343994  val: loss: 2153831.75 acc: 0.792271077632904\n",
      "step: 36995\n",
      "train: loss: 212882.59375 acc: 0.9691309332847595  val: loss: 569108.625 acc: 0.7791218757629395\n",
      "step: 37000\n",
      "train: loss: 1053052.75 acc: 0.908673882484436  val: loss: 774460.375 acc: 0.8583577871322632\n",
      "step: 37005\n",
      "train: loss: 914177.125 acc: 0.9724899530410767  val: loss: 637190.0 acc: 0.882112979888916\n",
      "step: 37010\n",
      "train: loss: 1357178.0 acc: 0.9534064531326294  val: loss: 420195.71875 acc: 0.8944354057312012\n",
      "step: 37015\n",
      "train: loss: 615218.25 acc: 0.9684150218963623  val: loss: 949755.75 acc: 0.619945764541626\n",
      "step: 37020\n",
      "train: loss: 437043.125 acc: 0.9300515055656433  val: loss: 440460.59375 acc: 0.3606224060058594\n",
      "step: 37025\n",
      "train: loss: 409679.8125 acc: 0.9742240905761719  val: loss: 3937698.25 acc: -0.6672022342681885\n",
      "step: 37030\n",
      "train: loss: 225350.09375 acc: 0.9768915176391602  val: loss: 582237.375 acc: 0.7060304284095764\n",
      "step: 37035\n",
      "train: loss: 825036.25 acc: 0.7877013683319092  val: loss: 674910.9375 acc: 0.8097420930862427\n",
      "step: 37040\n",
      "train: loss: 2641645.25 acc: 0.5967631340026855  val: loss: 521942.78125 acc: 0.7516168355941772\n",
      "step: 37045\n",
      "train: loss: 1570121.75 acc: 0.33950066566467285  val: loss: 752568.125 acc: 0.5216421484947205\n",
      "step: 37050\n",
      "train: loss: 652354.625 acc: 0.6930283308029175  val: loss: 352344.125 acc: 0.8296172618865967\n",
      "step: 37055\n",
      "train: loss: 572465.1875 acc: 0.7753549814224243  val: loss: 608880.375 acc: 0.7993067502975464\n",
      "step: 37060\n",
      "train: loss: 659656.125 acc: 0.6872941255569458  val: loss: 319120.40625 acc: 0.9322912096977234\n",
      "step: 37065\n",
      "train: loss: 2182898.0 acc: 0.46474236249923706  val: loss: 1596392.375 acc: 0.6752954721450806\n",
      "step: 37070\n",
      "train: loss: 705302.5 acc: 0.6106352806091309  val: loss: 1964138.0 acc: 0.540206253528595\n",
      "step: 37075\n",
      "train: loss: 503917.4375 acc: 0.685834527015686  val: loss: 2243177.25 acc: 0.6411021947860718\n",
      "step: 37080\n",
      "train: loss: 198811.71875 acc: 0.8139776587486267  val: loss: 1061222.75 acc: 0.618087649345398\n",
      "step: 37085\n",
      "train: loss: 202720.359375 acc: 0.8655142784118652  val: loss: 2144614.75 acc: 0.6270668506622314\n",
      "step: 37090\n",
      "train: loss: 35032.66796875 acc: 0.9719552993774414  val: loss: 979082.875 acc: 0.7092856764793396\n",
      "step: 37095\n",
      "train: loss: 524031.9375 acc: 0.7981425523757935  val: loss: 4483476.5 acc: 0.6519178152084351\n",
      "step: 37100\n",
      "train: loss: 609688.25 acc: 0.7972275614738464  val: loss: 2192164.0 acc: 0.7024115324020386\n",
      "step: 37105\n",
      "train: loss: 194987.453125 acc: 0.8546584844589233  val: loss: 1302138.0 acc: 0.7210462689399719\n",
      "step: 37110\n",
      "train: loss: 80044.953125 acc: 0.9130533933639526  val: loss: 858321.6875 acc: 0.7774785757064819\n",
      "step: 37115\n",
      "train: loss: 157842.859375 acc: 0.8462072610855103  val: loss: 2765106.25 acc: 0.6842948198318481\n",
      "step: 37120\n",
      "train: loss: 179222.6875 acc: 0.830761730670929  val: loss: 481706.0625 acc: 0.670905351638794\n",
      "step: 37125\n",
      "train: loss: 569878.375 acc: 0.4995676279067993  val: loss: 2556342.5 acc: 0.6659037470817566\n",
      "step: 37130\n",
      "train: loss: 608133.4375 acc: 0.7019120454788208  val: loss: 2659382.75 acc: 0.6168053150177002\n",
      "step: 37135\n",
      "train: loss: 538820.0625 acc: 0.7528656721115112  val: loss: 1086646.875 acc: 0.7264631986618042\n",
      "step: 37140\n",
      "train: loss: 57414.55859375 acc: 0.9423402547836304  val: loss: 2687043.5 acc: 0.6055072546005249\n",
      "step: 37145\n",
      "train: loss: 673395.125 acc: 0.7546585202217102  val: loss: 4858477.0 acc: 0.5192769765853882\n",
      "step: 37150\n",
      "train: loss: 1466779.25 acc: 0.7618809938430786  val: loss: 3557113.75 acc: 0.6837527751922607\n",
      "step: 37155\n",
      "train: loss: 873761.625 acc: 0.8693810701370239  val: loss: 1025994.5 acc: 0.7919190526008606\n",
      "step: 37160\n",
      "train: loss: 808492.625 acc: 0.9016869068145752  val: loss: 2151631.75 acc: 0.6889161467552185\n",
      "step: 37165\n",
      "train: loss: 449767.46875 acc: 0.9434061646461487  val: loss: 786010.0 acc: 0.821568489074707\n",
      "step: 37170\n",
      "train: loss: 304374.15625 acc: 0.9355754256248474  val: loss: 737007.875 acc: 0.8226198554039001\n",
      "step: 37175\n",
      "train: loss: 409692.96875 acc: 0.9403952360153198  val: loss: 1533569.375 acc: 0.8801236748695374\n",
      "step: 37180\n",
      "train: loss: 183183.71875 acc: 0.9834513068199158  val: loss: 1130859.125 acc: 0.6893293261528015\n",
      "step: 37185\n",
      "train: loss: 86615.53125 acc: 0.9919190406799316  val: loss: 1090393.375 acc: 0.4705101251602173\n",
      "step: 37190\n",
      "train: loss: 89767.203125 acc: 0.992342472076416  val: loss: 2928342.5 acc: -1.1214265823364258\n",
      "step: 37195\n",
      "train: loss: 667383.9375 acc: 0.8943504095077515  val: loss: 1871889.125 acc: 0.1612849235534668\n",
      "step: 37200\n",
      "train: loss: 106049.2734375 acc: 0.9603980183601379  val: loss: 1772196.375 acc: 0.3338087201118469\n",
      "step: 37205\n",
      "train: loss: 71069.765625 acc: 0.7811462879180908  val: loss: 823550.0 acc: 0.554369330406189\n",
      "step: 37210\n",
      "train: loss: 74705.6171875 acc: 0.9607897400856018  val: loss: 775061.5 acc: 0.9237082600593567\n",
      "step: 37215\n",
      "train: loss: 13219.4716796875 acc: 0.9952192306518555  val: loss: 1551864.5 acc: 0.35281628370285034\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 37220\n",
      "train: loss: 28702.42578125 acc: 0.9626099467277527  val: loss: 298394.0 acc: 0.8949366211891174\n",
      "step: 37225\n",
      "train: loss: 23923.02734375 acc: 0.9912715554237366  val: loss: 1626727.5 acc: 0.49559444189071655\n",
      "step: 37230\n",
      "train: loss: 19453.35546875 acc: 0.8789021968841553  val: loss: 1568029.5 acc: 0.8443279266357422\n",
      "step: 37235\n",
      "train: loss: 13492.1611328125 acc: 0.9908236265182495  val: loss: 2901241.5 acc: 0.32636159658432007\n",
      "step: 37240\n",
      "train: loss: 8199.2001953125 acc: 0.9754627346992493  val: loss: 1091682.25 acc: 0.8668941259384155\n",
      "step: 37245\n",
      "train: loss: 10399.5390625 acc: 0.9829596877098083  val: loss: 622912.25 acc: 0.928221583366394\n",
      "step: 37250\n",
      "train: loss: 16104.8369140625 acc: 0.973279595375061  val: loss: 2426728.5 acc: 0.6104571223258972\n",
      "step: 37255\n",
      "train: loss: 34617.70703125 acc: 0.9858424067497253  val: loss: 936865.25 acc: 0.8062192797660828\n",
      "step: 37260\n",
      "train: loss: 18699.849609375 acc: 0.9863885045051575  val: loss: 1556377.625 acc: 0.2102522850036621\n",
      "step: 37265\n",
      "train: loss: 15053.15234375 acc: 0.9909763932228088  val: loss: 2294007.75 acc: 0.29437631368637085\n",
      "step: 37270\n",
      "train: loss: 10440.8642578125 acc: 0.993739664554596  val: loss: 789315.625 acc: 0.8511091470718384\n",
      "step: 37275\n",
      "train: loss: 10297.2861328125 acc: 0.992143988609314  val: loss: 573297.0625 acc: 0.9011562466621399\n",
      "step: 37280\n",
      "train: loss: 10608.5810546875 acc: 0.9810367822647095  val: loss: 1873299.625 acc: 0.3466467261314392\n",
      "step: 37285\n",
      "train: loss: 6828.357421875 acc: 0.9968981146812439  val: loss: 1304408.5 acc: 0.6412310004234314\n",
      "step: 37290\n",
      "train: loss: 15522.5244140625 acc: 0.9967007040977478  val: loss: 1964386.0 acc: 0.5926607251167297\n",
      "step: 37295\n",
      "train: loss: 12221.5830078125 acc: 0.995302677154541  val: loss: 1319324.25 acc: 0.8617831468582153\n",
      "step: 37300\n",
      "train: loss: 24972.533203125 acc: 0.9878149032592773  val: loss: 1004031.75 acc: 0.5251426100730896\n",
      "step: 37305\n",
      "train: loss: 28887.439453125 acc: 0.9881057143211365  val: loss: 663048.4375 acc: 0.8556186556816101\n",
      "step: 37310\n",
      "train: loss: 202920.078125 acc: 0.9633201956748962  val: loss: 850934.6875 acc: 0.8745540976524353\n",
      "step: 37315\n",
      "train: loss: 43559.84375 acc: 0.9708764553070068  val: loss: 1150342.25 acc: 0.49524664878845215\n",
      "step: 37320\n",
      "train: loss: 282515.75 acc: 0.8276534080505371  val: loss: 1155716.125 acc: 0.6542637348175049\n",
      "step: 37325\n",
      "train: loss: 406125.34375 acc: 0.8739776015281677  val: loss: 2937828.75 acc: -1.1200799942016602\n",
      "step: 37330\n",
      "train: loss: 649722.125 acc: 0.9302684664726257  val: loss: 360879.6875 acc: 0.8455155491828918\n",
      "step: 37335\n",
      "train: loss: 276344.59375 acc: 0.9344454407691956  val: loss: 709889.0625 acc: 0.872677206993103\n",
      "step: 37340\n",
      "train: loss: 439879.40625 acc: 0.9463651776313782  val: loss: 399083.6875 acc: 0.9466495513916016\n",
      "step: 37345\n",
      "train: loss: 234832.71875 acc: 0.9666687250137329  val: loss: 351099.15625 acc: 0.9558018445968628\n",
      "step: 37350\n",
      "train: loss: 1536521.25 acc: 0.9185587167739868  val: loss: 1501124.0 acc: 0.6678204536437988\n",
      "step: 37355\n",
      "train: loss: 415868.21875 acc: 0.9700121879577637  val: loss: 244577.046875 acc: 0.9713037610054016\n",
      "step: 37360\n",
      "train: loss: 230954.046875 acc: 0.9769400954246521  val: loss: 876740.375 acc: 0.7113063931465149\n",
      "step: 37365\n",
      "train: loss: 381908.3125 acc: 0.9692365527153015  val: loss: 612398.3125 acc: 0.8899503350257874\n",
      "step: 37370\n",
      "train: loss: 377367.9375 acc: 0.9744950532913208  val: loss: 844419.25 acc: 0.7431304454803467\n",
      "step: 37375\n",
      "train: loss: 673098.875 acc: 0.9769789576530457  val: loss: 1819678.75 acc: 0.6855151057243347\n",
      "step: 37380\n",
      "train: loss: 1495045.375 acc: 0.926423192024231  val: loss: 438762.15625 acc: 0.8646541237831116\n",
      "step: 37385\n",
      "train: loss: 1084468.625 acc: 0.9340049028396606  val: loss: 327414.46875 acc: 0.8948177695274353\n",
      "step: 37390\n",
      "train: loss: 571566.1875 acc: 0.9585859179496765  val: loss: 278361.21875 acc: 0.9516043663024902\n",
      "step: 37395\n",
      "train: loss: 1144963.0 acc: 0.8955278396606445  val: loss: 659875.5 acc: 0.9198331832885742\n",
      "step: 37400\n",
      "train: loss: 177202.78125 acc: 0.9657269716262817  val: loss: 454003.09375 acc: 0.919662594795227\n",
      "step: 37405\n",
      "train: loss: 3407718.75 acc: 0.4401332139968872  val: loss: 760557.3125 acc: 0.8538913726806641\n",
      "step: 37410\n",
      "train: loss: 438272.5625 acc: 0.7108258008956909  val: loss: 1991503.75 acc: 0.6929490566253662\n",
      "step: 37415\n",
      "train: loss: 500041.28125 acc: 0.812049925327301  val: loss: 1638284.125 acc: 0.773120105266571\n",
      "step: 37420\n",
      "train: loss: 740974.4375 acc: 0.7632271647453308  val: loss: 1759348.25 acc: 0.7386896014213562\n",
      "step: 37425\n",
      "train: loss: 847734.5 acc: 0.7384569644927979  val: loss: 1083875.25 acc: 0.8717628717422485\n",
      "step: 37430\n",
      "train: loss: 354640.625 acc: 0.8033018112182617  val: loss: 2882218.25 acc: 0.5963821411132812\n",
      "step: 37435\n",
      "train: loss: 86332.875 acc: 0.9237755537033081  val: loss: 4242521.5 acc: 0.5105665922164917\n",
      "step: 37440\n",
      "train: loss: 513330.71875 acc: 0.795340895652771  val: loss: 592874.0 acc: 0.6789426803588867\n",
      "step: 37445\n",
      "train: loss: 128413.65625 acc: 0.8821882009506226  val: loss: 1270764.5 acc: 0.6137386560440063\n",
      "step: 37450\n",
      "train: loss: 70243.953125 acc: 0.946260929107666  val: loss: 4744844.5 acc: 0.5298707485198975\n",
      "step: 37455\n",
      "train: loss: 125974.1484375 acc: 0.920638382434845  val: loss: 3787911.0 acc: 0.5331114530563354\n",
      "step: 37460\n",
      "train: loss: 268157.3125 acc: 0.8631014227867126  val: loss: 2748743.0 acc: 0.5427840948104858\n",
      "step: 37465\n",
      "train: loss: 179656.125 acc: 0.8917142748832703  val: loss: 2953091.0 acc: 0.5163521766662598\n",
      "step: 37470\n",
      "train: loss: 144277.765625 acc: 0.9089961051940918  val: loss: 512870.59375 acc: 0.7869858145713806\n",
      "step: 37475\n",
      "train: loss: 162624.28125 acc: 0.8509132266044617  val: loss: 4895188.5 acc: 0.5458971261978149\n",
      "step: 37480\n",
      "train: loss: 226313.0625 acc: 0.8466672897338867  val: loss: 2460259.75 acc: 0.5848211646080017\n",
      "step: 37485\n",
      "train: loss: 38284.01953125 acc: 0.9687001705169678  val: loss: 2864174.5 acc: 0.5907283425331116\n",
      "step: 37490\n",
      "train: loss: 220331.375 acc: 0.7620518207550049  val: loss: 2219988.5 acc: 0.6837604641914368\n",
      "step: 37495\n",
      "train: loss: 122508.4921875 acc: 0.9113878607749939  val: loss: 1657447.25 acc: 0.6824432611465454\n",
      "step: 37500\n",
      "train: loss: 693858.9375 acc: 0.7472724914550781  val: loss: 529134.3125 acc: 0.824755072593689\n",
      "step: 37505\n",
      "train: loss: 282360.0625 acc: 0.772865355014801  val: loss: 579353.0625 acc: 0.7108526229858398\n",
      "step: 37510\n",
      "train: loss: 513514.875 acc: 0.7715164422988892  val: loss: 1888290.75 acc: 0.6226727962493896\n",
      "step: 37515\n",
      "train: loss: 925659.6875 acc: 0.6328854560852051  val: loss: 1407619.125 acc: 0.7566661238670349\n",
      "step: 37520\n",
      "train: loss: 1068802.625 acc: 0.8443455100059509  val: loss: 1996378.75 acc: 0.7467397451400757\n",
      "step: 37525\n",
      "train: loss: 685563.1875 acc: 0.9331651329994202  val: loss: 171463.859375 acc: 0.9560338258743286\n",
      "step: 37530\n",
      "train: loss: 217586.703125 acc: 0.9803239703178406  val: loss: 943343.875 acc: 0.8858455419540405\n",
      "step: 37535\n",
      "train: loss: 771734.3125 acc: 0.9125369191169739  val: loss: 415964.96875 acc: 0.8475132584571838\n",
      "step: 37540\n",
      "train: loss: 237753.3125 acc: 0.957683801651001  val: loss: 2206120.0 acc: 0.28267890214920044\n",
      "step: 37545\n",
      "train: loss: 156046.203125 acc: 0.9818602800369263  val: loss: 1070809.75 acc: 0.8537744283676147\n",
      "step: 37550\n",
      "train: loss: 66207.234375 acc: 0.9940836429595947  val: loss: 1991731.375 acc: 0.5954433679580688\n",
      "step: 37555\n",
      "train: loss: 24024.6875 acc: 0.9982687830924988  val: loss: 1055225.25 acc: 0.7566818594932556\n",
      "step: 37560\n",
      "train: loss: 36139.2890625 acc: 0.9963945746421814  val: loss: 1963755.75 acc: 0.7693745493888855\n",
      "step: 37565\n",
      "train: loss: 31573.572265625 acc: 0.9953290820121765  val: loss: 1959456.625 acc: 0.7180923819541931\n",
      "step: 37570\n",
      "train: loss: 22889.330078125 acc: 0.9883780479431152  val: loss: 606798.75 acc: 0.8803372383117676\n",
      "step: 37575\n",
      "train: loss: 23200.7421875 acc: 0.9952071309089661  val: loss: 1805617.25 acc: 0.611816942691803\n",
      "step: 37580\n",
      "train: loss: 10691.591796875 acc: 0.9958215951919556  val: loss: 982641.6875 acc: 0.7797532081604004\n",
      "step: 37585\n",
      "train: loss: 7839.68701171875 acc: 0.984478235244751  val: loss: 451340.0625 acc: 0.911217212677002\n",
      "step: 37590\n",
      "train: loss: 14012.783203125 acc: 0.9906216859817505  val: loss: 789760.4375 acc: 0.8811386227607727\n",
      "step: 37595\n",
      "train: loss: 7138.88330078125 acc: 0.9862790703773499  val: loss: 1841326.75 acc: 0.17037194967269897\n",
      "step: 37600\n",
      "train: loss: 5837.87841796875 acc: 0.980478823184967  val: loss: 874271.8125 acc: 0.9270480275154114\n",
      "step: 37605\n",
      "train: loss: 10156.923828125 acc: 0.983362078666687  val: loss: 1239871.0 acc: 0.6103280782699585\n",
      "step: 37610\n",
      "train: loss: 26436.357421875 acc: 0.9668281674385071  val: loss: 829564.625 acc: 0.7789899706840515\n",
      "step: 37615\n",
      "train: loss: 18415.5390625 acc: 0.9651463031768799  val: loss: 346439.0625 acc: 0.9174675941467285\n",
      "step: 37620\n",
      "train: loss: 36834.36328125 acc: 0.9880436658859253  val: loss: 1982260.375 acc: 0.5447051525115967\n",
      "step: 37625\n",
      "train: loss: 16723.66015625 acc: 0.9844834208488464  val: loss: 1228703.25 acc: 0.6488885879516602\n",
      "step: 37630\n",
      "train: loss: 7202.22509765625 acc: 0.9955083727836609  val: loss: 1387968.125 acc: 0.38496142625808716\n",
      "step: 37635\n",
      "train: loss: 49386.24609375 acc: 0.9743901491165161  val: loss: 52346.48046875 acc: 0.9901733994483948\n",
      "step: 37640\n",
      "train: loss: 43066.72265625 acc: 0.9800773859024048  val: loss: 979377.3125 acc: 0.8385010957717896\n",
      "step: 37645\n",
      "train: loss: 9151.0458984375 acc: 0.9649801254272461  val: loss: 724035.8125 acc: 0.7936407923698425\n",
      "step: 37650\n",
      "train: loss: 11207.65625 acc: 0.9937909841537476  val: loss: 456084.09375 acc: 0.9107636213302612\n",
      "step: 37655\n",
      "train: loss: 29391.951171875 acc: 0.9928110241889954  val: loss: 1402226.25 acc: 0.5111511945724487\n",
      "step: 37660\n",
      "train: loss: 16933.12890625 acc: 0.9938749074935913  val: loss: 115707.3671875 acc: 0.9701380729675293\n",
      "step: 37665\n",
      "train: loss: 9776.1552734375 acc: 0.9949173331260681  val: loss: 713182.0625 acc: 0.8571180701255798\n",
      "step: 37670\n",
      "train: loss: 22306.92578125 acc: 0.9873462319374084  val: loss: 268461.28125 acc: 0.910064160823822\n",
      "step: 37675\n",
      "train: loss: 72955.625 acc: 0.9797914624214172  val: loss: 514814.84375 acc: 0.9304561018943787\n",
      "step: 37680\n",
      "train: loss: 72696.265625 acc: 0.9755124449729919  val: loss: 136412.015625 acc: 0.9625787138938904\n",
      "step: 37685\n",
      "train: loss: 197074.703125 acc: 0.9287652969360352  val: loss: 275878.75 acc: 0.9059723615646362\n",
      "step: 37690\n",
      "train: loss: 60795.8671875 acc: 0.9927049875259399  val: loss: 828129.75 acc: 0.7202432155609131\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 37695\n",
      "train: loss: 739391.125 acc: 0.8931971788406372  val: loss: 1569897.75 acc: 0.7424056529998779\n",
      "step: 37700\n",
      "train: loss: 166900.515625 acc: 0.9716037511825562  val: loss: 1323894.625 acc: 0.549467921257019\n",
      "step: 37705\n",
      "train: loss: 83086.421875 acc: 0.9877278208732605  val: loss: 429945.96875 acc: 0.8542540073394775\n",
      "step: 37710\n",
      "train: loss: 278527.96875 acc: 0.9568153619766235  val: loss: 186200.796875 acc: 0.9228748083114624\n",
      "step: 37715\n",
      "train: loss: 463409.5 acc: 0.9589246511459351  val: loss: 434973.03125 acc: 0.9137921333312988\n",
      "step: 37720\n",
      "train: loss: 338815.90625 acc: 0.9725073575973511  val: loss: 2592531.25 acc: 0.039942383766174316\n",
      "step: 37725\n",
      "train: loss: 199528.015625 acc: 0.9766809940338135  val: loss: 956659.9375 acc: 0.9027870297431946\n",
      "step: 37730\n",
      "train: loss: 186950.0625 acc: 0.9865143895149231  val: loss: 892617.3125 acc: 0.8825428485870361\n",
      "step: 37735\n",
      "train: loss: 351182.1875 acc: 0.9781269431114197  val: loss: 736428.75 acc: 0.8072392344474792\n",
      "step: 37740\n",
      "train: loss: 916789.8125 acc: 0.9774041175842285  val: loss: 726751.6875 acc: 0.9092238545417786\n",
      "step: 37745\n",
      "train: loss: 916810.875 acc: 0.9668677449226379  val: loss: 522563.0625 acc: 0.9189367294311523\n",
      "step: 37750\n",
      "train: loss: 1097507.25 acc: 0.9385336637496948  val: loss: 649790.3125 acc: 0.9291161298751831\n",
      "step: 37755\n",
      "train: loss: 381740.0625 acc: 0.9600566029548645  val: loss: 968460.25 acc: 0.8787044286727905\n",
      "step: 37760\n",
      "train: loss: 239093.515625 acc: 0.9643723368644714  val: loss: 1197212.625 acc: 0.8137475848197937\n",
      "step: 37765\n",
      "train: loss: 178014.765625 acc: 0.9365683197975159  val: loss: 232131.5 acc: 0.900541365146637\n",
      "step: 37770\n",
      "train: loss: 1564894.375 acc: 0.7787560224533081  val: loss: 1722018.875 acc: 0.8270687460899353\n",
      "step: 37775\n",
      "train: loss: 458650.34375 acc: 0.8069114089012146  val: loss: 3727832.25 acc: 0.5714117884635925\n",
      "step: 37780\n",
      "train: loss: 310145.03125 acc: 0.84360671043396  val: loss: 2941808.0 acc: 0.630850076675415\n",
      "step: 37785\n",
      "train: loss: 426897.40625 acc: 0.834563136100769  val: loss: 470953.84375 acc: 0.8846143484115601\n",
      "step: 37790\n",
      "train: loss: 960343.0625 acc: 0.6322319507598877  val: loss: 2452630.75 acc: 0.7396891713142395\n",
      "step: 37795\n",
      "train: loss: 1111662.125 acc: 0.7353411912918091  val: loss: 2087924.5 acc: 0.5845824480056763\n",
      "step: 37800\n",
      "train: loss: 142841.765625 acc: 0.8898727893829346  val: loss: 4181649.5 acc: 0.5045630931854248\n",
      "step: 37805\n",
      "train: loss: 76480.71875 acc: 0.9433269500732422  val: loss: 569421.5625 acc: 0.7491875886917114\n",
      "step: 37810\n",
      "train: loss: 155481.078125 acc: 0.8772246837615967  val: loss: 3652463.25 acc: 0.5524961948394775\n",
      "step: 37815\n",
      "train: loss: 481856.46875 acc: 0.7867902517318726  val: loss: 1980704.75 acc: 0.6598788499832153\n",
      "step: 37820\n",
      "train: loss: 64067.5390625 acc: 0.9547931551933289  val: loss: 2776399.5 acc: 0.5560818910598755\n",
      "step: 37825\n",
      "train: loss: 21276.611328125 acc: 0.9831889271736145  val: loss: 1828692.125 acc: 0.6378148794174194\n",
      "step: 37830\n",
      "train: loss: 273625.03125 acc: 0.8475854396820068  val: loss: 1263108.5 acc: 0.6685114502906799\n",
      "step: 37835\n",
      "train: loss: 111034.0703125 acc: 0.9190834760665894  val: loss: 227061.671875 acc: 0.8818321228027344\n",
      "step: 37840\n",
      "train: loss: 70898.1796875 acc: 0.9315905570983887  val: loss: 910043.3125 acc: 0.7364969253540039\n",
      "step: 37845\n",
      "train: loss: 67079.3984375 acc: 0.9143686294555664  val: loss: 674690.125 acc: 0.7164182662963867\n",
      "step: 37850\n",
      "train: loss: 183461.28125 acc: 0.8849713802337646  val: loss: 740540.125 acc: 0.7267143130302429\n",
      "step: 37855\n",
      "train: loss: 803851.5625 acc: 0.7173150181770325  val: loss: 511808.96875 acc: 0.7803130149841309\n",
      "step: 37860\n",
      "train: loss: 147153.1875 acc: 0.8591924905776978  val: loss: 2254045.25 acc: 0.6247940063476562\n",
      "step: 37865\n",
      "train: loss: 377292.90625 acc: 0.7849911451339722  val: loss: 286076.9375 acc: 0.8202390670776367\n",
      "step: 37870\n",
      "train: loss: 180725.96875 acc: 0.8750826120376587  val: loss: 3394656.25 acc: 0.5698331594467163\n",
      "step: 37875\n",
      "train: loss: 104855.4765625 acc: 0.9168976545333862  val: loss: 5991357.5 acc: 0.5661177635192871\n",
      "step: 37880\n",
      "train: loss: 789916.6875 acc: 0.7446897029876709  val: loss: 1197300.25 acc: 0.7176375389099121\n",
      "step: 37885\n",
      "train: loss: 663172.1875 acc: 0.908656656742096  val: loss: 910800.1875 acc: 0.8752198219299316\n",
      "step: 37890\n",
      "train: loss: 694731.9375 acc: 0.9292135238647461  val: loss: 770695.0625 acc: 0.7398636341094971\n",
      "step: 37895\n",
      "train: loss: 73100.8046875 acc: 0.9948084354400635  val: loss: 1008245.9375 acc: 0.8299952745437622\n",
      "step: 37900\n",
      "train: loss: 646808.125 acc: 0.9120292663574219  val: loss: 480379.34375 acc: 0.9548764228820801\n",
      "step: 37905\n",
      "train: loss: 61901.203125 acc: 0.9861416220664978  val: loss: 2821148.75 acc: 0.5844696760177612\n",
      "step: 37910\n",
      "train: loss: 71685.8515625 acc: 0.9908109307289124  val: loss: 297611.4375 acc: 0.9399800300598145\n",
      "step: 37915\n",
      "train: loss: 76692.078125 acc: 0.9920442700386047  val: loss: 936450.5625 acc: 0.8343294858932495\n",
      "step: 37920\n",
      "train: loss: 58227.78125 acc: 0.9956619143486023  val: loss: 1449355.75 acc: 0.6188252568244934\n",
      "step: 37925\n",
      "train: loss: 15001.91015625 acc: 0.9987090229988098  val: loss: 1249531.25 acc: 0.8622512817382812\n",
      "step: 37930\n",
      "train: loss: 45313.2109375 acc: 0.9930136203765869  val: loss: 610010.9375 acc: 0.7178539037704468\n",
      "step: 37935\n",
      "train: loss: 28058.91796875 acc: 0.9902619123458862  val: loss: 1758118.5 acc: -0.05622506141662598\n",
      "step: 37940\n",
      "train: loss: 10856.0537109375 acc: 0.9907898306846619  val: loss: 558494.4375 acc: 0.8272920846939087\n",
      "step: 37945\n",
      "train: loss: 48090.00390625 acc: 0.9450962543487549  val: loss: 1233647.5 acc: 0.8680046796798706\n",
      "step: 37950\n",
      "train: loss: 2294.85791015625 acc: 0.9922694563865662  val: loss: 1676786.375 acc: 0.20719200372695923\n",
      "step: 37955\n",
      "train: loss: 7638.27099609375 acc: 0.9675417542457581  val: loss: 702555.9375 acc: 0.7633663415908813\n",
      "step: 37960\n",
      "train: loss: 9643.755859375 acc: 0.9879381656646729  val: loss: 1007077.1875 acc: 0.8538092970848083\n",
      "step: 37965\n",
      "train: loss: 7113.771484375 acc: 0.9805303812026978  val: loss: 178622.0625 acc: 0.9683590531349182\n",
      "step: 37970\n",
      "train: loss: 10641.8828125 acc: 0.97742760181427  val: loss: 665218.0625 acc: 0.5122185945510864\n",
      "step: 37975\n",
      "train: loss: 10801.8037109375 acc: 0.9875423312187195  val: loss: 1105364.125 acc: 0.012739837169647217\n",
      "step: 37980\n",
      "train: loss: 14748.220703125 acc: 0.983624279499054  val: loss: 668144.8125 acc: 0.7867711186408997\n",
      "step: 37985\n",
      "train: loss: 12570.048828125 acc: 0.9870750904083252  val: loss: 665730.125 acc: 0.7848454117774963\n",
      "step: 37990\n",
      "train: loss: 33269.41015625 acc: 0.9864141345024109  val: loss: 842660.8125 acc: 0.7606949806213379\n",
      "step: 37995\n",
      "train: loss: 92565.890625 acc: 0.9395170211791992  val: loss: 381521.65625 acc: 0.7937712669372559\n",
      "step: 38000\n",
      "train: loss: 7354.7744140625 acc: 0.9959253668785095  val: loss: 568516.625 acc: 0.9426041841506958\n",
      "step: 38005\n",
      "train: loss: 8874.31640625 acc: 0.9902180433273315  val: loss: 1466817.25 acc: 0.4216853976249695\n",
      "step: 38010\n",
      "train: loss: 7587.3134765625 acc: 0.9838870167732239  val: loss: 474303.6875 acc: 0.8917011022567749\n",
      "step: 38015\n",
      "train: loss: 12255.34375 acc: 0.994820237159729  val: loss: 1558417.0 acc: 0.5187513828277588\n",
      "step: 38020\n",
      "train: loss: 24836.28125 acc: 0.9924103617668152  val: loss: 427523.96875 acc: 0.8034326434135437\n",
      "step: 38025\n",
      "train: loss: 21833.322265625 acc: 0.9950008392333984  val: loss: 1260542.0 acc: 0.8118495941162109\n",
      "step: 38030\n",
      "train: loss: 22909.396484375 acc: 0.9919832348823547  val: loss: 784516.1875 acc: 0.7858781814575195\n",
      "step: 38035\n",
      "train: loss: 13347.876953125 acc: 0.9956385493278503  val: loss: 307334.5 acc: 0.9657589197158813\n",
      "step: 38040\n",
      "train: loss: 66535.3203125 acc: 0.9775800704956055  val: loss: 1298587.625 acc: 0.9059934616088867\n",
      "step: 38045\n",
      "train: loss: 291285.0 acc: 0.8500214219093323  val: loss: 452997.0625 acc: 0.9486566185951233\n",
      "step: 38050\n",
      "train: loss: 9376.6083984375 acc: 0.9905018210411072  val: loss: 287246.03125 acc: 0.9119989275932312\n",
      "step: 38055\n",
      "train: loss: 665362.3125 acc: 0.7203831672668457  val: loss: 1062724.0 acc: 0.9079774618148804\n",
      "step: 38060\n",
      "train: loss: 235214.421875 acc: 0.9752159714698792  val: loss: 939620.9375 acc: 0.7475925087928772\n",
      "step: 38065\n",
      "train: loss: 172011.296875 acc: 0.9805189967155457  val: loss: 385793.0 acc: 0.9566170573234558\n",
      "step: 38070\n",
      "train: loss: 65693.484375 acc: 0.9929338097572327  val: loss: 934793.8125 acc: 0.8689215183258057\n",
      "step: 38075\n",
      "train: loss: 241928.84375 acc: 0.9642024040222168  val: loss: 921005.3125 acc: 0.7963306903839111\n",
      "step: 38080\n",
      "train: loss: 307151.0625 acc: 0.9491061568260193  val: loss: 4058041.25 acc: -0.17295575141906738\n",
      "step: 38085\n",
      "train: loss: 598536.1875 acc: 0.9511401653289795  val: loss: 1920123.5 acc: 0.19582605361938477\n",
      "step: 38090\n",
      "train: loss: 106683.421875 acc: 0.97342449426651  val: loss: 1452005.25 acc: 0.6215188503265381\n",
      "step: 38095\n",
      "train: loss: 149825.609375 acc: 0.9863445162773132  val: loss: 1283813.75 acc: 0.5795153379440308\n",
      "step: 38100\n",
      "train: loss: 486470.84375 acc: 0.9815411567687988  val: loss: 1005615.25 acc: 0.8475778698921204\n",
      "step: 38105\n",
      "train: loss: 3010868.25 acc: 0.922535240650177  val: loss: 1466816.75 acc: 0.6934431791305542\n",
      "step: 38110\n",
      "train: loss: 1337106.875 acc: 0.9449551701545715  val: loss: 1350859.5 acc: -0.3267616033554077\n",
      "step: 38115\n",
      "train: loss: 1814482.5 acc: 0.8825628757476807  val: loss: 558117.8125 acc: 0.8962292671203613\n",
      "step: 38120\n",
      "train: loss: 273855.25 acc: 0.9762567281723022  val: loss: 1305208.375 acc: 0.8465710878372192\n",
      "step: 38125\n",
      "train: loss: 794193.25 acc: 0.8991599678993225  val: loss: 1938658.375 acc: 0.33487778902053833\n",
      "step: 38130\n",
      "train: loss: 576658.6875 acc: 0.9057921171188354  val: loss: 427141.0625 acc: 0.9300302863121033\n",
      "step: 38135\n",
      "train: loss: 2194036.75 acc: 0.3104495406150818  val: loss: 1620768.375 acc: 0.7245008945465088\n",
      "step: 38140\n",
      "train: loss: 1372241.5 acc: 0.7404934167861938  val: loss: 936119.625 acc: 0.7421049475669861\n",
      "step: 38145\n",
      "train: loss: 753979.8125 acc: 0.7242415547370911  val: loss: 1039452.75 acc: 0.7340133190155029\n",
      "step: 38150\n",
      "train: loss: 1139456.75 acc: 0.7717177271842957  val: loss: 802979.0 acc: 0.789508581161499\n",
      "step: 38155\n",
      "train: loss: 334845.34375 acc: 0.8887666463851929  val: loss: 307275.71875 acc: 0.8925096988677979\n",
      "step: 38160\n",
      "train: loss: 819734.3125 acc: 0.7564994096755981  val: loss: 758596.625 acc: 0.6910816431045532\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 38165\n",
      "train: loss: 271471.84375 acc: 0.8641334772109985  val: loss: 5377641.5 acc: 0.5462799072265625\n",
      "step: 38170\n",
      "train: loss: 146731.21875 acc: 0.9145798683166504  val: loss: 746977.6875 acc: 0.7243003845214844\n",
      "step: 38175\n",
      "train: loss: 82732.09375 acc: 0.9384686350822449  val: loss: 947049.8125 acc: 0.7495934367179871\n",
      "step: 38180\n",
      "train: loss: 47685.66796875 acc: 0.958241879940033  val: loss: 1787838.375 acc: 0.5795718431472778\n",
      "step: 38185\n",
      "train: loss: 58938.07421875 acc: 0.9538891315460205  val: loss: 546956.9375 acc: 0.7071328163146973\n",
      "step: 38190\n",
      "train: loss: 48232.9296875 acc: 0.9656376242637634  val: loss: 1228677.125 acc: 0.6819103956222534\n",
      "step: 38195\n",
      "train: loss: 64028.23828125 acc: 0.9506150484085083  val: loss: 1712558.25 acc: 0.6796774864196777\n",
      "step: 38200\n",
      "train: loss: 27567.5 acc: 0.9769842028617859  val: loss: 2033650.75 acc: 0.6308217644691467\n",
      "step: 38205\n",
      "train: loss: 206435.234375 acc: 0.8666384816169739  val: loss: 1932447.0 acc: 0.6473186612129211\n",
      "step: 38210\n",
      "train: loss: 154808.328125 acc: 0.8775262832641602  val: loss: 1399542.75 acc: 0.6610071063041687\n",
      "step: 38215\n",
      "train: loss: 317146.15625 acc: 0.7872024178504944  val: loss: 822949.1875 acc: 0.7345885634422302\n",
      "step: 38220\n",
      "train: loss: 198407.796875 acc: 0.790530800819397  val: loss: 2472245.5 acc: 0.6826690435409546\n",
      "step: 38225\n",
      "train: loss: 284911.875 acc: 0.8305606245994568  val: loss: 1058915.5 acc: 0.7108352780342102\n",
      "step: 38230\n",
      "train: loss: 414176.53125 acc: 0.7666075825691223  val: loss: 1301368.25 acc: 0.6435781717300415\n",
      "step: 38235\n",
      "train: loss: 189936.4375 acc: 0.868536114692688  val: loss: 3131978.0 acc: 0.6087706089019775\n",
      "step: 38240\n",
      "train: loss: 739961.5625 acc: 0.6278643012046814  val: loss: 900733.0625 acc: 0.6838545799255371\n",
      "step: 38245\n",
      "train: loss: 576838.0625 acc: 0.8104070425033569  val: loss: 1113867.25 acc: 0.7628422379493713\n",
      "step: 38250\n",
      "train: loss: 516674.75 acc: 0.8712570667266846  val: loss: 397167.21875 acc: 0.6602078676223755\n",
      "step: 38255\n",
      "train: loss: 832380.6875 acc: 0.9001802802085876  val: loss: 192997.921875 acc: 0.9645617008209229\n",
      "step: 38260\n",
      "train: loss: 225634.609375 acc: 0.9742887020111084  val: loss: 749257.8125 acc: 0.79659503698349\n",
      "step: 38265\n",
      "train: loss: 211065.4375 acc: 0.9786007404327393  val: loss: 152934.046875 acc: 0.9404351711273193\n",
      "step: 38270\n",
      "train: loss: 158072.40625 acc: 0.9677754640579224  val: loss: 197079.921875 acc: 0.958392858505249\n",
      "step: 38275\n",
      "train: loss: 131906.203125 acc: 0.979289174079895  val: loss: 1058595.125 acc: 0.7517701387405396\n",
      "step: 38280\n",
      "train: loss: 55825.58984375 acc: 0.9948946833610535  val: loss: 304347.375 acc: 0.8367447853088379\n",
      "step: 38285\n",
      "train: loss: 69716.78125 acc: 0.9952554702758789  val: loss: 940734.0 acc: 0.5290910005569458\n",
      "step: 38290\n",
      "train: loss: 55919.90234375 acc: 0.9949403405189514  val: loss: 278360.28125 acc: 0.956994891166687\n",
      "step: 38295\n",
      "train: loss: 57008.7578125 acc: 0.9927531480789185  val: loss: 105722.4140625 acc: 0.9832286238670349\n",
      "step: 38300\n",
      "train: loss: 13972.6025390625 acc: 0.9884557723999023  val: loss: 1096246.375 acc: 0.7503497004508972\n",
      "step: 38305\n",
      "train: loss: 15637.876953125 acc: 0.9958376288414001  val: loss: 909244.0625 acc: 0.8416359424591064\n",
      "step: 38310\n",
      "train: loss: 10533.966796875 acc: 0.9947831630706787  val: loss: 1256170.5 acc: 0.39001065492630005\n",
      "step: 38315\n",
      "train: loss: 12120.1796875 acc: 0.986476480960846  val: loss: 892133.0625 acc: 0.8651137948036194\n",
      "step: 38320\n",
      "train: loss: 34523.67578125 acc: 0.9814389944076538  val: loss: 492369.75 acc: 0.8003162145614624\n",
      "step: 38325\n",
      "train: loss: 21474.04296875 acc: 0.9665945172309875  val: loss: 416223.65625 acc: 0.9157716631889343\n",
      "step: 38330\n",
      "train: loss: 4387.36669921875 acc: 0.9832972288131714  val: loss: 413575.1875 acc: 0.8252216577529907\n",
      "step: 38335\n",
      "train: loss: 6459.62109375 acc: 0.9907003045082092  val: loss: 1013298.3125 acc: 0.8607909679412842\n",
      "step: 38340\n",
      "train: loss: 14411.9990234375 acc: 0.9717284440994263  val: loss: 971509.5625 acc: 0.9344978332519531\n",
      "step: 38345\n",
      "train: loss: 8549.5830078125 acc: 0.9856084585189819  val: loss: 802181.75 acc: 0.7910927534103394\n",
      "step: 38350\n",
      "train: loss: 11319.5283203125 acc: 0.9932569265365601  val: loss: 178047.046875 acc: 0.9789862036705017\n",
      "step: 38355\n",
      "train: loss: 29177.314453125 acc: 0.989452064037323  val: loss: 648792.375 acc: 0.9030370116233826\n",
      "step: 38360\n",
      "train: loss: 88168.9765625 acc: 0.9395528435707092  val: loss: 1806325.5 acc: 0.8836488127708435\n",
      "step: 38365\n",
      "train: loss: 11734.5634765625 acc: 0.991783082485199  val: loss: 533922.3125 acc: 0.8904469013214111\n",
      "step: 38370\n",
      "train: loss: 18581.025390625 acc: 0.9912031888961792  val: loss: 2397505.25 acc: 0.6027150750160217\n",
      "step: 38375\n",
      "train: loss: 6065.1640625 acc: 0.9927909970283508  val: loss: 2015779.75 acc: 0.7408647537231445\n",
      "step: 38380\n",
      "train: loss: 15497.310546875 acc: 0.9913672208786011  val: loss: 1465737.875 acc: 0.5139324069023132\n",
      "step: 38385\n",
      "train: loss: 8045.3876953125 acc: 0.9976123571395874  val: loss: 956055.0 acc: 0.8911896347999573\n",
      "step: 38390\n",
      "train: loss: 35840.1640625 acc: 0.9856597781181335  val: loss: 858394.875 acc: 0.8188568353652954\n",
      "step: 38395\n",
      "train: loss: 25678.923828125 acc: 0.9928279519081116  val: loss: 1650259.5 acc: 0.17328017950057983\n",
      "step: 38400\n",
      "train: loss: 21742.783203125 acc: 0.9922524690628052  val: loss: 1749546.375 acc: 0.8291398286819458\n",
      "step: 38405\n",
      "train: loss: 39102.546875 acc: 0.9874905943870544  val: loss: 1245225.5 acc: 0.70842444896698\n",
      "step: 38410\n",
      "train: loss: 107931.7734375 acc: 0.9672208428382874  val: loss: 2121063.5 acc: 0.8103529214859009\n",
      "step: 38415\n",
      "train: loss: 160500.625 acc: 0.8927928805351257  val: loss: 1532833.125 acc: 0.5760062336921692\n",
      "step: 38420\n",
      "train: loss: 220606.15625 acc: 0.9586356282234192  val: loss: 1056880.875 acc: 0.9182882905006409\n",
      "step: 38425\n",
      "train: loss: 164373.609375 acc: 0.9749298691749573  val: loss: 1709325.875 acc: 0.8144073486328125\n",
      "step: 38430\n",
      "train: loss: 282865.34375 acc: 0.9687245488166809  val: loss: 874291.375 acc: 0.7932310104370117\n",
      "step: 38435\n",
      "train: loss: 40385.27734375 acc: 0.9947096705436707  val: loss: 2021176.0 acc: -0.26176679134368896\n",
      "step: 38440\n",
      "train: loss: 110793.4296875 acc: 0.9857982993125916  val: loss: 854318.0 acc: 0.7979936599731445\n",
      "step: 38445\n",
      "train: loss: 300510.84375 acc: 0.9775630831718445  val: loss: 1629870.75 acc: 0.536677360534668\n",
      "step: 38450\n",
      "train: loss: 360113.75 acc: 0.978281557559967  val: loss: 1676698.25 acc: 0.5155600905418396\n",
      "step: 38455\n",
      "train: loss: 268108.40625 acc: 0.9796984791755676  val: loss: 1928243.5 acc: 0.8114851117134094\n",
      "step: 38460\n",
      "train: loss: 199486.75 acc: 0.9759800434112549  val: loss: 864224.9375 acc: 0.8528746366500854\n",
      "step: 38465\n",
      "train: loss: 998986.75 acc: 0.9603952765464783  val: loss: 296197.78125 acc: 0.9217038154602051\n",
      "step: 38470\n",
      "train: loss: 1500433.625 acc: 0.9598330855369568  val: loss: 1769240.25 acc: 0.3364598751068115\n",
      "step: 38475\n",
      "train: loss: 665753.5 acc: 0.9799998998641968  val: loss: 595068.1875 acc: 0.8907013535499573\n",
      "step: 38480\n",
      "train: loss: 928871.625 acc: 0.9462617039680481  val: loss: 425621.6875 acc: 0.9430387020111084\n",
      "step: 38485\n",
      "train: loss: 307324.4375 acc: 0.9656592607498169  val: loss: 2001182.25 acc: 0.12284737825393677\n",
      "step: 38490\n",
      "train: loss: 791711.1875 acc: 0.8822831511497498  val: loss: 525495.8125 acc: 0.9212388396263123\n",
      "step: 38495\n",
      "train: loss: 220785.84375 acc: 0.975227415561676  val: loss: 585513.0625 acc: 0.7543473243713379\n",
      "step: 38500\n",
      "train: loss: 1284850.0 acc: 0.6948793530464172  val: loss: 1362017.75 acc: 0.5059370994567871\n",
      "step: 38505\n",
      "train: loss: 1190420.75 acc: 0.6903610229492188  val: loss: 3345486.75 acc: 0.6924325227737427\n",
      "step: 38510\n",
      "train: loss: 581384.75 acc: 0.79482102394104  val: loss: 2795164.0 acc: 0.6026697158813477\n",
      "step: 38515\n",
      "train: loss: 222618.015625 acc: 0.8405101299285889  val: loss: 1406854.125 acc: 0.7169879674911499\n",
      "step: 38520\n",
      "train: loss: 427709.0 acc: 0.8375619053840637  val: loss: 1168999.625 acc: 0.8248233795166016\n",
      "step: 38525\n",
      "train: loss: 831920.9375 acc: 0.6840355396270752  val: loss: 988271.0 acc: 0.7395875453948975\n",
      "step: 38530\n",
      "train: loss: 169114.65625 acc: 0.8954917788505554  val: loss: 329984.0 acc: 0.7796201705932617\n",
      "step: 38535\n",
      "train: loss: 551294.125 acc: 0.7671156525611877  val: loss: 2913089.5 acc: 0.591572105884552\n",
      "step: 38540\n",
      "train: loss: 101033.90625 acc: 0.9236816167831421  val: loss: 1538062.75 acc: 0.6166127920150757\n",
      "step: 38545\n",
      "train: loss: 148566.328125 acc: 0.885674238204956  val: loss: 1865174.875 acc: 0.6525847315788269\n",
      "step: 38550\n",
      "train: loss: 20625.283203125 acc: 0.9804038405418396  val: loss: 2298360.25 acc: 0.6269343495368958\n",
      "step: 38555\n",
      "train: loss: 47949.33984375 acc: 0.9646151065826416  val: loss: 573872.8125 acc: 0.7007300853729248\n",
      "step: 38560\n",
      "train: loss: 72746.765625 acc: 0.9459684491157532  val: loss: 611769.5625 acc: 0.7072768211364746\n",
      "step: 38565\n",
      "train: loss: 73843.34375 acc: 0.9451223015785217  val: loss: 1839962.125 acc: 0.6221762895584106\n",
      "step: 38570\n",
      "train: loss: 249151.9375 acc: 0.8436998128890991  val: loss: 316369.40625 acc: 0.7712200284004211\n",
      "step: 38575\n",
      "train: loss: 262072.875 acc: 0.8305500149726868  val: loss: 2495699.75 acc: 0.6276494264602661\n",
      "step: 38580\n",
      "train: loss: 127994.1171875 acc: 0.9034914374351501  val: loss: 483933.875 acc: 0.7202096581459045\n",
      "step: 38585\n",
      "train: loss: 638445.1875 acc: 0.7819836139678955  val: loss: 787030.25 acc: 0.7422440052032471\n",
      "step: 38590\n",
      "train: loss: 52388.99609375 acc: 0.9575373530387878  val: loss: 4251063.5 acc: 0.5199381113052368\n",
      "step: 38595\n",
      "train: loss: 257340.015625 acc: 0.7774708867073059  val: loss: 2184563.25 acc: 0.6230352520942688\n",
      "step: 38600\n",
      "train: loss: 296823.53125 acc: 0.8044931292533875  val: loss: 589098.6875 acc: 0.7418169379234314\n",
      "step: 38605\n",
      "train: loss: 104058.8671875 acc: 0.9249919652938843  val: loss: 902478.4375 acc: 0.6562855243682861\n",
      "step: 38610\n",
      "train: loss: 487656.21875 acc: 0.8240772485733032  val: loss: 1737249.25 acc: 0.7421050667762756\n",
      "step: 38615\n",
      "train: loss: 910572.1875 acc: 0.84254390001297  val: loss: 189173.75 acc: 0.9698917865753174\n",
      "step: 38620\n",
      "train: loss: 869919.8125 acc: 0.8716355562210083  val: loss: 290744.0625 acc: 0.9246903657913208\n",
      "step: 38625\n",
      "train: loss: 126208.5703125 acc: 0.9896215796470642  val: loss: 685656.5 acc: 0.7227832078933716\n",
      "step: 38630\n",
      "train: loss: 435776.90625 acc: 0.9327889680862427  val: loss: 481053.5 acc: 0.8682688474655151\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 38635\n",
      "train: loss: 44066.359375 acc: 0.9917922616004944  val: loss: 287592.71875 acc: 0.9200292825698853\n",
      "step: 38640\n",
      "train: loss: 90716.4921875 acc: 0.9870829582214355  val: loss: 1294881.375 acc: 0.8221635818481445\n",
      "step: 38645\n",
      "train: loss: 87793.9453125 acc: 0.992116391658783  val: loss: 811818.375 acc: 0.8877279162406921\n",
      "step: 38650\n",
      "train: loss: 56751.21484375 acc: 0.9932077527046204  val: loss: 1405382.125 acc: 0.8855201005935669\n",
      "step: 38655\n",
      "train: loss: 53852.90234375 acc: 0.9934336543083191  val: loss: 1557526.625 acc: 0.7505315542221069\n",
      "step: 38660\n",
      "train: loss: 26842.87890625 acc: 0.9959565997123718  val: loss: 598240.625 acc: 0.6889142990112305\n",
      "step: 38665\n",
      "train: loss: 30044.0625 acc: 0.993614912033081  val: loss: 910141.0625 acc: 0.8975233435630798\n",
      "step: 38670\n",
      "train: loss: 16284.990234375 acc: 0.9958581328392029  val: loss: 506755.8125 acc: 0.9110678434371948\n",
      "step: 38675\n",
      "train: loss: 10417.8056640625 acc: 0.9937348365783691  val: loss: 1658030.625 acc: 0.5316469669342041\n",
      "step: 38680\n",
      "train: loss: 26272.763671875 acc: 0.9617711305618286  val: loss: 800931.4375 acc: 0.9165666103363037\n",
      "step: 38685\n",
      "train: loss: 13083.4267578125 acc: 0.992401123046875  val: loss: 1891770.875 acc: 0.8580220341682434\n",
      "step: 38690\n",
      "train: loss: 16547.4453125 acc: 0.9812095165252686  val: loss: 501882.125 acc: 0.8112125396728516\n",
      "step: 38695\n",
      "train: loss: 248711.78125 acc: 0.8245453834533691  val: loss: 1674670.375 acc: 0.878593385219574\n",
      "step: 38700\n",
      "train: loss: 3526.168701171875 acc: 0.9974532127380371  val: loss: 1498716.25 acc: 0.5700418949127197\n",
      "step: 38705\n",
      "train: loss: 2915.77392578125 acc: 0.9912390112876892  val: loss: 2880500.25 acc: 0.5289841294288635\n",
      "step: 38710\n",
      "train: loss: 15240.4677734375 acc: 0.9746062755584717  val: loss: 1716253.375 acc: 0.715196967124939\n",
      "step: 38715\n",
      "train: loss: 58717.86328125 acc: 0.9747938513755798  val: loss: 1171399.125 acc: 0.9134825468063354\n",
      "step: 38720\n",
      "train: loss: 8089.763671875 acc: 0.9955855011940002  val: loss: 1568711.625 acc: 0.3144768476486206\n",
      "step: 38725\n",
      "train: loss: 75771.3515625 acc: 0.9649220108985901  val: loss: 484685.5625 acc: 0.9107789397239685\n",
      "step: 38730\n",
      "train: loss: 13713.3203125 acc: 0.9932438135147095  val: loss: 1552464.5 acc: 0.6582626700401306\n",
      "step: 38735\n",
      "train: loss: 7479.05908203125 acc: 0.9939228296279907  val: loss: 2061156.625 acc: 0.6774865388870239\n",
      "step: 38740\n",
      "train: loss: 7151.703125 acc: 0.9864920973777771  val: loss: 792076.8125 acc: 0.8102670311927795\n",
      "step: 38745\n",
      "train: loss: 6243.0400390625 acc: 0.9956881999969482  val: loss: 4016896.5 acc: 0.7026240229606628\n",
      "step: 38750\n",
      "train: loss: 24289.625 acc: 0.9921799898147583  val: loss: 535166.125 acc: 0.8444786667823792\n",
      "step: 38755\n",
      "train: loss: 28398.169921875 acc: 0.9936755895614624  val: loss: 1628109.0 acc: 0.15281182527542114\n",
      "step: 38760\n",
      "train: loss: 6960.2392578125 acc: 0.9981962442398071  val: loss: 2041809.25 acc: 0.44666391611099243\n",
      "step: 38765\n",
      "train: loss: 20779.55859375 acc: 0.9943185448646545  val: loss: 2488548.5 acc: 0.4819207191467285\n",
      "step: 38770\n",
      "train: loss: 30802.44921875 acc: 0.9924975633621216  val: loss: 4065725.75 acc: -0.14742088317871094\n",
      "step: 38775\n",
      "train: loss: 92605.96875 acc: 0.9830266833305359  val: loss: 1170410.125 acc: 0.5335503220558167\n",
      "step: 38780\n",
      "train: loss: 151160.625 acc: 0.8805634379386902  val: loss: 999548.625 acc: 0.7603352665901184\n",
      "step: 38785\n",
      "train: loss: 44209.65625 acc: 0.9912604689598083  val: loss: 2963980.25 acc: 0.0024671554565429688\n",
      "step: 38790\n",
      "train: loss: 191217.234375 acc: 0.9747211337089539  val: loss: 1073499.625 acc: 0.6652216911315918\n",
      "step: 38795\n",
      "train: loss: 26982.51171875 acc: 0.9976080656051636  val: loss: 1910901.75 acc: 0.6629153490066528\n",
      "step: 38800\n",
      "train: loss: 15290.0556640625 acc: 0.99776691198349  val: loss: 842575.9375 acc: 0.5805102586746216\n",
      "step: 38805\n",
      "train: loss: 489521.78125 acc: 0.9478699564933777  val: loss: 841192.9375 acc: 0.7021938562393188\n",
      "step: 38810\n",
      "train: loss: 445125.1875 acc: 0.9250904321670532  val: loss: 1134075.25 acc: 0.698708176612854\n",
      "step: 38815\n",
      "train: loss: 1396289.0 acc: 0.9383317232131958  val: loss: 2545556.25 acc: 0.6033802628517151\n",
      "step: 38820\n",
      "train: loss: 214595.609375 acc: 0.9747450947761536  val: loss: 2014195.0 acc: 0.7487258911132812\n",
      "step: 38825\n",
      "train: loss: 239174.328125 acc: 0.9803289771080017  val: loss: 658471.8125 acc: 0.8575963973999023\n",
      "step: 38830\n",
      "train: loss: 427053.46875 acc: 0.9702329635620117  val: loss: 1086252.25 acc: 0.7171815633773804\n",
      "step: 38835\n",
      "train: loss: 696990.25 acc: 0.9774919748306274  val: loss: 333840.8125 acc: 0.7755138278007507\n",
      "step: 38840\n",
      "train: loss: 847289.1875 acc: 0.9748410582542419  val: loss: 325199.90625 acc: 0.8861682415008545\n",
      "step: 38845\n",
      "train: loss: 811462.0625 acc: 0.9510895609855652  val: loss: 811792.1875 acc: 0.8983563184738159\n",
      "step: 38850\n",
      "train: loss: 1111487.375 acc: 0.9291267991065979  val: loss: 1267492.125 acc: 0.728291392326355\n",
      "step: 38855\n",
      "train: loss: 1418497.625 acc: 0.9056607484817505  val: loss: 880716.1875 acc: 0.8090611100196838\n",
      "step: 38860\n",
      "train: loss: 272705.5 acc: 0.9611247181892395  val: loss: 1337835.75 acc: 0.7457709312438965\n",
      "step: 38865\n",
      "train: loss: 942344.25 acc: 0.8696843385696411  val: loss: 245129.234375 acc: 0.938809335231781\n",
      "step: 38870\n",
      "train: loss: 1717163.375 acc: 0.4630975127220154  val: loss: 804005.1875 acc: 0.8623554706573486\n",
      "step: 38875\n",
      "train: loss: 744103.8125 acc: 0.0381585955619812  val: loss: 1296241.125 acc: 0.7001593112945557\n",
      "step: 38880\n",
      "train: loss: 385156.6875 acc: 0.7167844772338867  val: loss: 814801.0625 acc: 0.7176617383956909\n",
      "step: 38885\n",
      "train: loss: 758099.8125 acc: 0.8317400217056274  val: loss: 593283.6875 acc: 0.8485864400863647\n",
      "step: 38890\n",
      "train: loss: 520894.125 acc: 0.7368217706680298  val: loss: 1336098.5 acc: 0.6352958083152771\n",
      "step: 38895\n",
      "train: loss: 374012.0 acc: 0.8258776068687439  val: loss: 1341892.25 acc: 0.635884702205658\n",
      "step: 38900\n",
      "train: loss: 276694.0625 acc: 0.8515874147415161  val: loss: 517727.78125 acc: 0.7086021900177002\n",
      "step: 38905\n",
      "train: loss: 444680.6875 acc: 0.7815274000167847  val: loss: 1900145.875 acc: 0.6549410820007324\n",
      "step: 38910\n",
      "train: loss: 102716.8515625 acc: 0.9239493608474731  val: loss: 4683867.5 acc: 0.517428994178772\n",
      "step: 38915\n",
      "train: loss: 59356.18359375 acc: 0.9522212743759155  val: loss: 638036.125 acc: 0.7077548503875732\n",
      "step: 38920\n",
      "train: loss: 53516.703125 acc: 0.9605557322502136  val: loss: 2220293.25 acc: 0.6215572357177734\n",
      "step: 38925\n",
      "train: loss: 168292.34375 acc: 0.8906984329223633  val: loss: 1667527.25 acc: 0.685072660446167\n",
      "step: 38930\n",
      "train: loss: 150124.984375 acc: 0.8724096417427063  val: loss: 1112291.5 acc: 0.6496636271476746\n",
      "step: 38935\n",
      "train: loss: 204744.96875 acc: 0.8687027096748352  val: loss: 998255.5 acc: 0.7411073446273804\n",
      "step: 38940\n",
      "train: loss: 111052.7890625 acc: 0.8936047554016113  val: loss: 1361462.5 acc: 0.6735266447067261\n",
      "step: 38945\n",
      "train: loss: 492763.6875 acc: 0.7204521894454956  val: loss: 1476722.75 acc: 0.6958723068237305\n",
      "step: 38950\n",
      "train: loss: 224584.125 acc: 0.7932258248329163  val: loss: 799949.3125 acc: 0.7472072839736938\n",
      "step: 38955\n",
      "train: loss: 391967.71875 acc: 0.7961230874061584  val: loss: 1971921.125 acc: 0.650670051574707\n",
      "step: 38960\n",
      "train: loss: 271953.3125 acc: 0.8481817245483398  val: loss: 1024301.6875 acc: 0.7083325982093811\n",
      "step: 38965\n",
      "train: loss: 74376.859375 acc: 0.935795783996582  val: loss: 3300895.75 acc: 0.5447733402252197\n",
      "step: 38970\n",
      "train: loss: 524852.5 acc: 0.7439039945602417  val: loss: 5630827.5 acc: 0.5377885699272156\n",
      "step: 38975\n",
      "train: loss: 737954.0 acc: 0.719511866569519  val: loss: 3076607.25 acc: 0.5185283422470093\n",
      "step: 38980\n",
      "train: loss: 1267745.25 acc: 0.7893855571746826  val: loss: 1352444.125 acc: 0.8431520462036133\n",
      "step: 38985\n",
      "train: loss: 507603.6875 acc: 0.9435068368911743  val: loss: 1237567.25 acc: 0.7609376907348633\n",
      "step: 38990\n",
      "train: loss: 110536.328125 acc: 0.9929569363594055  val: loss: 406212.5625 acc: 0.8278304934501648\n",
      "step: 38995\n",
      "train: loss: 176945.59375 acc: 0.9785157442092896  val: loss: 962339.125 acc: 0.6714597940444946\n",
      "step: 39000\n",
      "train: loss: 171997.390625 acc: 0.9630845785140991  val: loss: 1201662.875 acc: 0.7067134380340576\n",
      "step: 39005\n",
      "train: loss: 130495.390625 acc: 0.9784928560256958  val: loss: 1384386.125 acc: 0.888971209526062\n",
      "step: 39010\n",
      "train: loss: 115582.4765625 acc: 0.9816276431083679  val: loss: 136717.8125 acc: 0.8865456581115723\n",
      "step: 39015\n",
      "train: loss: 44382.22265625 acc: 0.9967399835586548  val: loss: 1647959.0 acc: 0.7143903970718384\n",
      "step: 39020\n",
      "train: loss: 32034.572265625 acc: 0.9972633719444275  val: loss: 701588.4375 acc: 0.8948731422424316\n",
      "step: 39025\n",
      "train: loss: 65095.12890625 acc: 0.9927741289138794  val: loss: 1064663.375 acc: 0.8579944968223572\n",
      "step: 39030\n",
      "train: loss: 49757.69140625 acc: 0.9776126742362976  val: loss: 876787.9375 acc: 0.8326666355133057\n",
      "step: 39035\n",
      "train: loss: 24759.1484375 acc: 0.9882253408432007  val: loss: 2209659.75 acc: -0.22087574005126953\n",
      "step: 39040\n",
      "train: loss: 22500.916015625 acc: 0.9872512221336365  val: loss: 2436803.75 acc: 0.09716188907623291\n",
      "step: 39045\n",
      "train: loss: 14964.068359375 acc: 0.9771813154220581  val: loss: 2438982.25 acc: 0.2722238302230835\n",
      "step: 39050\n",
      "train: loss: 6332.357421875 acc: 0.9867978692054749  val: loss: 1898035.875 acc: 0.5895204544067383\n",
      "step: 39055\n",
      "train: loss: 13387.4521484375 acc: 0.9562041759490967  val: loss: 946518.625 acc: 0.7810260057449341\n",
      "step: 39060\n",
      "train: loss: 26954.72265625 acc: 0.8870845437049866  val: loss: 1845715.25 acc: 0.7823910713195801\n",
      "step: 39065\n",
      "train: loss: 4831.646484375 acc: 0.9893979430198669  val: loss: 1184343.0 acc: 0.8164268732070923\n",
      "step: 39070\n",
      "train: loss: 6050.67138671875 acc: 0.9756578803062439  val: loss: 2488205.75 acc: 0.5838959813117981\n",
      "step: 39075\n",
      "train: loss: 11776.7666015625 acc: 0.9804599285125732  val: loss: 605160.75 acc: 0.782156229019165\n",
      "step: 39080\n",
      "train: loss: 34868.41796875 acc: 0.9718615412712097  val: loss: 1419095.375 acc: 0.44118833541870117\n",
      "step: 39085\n",
      "train: loss: 25573.69140625 acc: 0.9853305816650391  val: loss: 1179489.5 acc: 0.5301884412765503\n",
      "step: 39090\n",
      "train: loss: 33091.8125 acc: 0.9867372512817383  val: loss: 1857194.125 acc: 0.09367102384567261\n",
      "step: 39095\n",
      "train: loss: 7605.27734375 acc: 0.9966702461242676  val: loss: 1859311.0 acc: 0.7724886536598206\n",
      "step: 39100\n",
      "train: loss: 38636.2421875 acc: 0.9867169260978699  val: loss: 2340863.5 acc: -0.5066205263137817\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 39105\n",
      "train: loss: 6571.45361328125 acc: 0.9863845705986023  val: loss: 2497461.25 acc: 0.2468164563179016\n",
      "step: 39110\n",
      "train: loss: 5861.892578125 acc: 0.9925575852394104  val: loss: 4345921.0 acc: -0.1785120964050293\n",
      "step: 39115\n",
      "train: loss: 99619.703125 acc: 0.9602080583572388  val: loss: 2361920.25 acc: 0.20708978176116943\n",
      "step: 39120\n",
      "train: loss: 21025.91796875 acc: 0.9944215416908264  val: loss: 1305095.5 acc: 0.8619705438613892\n",
      "step: 39125\n",
      "train: loss: 22548.396484375 acc: 0.9932716488838196  val: loss: 1098221.625 acc: 0.7652159929275513\n",
      "step: 39130\n",
      "train: loss: 56284.109375 acc: 0.9842292666435242  val: loss: 469384.03125 acc: 0.9222361445426941\n",
      "step: 39135\n",
      "train: loss: 22923.345703125 acc: 0.9855490326881409  val: loss: 2459813.25 acc: 0.3964802622795105\n",
      "step: 39140\n",
      "train: loss: 160122.34375 acc: 0.9688428640365601  val: loss: 1258419.125 acc: 0.8347960710525513\n",
      "step: 39145\n",
      "train: loss: 204371.5 acc: 0.9299004673957825  val: loss: 869105.25 acc: 0.7287237644195557\n",
      "step: 39150\n",
      "train: loss: 559022.8125 acc: 0.6998097896575928  val: loss: 754278.8125 acc: 0.8741601705551147\n",
      "step: 39155\n",
      "train: loss: 14604.5673828125 acc: 0.9966747164726257  val: loss: 540472.375 acc: 0.9125895500183105\n",
      "step: 39160\n",
      "train: loss: 128810.375 acc: 0.9864486455917358  val: loss: 949398.8125 acc: 0.8539458513259888\n",
      "step: 39165\n",
      "train: loss: 41964.2109375 acc: 0.9954752326011658  val: loss: 690894.1875 acc: 0.8865198493003845\n",
      "step: 39170\n",
      "train: loss: 499095.1875 acc: 0.9563815593719482  val: loss: 140210.875 acc: 0.9578489661216736\n",
      "step: 39175\n",
      "train: loss: 145781.796875 acc: 0.9710140228271484  val: loss: 455490.25 acc: 0.9084146022796631\n",
      "step: 39180\n",
      "train: loss: 491535.40625 acc: 0.9665715098381042  val: loss: 2029580.625 acc: 0.3251078128814697\n",
      "step: 39185\n",
      "train: loss: 344230.71875 acc: 0.9641287326812744  val: loss: 502282.59375 acc: 0.7172641754150391\n",
      "step: 39190\n",
      "train: loss: 148567.625 acc: 0.954960823059082  val: loss: 689508.1875 acc: 0.7919794917106628\n",
      "step: 39195\n",
      "train: loss: 459908.625 acc: 0.9755524396896362  val: loss: 637389.875 acc: 0.7850792407989502\n",
      "step: 39200\n",
      "train: loss: 317649.84375 acc: 0.98622727394104  val: loss: 369052.78125 acc: 0.9371919631958008\n",
      "step: 39205\n",
      "train: loss: 1068766.125 acc: 0.9580004215240479  val: loss: 1022308.1875 acc: 0.5556850433349609\n",
      "step: 39210\n",
      "train: loss: 516093.59375 acc: 0.9693543910980225  val: loss: 1538799.875 acc: 0.6268230080604553\n",
      "step: 39215\n",
      "train: loss: 548470.8125 acc: 0.9616019129753113  val: loss: 641484.75 acc: 0.8940258622169495\n",
      "step: 39220\n",
      "train: loss: 540123.125 acc: 0.9532263278961182  val: loss: 1346776.125 acc: 0.8344895839691162\n",
      "step: 39225\n",
      "train: loss: 330290.0 acc: 0.9481490254402161  val: loss: 688990.1875 acc: 0.7696785926818848\n",
      "step: 39230\n",
      "train: loss: 689801.625 acc: 0.8952039480209351  val: loss: 581479.5 acc: 0.8447805643081665\n",
      "step: 39235\n",
      "train: loss: 1283299.125 acc: 0.7713038921356201  val: loss: 1073857.625 acc: 0.6710954308509827\n",
      "step: 39240\n",
      "train: loss: 1077276.375 acc: 0.6167933940887451  val: loss: 379320.46875 acc: 0.7238019108772278\n",
      "step: 39245\n",
      "train: loss: 498594.5 acc: 0.7989140152931213  val: loss: 2115644.5 acc: 0.6752980947494507\n",
      "step: 39250\n",
      "train: loss: 364904.21875 acc: 0.8671700954437256  val: loss: 2092492.125 acc: 0.6934486627578735\n",
      "step: 39255\n",
      "train: loss: 691649.4375 acc: 0.7652142643928528  val: loss: 959951.625 acc: 0.8028520345687866\n",
      "step: 39260\n",
      "train: loss: 266296.6875 acc: 0.87469881772995  val: loss: 1991654.5 acc: 0.6499786972999573\n",
      "step: 39265\n",
      "train: loss: 517873.78125 acc: 0.7631965279579163  val: loss: 907040.1875 acc: 0.6914961338043213\n",
      "step: 39270\n",
      "train: loss: 35231.7265625 acc: 0.973560094833374  val: loss: 1735766.875 acc: 0.5697950720787048\n",
      "step: 39275\n",
      "train: loss: 53233.55078125 acc: 0.9513376355171204  val: loss: 1162370.125 acc: 0.6306288242340088\n",
      "step: 39280\n",
      "train: loss: 29878.69140625 acc: 0.9757985472679138  val: loss: 3094605.25 acc: 0.5521911382675171\n",
      "step: 39285\n",
      "train: loss: 91598.3125 acc: 0.9355713725090027  val: loss: 2299038.25 acc: 0.5524177551269531\n",
      "step: 39290\n",
      "train: loss: 228018.296875 acc: 0.8618656396865845  val: loss: 699278.8125 acc: 0.7428836226463318\n",
      "step: 39295\n",
      "train: loss: 235891.546875 acc: 0.8459547758102417  val: loss: 4488757.0 acc: 0.5260973572731018\n",
      "step: 39300\n",
      "train: loss: 132103.5 acc: 0.8868572115898132  val: loss: 1385969.125 acc: 0.615135669708252\n",
      "step: 39305\n",
      "train: loss: 76073.5703125 acc: 0.9231962561607361  val: loss: 3831699.75 acc: 0.520390510559082\n",
      "step: 39310\n",
      "train: loss: 45217.32421875 acc: 0.955317497253418  val: loss: 1045850.8125 acc: 0.6734232902526855\n",
      "step: 39315\n",
      "train: loss: 35264.921875 acc: 0.9627971649169922  val: loss: 1052085.125 acc: 0.7384928464889526\n",
      "step: 39320\n",
      "train: loss: 225984.953125 acc: 0.8642135858535767  val: loss: 1303419.5 acc: 0.6621103286743164\n",
      "step: 39325\n",
      "train: loss: 537523.125 acc: 0.7443250417709351  val: loss: 2846676.0 acc: 0.5974703431129456\n",
      "step: 39330\n",
      "train: loss: 478328.21875 acc: 0.7527422904968262  val: loss: 1996118.25 acc: 0.6207886338233948\n",
      "step: 39335\n",
      "train: loss: 437491.09375 acc: 0.7940291166305542  val: loss: 3429158.0 acc: 0.5794090032577515\n",
      "step: 39340\n",
      "train: loss: 147867.09375 acc: 0.8988636136054993  val: loss: 2288657.75 acc: 0.6013147830963135\n",
      "step: 39345\n",
      "train: loss: 2142568.5 acc: 0.7316929697990417  val: loss: 1808766.75 acc: 0.7103636860847473\n",
      "step: 39350\n",
      "train: loss: 678840.75 acc: 0.8679604530334473  val: loss: 729081.625 acc: 0.4913007616996765\n",
      "step: 39355\n",
      "train: loss: 1295478.875 acc: 0.8921914100646973  val: loss: 1258735.25 acc: 0.8256064057350159\n",
      "step: 39360\n",
      "train: loss: 430982.6875 acc: 0.9533208608627319  val: loss: 827522.4375 acc: 0.8549391627311707\n",
      "step: 39365\n",
      "train: loss: 114485.1953125 acc: 0.983446478843689  val: loss: 1461470.25 acc: -0.1297447681427002\n",
      "step: 39370\n",
      "train: loss: 47888.55078125 acc: 0.9914072751998901  val: loss: 1973646.125 acc: 0.7049411535263062\n",
      "step: 39375\n",
      "train: loss: 52310.5 acc: 0.9950379729270935  val: loss: 1314461.875 acc: 0.46092331409454346\n",
      "step: 39380\n",
      "train: loss: 40164.5859375 acc: 0.996385395526886  val: loss: 1410258.5 acc: 0.8628358840942383\n",
      "step: 39385\n",
      "train: loss: 12741.4833984375 acc: 0.9991529583930969  val: loss: 704441.5625 acc: 0.8649646043777466\n",
      "step: 39390\n",
      "train: loss: 52857.74609375 acc: 0.9943685531616211  val: loss: 2429916.25 acc: 0.7676216959953308\n",
      "step: 39395\n",
      "train: loss: 41456.265625 acc: 0.9862357378005981  val: loss: 431538.1875 acc: 0.7880933284759521\n",
      "step: 39400\n",
      "train: loss: 30276.009765625 acc: 0.9725896120071411  val: loss: 2078885.125 acc: 0.1980726718902588\n",
      "step: 39405\n",
      "train: loss: 2838.189208984375 acc: 0.9949557185173035  val: loss: 1347529.75 acc: 0.804233193397522\n",
      "step: 39410\n",
      "train: loss: 10798.1611328125 acc: 0.9806744456291199  val: loss: 950573.625 acc: 0.8758863806724548\n",
      "step: 39415\n",
      "train: loss: 13273.150390625 acc: 0.9778850078582764  val: loss: 1989198.0 acc: -1.402482271194458\n",
      "step: 39420\n",
      "train: loss: 12824.8525390625 acc: 0.9919896721839905  val: loss: 1252992.625 acc: 0.8217134475708008\n",
      "step: 39425\n",
      "train: loss: 15498.4755859375 acc: 0.9745752215385437  val: loss: 2388217.0 acc: -0.03834080696105957\n",
      "step: 39430\n",
      "train: loss: 11392.677734375 acc: 0.962361216545105  val: loss: 1898504.375 acc: 0.41970062255859375\n",
      "step: 39435\n",
      "train: loss: 11492.2939453125 acc: 0.9707514047622681  val: loss: 972183.0 acc: 0.8818355202674866\n",
      "step: 39440\n",
      "train: loss: 31449.5234375 acc: 0.9581325650215149  val: loss: 447511.40625 acc: 0.8870221376419067\n",
      "step: 39445\n",
      "train: loss: 63088.3671875 acc: 0.9730198383331299  val: loss: 2352860.0 acc: 0.6871817111968994\n",
      "step: 39450\n",
      "train: loss: 11348.3369140625 acc: 0.9925986528396606  val: loss: 1840659.875 acc: 0.6378962993621826\n",
      "step: 39455\n",
      "train: loss: 11986.26171875 acc: 0.9920077919960022  val: loss: 3867931.25 acc: -0.25933265686035156\n",
      "step: 39460\n",
      "train: loss: 4359.86083984375 acc: 0.9960877895355225  val: loss: 1496832.375 acc: 0.15157616138458252\n",
      "step: 39465\n",
      "train: loss: 208903.28125 acc: 0.9152330160140991  val: loss: 1177351.0 acc: 0.8078192472457886\n",
      "step: 39470\n",
      "train: loss: 14462.8740234375 acc: 0.990125298500061  val: loss: 1649626.875 acc: 0.8339133262634277\n",
      "step: 39475\n",
      "train: loss: 7529.14404296875 acc: 0.9857752323150635  val: loss: 1247934.0 acc: 0.8707836866378784\n",
      "step: 39480\n",
      "train: loss: 26815.30859375 acc: 0.9874547123908997  val: loss: 1190091.875 acc: 0.2837345004081726\n",
      "step: 39485\n",
      "train: loss: 16082.791015625 acc: 0.9933789968490601  val: loss: 869582.5625 acc: 0.9272679090499878\n",
      "step: 39490\n",
      "train: loss: 12524.1591796875 acc: 0.9949809908866882  val: loss: 1207126.375 acc: 0.5988129377365112\n",
      "step: 39495\n",
      "train: loss: 11303.341796875 acc: 0.9964607954025269  val: loss: 198337.3125 acc: 0.9374319911003113\n",
      "step: 39500\n",
      "train: loss: 27424.958984375 acc: 0.9861901998519897  val: loss: 2427723.25 acc: 0.6409980058670044\n",
      "step: 39505\n",
      "train: loss: 58853.96875 acc: 0.9844341278076172  val: loss: 2078925.875 acc: 0.2943982481956482\n",
      "step: 39510\n",
      "train: loss: 126116.8984375 acc: 0.9586771726608276  val: loss: 960870.125 acc: 0.5898793935775757\n",
      "step: 39515\n",
      "train: loss: 44308.92578125 acc: 0.9649251699447632  val: loss: 1635506.625 acc: 0.1917426586151123\n",
      "step: 39520\n",
      "train: loss: 861457.25 acc: 0.6934959292411804  val: loss: 181903.265625 acc: 0.880658745765686\n",
      "step: 39525\n",
      "train: loss: 754483.75 acc: 0.9247637987136841  val: loss: 149026.890625 acc: 0.9211602210998535\n",
      "step: 39530\n",
      "train: loss: 59234.51171875 acc: 0.9942936897277832  val: loss: 302470.59375 acc: 0.8939575552940369\n",
      "step: 39535\n",
      "train: loss: 19298.68359375 acc: 0.9968054890632629  val: loss: 1360835.0 acc: 0.24885571002960205\n",
      "step: 39540\n",
      "train: loss: 468341.71875 acc: 0.9452900886535645  val: loss: 311871.40625 acc: 0.9380946159362793\n",
      "step: 39545\n",
      "train: loss: 884575.625 acc: 0.9539403319358826  val: loss: 565193.5 acc: 0.8516958951950073\n",
      "step: 39550\n",
      "train: loss: 355116.75 acc: 0.982321560382843  val: loss: 746349.0 acc: 0.8400209546089172\n",
      "step: 39555\n",
      "train: loss: 175795.25 acc: 0.980787992477417  val: loss: 437995.4375 acc: 0.8956363201141357\n",
      "step: 39560\n",
      "train: loss: 1098336.25 acc: 0.9270370602607727  val: loss: 1324672.125 acc: 0.6931129693984985\n",
      "step: 39565\n",
      "train: loss: 3532472.5 acc: 0.9047829508781433  val: loss: 710006.9375 acc: 0.8337633609771729\n",
      "step: 39570\n",
      "train: loss: 500190.75 acc: 0.9783416390419006  val: loss: 417302.875 acc: 0.8649868369102478\n",
      "step: 39575\n",
      "train: loss: 372157.1875 acc: 0.9686403870582581  val: loss: 360672.15625 acc: 0.9458290934562683\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 39580\n",
      "train: loss: 920714.8125 acc: 0.9394837617874146  val: loss: 643175.5625 acc: 0.7403565645217896\n",
      "step: 39585\n",
      "train: loss: 473012.90625 acc: 0.9391040205955505  val: loss: 1121844.5 acc: 0.8070528507232666\n",
      "step: 39590\n",
      "train: loss: 435554.65625 acc: 0.933678925037384  val: loss: 668001.25 acc: 0.9130648970603943\n",
      "step: 39595\n",
      "train: loss: 190325.578125 acc: 0.9490167498588562  val: loss: 981687.625 acc: 0.8776651620864868\n",
      "step: 39600\n",
      "train: loss: 1358270.875 acc: 0.7208508253097534  val: loss: 597869.25 acc: 0.7848697900772095\n",
      "step: 39605\n",
      "train: loss: 1871254.375 acc: 0.6566164493560791  val: loss: 4396679.5 acc: 0.6147779226303101\n",
      "step: 39610\n",
      "train: loss: 543260.9375 acc: 0.7511757016181946  val: loss: 1751547.875 acc: 0.7428333163261414\n",
      "step: 39615\n",
      "train: loss: 479380.4375 acc: 0.8208861351013184  val: loss: 2673633.5 acc: 0.7260580062866211\n",
      "step: 39620\n",
      "train: loss: 1039398.5 acc: 0.7475851774215698  val: loss: 4041577.0 acc: 0.6553963422775269\n",
      "step: 39625\n",
      "train: loss: 285555.53125 acc: 0.7737308144569397  val: loss: 2080327.125 acc: 0.6421380043029785\n",
      "step: 39630\n",
      "train: loss: 208319.328125 acc: 0.8565427660942078  val: loss: 4015583.5 acc: 0.556067943572998\n",
      "step: 39635\n",
      "train: loss: 720882.3125 acc: 0.7253006100654602  val: loss: 2346708.75 acc: 0.6685965061187744\n",
      "step: 39640\n",
      "train: loss: 257444.515625 acc: 0.8473373055458069  val: loss: 895493.5 acc: 0.744178056716919\n",
      "step: 39645\n",
      "train: loss: 67832.4921875 acc: 0.9559807777404785  val: loss: 3069573.75 acc: 0.5666755437850952\n",
      "step: 39650\n",
      "train: loss: 81318.40625 acc: 0.9431668519973755  val: loss: 2546642.5 acc: 0.614917516708374\n",
      "step: 39655\n",
      "train: loss: 114840.078125 acc: 0.9175141453742981  val: loss: 5475877.5 acc: 0.5885465145111084\n",
      "step: 39660\n",
      "train: loss: 601357.875 acc: 0.7888801097869873  val: loss: 3488711.25 acc: 0.6324141025543213\n",
      "step: 39665\n",
      "train: loss: 101759.140625 acc: 0.9259030222892761  val: loss: 2123319.25 acc: 0.596881628036499\n",
      "step: 39670\n",
      "train: loss: 20586.578125 acc: 0.980444610118866  val: loss: 852066.375 acc: 0.7007045745849609\n",
      "step: 39675\n",
      "train: loss: 95365.03125 acc: 0.9217196106910706  val: loss: 2896338.75 acc: 0.5678672790527344\n",
      "step: 39680\n",
      "train: loss: 131827.03125 acc: 0.8968366384506226  val: loss: 2350114.25 acc: 0.6550055742263794\n",
      "step: 39685\n",
      "train: loss: 269286.15625 acc: 0.7324678301811218  val: loss: 742489.875 acc: 0.6770449876785278\n",
      "step: 39690\n",
      "train: loss: 283536.875 acc: 0.8506418466567993  val: loss: 1019291.375 acc: 0.7382266521453857\n",
      "step: 39695\n",
      "train: loss: 45641.31640625 acc: 0.9514271020889282  val: loss: 613605.25 acc: 0.757473349571228\n",
      "step: 39700\n",
      "train: loss: 148450.453125 acc: 0.858588457107544  val: loss: 1230119.375 acc: 0.7522847056388855\n",
      "step: 39705\n",
      "train: loss: 149550.390625 acc: 0.8927103281021118  val: loss: 3210279.25 acc: 0.49576735496520996\n",
      "step: 39710\n",
      "train: loss: 1420963.875 acc: 0.7508637309074402  val: loss: 914662.8125 acc: 0.8273035883903503\n",
      "step: 39715\n",
      "train: loss: 602290.375 acc: 0.8357502222061157  val: loss: 1397457.25 acc: 0.7985010147094727\n",
      "step: 39720\n",
      "train: loss: 216312.953125 acc: 0.9763340353965759  val: loss: 1100927.0 acc: 0.6613175272941589\n",
      "step: 39725\n",
      "train: loss: 197299.65625 acc: 0.980398952960968  val: loss: 583202.8125 acc: 0.8152296543121338\n",
      "step: 39730\n",
      "train: loss: 118349.8125 acc: 0.9746777415275574  val: loss: 672286.75 acc: 0.7707628011703491\n",
      "step: 39735\n",
      "train: loss: 220522.34375 acc: 0.9463662505149841  val: loss: 1605505.125 acc: 0.8045676946640015\n",
      "step: 39740\n",
      "train: loss: 74630.28125 acc: 0.9934456944465637  val: loss: 936912.125 acc: 0.7431333065032959\n",
      "step: 39745\n",
      "train: loss: 35456.47265625 acc: 0.9976060390472412  val: loss: 946367.6875 acc: 0.7427494525909424\n",
      "step: 39750\n",
      "train: loss: 29545.017578125 acc: 0.9974746108055115  val: loss: 2930033.75 acc: 0.056314706802368164\n",
      "step: 39755\n",
      "train: loss: 32935.8046875 acc: 0.9962506294250488  val: loss: 1840494.25 acc: 0.5654030442237854\n",
      "step: 39760\n",
      "train: loss: 24663.306640625 acc: 0.9967707991600037  val: loss: 1313349.625 acc: 0.8085646629333496\n",
      "step: 39765\n",
      "train: loss: 29292.3671875 acc: 0.9855594635009766  val: loss: 197179.875 acc: 0.9267639517784119\n",
      "step: 39770\n",
      "train: loss: 4708.76171875 acc: 0.9846470952033997  val: loss: 1938282.625 acc: 0.5379345417022705\n",
      "step: 39775\n",
      "train: loss: 6131.02685546875 acc: 0.9875174760818481  val: loss: 832696.625 acc: 0.8208152055740356\n",
      "step: 39780\n",
      "train: loss: 6087.15869140625 acc: 0.9916236996650696  val: loss: 269264.375 acc: 0.932433545589447\n",
      "step: 39785\n",
      "train: loss: 16390.23046875 acc: 0.9957554340362549  val: loss: 405364.96875 acc: 0.8747192621231079\n",
      "step: 39790\n",
      "train: loss: 13460.107421875 acc: 0.9753565192222595  val: loss: 209105.671875 acc: 0.909398078918457\n",
      "step: 39795\n",
      "train: loss: 6859.34521484375 acc: 0.9836035966873169  val: loss: 487402.75 acc: 0.8386945724487305\n",
      "step: 39800\n",
      "train: loss: 6578.1962890625 acc: 0.9894125461578369  val: loss: 2369187.25 acc: 0.6273243427276611\n",
      "step: 39805\n",
      "train: loss: 13262.4541015625 acc: 0.9687207341194153  val: loss: 3023130.25 acc: 0.33217644691467285\n",
      "step: 39810\n",
      "train: loss: 9869.4052734375 acc: 0.9764464497566223  val: loss: 1018728.6875 acc: 0.8202348947525024\n",
      "step: 39815\n",
      "train: loss: 28496.6328125 acc: 0.9812877178192139  val: loss: 1542891.5 acc: 0.6965193748474121\n",
      "step: 39820\n",
      "train: loss: 18836.732421875 acc: 0.9886820316314697  val: loss: 1026709.5 acc: 0.8406758904457092\n",
      "step: 39825\n",
      "train: loss: 5405.42431640625 acc: 0.9954856038093567  val: loss: 982502.375 acc: 0.5671705603599548\n",
      "step: 39830\n",
      "train: loss: 9768.6962890625 acc: 0.9952783584594727  val: loss: 3326576.25 acc: 0.10994017124176025\n",
      "step: 39835\n",
      "train: loss: 7403.6943359375 acc: 0.9904056191444397  val: loss: 506098.5625 acc: 0.9002174139022827\n",
      "step: 39840\n",
      "train: loss: 7217.27099609375 acc: 0.9792578220367432  val: loss: 443083.96875 acc: 0.9284510612487793\n",
      "step: 39845\n",
      "train: loss: 8264.650390625 acc: 0.9926472902297974  val: loss: 779225.1875 acc: 0.7190203666687012\n",
      "step: 39850\n",
      "train: loss: 12714.8984375 acc: 0.9951605796813965  val: loss: 192675.03125 acc: 0.9682191610336304\n",
      "step: 39855\n",
      "train: loss: 27521.05859375 acc: 0.9920634627342224  val: loss: 254989.859375 acc: 0.8964332938194275\n",
      "step: 39860\n",
      "train: loss: 16076.994140625 acc: 0.9954169988632202  val: loss: 1196515.125 acc: 0.8150638937950134\n",
      "step: 39865\n",
      "train: loss: 61454.34765625 acc: 0.9758481979370117  val: loss: 56805.63671875 acc: 0.9875199198722839\n",
      "step: 39870\n",
      "train: loss: 292129.40625 acc: 0.9399261474609375  val: loss: 335977.21875 acc: 0.9556376934051514\n",
      "step: 39875\n",
      "train: loss: 146716.921875 acc: 0.9600750207901001  val: loss: 1046197.9375 acc: 0.7879025936126709\n",
      "step: 39880\n",
      "train: loss: 18100.2890625 acc: 0.9900020360946655  val: loss: 1014381.625 acc: 0.901765763759613\n",
      "step: 39885\n",
      "train: loss: 45950.1484375 acc: 0.9919740557670593  val: loss: 1407121.375 acc: 0.7914329767227173\n",
      "step: 39890\n",
      "train: loss: 147234.890625 acc: 0.9871944189071655  val: loss: 772687.5625 acc: 0.9034013748168945\n",
      "step: 39895\n",
      "train: loss: 147109.78125 acc: 0.9878138899803162  val: loss: 469969.46875 acc: 0.9554247260093689\n",
      "step: 39900\n",
      "train: loss: 25668.142578125 acc: 0.9961779713630676  val: loss: 855189.5 acc: 0.864355742931366\n",
      "step: 39905\n",
      "train: loss: 494475.53125 acc: 0.9580208659172058  val: loss: 1052736.0 acc: 0.9196520447731018\n",
      "step: 39910\n",
      "train: loss: 318838.71875 acc: 0.9512439370155334  val: loss: 818794.375 acc: 0.9020918011665344\n",
      "step: 39915\n",
      "train: loss: 492628.5 acc: 0.959900438785553  val: loss: 3337170.0 acc: 0.687639594078064\n",
      "step: 39920\n",
      "train: loss: 208042.671875 acc: 0.9829537272453308  val: loss: 463447.9375 acc: 0.8011162877082825\n",
      "step: 39925\n",
      "train: loss: 363976.09375 acc: 0.9748099446296692  val: loss: 489386.03125 acc: 0.8601909279823303\n",
      "step: 39930\n",
      "train: loss: 747908.25 acc: 0.9612578749656677  val: loss: 660577.125 acc: 0.955359935760498\n",
      "step: 39935\n",
      "train: loss: 3196985.75 acc: 0.9094455242156982  val: loss: 1785768.5 acc: 0.6564409732818604\n",
      "step: 39940\n",
      "train: loss: 1192450.5 acc: 0.9310986995697021  val: loss: 501916.125 acc: 0.9600557088851929\n",
      "step: 39945\n",
      "train: loss: 152306.96875 acc: 0.9857699275016785  val: loss: 934677.75 acc: 0.8057888746261597\n",
      "step: 39950\n",
      "train: loss: 120279.9296875 acc: 0.9912127256393433  val: loss: 965191.25 acc: 0.5987417697906494\n",
      "step: 39955\n",
      "train: loss: 209411.078125 acc: 0.9565904140472412  val: loss: 1945798.5 acc: 0.735480546951294\n",
      "step: 39960\n",
      "train: loss: 403077.46875 acc: 0.9326675534248352  val: loss: 3136203.25 acc: 0.6086127758026123\n",
      "step: 39965\n",
      "train: loss: 1542204.5 acc: 0.621755838394165  val: loss: 836595.125 acc: 0.8656825423240662\n",
      "step: 39970\n",
      "train: loss: 1218237.375 acc: 0.7281027436256409  val: loss: 2437359.75 acc: 0.671134352684021\n",
      "step: 39975\n",
      "train: loss: 474951.59375 acc: 0.7344666719436646  val: loss: 601588.625 acc: 0.6589212417602539\n",
      "step: 39980\n",
      "train: loss: 1154139.375 acc: 0.5673386454582214  val: loss: 852149.375 acc: 0.7977604866027832\n",
      "step: 39985\n",
      "train: loss: 699066.0 acc: 0.8063773512840271  val: loss: 1241783.5 acc: 0.7171682119369507\n",
      "step: 39990\n",
      "train: loss: 984001.375 acc: 0.6777887344360352  val: loss: 2135056.25 acc: 0.6197226047515869\n",
      "step: 39995\n",
      "train: loss: 273344.40625 acc: 0.8317253589630127  val: loss: 523307.25 acc: 0.7203980684280396\n",
      "step: 40000\n",
      "train: loss: 656331.875 acc: 0.7488259077072144  val: loss: 264027.59375 acc: 0.780563235282898\n",
      "step: 40005\n",
      "train: loss: 141808.640625 acc: 0.9012480974197388  val: loss: 2352256.75 acc: 0.5639700889587402\n",
      "step: 40010\n",
      "train: loss: 154157.8125 acc: 0.8910571336746216  val: loss: 197153.84375 acc: 0.8631008863449097\n",
      "step: 40015\n",
      "train: loss: 59936.69921875 acc: 0.9518416523933411  val: loss: 692263.9375 acc: 0.7797139883041382\n",
      "step: 40020\n",
      "train: loss: 118626.6953125 acc: 0.9219173789024353  val: loss: 2132647.25 acc: 0.6136756539344788\n",
      "step: 40025\n",
      "train: loss: 461354.09375 acc: 0.7713660001754761  val: loss: 817041.1875 acc: 0.7635557651519775\n",
      "step: 40030\n",
      "train: loss: 240372.109375 acc: 0.8562071323394775  val: loss: 2946175.25 acc: 0.47512853145599365\n",
      "step: 40035\n",
      "train: loss: 44565.88671875 acc: 0.9600808620452881  val: loss: 1156798.875 acc: 0.6305845975875854\n",
      "step: 40040\n",
      "train: loss: 80533.9453125 acc: 0.9292027354240417  val: loss: 2649861.0 acc: 0.5929417610168457\n",
      "step: 40045\n",
      "train: loss: 266925.1875 acc: 0.8504267334938049  val: loss: 685947.625 acc: 0.7567505240440369\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 40050\n",
      "train: loss: 557936.5 acc: 0.7593279480934143  val: loss: 2469037.75 acc: 0.6112757921218872\n",
      "step: 40055\n",
      "train: loss: 500897.9375 acc: 0.7553482055664062  val: loss: 1997721.125 acc: 0.6429241895675659\n",
      "step: 40060\n",
      "train: loss: 271515.4375 acc: 0.7953369617462158  val: loss: 2227537.0 acc: 0.6348222494125366\n",
      "step: 40065\n",
      "train: loss: 312994.6875 acc: 0.8281316161155701  val: loss: 678981.625 acc: 0.723297119140625\n",
      "step: 40070\n",
      "train: loss: 654471.0 acc: 0.7514114379882812  val: loss: 1530143.125 acc: 0.6623344421386719\n",
      "step: 40075\n",
      "train: loss: 2024201.375 acc: 0.6680588722229004  val: loss: 1284791.375 acc: 0.7366599440574646\n",
      "step: 40080\n",
      "train: loss: 579334.4375 acc: 0.8779703974723816  val: loss: 859311.5 acc: 0.6993943452835083\n",
      "step: 40085\n",
      "train: loss: 278382.9375 acc: 0.967719554901123  val: loss: 1552258.125 acc: 0.7211803197860718\n",
      "step: 40090\n",
      "train: loss: 448938.5625 acc: 0.9433704018592834  val: loss: 700740.5 acc: 0.8302507400512695\n",
      "step: 40095\n",
      "train: loss: 130817.4609375 acc: 0.980162501335144  val: loss: 1639786.125 acc: 0.7258930206298828\n",
      "step: 40100\n",
      "train: loss: 518450.59375 acc: 0.9052247405052185  val: loss: 936262.625 acc: 0.8900429606437683\n",
      "step: 40105\n",
      "train: loss: 120880.640625 acc: 0.9865713119506836  val: loss: 326674.53125 acc: 0.8876338601112366\n",
      "step: 40110\n",
      "train: loss: 27675.884765625 acc: 0.9979290962219238  val: loss: 670091.625 acc: 0.8296785950660706\n",
      "step: 40115\n",
      "train: loss: 64540.59375 acc: 0.9952197670936584  val: loss: 1153191.25 acc: 0.7072519063949585\n",
      "step: 40120\n",
      "train: loss: 59738.59765625 acc: 0.991758406162262  val: loss: 878794.5 acc: 0.8467962741851807\n",
      "step: 40125\n",
      "train: loss: 45221.33984375 acc: 0.9886154532432556  val: loss: 1002082.75 acc: 0.7036041021347046\n",
      "step: 40130\n",
      "train: loss: 40968.82421875 acc: 0.9760138392448425  val: loss: 1451705.0 acc: 0.8504032492637634\n",
      "step: 40135\n",
      "train: loss: 13828.2646484375 acc: 0.9919273257255554  val: loss: 653616.375 acc: 0.7656959295272827\n",
      "step: 40140\n",
      "train: loss: 1636.8433837890625 acc: 0.9994299411773682  val: loss: 1893171.875 acc: -0.6862510442733765\n",
      "step: 40145\n",
      "train: loss: 12468.2890625 acc: 0.9919602870941162  val: loss: 894136.3125 acc: 0.6331368684768677\n",
      "step: 40150\n",
      "train: loss: 8219.9521484375 acc: 0.9969772696495056  val: loss: 307106.75 acc: 0.8514633178710938\n",
      "step: 40155\n",
      "train: loss: 11097.2060546875 acc: 0.9737229347229004  val: loss: 718307.125 acc: 0.9012764096260071\n",
      "step: 40160\n",
      "train: loss: 4270.958984375 acc: 0.9902763366699219  val: loss: 479202.65625 acc: 0.7791942358016968\n",
      "step: 40165\n",
      "train: loss: 9935.037109375 acc: 0.9894884824752808  val: loss: 1092711.125 acc: 0.6160690784454346\n",
      "step: 40170\n",
      "train: loss: 11532.1845703125 acc: 0.9777495265007019  val: loss: 814677.1875 acc: 0.8101160526275635\n",
      "step: 40175\n",
      "train: loss: 23352.447265625 acc: 0.972700834274292  val: loss: 665571.0625 acc: 0.7664815187454224\n",
      "step: 40180\n",
      "train: loss: 19877.640625 acc: 0.9815335869789124  val: loss: 148346.0 acc: 0.9679094552993774\n",
      "step: 40185\n",
      "train: loss: 18992.216796875 acc: 0.9895293116569519  val: loss: 1545274.375 acc: 0.015230000019073486\n",
      "step: 40190\n",
      "train: loss: 13276.3125 acc: 0.9925286769866943  val: loss: 811991.5 acc: 0.7985187768936157\n",
      "step: 40195\n",
      "train: loss: 10778.328125 acc: 0.99371737241745  val: loss: 605171.5 acc: 0.8937634825706482\n",
      "step: 40200\n",
      "train: loss: 6336.63330078125 acc: 0.9931888580322266  val: loss: 244149.734375 acc: 0.9375121593475342\n",
      "step: 40205\n",
      "train: loss: 5558.87353515625 acc: 0.9959318041801453  val: loss: 936484.25 acc: 0.7572613954544067\n",
      "step: 40210\n",
      "train: loss: 17398.658203125 acc: 0.9912208914756775  val: loss: 599468.0 acc: 0.9128254055976868\n",
      "step: 40215\n",
      "train: loss: 25334.029296875 acc: 0.9903072714805603  val: loss: 504543.34375 acc: 0.901852548122406\n",
      "step: 40220\n",
      "train: loss: 27456.96484375 acc: 0.9873877763748169  val: loss: 1215485.375 acc: 0.8654845952987671\n",
      "step: 40225\n",
      "train: loss: 12237.6279296875 acc: 0.9923790693283081  val: loss: 392921.125 acc: 0.92058926820755\n",
      "step: 40230\n",
      "train: loss: 25264.927734375 acc: 0.9850677847862244  val: loss: 612014.1875 acc: 0.9536522030830383\n",
      "step: 40235\n",
      "train: loss: 38164.07421875 acc: 0.9870090484619141  val: loss: 591905.9375 acc: 0.9512112140655518\n",
      "step: 40240\n",
      "train: loss: 51779.3125 acc: 0.9759042263031006  val: loss: 895987.0625 acc: 0.8841091990470886\n",
      "step: 40245\n",
      "train: loss: 192723.1875 acc: 0.9066072702407837  val: loss: 185801.328125 acc: 0.9465950727462769\n",
      "step: 40250\n",
      "train: loss: 98539.2421875 acc: 0.9824143052101135  val: loss: 1033876.875 acc: 0.3084426522254944\n",
      "step: 40255\n",
      "train: loss: 87365.2890625 acc: 0.990320086479187  val: loss: 1141047.625 acc: 0.8000062108039856\n",
      "step: 40260\n",
      "train: loss: 85209.5703125 acc: 0.9914984107017517  val: loss: 851436.375 acc: 0.9187367558479309\n",
      "step: 40265\n",
      "train: loss: 38178.83984375 acc: 0.9909888505935669  val: loss: 957269.375 acc: 0.6751201152801514\n",
      "step: 40270\n",
      "train: loss: 398208.71875 acc: 0.9566947221755981  val: loss: 857775.375 acc: 0.8390886783599854\n",
      "step: 40275\n",
      "train: loss: 414856.375 acc: 0.9556111097335815  val: loss: 1122940.75 acc: 0.7806890606880188\n",
      "step: 40280\n",
      "train: loss: 477589.8125 acc: 0.9684183597564697  val: loss: 2683395.25 acc: 0.7033356428146362\n",
      "step: 40285\n",
      "train: loss: 2145177.75 acc: 0.8743083477020264  val: loss: 427946.5625 acc: 0.9086046814918518\n",
      "step: 40290\n",
      "train: loss: 155519.03125 acc: 0.936995804309845  val: loss: 477210.53125 acc: 0.6250241994857788\n",
      "step: 40295\n",
      "train: loss: 400352.78125 acc: 0.9806502461433411  val: loss: 899220.6875 acc: 0.8539292216300964\n",
      "step: 40300\n",
      "train: loss: 638988.375 acc: 0.9824666976928711  val: loss: 1293941.625 acc: 0.7250012159347534\n",
      "step: 40305\n",
      "train: loss: 2084360.25 acc: 0.9326106905937195  val: loss: 2148978.25 acc: 0.4756167531013489\n",
      "step: 40310\n",
      "train: loss: 748783.75 acc: 0.9663531184196472  val: loss: 2177304.0 acc: 0.7738257646560669\n",
      "step: 40315\n",
      "train: loss: 545627.75 acc: 0.9620460271835327  val: loss: 902920.25 acc: 0.89134681224823\n",
      "step: 40320\n",
      "train: loss: 196735.421875 acc: 0.9434379935264587  val: loss: 174524.671875 acc: 0.950106143951416\n",
      "step: 40325\n",
      "train: loss: 397841.75 acc: 0.9430090188980103  val: loss: 1248273.875 acc: 0.6127490997314453\n",
      "step: 40330\n",
      "train: loss: 481619.21875 acc: 0.9381783604621887  val: loss: 790109.75 acc: 0.7355118989944458\n",
      "step: 40335\n",
      "train: loss: 673433.25 acc: 0.6212576627731323  val: loss: 2420283.5 acc: 0.6159641742706299\n",
      "step: 40340\n",
      "train: loss: 608170.1875 acc: 0.7732391357421875  val: loss: 803459.5625 acc: 0.6508594751358032\n",
      "step: 40345\n",
      "train: loss: 323128.1875 acc: 0.808591902256012  val: loss: 778733.125 acc: 0.7640179991722107\n",
      "step: 40350\n",
      "train: loss: 452284.03125 acc: 0.8327031135559082  val: loss: 2419298.0 acc: 0.6300933361053467\n",
      "step: 40355\n",
      "train: loss: 508129.875 acc: 0.7537990808486938  val: loss: 388139.09375 acc: 0.7936859130859375\n",
      "step: 40360\n",
      "train: loss: 493113.65625 acc: 0.7609981298446655  val: loss: 2184577.75 acc: 0.6437042951583862\n",
      "step: 40365\n",
      "train: loss: 195224.390625 acc: 0.8891900777816772  val: loss: 3443756.25 acc: 0.5359756946563721\n",
      "step: 40370\n",
      "train: loss: 36952.21484375 acc: 0.9674195647239685  val: loss: 1625174.375 acc: 0.637117326259613\n",
      "step: 40375\n",
      "train: loss: 32971.18359375 acc: 0.9726142287254333  val: loss: 3859890.5 acc: 0.5661741495132446\n",
      "step: 40380\n",
      "train: loss: 260374.6875 acc: 0.8647326231002808  val: loss: 3477474.0 acc: 0.5437413454055786\n",
      "step: 40385\n",
      "train: loss: 128000.1875 acc: 0.9089500308036804  val: loss: 2981631.5 acc: 0.5256388187408447\n",
      "step: 40390\n",
      "train: loss: 80861.2109375 acc: 0.9418196082115173  val: loss: 1941003.5 acc: 0.6261270046234131\n",
      "step: 40395\n",
      "train: loss: 368430.6875 acc: 0.8095643520355225  val: loss: 4305110.0 acc: 0.5683820247650146\n",
      "step: 40400\n",
      "train: loss: 265866.125 acc: 0.8419011235237122  val: loss: 2112749.25 acc: 0.6649870276451111\n",
      "step: 40405\n",
      "train: loss: 269919.90625 acc: 0.8264169692993164  val: loss: 91971.8984375 acc: 0.9273675084114075\n",
      "step: 40410\n",
      "train: loss: 108921.65625 acc: 0.9178118705749512  val: loss: 1812082.875 acc: 0.675968587398529\n",
      "step: 40415\n",
      "train: loss: 213825.8125 acc: 0.8765419721603394  val: loss: 3121822.75 acc: 0.5484911203384399\n",
      "step: 40420\n",
      "train: loss: 248141.90625 acc: 0.7886466979980469  val: loss: 2386077.25 acc: 0.5669147968292236\n",
      "step: 40425\n",
      "train: loss: 143663.359375 acc: 0.8424104452133179  val: loss: 1131804.5 acc: 0.6094059944152832\n",
      "step: 40430\n",
      "train: loss: 29002.6953125 acc: 0.9681404232978821  val: loss: 1487520.0 acc: 0.6451796293258667\n",
      "step: 40435\n",
      "train: loss: 641147.4375 acc: 0.7390782833099365  val: loss: 229526.703125 acc: 0.8413758873939514\n",
      "step: 40440\n",
      "train: loss: 1429584.5 acc: 0.6805923581123352  val: loss: 1037063.8125 acc: 0.7580991387367249\n",
      "step: 40445\n",
      "train: loss: 948850.5625 acc: 0.7385025024414062  val: loss: 836288.625 acc: 0.8287168741226196\n",
      "step: 40450\n",
      "train: loss: 208051.921875 acc: 0.9773493409156799  val: loss: 863151.75 acc: 0.6644527912139893\n",
      "step: 40455\n",
      "train: loss: 275201.09375 acc: 0.9754704833030701  val: loss: 1035549.375 acc: -0.036855220794677734\n",
      "step: 40460\n",
      "train: loss: 623844.75 acc: 0.9110546112060547  val: loss: 681664.0625 acc: 0.8923979997634888\n",
      "step: 40465\n",
      "train: loss: 1060260.5 acc: 0.6595733165740967  val: loss: 324563.875 acc: 0.9261050224304199\n",
      "step: 40470\n",
      "train: loss: 245063.84375 acc: 0.9691523909568787  val: loss: 535454.8125 acc: 0.8453490734100342\n",
      "step: 40475\n",
      "train: loss: 58907.9375 acc: 0.994541347026825  val: loss: 1170117.375 acc: 0.5952715873718262\n",
      "step: 40480\n",
      "train: loss: 80324.0 acc: 0.994801938533783  val: loss: 207177.609375 acc: 0.9390296339988708\n",
      "step: 40485\n",
      "train: loss: 44363.0390625 acc: 0.9960052967071533  val: loss: 253597.40625 acc: 0.9123323559761047\n",
      "step: 40490\n",
      "train: loss: 38719.24609375 acc: 0.9943691492080688  val: loss: 564273.875 acc: 0.7883862257003784\n",
      "step: 40495\n",
      "train: loss: 12192.06640625 acc: 0.9972801804542542  val: loss: 416545.59375 acc: 0.894365131855011\n",
      "step: 40500\n",
      "train: loss: 28958.365234375 acc: 0.9813799262046814  val: loss: 871470.0 acc: 0.8847050666809082\n",
      "step: 40505\n",
      "train: loss: 13089.3564453125 acc: 0.9594811201095581  val: loss: 618912.8125 acc: 0.9203764796257019\n",
      "step: 40510\n",
      "train: loss: 8175.37939453125 acc: 0.9855867028236389  val: loss: 2202738.0 acc: 0.4532567262649536\n",
      "step: 40515\n",
      "train: loss: 15961.1845703125 acc: 0.9837355613708496  val: loss: 429609.75 acc: 0.8207002282142639\n",
      "step: 40520\n",
      "train: loss: 12533.06640625 acc: 0.9707456231117249  val: loss: 510517.65625 acc: 0.9487638473510742\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 40525\n",
      "train: loss: 14453.1064453125 acc: 0.9678022265434265  val: loss: 1933792.875 acc: 0.8239361047744751\n",
      "step: 40530\n",
      "train: loss: 23171.23046875 acc: 0.9627689123153687  val: loss: 551100.3125 acc: 0.8849520683288574\n",
      "step: 40535\n",
      "train: loss: 11441.66015625 acc: 0.9756490588188171  val: loss: 916325.125 acc: 0.8644919991493225\n",
      "step: 40540\n",
      "train: loss: 5300.87841796875 acc: 0.9800113439559937  val: loss: 1121660.875 acc: 0.8048562407493591\n",
      "step: 40545\n",
      "train: loss: 7893.12158203125 acc: 0.9947088956832886  val: loss: 3253558.25 acc: 0.5444754958152771\n",
      "step: 40550\n",
      "train: loss: 13310.3349609375 acc: 0.9940754175186157  val: loss: 391523.875 acc: 0.9410711526870728\n",
      "step: 40555\n",
      "train: loss: 82652.875 acc: 0.9376710057258606  val: loss: 1435141.625 acc: 0.847348690032959\n",
      "step: 40560\n",
      "train: loss: 5859.396484375 acc: 0.9974104762077332  val: loss: 1261417.5 acc: 0.7204513549804688\n",
      "step: 40565\n",
      "train: loss: 32847.76171875 acc: 0.9768543243408203  val: loss: 993458.8125 acc: 0.8536662459373474\n",
      "step: 40570\n",
      "train: loss: 14693.90234375 acc: 0.9857805967330933  val: loss: 487111.84375 acc: 0.9348049759864807\n",
      "step: 40575\n",
      "train: loss: 10675.560546875 acc: 0.9931429624557495  val: loss: 1201438.875 acc: 0.8830084800720215\n",
      "step: 40580\n",
      "train: loss: 30981.751953125 acc: 0.9909363389015198  val: loss: 1869152.625 acc: 0.5389626026153564\n",
      "step: 40585\n",
      "train: loss: 23396.224609375 acc: 0.989201545715332  val: loss: 1119493.75 acc: 0.7236055731773376\n",
      "step: 40590\n",
      "train: loss: 27369.404296875 acc: 0.994459867477417  val: loss: 1683221.125 acc: 0.38014042377471924\n",
      "step: 40595\n",
      "train: loss: 31111.181640625 acc: 0.9862067103385925  val: loss: 1925081.0 acc: 0.6969764232635498\n",
      "step: 40600\n",
      "train: loss: 77016.7109375 acc: 0.9788883328437805  val: loss: 1170076.125 acc: 0.8306013345718384\n",
      "step: 40605\n",
      "train: loss: 76991.1484375 acc: 0.9780133962631226  val: loss: 677343.0625 acc: 0.9067421555519104\n",
      "step: 40610\n",
      "train: loss: 36379.0234375 acc: 0.9850773811340332  val: loss: 3270816.75 acc: 0.6030492186546326\n",
      "step: 40615\n",
      "train: loss: 16791.443359375 acc: 0.9947724938392639  val: loss: 1492870.375 acc: 0.5933421850204468\n",
      "step: 40620\n",
      "train: loss: 69223.1796875 acc: 0.991087019443512  val: loss: 1016539.625 acc: 0.6119522452354431\n",
      "step: 40625\n",
      "train: loss: 221316.953125 acc: 0.9764103889465332  val: loss: 982588.3125 acc: 0.7400691509246826\n",
      "step: 40630\n",
      "train: loss: 62223.46875 acc: 0.9937195181846619  val: loss: 690732.0 acc: 0.6361016035079956\n",
      "step: 40635\n",
      "train: loss: 245285.671875 acc: 0.9530037641525269  val: loss: 1457983.125 acc: 0.3968348503112793\n",
      "step: 40640\n",
      "train: loss: 210147.125 acc: 0.9704881906509399  val: loss: 1159815.5 acc: 0.7586634755134583\n",
      "step: 40645\n",
      "train: loss: 547718.8125 acc: 0.95787513256073  val: loss: 438704.59375 acc: 0.9285854697227478\n",
      "step: 40650\n",
      "train: loss: 216541.671875 acc: 0.9753692746162415  val: loss: 2045528.125 acc: 0.5348637700080872\n",
      "step: 40655\n",
      "train: loss: 176457.703125 acc: 0.9739142656326294  val: loss: 2823210.25 acc: 0.45168590545654297\n",
      "step: 40660\n",
      "train: loss: 1210716.0 acc: 0.95602947473526  val: loss: 1087464.75 acc: 0.5914943218231201\n",
      "step: 40665\n",
      "train: loss: 740395.375 acc: 0.9791194200515747  val: loss: 579093.25 acc: 0.8625173568725586\n",
      "step: 40670\n",
      "train: loss: 1423361.0 acc: 0.9506319165229797  val: loss: 3720371.75 acc: -0.4632219076156616\n",
      "step: 40675\n",
      "train: loss: 2258334.25 acc: 0.7894479036331177  val: loss: 859151.875 acc: 0.7483682632446289\n",
      "step: 40680\n",
      "train: loss: 671214.0 acc: 0.9455220103263855  val: loss: 1583574.5 acc: 0.40298765897750854\n",
      "step: 40685\n",
      "train: loss: 1820716.0 acc: 0.803770899772644  val: loss: 1036700.125 acc: 0.8217467069625854\n",
      "step: 40690\n",
      "train: loss: 1058731.625 acc: 0.9285215139389038  val: loss: 939865.375 acc: 0.9233055710792542\n",
      "step: 40695\n",
      "train: loss: 640975.75 acc: 0.8693418502807617  val: loss: 1929090.375 acc: 0.7610394954681396\n",
      "step: 40700\n",
      "train: loss: 943999.125 acc: 0.6743358969688416  val: loss: 2116503.5 acc: 0.7018569707870483\n",
      "step: 40705\n",
      "train: loss: 1221187.375 acc: 0.6622202396392822  val: loss: 678363.0 acc: 0.8383281826972961\n",
      "step: 40710\n",
      "train: loss: 354740.375 acc: 0.6617786884307861  val: loss: 610847.5625 acc: 0.7971317768096924\n",
      "step: 40715\n",
      "train: loss: 323050.53125 acc: 0.8631486296653748  val: loss: 1403802.625 acc: 0.8084088563919067\n",
      "step: 40720\n",
      "train: loss: 351847.0625 acc: 0.8230736255645752  val: loss: 2053549.5 acc: 0.6580028533935547\n",
      "step: 40725\n",
      "train: loss: 591367.375 acc: 0.7566325068473816  val: loss: 2083286.5 acc: 0.6421018838882446\n",
      "step: 40730\n",
      "train: loss: 42966.73828125 acc: 0.965549111366272  val: loss: 2647352.5 acc: 0.5695328712463379\n",
      "step: 40735\n",
      "train: loss: 316326.40625 acc: 0.8236916065216064  val: loss: 1262161.125 acc: 0.6527563333511353\n",
      "step: 40740\n",
      "train: loss: 215572.59375 acc: 0.8793399930000305  val: loss: 975557.875 acc: 0.6699923276901245\n",
      "step: 40745\n",
      "train: loss: 133275.84375 acc: 0.9005124568939209  val: loss: 490231.40625 acc: 0.761470377445221\n",
      "step: 40750\n",
      "train: loss: 403205.3125 acc: 0.7801545858383179  val: loss: 473365.59375 acc: 0.7755874991416931\n",
      "step: 40755\n",
      "train: loss: 78799.953125 acc: 0.9466025829315186  val: loss: 1415578.375 acc: 0.6484047174453735\n",
      "step: 40760\n",
      "train: loss: 137540.140625 acc: 0.9160231947898865  val: loss: 822972.1875 acc: 0.6873208284378052\n",
      "step: 40765\n",
      "train: loss: 138415.640625 acc: 0.8913629055023193  val: loss: 465503.53125 acc: 0.7303290963172913\n",
      "step: 40770\n",
      "train: loss: 51244.19140625 acc: 0.9529251456260681  val: loss: 491183.59375 acc: 0.7435058951377869\n",
      "step: 40775\n",
      "train: loss: 509271.75 acc: 0.7516088485717773  val: loss: 330583.75 acc: 0.7891421318054199\n",
      "step: 40780\n",
      "train: loss: 297275.71875 acc: 0.8288180828094482  val: loss: 1190323.125 acc: 0.7016658782958984\n",
      "step: 40785\n",
      "train: loss: 484788.1875 acc: 0.7720919251441956  val: loss: 472607.59375 acc: 0.7684153318405151\n",
      "step: 40790\n",
      "train: loss: 125626.4375 acc: 0.8817678093910217  val: loss: 3688958.5 acc: 0.5327810645103455\n",
      "step: 40795\n",
      "train: loss: 39239.4296875 acc: 0.9523411989212036  val: loss: 703447.125 acc: 0.7099051475524902\n",
      "step: 40800\n",
      "train: loss: 234345.75 acc: 0.874732494354248  val: loss: 584294.9375 acc: 0.7535516023635864\n",
      "step: 40805\n",
      "train: loss: 439849.875 acc: 0.7929942011833191  val: loss: 569652.125 acc: 0.780579686164856\n",
      "step: 40810\n",
      "train: loss: 324561.1875 acc: 0.9572799205780029  val: loss: 668969.4375 acc: 0.8320693373680115\n",
      "step: 40815\n",
      "train: loss: 890429.125 acc: 0.9072868824005127  val: loss: 125506.9453125 acc: 0.9600465297698975\n",
      "step: 40820\n",
      "train: loss: 1110656.5 acc: 0.7773997783660889  val: loss: 142473.921875 acc: 0.9699925184249878\n",
      "step: 40825\n",
      "train: loss: 84906.2265625 acc: 0.9925767183303833  val: loss: 800253.5625 acc: 0.8016392588615417\n",
      "step: 40830\n",
      "train: loss: 135911.609375 acc: 0.9783575534820557  val: loss: 185010.640625 acc: 0.9652806520462036\n",
      "step: 40835\n",
      "train: loss: 67551.1171875 acc: 0.9838795065879822  val: loss: 776268.625 acc: 0.786457896232605\n",
      "step: 40840\n",
      "train: loss: 37343.921875 acc: 0.9970836639404297  val: loss: 1399646.875 acc: 0.8984344005584717\n",
      "step: 40845\n",
      "train: loss: 54256.64453125 acc: 0.9956346750259399  val: loss: 1446936.375 acc: 0.8814436793327332\n",
      "step: 40850\n",
      "train: loss: 29290.74609375 acc: 0.9975123405456543  val: loss: 989229.0 acc: 0.8055287599563599\n",
      "step: 40855\n",
      "train: loss: 19436.212890625 acc: 0.9969302415847778  val: loss: 332257.0 acc: 0.8676190376281738\n",
      "step: 40860\n",
      "train: loss: 24069.935546875 acc: 0.9961816668510437  val: loss: 726946.125 acc: 0.9202238917350769\n",
      "step: 40865\n",
      "train: loss: 18210.060546875 acc: 0.9912056922912598  val: loss: 762767.875 acc: 0.6354809999465942\n",
      "step: 40870\n",
      "train: loss: 18474.26953125 acc: 0.9897920489311218  val: loss: 253770.046875 acc: 0.9423805475234985\n",
      "step: 40875\n",
      "train: loss: 12436.60546875 acc: 0.9845014214515686  val: loss: 836321.75 acc: 0.9157935380935669\n",
      "step: 40880\n",
      "train: loss: 17076.3046875 acc: 0.9754700064659119  val: loss: 1974494.125 acc: 0.0037001371383666992\n",
      "step: 40885\n",
      "train: loss: 9642.439453125 acc: 0.9762606024742126  val: loss: 1734766.75 acc: 0.7850392460823059\n",
      "step: 40890\n",
      "train: loss: 4783.626953125 acc: 0.9909942150115967  val: loss: 1353084.5 acc: 0.5729553699493408\n",
      "step: 40895\n",
      "train: loss: 10280.5341796875 acc: 0.9784550070762634  val: loss: 1955453.625 acc: 0.8284187912940979\n",
      "step: 40900\n",
      "train: loss: 8071.419921875 acc: 0.9824479818344116  val: loss: 546300.875 acc: 0.363842248916626\n",
      "step: 40905\n",
      "train: loss: 10261.5966796875 acc: 0.9855536818504333  val: loss: 1817039.0 acc: 0.37952178716659546\n",
      "step: 40910\n",
      "train: loss: 16306.0986328125 acc: 0.9933855533599854  val: loss: 424601.53125 acc: 0.8459864258766174\n",
      "step: 40915\n",
      "train: loss: 25380.29296875 acc: 0.9895423054695129  val: loss: 1082750.375 acc: 0.8291649222373962\n",
      "step: 40920\n",
      "train: loss: 83848.7109375 acc: 0.9212990999221802  val: loss: 1055822.0 acc: 0.7076422572135925\n",
      "step: 40925\n",
      "train: loss: 44556.4453125 acc: 0.9801511764526367  val: loss: 3586929.25 acc: 0.21617043018341064\n",
      "step: 40930\n",
      "train: loss: 9273.529296875 acc: 0.9927831292152405  val: loss: 4038891.25 acc: -0.9182496070861816\n",
      "step: 40935\n",
      "train: loss: 6437.68994140625 acc: 0.9922014474868774  val: loss: 1428314.875 acc: 0.6739825010299683\n",
      "step: 40940\n",
      "train: loss: 6956.02978515625 acc: 0.9959791302680969  val: loss: 1368135.625 acc: 0.8792033791542053\n",
      "step: 40945\n",
      "train: loss: 30685.337890625 acc: 0.9881846308708191  val: loss: 1578191.375 acc: 0.6645410060882568\n",
      "step: 40950\n",
      "train: loss: 34036.1953125 acc: 0.99085932970047  val: loss: 887268.25 acc: 0.825020968914032\n",
      "step: 40955\n",
      "train: loss: 53935.3984375 acc: 0.9852811098098755  val: loss: 2556386.75 acc: 0.6802411079406738\n",
      "step: 40960\n",
      "train: loss: 25152.173828125 acc: 0.9857418537139893  val: loss: 2516976.75 acc: 0.4902874231338501\n",
      "step: 40965\n",
      "train: loss: 52175.73828125 acc: 0.9880152344703674  val: loss: 1983169.375 acc: 0.6694763898849487\n",
      "step: 40970\n",
      "train: loss: 214770.46875 acc: 0.927105724811554  val: loss: 1317848.125 acc: -0.22405970096588135\n",
      "step: 40975\n",
      "train: loss: 53848.1640625 acc: 0.9687857031822205  val: loss: 954446.375 acc: 0.7455806136131287\n",
      "step: 40980\n",
      "train: loss: 56510.69921875 acc: 0.9919945597648621  val: loss: 1790584.875 acc: 0.7228952050209045\n",
      "step: 40985\n",
      "train: loss: 30292.931640625 acc: 0.9958820343017578  val: loss: 1390756.5 acc: 0.8601851463317871\n",
      "step: 40990\n",
      "train: loss: 35204.42578125 acc: 0.9965770244598389  val: loss: 3044052.25 acc: 0.0027459263801574707\n",
      "step: 40995\n",
      "train: loss: 28190.5390625 acc: 0.996571958065033  val: loss: 1339016.25 acc: 0.8668578267097473\n",
      "step: 41000\n",
      "train: loss: 552838.0625 acc: 0.9556527137756348  val: loss: 2848329.5 acc: 0.4847090244293213\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 41005\n",
      "train: loss: 622062.8125 acc: 0.9594945311546326  val: loss: 1020970.6875 acc: 0.5493701696395874\n",
      "step: 41010\n",
      "train: loss: 471674.21875 acc: 0.9634225964546204  val: loss: 964267.9375 acc: 0.8521242141723633\n",
      "step: 41015\n",
      "train: loss: 172261.96875 acc: 0.9761470556259155  val: loss: 776424.875 acc: 0.49200141429901123\n",
      "step: 41020\n",
      "train: loss: 175504.34375 acc: 0.9755338430404663  val: loss: 1501293.125 acc: 0.8634238243103027\n",
      "step: 41025\n",
      "train: loss: 667715.25 acc: 0.9637940526008606  val: loss: 685182.25 acc: 0.8605571985244751\n",
      "step: 41030\n",
      "train: loss: 4497744.5 acc: 0.8250746726989746  val: loss: 469689.125 acc: 0.9505280256271362\n",
      "step: 41035\n",
      "train: loss: 1269558.5 acc: 0.9525925517082214  val: loss: 1308561.25 acc: 0.6778974533081055\n",
      "step: 41040\n",
      "train: loss: 895555.8125 acc: 0.9560920596122742  val: loss: 858440.9375 acc: 0.8447262048721313\n",
      "step: 41045\n",
      "train: loss: 511754.84375 acc: 0.9629940986633301  val: loss: 499763.6875 acc: 0.8423755764961243\n",
      "step: 41050\n",
      "train: loss: 1023500.125 acc: 0.9038918018341064  val: loss: 285487.40625 acc: 0.9220138788223267\n",
      "step: 41055\n",
      "train: loss: 957594.625 acc: 0.90440434217453  val: loss: 1164584.375 acc: 0.6381781101226807\n",
      "step: 41060\n",
      "train: loss: 408215.3125 acc: 0.9500764608383179  val: loss: 465433.1875 acc: 0.8531727194786072\n",
      "step: 41065\n",
      "train: loss: 1271313.625 acc: 0.327400803565979  val: loss: 1134699.375 acc: 0.8132537603378296\n",
      "step: 41070\n",
      "train: loss: 671981.875 acc: 0.6534177660942078  val: loss: 2659170.75 acc: 0.5830909013748169\n",
      "step: 41075\n",
      "train: loss: 713819.4375 acc: 0.7957708835601807  val: loss: 543675.875 acc: 0.6400296688079834\n",
      "step: 41080\n",
      "train: loss: 513552.5625 acc: 0.7866527438163757  val: loss: 2604853.75 acc: 0.6829597353935242\n",
      "step: 41085\n",
      "train: loss: 486597.75 acc: 0.8044801950454712  val: loss: 1203157.5 acc: 0.6913607120513916\n",
      "step: 41090\n",
      "train: loss: 273309.84375 acc: 0.8508639335632324  val: loss: 530236.375 acc: 0.7591304183006287\n",
      "step: 41095\n",
      "train: loss: 28310.357421875 acc: 0.9785693287849426  val: loss: 735553.5 acc: 0.6544656753540039\n",
      "step: 41100\n",
      "train: loss: 574282.25 acc: 0.7318551540374756  val: loss: 560165.0 acc: 0.6556852459907532\n",
      "step: 41105\n",
      "train: loss: 11654.8212890625 acc: 0.9892087578773499  val: loss: 628576.25 acc: 0.7289528250694275\n",
      "step: 41110\n",
      "train: loss: 134262.625 acc: 0.9145398736000061  val: loss: 942973.1875 acc: 0.6896795034408569\n",
      "step: 41115\n",
      "train: loss: 56017.9296875 acc: 0.9597522616386414  val: loss: 3447298.75 acc: 0.4811517000198364\n",
      "step: 41120\n",
      "train: loss: 182534.609375 acc: 0.8875150084495544  val: loss: 890284.3125 acc: 0.6686156988143921\n",
      "step: 41125\n",
      "train: loss: 106101.6015625 acc: 0.9269774556159973  val: loss: 3293507.5 acc: 0.5467061996459961\n",
      "step: 41130\n",
      "train: loss: 113428.46875 acc: 0.9265658259391785  val: loss: 1040578.75 acc: 0.7324521541595459\n",
      "step: 41135\n",
      "train: loss: 39023.9453125 acc: 0.9613246321678162  val: loss: 2512890.5 acc: 0.6289982795715332\n",
      "step: 41140\n",
      "train: loss: 344965.9375 acc: 0.75724858045578  val: loss: 627438.5 acc: 0.7009369134902954\n",
      "step: 41145\n",
      "train: loss: 421643.34375 acc: 0.7760018706321716  val: loss: 3858633.25 acc: 0.5665000677108765\n",
      "step: 41150\n",
      "train: loss: 92954.1328125 acc: 0.9075960516929626  val: loss: 3226072.25 acc: 0.48493874073028564\n",
      "step: 41155\n",
      "train: loss: 75320.9609375 acc: 0.9249426126480103  val: loss: 1027474.625 acc: 0.7356122136116028\n",
      "step: 41160\n",
      "train: loss: 115876.2890625 acc: 0.8925332427024841  val: loss: 1417891.25 acc: 0.6705994009971619\n",
      "step: 41165\n",
      "train: loss: 322555.375 acc: 0.8024724721908569  val: loss: 3488571.25 acc: 0.5471837520599365\n",
      "step: 41170\n",
      "train: loss: 1005528.875 acc: 0.7509799003601074  val: loss: 4235221.5 acc: 0.630378007888794\n",
      "step: 41175\n",
      "train: loss: 815815.0625 acc: 0.8767088055610657  val: loss: 1875751.875 acc: 0.8589417338371277\n",
      "step: 41180\n",
      "train: loss: 1647021.125 acc: 0.685937762260437  val: loss: 1818336.625 acc: 0.8054372072219849\n",
      "step: 41185\n",
      "train: loss: 1101690.5 acc: 0.8867849111557007  val: loss: 147009.453125 acc: 0.9451919794082642\n",
      "step: 41190\n",
      "train: loss: 54889.53125 acc: 0.9934085607528687  val: loss: 1552882.625 acc: 0.8817448616027832\n",
      "step: 41195\n",
      "train: loss: 77463.390625 acc: 0.9908732175827026  val: loss: 267208.84375 acc: 0.9467509388923645\n",
      "step: 41200\n",
      "train: loss: 100504.6015625 acc: 0.979997992515564  val: loss: 1050237.375 acc: 0.9121872186660767\n",
      "step: 41205\n",
      "train: loss: 39959.4765625 acc: 0.9966712594032288  val: loss: 984468.25 acc: 0.8138650059700012\n",
      "step: 41210\n",
      "train: loss: 82252.3046875 acc: 0.9934180378913879  val: loss: 816222.5625 acc: 0.441652774810791\n",
      "step: 41215\n",
      "train: loss: 27830.38671875 acc: 0.9977162480354309  val: loss: 2318456.75 acc: 0.6166459321975708\n",
      "step: 41220\n",
      "train: loss: 28946.01171875 acc: 0.9957445859909058  val: loss: 3067329.5 acc: 0.5200937390327454\n",
      "step: 41225\n",
      "train: loss: 26766.39453125 acc: 0.9958139657974243  val: loss: 1612747.875 acc: 0.7379765510559082\n",
      "step: 41230\n",
      "train: loss: 8494.5732421875 acc: 0.9967100620269775  val: loss: 2154809.0 acc: 0.7742050886154175\n",
      "step: 41235\n",
      "train: loss: 6305.724609375 acc: 0.997489869594574  val: loss: 595088.875 acc: 0.7871399521827698\n",
      "step: 41240\n",
      "train: loss: 10389.5029296875 acc: 0.9757964015007019  val: loss: 363372.4375 acc: 0.9245297312736511\n",
      "step: 41245\n",
      "train: loss: 6944.53759765625 acc: 0.9917174577713013  val: loss: 2158837.5 acc: -0.5747355222702026\n",
      "step: 41250\n",
      "train: loss: 235221.96875 acc: 0.6080812215805054  val: loss: 1110281.125 acc: 0.6281459331512451\n",
      "step: 41255\n",
      "train: loss: 11446.7685546875 acc: 0.9864175915718079  val: loss: 1235598.375 acc: 0.8143216371536255\n",
      "step: 41260\n",
      "train: loss: 9634.5283203125 acc: 0.9931766390800476  val: loss: 1105445.25 acc: 0.40675634145736694\n",
      "step: 41265\n",
      "train: loss: 2950.925048828125 acc: 0.9849690794944763  val: loss: 1110249.625 acc: 0.7818084359169006\n",
      "step: 41270\n",
      "train: loss: 7232.6240234375 acc: 0.9837580919265747  val: loss: 1518527.875 acc: 0.6500738859176636\n",
      "step: 41275\n",
      "train: loss: 59189.71875 acc: 0.9733191132545471  val: loss: 2873174.0 acc: 0.5159324407577515\n",
      "step: 41280\n",
      "train: loss: 8914.73046875 acc: 0.9940870404243469  val: loss: 1067341.5 acc: 0.5958179235458374\n",
      "step: 41285\n",
      "train: loss: 9759.6181640625 acc: 0.9956955909729004  val: loss: 422290.6875 acc: 0.9232901334762573\n",
      "step: 41290\n",
      "train: loss: 18378.54296875 acc: 0.9901213645935059  val: loss: 1615083.875 acc: 0.698147177696228\n",
      "step: 41295\n",
      "train: loss: 77112.3203125 acc: 0.9294959306716919  val: loss: 1763136.25 acc: 0.00461810827255249\n",
      "step: 41300\n",
      "train: loss: 6914.90673828125 acc: 0.9934592843055725  val: loss: 1459227.0 acc: 0.7515846490859985\n",
      "step: 41305\n",
      "train: loss: 4773.92822265625 acc: 0.9942107200622559  val: loss: 2802234.0 acc: 0.2228107452392578\n",
      "step: 41310\n",
      "train: loss: 24352.662109375 acc: 0.9906521439552307  val: loss: 2219251.5 acc: 0.6393914818763733\n",
      "step: 41315\n",
      "train: loss: 32932.984375 acc: 0.992656946182251  val: loss: 754551.8125 acc: 0.9127040505409241\n",
      "step: 41320\n",
      "train: loss: 23791.2265625 acc: 0.9887995719909668  val: loss: 3126574.5 acc: 0.12967413663864136\n",
      "step: 41325\n",
      "train: loss: 8567.794921875 acc: 0.9962146282196045  val: loss: 1471646.625 acc: 0.633142352104187\n",
      "step: 41330\n",
      "train: loss: 69278.9296875 acc: 0.9861552715301514  val: loss: 557742.5 acc: 0.8200666308403015\n",
      "step: 41335\n",
      "train: loss: 75971.421875 acc: 0.9746721982955933  val: loss: 651071.8125 acc: 0.22628533840179443\n",
      "step: 41340\n",
      "train: loss: 77270.75 acc: 0.9732924103736877  val: loss: 1895796.875 acc: 0.7271286845207214\n",
      "step: 41345\n",
      "train: loss: 77936.0546875 acc: 0.9836176633834839  val: loss: 2295406.25 acc: 0.4260759949684143\n",
      "step: 41350\n",
      "train: loss: 29092.3984375 acc: 0.9957149624824524  val: loss: 91023.0625 acc: 0.9751862287521362\n",
      "step: 41355\n",
      "train: loss: 106240.59375 acc: 0.9903433918952942  val: loss: 1563070.75 acc: 0.7274287939071655\n",
      "step: 41360\n",
      "train: loss: 62686.8125 acc: 0.9912973046302795  val: loss: 834426.625 acc: 0.8001989722251892\n",
      "step: 41365\n",
      "train: loss: 393304.90625 acc: 0.9493690133094788  val: loss: 755025.5 acc: 0.8588425517082214\n",
      "step: 41370\n",
      "train: loss: 274003.375 acc: 0.967052698135376  val: loss: 1590381.0 acc: 0.6765282154083252\n",
      "step: 41375\n",
      "train: loss: 536394.8125 acc: 0.9613584280014038  val: loss: 462286.53125 acc: 0.9243783950805664\n",
      "step: 41380\n",
      "train: loss: 321911.3125 acc: 0.9546853303909302  val: loss: 425747.8125 acc: 0.8944072127342224\n",
      "step: 41385\n",
      "train: loss: 200140.453125 acc: 0.9668967127799988  val: loss: 1183794.125 acc: 0.8034448623657227\n",
      "step: 41390\n",
      "train: loss: 347478.5 acc: 0.9831438660621643  val: loss: 1842903.125 acc: 0.1662464737892151\n",
      "step: 41395\n",
      "train: loss: 913611.875 acc: 0.9668976664543152  val: loss: 660430.0625 acc: 0.8729164600372314\n",
      "step: 41400\n",
      "train: loss: 1502725.625 acc: 0.8988777995109558  val: loss: 599189.625 acc: 0.8515315651893616\n",
      "step: 41405\n",
      "train: loss: 1242920.625 acc: 0.9502772092819214  val: loss: 455398.03125 acc: 0.9263575673103333\n",
      "step: 41410\n",
      "train: loss: 1718647.875 acc: 0.8836514949798584  val: loss: 1515243.5 acc: 0.5733667612075806\n",
      "step: 41415\n",
      "train: loss: 853037.5 acc: 0.9279170632362366  val: loss: 1637037.875 acc: 0.7842861413955688\n",
      "step: 41420\n",
      "train: loss: 640537.6875 acc: 0.9339695572853088  val: loss: 279809.3125 acc: 0.9324737191200256\n",
      "step: 41425\n",
      "train: loss: 759026.4375 acc: 0.9183512330055237  val: loss: 419811.28125 acc: 0.8385664820671082\n",
      "step: 41430\n",
      "train: loss: 1531297.625 acc: 0.5010080933570862  val: loss: 483958.0 acc: 0.8356682062149048\n",
      "step: 41435\n",
      "train: loss: 624006.125 acc: 0.7094905972480774  val: loss: 653755.75 acc: 0.6354714632034302\n",
      "step: 41440\n",
      "train: loss: 334460.375 acc: 0.7461869716644287  val: loss: 2370952.25 acc: 0.6779145002365112\n",
      "step: 41445\n",
      "train: loss: 704470.1875 acc: 0.8148389458656311  val: loss: 760171.0 acc: 0.8338594436645508\n",
      "step: 41450\n",
      "train: loss: 534998.625 acc: 0.8186519145965576  val: loss: 1921992.375 acc: 0.7143151760101318\n",
      "step: 41455\n",
      "train: loss: 388616.125 acc: 0.7064573764801025  val: loss: 862680.0 acc: 0.7336055040359497\n",
      "step: 41460\n",
      "train: loss: 930565.75 acc: 0.7231600284576416  val: loss: 2325451.25 acc: 0.5893934369087219\n",
      "step: 41465\n",
      "train: loss: 396129.09375 acc: 0.825370192527771  val: loss: 2070596.125 acc: 0.6115061044692993\n",
      "step: 41470\n",
      "train: loss: 424791.6875 acc: 0.7943310737609863  val: loss: 4364205.5 acc: 0.5319989919662476\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 41475\n",
      "train: loss: 57514.734375 acc: 0.9565693140029907  val: loss: 5832339.5 acc: 0.5099296569824219\n",
      "step: 41480\n",
      "train: loss: 18317.259765625 acc: 0.9847238659858704  val: loss: 1616810.5 acc: 0.6471468210220337\n",
      "step: 41485\n",
      "train: loss: 43746.890625 acc: 0.9674314856529236  val: loss: 3745211.25 acc: 0.4909821152687073\n",
      "step: 41490\n",
      "train: loss: 379146.1875 acc: 0.8123579025268555  val: loss: 1478091.5 acc: 0.6531035304069519\n",
      "step: 41495\n",
      "train: loss: 67035.5546875 acc: 0.9452043771743774  val: loss: 2642026.25 acc: 0.4639654755592346\n",
      "step: 41500\n",
      "train: loss: 159648.421875 acc: 0.8005768656730652  val: loss: 326738.4375 acc: 0.8112659454345703\n",
      "step: 41505\n",
      "train: loss: 201246.453125 acc: 0.8579931259155273  val: loss: 3710058.5 acc: 0.5532101392745972\n",
      "step: 41510\n",
      "train: loss: 116731.546875 acc: 0.9269525408744812  val: loss: 4725775.5 acc: 0.5648486614227295\n",
      "step: 41515\n",
      "train: loss: 413611.6875 acc: 0.8152868747711182  val: loss: 2717628.0 acc: 0.5380685329437256\n",
      "step: 41520\n",
      "train: loss: 142025.515625 acc: 0.8933501243591309  val: loss: 2222075.5 acc: 0.6189162731170654\n",
      "step: 41525\n",
      "train: loss: 293183.1875 acc: 0.8492192625999451  val: loss: 3811342.5 acc: 0.5239157676696777\n",
      "step: 41530\n",
      "train: loss: 99628.2421875 acc: 0.9149540662765503  val: loss: 1141577.0 acc: 0.7086475491523743\n",
      "step: 41535\n",
      "train: loss: 601629.25 acc: 0.8097085356712341  val: loss: 2736433.75 acc: 0.6456405520439148\n",
      "step: 41540\n",
      "train: loss: 932701.5 acc: 0.8110105395317078  val: loss: 729731.875 acc: 0.8000671863555908\n",
      "step: 41545\n",
      "train: loss: 450470.1875 acc: 0.8262311220169067  val: loss: 1268194.625 acc: 0.8094176650047302\n",
      "step: 41550\n",
      "train: loss: 266532.875 acc: 0.9733471274375916  val: loss: 803580.375 acc: 0.5821968913078308\n",
      "step: 41555\n",
      "train: loss: 403446.46875 acc: 0.9432461857795715  val: loss: 1756403.625 acc: 0.7453862428665161\n",
      "step: 41560\n",
      "train: loss: 632090.6875 acc: 0.8738515377044678  val: loss: 1739924.375 acc: 0.8384279012680054\n",
      "step: 41565\n",
      "train: loss: 45987.55078125 acc: 0.9938671588897705  val: loss: 710647.375 acc: 0.8422600030899048\n",
      "step: 41570\n",
      "train: loss: 305674.75 acc: 0.9604004621505737  val: loss: 2793016.0 acc: 0.7646846175193787\n",
      "step: 41575\n",
      "train: loss: 61140.57421875 acc: 0.994100034236908  val: loss: 1460437.625 acc: 0.31933844089508057\n",
      "step: 41580\n",
      "train: loss: 27193.83984375 acc: 0.9973419904708862  val: loss: 722775.0 acc: 0.7315632104873657\n",
      "step: 41585\n",
      "train: loss: 50121.703125 acc: 0.9898600578308105  val: loss: 1250213.625 acc: 0.5225038528442383\n",
      "step: 41590\n",
      "train: loss: 33762.5625 acc: 0.9958522915840149  val: loss: 638722.625 acc: 0.7981129884719849\n",
      "step: 41595\n",
      "train: loss: 10700.4716796875 acc: 0.9975376129150391  val: loss: 1016498.375 acc: 0.7908667325973511\n",
      "step: 41600\n",
      "train: loss: 3849.125732421875 acc: 0.9981398582458496  val: loss: 1922445.375 acc: 0.4509636163711548\n",
      "step: 41605\n",
      "train: loss: 8004.43310546875 acc: 0.9893076419830322  val: loss: 445053.03125 acc: 0.6772780418395996\n",
      "step: 41610\n",
      "train: loss: 15547.1435546875 acc: 0.9755300879478455  val: loss: 1329944.25 acc: 0.4686213731765747\n",
      "step: 41615\n",
      "train: loss: 8496.099609375 acc: 0.9856757521629333  val: loss: 716838.0 acc: 0.8278799057006836\n",
      "step: 41620\n",
      "train: loss: 10899.904296875 acc: 0.9814803600311279  val: loss: 668633.6875 acc: 0.6910510659217834\n",
      "step: 41625\n",
      "train: loss: 4090.250732421875 acc: 0.9927027821540833  val: loss: 2557135.5 acc: 0.7260854840278625\n",
      "step: 41630\n",
      "train: loss: 10563.03125 acc: 0.9617882966995239  val: loss: 1311971.625 acc: 0.8033463954925537\n",
      "step: 41635\n",
      "train: loss: 16579.84375 acc: 0.9682673811912537  val: loss: 1256026.375 acc: 0.8654424548149109\n",
      "step: 41640\n",
      "train: loss: 31666.841796875 acc: 0.9820783734321594  val: loss: 2065534.25 acc: 0.7439640164375305\n",
      "step: 41645\n",
      "train: loss: 11939.4208984375 acc: 0.9910808801651001  val: loss: 2918089.5 acc: 0.562487006187439\n",
      "step: 41650\n",
      "train: loss: 8395.3212890625 acc: 0.9925471544265747  val: loss: 1793708.0 acc: 0.6573854684829712\n",
      "step: 41655\n",
      "train: loss: 19094.619140625 acc: 0.9899803996086121  val: loss: 627692.875 acc: 0.8877544403076172\n",
      "step: 41660\n",
      "train: loss: 9988.6123046875 acc: 0.9935848116874695  val: loss: 2630896.0 acc: 0.650903046131134\n",
      "step: 41665\n",
      "train: loss: 2983.742919921875 acc: 0.9958791136741638  val: loss: 1365471.75 acc: 0.67457115650177\n",
      "step: 41670\n",
      "train: loss: 6880.97119140625 acc: 0.9943602085113525  val: loss: 2221015.5 acc: 0.4740915298461914\n",
      "step: 41675\n",
      "train: loss: 34040.45703125 acc: 0.988662838935852  val: loss: 1149973.125 acc: 0.8415610790252686\n",
      "step: 41680\n",
      "train: loss: 21653.587890625 acc: 0.9955508708953857  val: loss: 650650.875 acc: 0.854583740234375\n",
      "step: 41685\n",
      "train: loss: 12182.92578125 acc: 0.9963091015815735  val: loss: 673842.8125 acc: 0.7521828413009644\n",
      "step: 41690\n",
      "train: loss: 12191.232421875 acc: 0.9963156580924988  val: loss: 724093.625 acc: -0.07414519786834717\n",
      "step: 41695\n",
      "train: loss: 32818.87109375 acc: 0.987555205821991  val: loss: 1606815.5 acc: 0.4029567837715149\n",
      "step: 41700\n",
      "train: loss: 45814.66015625 acc: 0.9844194650650024  val: loss: 974194.0 acc: 0.6718435287475586\n",
      "step: 41705\n",
      "train: loss: 30301.19140625 acc: 0.9918113946914673  val: loss: 593924.5625 acc: 0.770613431930542\n",
      "step: 41710\n",
      "train: loss: 52830.8515625 acc: 0.9805507063865662  val: loss: 1430542.125 acc: 0.675000786781311\n",
      "step: 41715\n",
      "train: loss: 55672.37890625 acc: 0.9904589056968689  val: loss: 1012766.8125 acc: 0.7654918432235718\n",
      "step: 41720\n",
      "train: loss: 45804.55078125 acc: 0.9949651956558228  val: loss: 933598.9375 acc: 0.8303232192993164\n",
      "step: 41725\n",
      "train: loss: 777251.3125 acc: 0.8458799123764038  val: loss: 592422.875 acc: 0.878583550453186\n",
      "step: 41730\n",
      "train: loss: 22777.369140625 acc: 0.9964277744293213  val: loss: 264084.09375 acc: 0.9302807450294495\n",
      "step: 41735\n",
      "train: loss: 199679.625 acc: 0.945595383644104  val: loss: 1491164.375 acc: 0.3501083254814148\n",
      "step: 41740\n",
      "train: loss: 923985.75 acc: 0.9628502130508423  val: loss: 1709865.125 acc: 0.7442799806594849\n",
      "step: 41745\n",
      "train: loss: 372401.46875 acc: 0.9679623246192932  val: loss: 707300.0 acc: 0.9055148363113403\n",
      "step: 41750\n",
      "train: loss: 194219.46875 acc: 0.980817437171936  val: loss: 388271.75 acc: 0.897972822189331\n",
      "step: 41755\n",
      "train: loss: 1919283.5 acc: 0.8895793557167053  val: loss: 417714.4375 acc: 0.8343749046325684\n",
      "step: 41760\n",
      "train: loss: 406437.09375 acc: 0.9896002411842346  val: loss: 248109.90625 acc: 0.9293010234832764\n",
      "step: 41765\n",
      "train: loss: 686967.875 acc: 0.9773519039154053  val: loss: 2840226.75 acc: -0.1505589485168457\n",
      "step: 41770\n",
      "train: loss: 1266276.125 acc: 0.9528919458389282  val: loss: 671228.875 acc: 0.9323168992996216\n",
      "step: 41775\n",
      "train: loss: 299174.59375 acc: 0.9583873748779297  val: loss: 429045.34375 acc: 0.8204739689826965\n",
      "step: 41780\n",
      "train: loss: 562816.6875 acc: 0.9526370167732239  val: loss: 983119.5625 acc: 0.9063017964363098\n",
      "step: 41785\n",
      "train: loss: 322310.5625 acc: 0.9235581159591675  val: loss: 480378.4375 acc: 0.9172080755233765\n",
      "step: 41790\n",
      "train: loss: 372554.90625 acc: 0.8762658834457397  val: loss: 351102.53125 acc: 0.9585589170455933\n",
      "step: 41795\n",
      "train: loss: 1806940.625 acc: 0.7435941100120544  val: loss: 1730455.125 acc: 0.8564331531524658\n",
      "step: 41800\n",
      "train: loss: 716396.75 acc: 0.7405140399932861  val: loss: 2844281.25 acc: 0.7171330451965332\n",
      "step: 41805\n",
      "train: loss: 446682.96875 acc: 0.8192340731620789  val: loss: 2429571.5 acc: 0.6810367703437805\n",
      "step: 41810\n",
      "train: loss: 392481.0625 acc: 0.8311700820922852  val: loss: 871078.6875 acc: 0.8023695349693298\n",
      "step: 41815\n",
      "train: loss: 908933.5 acc: 0.7793110609054565  val: loss: 612414.6875 acc: 0.8706420063972473\n",
      "step: 41820\n",
      "train: loss: 471139.125 acc: 0.7608538866043091  val: loss: 695406.4375 acc: 0.7835079431533813\n",
      "step: 41825\n",
      "train: loss: 70487.1171875 acc: 0.9577631950378418  val: loss: 1588208.75 acc: 0.6578313708305359\n",
      "step: 41830\n",
      "train: loss: 71068.5625 acc: 0.9403069615364075  val: loss: 5918250.5 acc: 0.45630723237991333\n",
      "step: 41835\n",
      "train: loss: 85469.0859375 acc: 0.8966175317764282  val: loss: 1801686.625 acc: 0.5854178071022034\n",
      "step: 41840\n",
      "train: loss: 21270.2109375 acc: 0.9829288125038147  val: loss: 3856599.0 acc: 0.4812735319137573\n",
      "step: 41845\n",
      "train: loss: 86541.5078125 acc: 0.9348936080932617  val: loss: 2606372.5 acc: 0.5479240417480469\n",
      "step: 41850\n",
      "train: loss: 10253.9599609375 acc: 0.9913816452026367  val: loss: 716616.875 acc: 0.6971099376678467\n",
      "step: 41855\n",
      "train: loss: 63594.58984375 acc: 0.9370385408401489  val: loss: 1077604.375 acc: 0.7081944942474365\n",
      "step: 41860\n",
      "train: loss: 101688.5546875 acc: 0.9133408665657043  val: loss: 2407418.5 acc: 0.5372954607009888\n",
      "step: 41865\n",
      "train: loss: 36824.984375 acc: 0.962891697883606  val: loss: 2809066.5 acc: 0.609981894493103\n",
      "step: 41870\n",
      "train: loss: 151502.734375 acc: 0.874248743057251  val: loss: 107768.046875 acc: 0.9052267670631409\n",
      "step: 41875\n",
      "train: loss: 270392.0625 acc: 0.8626059293746948  val: loss: 1755887.125 acc: 0.629440426826477\n",
      "step: 41880\n",
      "train: loss: 105380.0546875 acc: 0.879010796546936  val: loss: 168276.484375 acc: 0.8905928134918213\n",
      "step: 41885\n",
      "train: loss: 194861.921875 acc: 0.8466979265213013  val: loss: 1338592.25 acc: 0.7208755016326904\n",
      "step: 41890\n",
      "train: loss: 140955.140625 acc: 0.8843032121658325  val: loss: 1032411.625 acc: 0.693331241607666\n",
      "step: 41895\n",
      "train: loss: 313021.40625 acc: 0.8478104472160339  val: loss: 2342398.5 acc: 0.603005051612854\n",
      "step: 41900\n",
      "train: loss: 528154.625 acc: 0.7915710806846619  val: loss: 166230.34375 acc: 0.85093092918396\n",
      "step: 41905\n",
      "train: loss: 1620318.375 acc: 0.7038943767547607  val: loss: 2709780.5 acc: 0.6062482595443726\n",
      "step: 41910\n",
      "train: loss: 549551.1875 acc: 0.8970927000045776  val: loss: 752256.8125 acc: 0.6939529776573181\n",
      "step: 41915\n",
      "train: loss: 424336.125 acc: 0.9568126201629639  val: loss: 331923.90625 acc: 0.8898967504501343\n",
      "step: 41920\n",
      "train: loss: 219538.828125 acc: 0.9789865016937256  val: loss: 625676.875 acc: 0.7443124055862427\n",
      "step: 41925\n",
      "train: loss: 163839.34375 acc: 0.9531216621398926  val: loss: 430563.625 acc: 0.7539998292922974\n",
      "step: 41930\n",
      "train: loss: 163389.75 acc: 0.9657663702964783  val: loss: 489514.84375 acc: 0.6744970679283142\n",
      "step: 41935\n",
      "train: loss: 260492.640625 acc: 0.9808026552200317  val: loss: 1106992.875 acc: 0.7226531505584717\n",
      "step: 41940\n",
      "train: loss: 49454.3359375 acc: 0.9959598183631897  val: loss: 1349555.75 acc: 0.6975364089012146\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 41945\n",
      "train: loss: 69449.0625 acc: 0.9952459931373596  val: loss: 1994296.125 acc: 0.7344040274620056\n",
      "step: 41950\n",
      "train: loss: 85963.8515625 acc: 0.9910246729850769  val: loss: 2194312.0 acc: 0.5178955793380737\n",
      "step: 41955\n",
      "train: loss: 15926.9501953125 acc: 0.9958484172821045  val: loss: 333465.21875 acc: 0.942672848701477\n",
      "step: 41960\n",
      "train: loss: 14787.900390625 acc: 0.996250331401825  val: loss: 1282973.875 acc: 0.8500977754592896\n",
      "step: 41965\n",
      "train: loss: 15569.7333984375 acc: 0.9861496090888977  val: loss: 1436911.0 acc: 0.7787027359008789\n",
      "step: 41970\n",
      "train: loss: 13626.837890625 acc: 0.9952722787857056  val: loss: 2417236.0 acc: 0.6345913410186768\n",
      "step: 41975\n",
      "train: loss: 19765.4765625 acc: 0.9759002327919006  val: loss: 305554.78125 acc: 0.9352462291717529\n",
      "step: 41980\n",
      "train: loss: 10098.8779296875 acc: 0.9747912883758545  val: loss: 1110192.375 acc: 0.6991201043128967\n",
      "step: 41985\n",
      "train: loss: 12324.904296875 acc: 0.9776110649108887  val: loss: 1192468.5 acc: 0.8559713363647461\n",
      "step: 41990\n",
      "train: loss: 7423.1962890625 acc: 0.984351396560669  val: loss: 1739344.0 acc: 0.7948320508003235\n",
      "step: 41995\n",
      "train: loss: 18319.86328125 acc: 0.9609346985816956  val: loss: 1880679.75 acc: 0.8203036189079285\n",
      "step: 42000\n",
      "train: loss: 12463.3818359375 acc: 0.9667424559593201  val: loss: 439529.8125 acc: 0.8112175464630127\n",
      "step: 42005\n",
      "train: loss: 269811.625 acc: 0.8023008108139038  val: loss: 1366928.375 acc: 0.8356664180755615\n",
      "step: 42010\n",
      "train: loss: 27852.232421875 acc: 0.9711536169052124  val: loss: 531867.6875 acc: 0.9097153544425964\n",
      "step: 42015\n",
      "train: loss: 69360.09375 acc: 0.9713670015335083  val: loss: 271749.9375 acc: 0.9005804061889648\n",
      "step: 42020\n",
      "train: loss: 8149.67333984375 acc: 0.9942071437835693  val: loss: 1526717.75 acc: 0.7791228890419006\n",
      "step: 42025\n",
      "train: loss: 11375.8935546875 acc: 0.9933414459228516  val: loss: 2009993.625 acc: 0.33018672466278076\n",
      "step: 42030\n",
      "train: loss: 16412.80078125 acc: 0.985861599445343  val: loss: 755126.5 acc: 0.8678481578826904\n",
      "step: 42035\n",
      "train: loss: 7026.3701171875 acc: 0.988150954246521  val: loss: 2377540.0 acc: 0.021548807621002197\n",
      "step: 42040\n",
      "train: loss: 15068.1943359375 acc: 0.9922919273376465  val: loss: 336387.28125 acc: 0.9074212908744812\n",
      "step: 42045\n",
      "train: loss: 14227.5908203125 acc: 0.9946195483207703  val: loss: 231143.15625 acc: 0.9262959361076355\n",
      "step: 42050\n",
      "train: loss: 34391.7421875 acc: 0.9887679219245911  val: loss: 240962.640625 acc: 0.9400738477706909\n",
      "step: 42055\n",
      "train: loss: 10371.90234375 acc: 0.996768593788147  val: loss: 360221.625 acc: 0.9336279630661011\n",
      "step: 42060\n",
      "train: loss: 27589.12890625 acc: 0.9842288494110107  val: loss: 403174.40625 acc: 0.9197914600372314\n",
      "step: 42065\n",
      "train: loss: 82266.53125 acc: 0.981408417224884  val: loss: 1636085.625 acc: 0.670680046081543\n",
      "step: 42070\n",
      "train: loss: 93691.078125 acc: 0.9415602684020996  val: loss: 226585.578125 acc: 0.8587613105773926\n",
      "step: 42075\n",
      "train: loss: 21323.767578125 acc: 0.98714679479599  val: loss: 479333.71875 acc: 0.893797755241394\n",
      "step: 42080\n",
      "train: loss: 598841.3125 acc: 0.8664079904556274  val: loss: 226373.625 acc: 0.9417976140975952\n",
      "step: 42085\n",
      "train: loss: 750574.6875 acc: 0.9176302552223206  val: loss: 372835.625 acc: 0.9138336181640625\n",
      "step: 42090\n",
      "train: loss: 37003.40625 acc: 0.9945030808448792  val: loss: 1959342.5 acc: 0.7224937677383423\n",
      "step: 42095\n",
      "train: loss: 13125.0224609375 acc: 0.9981027245521545  val: loss: 2797804.0 acc: 0.5681159496307373\n",
      "step: 42100\n",
      "train: loss: 377523.46875 acc: 0.9538560509681702  val: loss: 1214426.375 acc: 0.833690881729126\n",
      "step: 42105\n",
      "train: loss: 289695.0625 acc: 0.9694420695304871  val: loss: 521192.78125 acc: 0.9353027939796448\n",
      "step: 42110\n",
      "train: loss: 449414.9375 acc: 0.9632842540740967  val: loss: 354135.28125 acc: 0.9063687920570374\n",
      "step: 42115\n",
      "train: loss: 297545.25 acc: 0.9732769131660461  val: loss: 831219.6875 acc: 0.7324742078781128\n",
      "step: 42120\n",
      "train: loss: 525957.9375 acc: 0.964136004447937  val: loss: 1260695.875 acc: 0.8758078813552856\n",
      "step: 42125\n",
      "train: loss: 336920.21875 acc: 0.9868791103363037  val: loss: 837247.4375 acc: 0.9335976839065552\n",
      "step: 42130\n",
      "train: loss: 324062.3125 acc: 0.9918917417526245  val: loss: 521658.90625 acc: 0.8536136150360107\n",
      "step: 42135\n",
      "train: loss: 1175806.625 acc: 0.9467007517814636  val: loss: 863032.0625 acc: 0.8448134660720825\n",
      "step: 42140\n",
      "train: loss: 1190986.5 acc: 0.9375368356704712  val: loss: 3724196.0 acc: 0.4691551923751831\n",
      "step: 42145\n",
      "train: loss: 208416.453125 acc: 0.9681138396263123  val: loss: 1376472.0 acc: 0.8493852615356445\n",
      "step: 42150\n",
      "train: loss: 722000.3125 acc: 0.8319036960601807  val: loss: 406769.4375 acc: 0.8382620811462402\n",
      "step: 42155\n",
      "train: loss: 233601.84375 acc: 0.9515388011932373  val: loss: 821349.75 acc: 0.8672031164169312\n",
      "step: 42160\n",
      "train: loss: 1085478.125 acc: 0.7460111975669861  val: loss: 1286238.125 acc: 0.8621392846107483\n",
      "step: 42165\n",
      "train: loss: 651978.4375 acc: 0.6542677283287048  val: loss: 496075.6875 acc: 0.815519392490387\n",
      "step: 42170\n",
      "train: loss: 762802.9375 acc: 0.7819958329200745  val: loss: 476447.53125 acc: 0.8755329847335815\n",
      "step: 42175\n",
      "train: loss: 689156.5625 acc: 0.7745370864868164  val: loss: 1530555.375 acc: 0.8474761843681335\n",
      "step: 42180\n",
      "train: loss: 801794.9375 acc: 0.8093822598457336  val: loss: 1672358.875 acc: 0.7645792961120605\n",
      "step: 42185\n",
      "train: loss: 1113464.75 acc: 0.4560244679450989  val: loss: 974410.0625 acc: 0.8693363666534424\n",
      "step: 42190\n",
      "train: loss: 901770.0625 acc: 0.6600474119186401  val: loss: 370003.09375 acc: 0.7266957759857178\n",
      "step: 42195\n",
      "train: loss: 269745.96875 acc: 0.8157271146774292  val: loss: 1993077.375 acc: 0.6932419538497925\n",
      "step: 42200\n",
      "train: loss: 132928.265625 acc: 0.8944977521896362  val: loss: 980668.6875 acc: 0.7465828657150269\n",
      "step: 42205\n",
      "train: loss: 80597.3359375 acc: 0.9286385178565979  val: loss: 1301230.75 acc: 0.7660810947418213\n",
      "step: 42210\n",
      "train: loss: 76894.2421875 acc: 0.9462441802024841  val: loss: 2082716.875 acc: 0.6632356643676758\n",
      "step: 42215\n",
      "train: loss: 34237.7734375 acc: 0.9727986454963684  val: loss: 1592631.875 acc: 0.7635156512260437\n",
      "step: 42220\n",
      "train: loss: 21474.763671875 acc: 0.9812203645706177  val: loss: 1366688.5 acc: 0.7247318029403687\n",
      "step: 42225\n",
      "train: loss: 117751.4765625 acc: 0.886970579624176  val: loss: 1016572.375 acc: 0.7262142896652222\n",
      "step: 42230\n",
      "train: loss: 207795.328125 acc: 0.8711299300193787  val: loss: 2222323.75 acc: 0.6599174737930298\n",
      "step: 42235\n",
      "train: loss: 42614.63671875 acc: 0.9590671062469482  val: loss: 1952226.25 acc: 0.6896616220474243\n",
      "step: 42240\n",
      "train: loss: 233969.578125 acc: 0.8533164262771606  val: loss: 3129499.75 acc: 0.6948173642158508\n",
      "step: 42245\n",
      "train: loss: 451992.4375 acc: 0.7742536664009094  val: loss: 454771.8125 acc: 0.7527197599411011\n",
      "step: 42250\n",
      "train: loss: 616052.6875 acc: 0.7380430102348328  val: loss: 1226219.625 acc: 0.7512457966804504\n",
      "step: 42255\n",
      "train: loss: 204762.421875 acc: 0.8283194303512573  val: loss: 2085658.375 acc: 0.6316697001457214\n",
      "step: 42260\n",
      "train: loss: 169016.6875 acc: 0.8565427660942078  val: loss: 1236144.375 acc: 0.6963379979133606\n",
      "step: 42265\n",
      "train: loss: 696109.0 acc: 0.7171933650970459  val: loss: 4448442.0 acc: 0.627900242805481\n",
      "step: 42270\n",
      "train: loss: 1763268.625 acc: 0.7053028345108032  val: loss: 2099124.5 acc: 0.6799407005310059\n",
      "step: 42275\n",
      "train: loss: 733577.4375 acc: 0.8224305510520935  val: loss: 540654.1875 acc: 0.8889225125312805\n",
      "step: 42280\n",
      "train: loss: 1271827.125 acc: 0.8930937051773071  val: loss: 1055954.25 acc: 0.6014959812164307\n",
      "step: 42285\n",
      "train: loss: 434388.84375 acc: 0.9628612399101257  val: loss: 285888.78125 acc: 0.8741471171379089\n",
      "step: 42290\n",
      "train: loss: 421494.53125 acc: 0.9571011662483215  val: loss: 221788.734375 acc: 0.9575420618057251\n",
      "step: 42295\n",
      "train: loss: 436560.34375 acc: 0.9190496802330017  val: loss: 1440064.5 acc: 0.7230753898620605\n",
      "step: 42300\n",
      "train: loss: 160213.90625 acc: 0.985216498374939  val: loss: 559382.6875 acc: 0.8913552165031433\n",
      "step: 42305\n",
      "train: loss: 196353.671875 acc: 0.9809369444847107  val: loss: 374638.0 acc: 0.887630045413971\n",
      "step: 42310\n",
      "train: loss: 111207.2421875 acc: 0.9930037260055542  val: loss: 481910.9375 acc: 0.8985320925712585\n",
      "step: 42315\n",
      "train: loss: 105701.9609375 acc: 0.9863035082817078  val: loss: 1233838.375 acc: 0.5749882459640503\n",
      "step: 42320\n",
      "train: loss: 75651.703125 acc: 0.98541659116745  val: loss: 1149075.25 acc: 0.6737844944000244\n",
      "step: 42325\n",
      "train: loss: 32410.0625 acc: 0.977560818195343  val: loss: 1006056.875 acc: 0.5108063220977783\n",
      "step: 42330\n",
      "train: loss: 22800.708984375 acc: 0.9874825477600098  val: loss: 345338.5 acc: 0.9132475852966309\n",
      "step: 42335\n",
      "train: loss: 50648.4375 acc: 0.9661548137664795  val: loss: 2441326.5 acc: -1.4773991107940674\n",
      "step: 42340\n",
      "train: loss: 16168.58203125 acc: 0.9830138087272644  val: loss: 1162741.25 acc: 0.8633769154548645\n",
      "step: 42345\n",
      "train: loss: 14145.228515625 acc: 0.9929466247558594  val: loss: 74790.953125 acc: 0.9875762462615967\n",
      "step: 42350\n",
      "train: loss: 176522.921875 acc: 0.708480954170227  val: loss: 1033985.75 acc: 0.8203214406967163\n",
      "step: 42355\n",
      "train: loss: 19300.169921875 acc: 0.9522616267204285  val: loss: 199331.9375 acc: 0.9418318271636963\n",
      "step: 42360\n",
      "train: loss: 19137.263671875 acc: 0.9754396080970764  val: loss: 1096311.875 acc: 0.6325085759162903\n",
      "step: 42365\n",
      "train: loss: 10311.3525390625 acc: 0.9873100519180298  val: loss: 808686.25 acc: 0.8623359799385071\n",
      "step: 42370\n",
      "train: loss: 22642.94140625 acc: 0.9551550149917603  val: loss: 881662.25 acc: 0.7562470436096191\n",
      "step: 42375\n",
      "train: loss: 30283.595703125 acc: 0.9724056720733643  val: loss: 435148.96875 acc: 0.8388241529464722\n",
      "step: 42380\n",
      "train: loss: 43001.0625 acc: 0.9779446721076965  val: loss: 310761.8125 acc: 0.8735266327857971\n",
      "step: 42385\n",
      "train: loss: 32378.212890625 acc: 0.9839166402816772  val: loss: 108462.7734375 acc: 0.971773087978363\n",
      "step: 42390\n",
      "train: loss: 26520.26171875 acc: 0.9834650754928589  val: loss: 898802.1875 acc: 0.8865985870361328\n",
      "step: 42395\n",
      "train: loss: 37679.25 acc: 0.9643236994743347  val: loss: 1244502.75 acc: 0.8649781346321106\n",
      "step: 42400\n",
      "train: loss: 16979.26953125 acc: 0.9477721452713013  val: loss: 183231.234375 acc: 0.937951922416687\n",
      "step: 42405\n",
      "train: loss: 19142.48828125 acc: 0.9897263646125793  val: loss: 1008130.125 acc: 0.7737406492233276\n",
      "step: 42410\n",
      "train: loss: 21972.638671875 acc: 0.9913038611412048  val: loss: 594243.3125 acc: 0.8167028427124023\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 42415\n",
      "train: loss: 11958.4013671875 acc: 0.9956865906715393  val: loss: 809772.25 acc: 0.9432965517044067\n",
      "step: 42420\n",
      "train: loss: 37303.765625 acc: 0.98750239610672  val: loss: 1833768.25 acc: 0.6509625315666199\n",
      "step: 42425\n",
      "train: loss: 50439.3515625 acc: 0.971734881401062  val: loss: 631669.1875 acc: 0.9403800368309021\n",
      "step: 42430\n",
      "train: loss: 150598.421875 acc: 0.9592905044555664  val: loss: 390000.09375 acc: 0.9255296587944031\n",
      "step: 42435\n",
      "train: loss: 84943.625 acc: 0.9567090272903442  val: loss: 1015401.0625 acc: 0.9357580542564392\n",
      "step: 42440\n",
      "train: loss: 58809.95703125 acc: 0.9070143699645996  val: loss: 976840.0 acc: 0.824840784072876\n",
      "step: 42445\n",
      "train: loss: 83834.171875 acc: 0.9846798181533813  val: loss: 699085.5 acc: 0.8411776423454285\n",
      "step: 42450\n",
      "train: loss: 171624.25 acc: 0.9815401434898376  val: loss: 2489772.25 acc: 0.042258381843566895\n",
      "step: 42455\n",
      "train: loss: 41349.32421875 acc: 0.9956912994384766  val: loss: 1384349.0 acc: 0.6181473135948181\n",
      "step: 42460\n",
      "train: loss: 103322.7734375 acc: 0.9790349006652832  val: loss: 2286213.5 acc: 0.3912186026573181\n",
      "step: 42465\n",
      "train: loss: 347950.6875 acc: 0.9690479040145874  val: loss: 2060243.75 acc: 0.793451189994812\n",
      "step: 42470\n",
      "train: loss: 197539.8125 acc: 0.9645057320594788  val: loss: 965808.4375 acc: 0.8298808932304382\n",
      "step: 42475\n",
      "train: loss: 1062930.5 acc: 0.9589439630508423  val: loss: 707584.8125 acc: 0.8366059064865112\n",
      "step: 42480\n",
      "train: loss: 278198.28125 acc: 0.9650474190711975  val: loss: 1938234.625 acc: 0.5654247999191284\n",
      "step: 42485\n",
      "train: loss: 360519.53125 acc: 0.9577642679214478  val: loss: 441268.625 acc: 0.8899675607681274\n",
      "step: 42490\n",
      "train: loss: 792485.0625 acc: 0.9700716137886047  val: loss: 251958.8125 acc: 0.9377826452255249\n",
      "step: 42495\n",
      "train: loss: 1199827.75 acc: 0.971914529800415  val: loss: 1783587.625 acc: 0.6820430755615234\n",
      "step: 42500\n",
      "train: loss: 1763467.5 acc: 0.9344181418418884  val: loss: 333590.75 acc: 0.9194276928901672\n",
      "step: 42505\n",
      "train: loss: 1116964.5 acc: 0.9341484904289246  val: loss: 2428609.5 acc: 0.2677410840988159\n",
      "step: 42510\n",
      "train: loss: 243366.8125 acc: 0.9376930594444275  val: loss: 1761553.25 acc: 0.6733996868133545\n",
      "step: 42515\n",
      "train: loss: 145283.828125 acc: 0.9702326059341431  val: loss: 861300.125 acc: 0.6206823587417603\n",
      "step: 42520\n",
      "train: loss: 584770.4375 acc: 0.8589073419570923  val: loss: 1432439.0 acc: -0.18585526943206787\n",
      "step: 42525\n",
      "train: loss: 2392911.0 acc: 0.7014693021774292  val: loss: 1622404.0 acc: 0.5971755981445312\n",
      "step: 42530\n",
      "train: loss: 873161.875 acc: 0.7067452669143677  val: loss: 871665.25 acc: 0.6816815137863159\n",
      "step: 42535\n",
      "train: loss: 1305069.375 acc: 0.42996883392333984  val: loss: 323784.125 acc: 0.4423102140426636\n",
      "step: 42540\n",
      "train: loss: 491794.5625 acc: 0.7182427644729614  val: loss: 645186.375 acc: 0.83473801612854\n",
      "step: 42545\n",
      "train: loss: 962626.0 acc: 0.62558913230896  val: loss: 793045.0 acc: 0.7774370908737183\n",
      "step: 42550\n",
      "train: loss: 1657659.875 acc: 0.7463851571083069  val: loss: 1401849.875 acc: 0.580111026763916\n",
      "step: 42555\n",
      "train: loss: 475888.9375 acc: 0.7659787535667419  val: loss: 1039654.3125 acc: 0.5061131715774536\n",
      "step: 42560\n",
      "train: loss: 248030.328125 acc: 0.8501778841018677  val: loss: 2197716.0 acc: 0.6334426999092102\n",
      "step: 42565\n",
      "train: loss: 205508.0625 acc: 0.8193139433860779  val: loss: 674195.5625 acc: 0.6493990421295166\n",
      "step: 42570\n",
      "train: loss: 135730.765625 acc: 0.8940238952636719  val: loss: 1924823.0 acc: 0.7258356809616089\n",
      "step: 42575\n",
      "train: loss: 124249.046875 acc: 0.9199714660644531  val: loss: 3481783.0 acc: 0.596865177154541\n",
      "step: 42580\n",
      "train: loss: 247356.84375 acc: 0.8688129782676697  val: loss: 1868613.625 acc: 0.7108370065689087\n",
      "step: 42585\n",
      "train: loss: 58249.12890625 acc: 0.9575015306472778  val: loss: 1009293.9375 acc: 0.7417099475860596\n",
      "step: 42590\n",
      "train: loss: 338911.28125 acc: 0.816914439201355  val: loss: 1186149.375 acc: 0.7407577037811279\n",
      "step: 42595\n",
      "train: loss: 301752.46875 acc: 0.8337056040763855  val: loss: 2426378.25 acc: 0.6667502522468567\n",
      "step: 42600\n",
      "train: loss: 73913.3125 acc: 0.8915800452232361  val: loss: 690370.1875 acc: 0.827349066734314\n",
      "step: 42605\n",
      "train: loss: 131170.140625 acc: 0.894067108631134  val: loss: 1088667.875 acc: 0.7168600559234619\n",
      "step: 42610\n",
      "train: loss: 418905.25 acc: 0.7976743578910828  val: loss: 2150982.5 acc: 0.6970589756965637\n",
      "step: 42615\n",
      "train: loss: 385115.5625 acc: 0.7617539167404175  val: loss: 3777148.75 acc: 0.5689412951469421\n",
      "step: 42620\n",
      "train: loss: 156488.0625 acc: 0.8818966746330261  val: loss: 3241160.75 acc: 0.5341808795928955\n",
      "step: 42625\n",
      "train: loss: 492613.5625 acc: 0.7499988675117493  val: loss: 1677172.125 acc: 0.6887109875679016\n",
      "step: 42630\n",
      "train: loss: 180674.3125 acc: 0.8365693688392639  val: loss: 506696.0 acc: 0.7657549381256104\n",
      "step: 42635\n",
      "train: loss: 1128620.25 acc: 0.7387652397155762  val: loss: 435802.25 acc: 0.8360346555709839\n",
      "step: 42640\n",
      "train: loss: 356628.5625 acc: 0.8872392177581787  val: loss: 1495410.875 acc: 0.5573925971984863\n",
      "step: 42645\n",
      "train: loss: 747956.1875 acc: 0.8920636177062988  val: loss: 1662611.125 acc: 0.769587516784668\n",
      "step: 42650\n",
      "train: loss: 584001.0625 acc: 0.9396923184394836  val: loss: 614529.4375 acc: 0.8660170435905457\n",
      "step: 42655\n",
      "train: loss: 353116.375 acc: 0.9559202790260315  val: loss: 401041.40625 acc: 0.8774799108505249\n",
      "step: 42660\n",
      "train: loss: 145522.96875 acc: 0.9771742820739746  val: loss: 793731.0 acc: 0.8719202876091003\n",
      "step: 42665\n",
      "train: loss: 102512.9296875 acc: 0.9835774898529053  val: loss: 409592.09375 acc: 0.917625904083252\n",
      "step: 42670\n",
      "train: loss: 116776.609375 acc: 0.9881480932235718  val: loss: 261452.46875 acc: 0.9330542683601379\n",
      "step: 42675\n",
      "train: loss: 68486.59375 acc: 0.9946357607841492  val: loss: 219927.8125 acc: 0.8831146359443665\n",
      "step: 42680\n",
      "train: loss: 95726.203125 acc: 0.9879727959632874  val: loss: 1732747.125 acc: 0.6548889875411987\n",
      "step: 42685\n",
      "train: loss: 90906.15625 acc: 0.9818431735038757  val: loss: 381934.8125 acc: 0.9561542868614197\n",
      "step: 42690\n",
      "train: loss: 50182.5234375 acc: 0.9769988656044006  val: loss: 732146.875 acc: 0.7988964319229126\n",
      "step: 42695\n",
      "train: loss: 33332.26171875 acc: 0.9847559332847595  val: loss: 446439.125 acc: 0.9411191344261169\n",
      "step: 42700\n",
      "train: loss: 19515.35546875 acc: 0.9470766186714172  val: loss: 430233.90625 acc: 0.8649275898933411\n",
      "step: 42705\n",
      "train: loss: 15392.5341796875 acc: 0.9729607105255127  val: loss: 578655.3125 acc: 0.892475962638855\n",
      "step: 42710\n",
      "train: loss: 14316.3388671875 acc: 0.9902480840682983  val: loss: 711228.6875 acc: 0.8798759579658508\n",
      "step: 42715\n",
      "train: loss: 17292.212890625 acc: 0.967475414276123  val: loss: 800646.0 acc: 0.739708423614502\n",
      "step: 42720\n",
      "train: loss: 35640.06640625 acc: 0.9590634107589722  val: loss: 1083378.625 acc: 0.7508193254470825\n",
      "step: 42725\n",
      "train: loss: 10929.7578125 acc: 0.973129153251648  val: loss: 1637384.375 acc: 0.8625180721282959\n",
      "step: 42730\n",
      "train: loss: 10172.5185546875 acc: 0.9857766032218933  val: loss: 435271.4375 acc: 0.9518449306488037\n",
      "step: 42735\n",
      "train: loss: 22382.505859375 acc: 0.9615370035171509  val: loss: 672009.0 acc: 0.8966895341873169\n",
      "step: 42740\n",
      "train: loss: 56601.40625 acc: 0.9718588590621948  val: loss: 1000718.125 acc: 0.8085665106773376\n",
      "step: 42745\n",
      "train: loss: 25228.185546875 acc: 0.9727265238761902  val: loss: 1553890.625 acc: 0.8778179883956909\n",
      "step: 42750\n",
      "train: loss: 34185.58984375 acc: 0.9776204824447632  val: loss: 487599.125 acc: 0.9368891716003418\n",
      "step: 42755\n",
      "train: loss: 21718.494140625 acc: 0.9903438091278076  val: loss: 1293386.375 acc: 0.8112434148788452\n",
      "step: 42760\n",
      "train: loss: 98420.671875 acc: 0.8958834409713745  val: loss: 2438095.75 acc: 0.7878789305686951\n",
      "step: 42765\n",
      "train: loss: 24531.49609375 acc: 0.9703123569488525  val: loss: 2264864.0 acc: 0.1951514482498169\n",
      "step: 42770\n",
      "train: loss: 12169.9287109375 acc: 0.9961680173873901  val: loss: 1273602.875 acc: 0.8072306513786316\n",
      "step: 42775\n",
      "train: loss: 32722.912109375 acc: 0.9870019555091858  val: loss: 1180106.5 acc: 0.5258678793907166\n",
      "step: 42780\n",
      "train: loss: 54887.1640625 acc: 0.9870747327804565  val: loss: 2283884.75 acc: 0.706163763999939\n",
      "step: 42785\n",
      "train: loss: 16830.97265625 acc: 0.9961796402931213  val: loss: 813154.3125 acc: 0.9027590751647949\n",
      "step: 42790\n",
      "train: loss: 69730.515625 acc: 0.9420172572135925  val: loss: 1762978.75 acc: 0.65224289894104\n",
      "step: 42795\n",
      "train: loss: 53090.2109375 acc: 0.9874805212020874  val: loss: 1971521.875 acc: 0.8090178966522217\n",
      "step: 42800\n",
      "train: loss: 255359.96875 acc: 0.9285792708396912  val: loss: 1056596.875 acc: 0.7629043459892273\n",
      "step: 42805\n",
      "train: loss: 53961.26171875 acc: 0.9664498567581177  val: loss: 818231.375 acc: 0.8339154720306396\n",
      "step: 42810\n",
      "train: loss: 790635.625 acc: 0.8601390719413757  val: loss: 536070.3125 acc: 0.7026780843734741\n",
      "step: 42815\n",
      "train: loss: 60244.52734375 acc: 0.9934965968132019  val: loss: 782835.4375 acc: 0.17160922288894653\n",
      "step: 42820\n",
      "train: loss: 85991.796875 acc: 0.9915105104446411  val: loss: 568087.25 acc: 0.9116635322570801\n",
      "step: 42825\n",
      "train: loss: 35901.43359375 acc: 0.9944314360618591  val: loss: 1221985.875 acc: -0.6041039228439331\n",
      "step: 42830\n",
      "train: loss: 125318.1796875 acc: 0.9827091693878174  val: loss: 1840434.25 acc: -1.3691656589508057\n",
      "step: 42835\n",
      "train: loss: 252372.40625 acc: 0.9800235033035278  val: loss: 1227453.875 acc: 0.8387073278427124\n",
      "step: 42840\n",
      "train: loss: 357036.28125 acc: 0.9757565259933472  val: loss: 427735.34375 acc: 0.9061327576637268\n",
      "step: 42845\n",
      "train: loss: 355410.375 acc: 0.9447340369224548  val: loss: 731752.0625 acc: 0.8666124939918518\n",
      "step: 42850\n",
      "train: loss: 353481.4375 acc: 0.8940120935440063  val: loss: 1288676.375 acc: 0.40149009227752686\n",
      "step: 42855\n",
      "train: loss: 671739.75 acc: 0.9702664613723755  val: loss: 693743.4375 acc: 0.7356163263320923\n",
      "step: 42860\n",
      "train: loss: 681738.6875 acc: 0.9672300815582275  val: loss: 2734611.0 acc: 0.35063672065734863\n",
      "step: 42865\n",
      "train: loss: 1639116.5 acc: 0.9252053499221802  val: loss: 853310.0 acc: 0.8759169578552246\n",
      "step: 42870\n",
      "train: loss: 1006683.75 acc: 0.9574797749519348  val: loss: 3559797.75 acc: 0.5228217840194702\n",
      "step: 42875\n",
      "train: loss: 235574.15625 acc: 0.9787117838859558  val: loss: 574807.625 acc: 0.8957497477531433\n",
      "step: 42880\n",
      "train: loss: 538337.4375 acc: 0.9534279108047485  val: loss: 481507.15625 acc: 0.9369749426841736\n",
      "step: 42885\n",
      "train: loss: 394342.96875 acc: 0.9633919596672058  val: loss: 657311.3125 acc: 0.837734580039978\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 42890\n",
      "train: loss: 1782443.5 acc: 0.8296640515327454  val: loss: 1306470.75 acc: 0.7151548862457275\n",
      "step: 42895\n",
      "train: loss: 636566.625 acc: 0.6502619385719299  val: loss: 1318966.5 acc: 0.8034515380859375\n",
      "step: 42900\n",
      "train: loss: 1081426.875 acc: 0.25354212522506714  val: loss: 1190749.875 acc: 0.7576189041137695\n",
      "step: 42905\n",
      "train: loss: 624836.1875 acc: 0.681076169013977  val: loss: 1386576.75 acc: 0.5283581018447876\n",
      "step: 42910\n",
      "train: loss: 472626.65625 acc: 0.8399538993835449  val: loss: 960211.125 acc: 0.7007951736450195\n",
      "step: 42915\n",
      "train: loss: 2525101.5 acc: -0.0344843864440918  val: loss: 498130.78125 acc: 0.8382470607757568\n",
      "step: 42920\n",
      "train: loss: 797129.375 acc: 0.24153989553451538  val: loss: 3202991.75 acc: 0.5724384784698486\n",
      "step: 42925\n",
      "train: loss: 1390280.625 acc: 0.5821306705474854  val: loss: 1286526.0 acc: 0.5888100862503052\n",
      "step: 42930\n",
      "train: loss: 397985.34375 acc: 0.7093119621276855  val: loss: 1567702.5 acc: 0.6003158092498779\n",
      "step: 42935\n",
      "train: loss: 337513.8125 acc: 0.7952432036399841  val: loss: 1230707.125 acc: 0.5749397873878479\n",
      "step: 42940\n",
      "train: loss: 61988.16015625 acc: 0.9482408165931702  val: loss: 1188476.5 acc: 0.6705389022827148\n",
      "step: 42945\n",
      "train: loss: 128236.0625 acc: 0.9183093309402466  val: loss: 2038557.375 acc: 0.6341545581817627\n",
      "step: 42950\n",
      "train: loss: 147547.328125 acc: 0.9142414927482605  val: loss: 1013034.25 acc: 0.6985849738121033\n",
      "step: 42955\n",
      "train: loss: 234121.40625 acc: 0.8788617253303528  val: loss: 609133.125 acc: 0.7817343473434448\n",
      "step: 42960\n",
      "train: loss: 78564.3515625 acc: 0.938374936580658  val: loss: 1581846.125 acc: 0.6742429733276367\n",
      "step: 42965\n",
      "train: loss: 79972.8359375 acc: 0.8981432914733887  val: loss: 104923.0078125 acc: 0.9179485440254211\n",
      "step: 42970\n",
      "train: loss: 57605.37109375 acc: 0.9500283002853394  val: loss: 3190239.5 acc: 0.6558038592338562\n",
      "step: 42975\n",
      "train: loss: 276835.28125 acc: 0.8553506135940552  val: loss: 415931.28125 acc: 0.7672119736671448\n",
      "step: 42980\n",
      "train: loss: 52266.09375 acc: 0.9465321898460388  val: loss: 3103168.5 acc: 0.5810793042182922\n",
      "step: 42985\n",
      "train: loss: 1022976.5 acc: 0.6327823400497437  val: loss: 892094.0625 acc: 0.7170764207839966\n",
      "step: 42990\n",
      "train: loss: 165706.90625 acc: 0.8748872876167297  val: loss: 146566.703125 acc: 0.8941287398338318\n",
      "step: 42995\n",
      "train: loss: 60200.56640625 acc: 0.9366415739059448  val: loss: 247571.59375 acc: 0.8337010145187378\n",
      "step: 43000\n",
      "train: loss: 1477037.375 acc: 0.7033671140670776  val: loss: 1100838.25 acc: 0.7983517050743103\n",
      "step: 43005\n",
      "train: loss: 649501.0 acc: 0.878351092338562  val: loss: 587549.5625 acc: 0.8327511548995972\n",
      "step: 43010\n",
      "train: loss: 524480.375 acc: 0.9459658861160278  val: loss: 1641101.5 acc: 0.7875769734382629\n",
      "step: 43015\n",
      "train: loss: 1126168.25 acc: 0.8752191066741943  val: loss: 352726.28125 acc: 0.917569100856781\n",
      "step: 43020\n",
      "train: loss: 213951.234375 acc: 0.9679511189460754  val: loss: 210568.796875 acc: 0.9475027322769165\n",
      "step: 43025\n",
      "train: loss: 140879.375 acc: 0.9833801984786987  val: loss: 560750.125 acc: 0.8732641339302063\n",
      "step: 43030\n",
      "train: loss: 87534.4453125 acc: 0.9893273115158081  val: loss: 1173070.75 acc: 0.9245724081993103\n",
      "step: 43035\n",
      "train: loss: 74794.9453125 acc: 0.9947534203529358  val: loss: 1185756.5 acc: 0.8083663582801819\n",
      "step: 43040\n",
      "train: loss: 92225.484375 acc: 0.9935241937637329  val: loss: 444821.71875 acc: 0.9332708120346069\n",
      "step: 43045\n",
      "train: loss: 93268.609375 acc: 0.9863443374633789  val: loss: 831452.375 acc: 0.9241389036178589\n",
      "step: 43050\n",
      "train: loss: 53016.8671875 acc: 0.9903379678726196  val: loss: 1368152.75 acc: 0.8624680638313293\n",
      "step: 43055\n",
      "train: loss: 50954.0859375 acc: 0.971626341342926  val: loss: 587794.6875 acc: 0.9002172946929932\n",
      "step: 43060\n",
      "train: loss: 20209.244140625 acc: 0.9955552816390991  val: loss: 332451.0625 acc: 0.952545166015625\n",
      "step: 43065\n",
      "train: loss: 25526.740234375 acc: 0.9852386713027954  val: loss: 520160.53125 acc: 0.9486728310585022\n",
      "step: 43070\n",
      "train: loss: 8974.5576171875 acc: 0.9939528703689575  val: loss: 1780927.375 acc: -0.11516523361206055\n",
      "step: 43075\n",
      "train: loss: 33461.8984375 acc: 0.9658903479576111  val: loss: 765643.6875 acc: 0.8713681101799011\n",
      "step: 43080\n",
      "train: loss: 23854.96875 acc: 0.9530372023582458  val: loss: 1255965.25 acc: 0.6812982559204102\n",
      "step: 43085\n",
      "train: loss: 13655.1884765625 acc: 0.9804345369338989  val: loss: 1421304.875 acc: 0.400951623916626\n",
      "step: 43090\n",
      "train: loss: 21193.341796875 acc: 0.9011605381965637  val: loss: 1078264.75 acc: 0.8928883075714111\n",
      "step: 43095\n",
      "train: loss: 13482.283203125 acc: 0.9785098433494568  val: loss: 1133309.875 acc: 0.8005368113517761\n",
      "step: 43100\n",
      "train: loss: 31694.51953125 acc: 0.9665650129318237  val: loss: 692748.9375 acc: 0.8620525598526001\n",
      "step: 43105\n",
      "train: loss: 67361.2734375 acc: 0.9712941646575928  val: loss: 2323657.5 acc: 0.7902336120605469\n",
      "step: 43110\n",
      "train: loss: 36634.68359375 acc: 0.9831048250198364  val: loss: 1241475.25 acc: 0.7675377130508423\n",
      "step: 43115\n",
      "train: loss: 18248.583984375 acc: 0.9837015867233276  val: loss: 1526363.25 acc: 0.8203332424163818\n",
      "step: 43120\n",
      "train: loss: 13187.208984375 acc: 0.9931296110153198  val: loss: 2511852.0 acc: 0.2703213095664978\n",
      "step: 43125\n",
      "train: loss: 38448.33984375 acc: 0.9616801738739014  val: loss: 688682.3125 acc: 0.5944321155548096\n",
      "step: 43130\n",
      "train: loss: 20028.982421875 acc: 0.9536566734313965  val: loss: 1127549.125 acc: 0.7325495481491089\n",
      "step: 43135\n",
      "train: loss: 106822.4453125 acc: 0.9341026544570923  val: loss: 1539826.125 acc: 0.8022761940956116\n",
      "step: 43140\n",
      "train: loss: 39525.328125 acc: 0.9892898201942444  val: loss: 2386687.5 acc: -0.11840903759002686\n",
      "step: 43145\n",
      "train: loss: 31830.04296875 acc: 0.9868103265762329  val: loss: 1657979.375 acc: 0.543658971786499\n",
      "step: 43150\n",
      "train: loss: 79230.0234375 acc: 0.9793236255645752  val: loss: 1889728.75 acc: 0.3421670198440552\n",
      "step: 43155\n",
      "train: loss: 33295.21875 acc: 0.9893988966941833  val: loss: 306106.75 acc: 0.9593966007232666\n",
      "step: 43160\n",
      "train: loss: 103187.2578125 acc: 0.9644890427589417  val: loss: 884085.6875 acc: 0.8390570282936096\n",
      "step: 43165\n",
      "train: loss: 55618.7890625 acc: 0.9672768115997314  val: loss: 1536682.125 acc: 0.2548442482948303\n",
      "step: 43170\n",
      "train: loss: 117658.4375 acc: 0.9511021971702576  val: loss: 878367.0625 acc: 0.5272022485733032\n",
      "step: 43175\n",
      "train: loss: 818365.625 acc: 0.6868305206298828  val: loss: 1195690.625 acc: 0.861691951751709\n",
      "step: 43180\n",
      "train: loss: 90248.1484375 acc: 0.9874755144119263  val: loss: 1604922.375 acc: 0.41939443349838257\n",
      "step: 43185\n",
      "train: loss: 75655.0703125 acc: 0.9935102462768555  val: loss: 319673.59375 acc: 0.897020697593689\n",
      "step: 43190\n",
      "train: loss: 277357.625 acc: 0.9706835746765137  val: loss: 3053891.25 acc: 0.6613444685935974\n",
      "step: 43195\n",
      "train: loss: 208166.125 acc: 0.9753521680831909  val: loss: 1775044.875 acc: 0.5527240037918091\n",
      "step: 43200\n",
      "train: loss: 400851.46875 acc: 0.9501649737358093  val: loss: 2101776.75 acc: 0.36297619342803955\n",
      "step: 43205\n",
      "train: loss: 501379.875 acc: 0.9822853803634644  val: loss: 1120607.25 acc: 0.6127631664276123\n",
      "step: 43210\n",
      "train: loss: 208762.78125 acc: 0.980458676815033  val: loss: 1254299.125 acc: 0.8237138986587524\n",
      "step: 43215\n",
      "train: loss: 307249.0 acc: 0.966313362121582  val: loss: 812688.3125 acc: 0.8895172476768494\n",
      "step: 43220\n",
      "train: loss: 212944.0625 acc: 0.9836990833282471  val: loss: 1409253.0 acc: 0.7544820308685303\n",
      "step: 43225\n",
      "train: loss: 899328.1875 acc: 0.9682072997093201  val: loss: 341763.65625 acc: 0.9104658365249634\n",
      "step: 43230\n",
      "train: loss: 1008059.3125 acc: 0.9632076025009155  val: loss: 1107662.5 acc: 0.7670075297355652\n",
      "step: 43235\n",
      "train: loss: 1579672.125 acc: 0.9342089891433716  val: loss: 1259486.0 acc: 0.6898223161697388\n",
      "step: 43240\n",
      "train: loss: 499001.96875 acc: 0.973426878452301  val: loss: 567394.8125 acc: 0.8178712129592896\n",
      "step: 43245\n",
      "train: loss: 540012.3125 acc: 0.8821841478347778  val: loss: 803046.625 acc: 0.6600459814071655\n",
      "step: 43250\n",
      "train: loss: 716228.5625 acc: 0.8869689106941223  val: loss: 1014224.3125 acc: 0.8458409905433655\n",
      "step: 43255\n",
      "train: loss: 1116765.375 acc: 0.6722309589385986  val: loss: 732261.4375 acc: 0.8809007406234741\n",
      "step: 43260\n",
      "train: loss: 716386.1875 acc: 0.7710631489753723  val: loss: 1292170.625 acc: 0.7057009935379028\n",
      "step: 43265\n",
      "train: loss: 1622304.125 acc: 0.7605817317962646  val: loss: 1507458.25 acc: 0.8358907699584961\n",
      "step: 43270\n",
      "train: loss: 1056908.75 acc: 0.32988303899765015  val: loss: 538518.625 acc: 0.8934630751609802\n",
      "step: 43275\n",
      "train: loss: 721126.625 acc: 0.8212810754776001  val: loss: 760729.75 acc: 0.619352400302887\n",
      "step: 43280\n",
      "train: loss: 1764447.0 acc: 0.0077288150787353516  val: loss: 820868.875 acc: 0.5834175944328308\n",
      "step: 43285\n",
      "train: loss: 374796.90625 acc: 0.7820647954940796  val: loss: 1931268.0 acc: 0.4832035303115845\n",
      "step: 43290\n",
      "train: loss: 608736.625 acc: 0.705787718296051  val: loss: 1777617.5 acc: 0.5847837924957275\n",
      "step: 43295\n",
      "train: loss: 377153.125 acc: 0.7700273990631104  val: loss: 1166608.625 acc: 0.5696772336959839\n",
      "step: 43300\n",
      "train: loss: 317465.21875 acc: 0.8080450892448425  val: loss: 741504.0 acc: 0.7230026721954346\n",
      "step: 43305\n",
      "train: loss: 33492.52734375 acc: 0.9731968641281128  val: loss: 1188984.875 acc: 0.6686277985572815\n",
      "step: 43310\n",
      "train: loss: 95610.390625 acc: 0.9367886185646057  val: loss: 493907.96875 acc: 0.742982029914856\n",
      "step: 43315\n",
      "train: loss: 94870.7578125 acc: 0.9375737905502319  val: loss: 1420214.5 acc: 0.7116988301277161\n",
      "step: 43320\n",
      "train: loss: 692253.5 acc: 0.757077693939209  val: loss: 861018.0 acc: 0.78952956199646\n",
      "step: 43325\n",
      "train: loss: 69102.5234375 acc: 0.9228807687759399  val: loss: 1830710.0 acc: 0.6411846876144409\n",
      "step: 43330\n",
      "train: loss: 94895.953125 acc: 0.8908181190490723  val: loss: 3088888.75 acc: 0.5516564846038818\n",
      "step: 43335\n",
      "train: loss: 12227.0419921875 acc: 0.9883136749267578  val: loss: 2792160.25 acc: 0.6426885724067688\n",
      "step: 43340\n",
      "train: loss: 82511.3203125 acc: 0.9347593784332275  val: loss: 4262285.0 acc: 0.5826518535614014\n",
      "step: 43345\n",
      "train: loss: 538689.4375 acc: 0.7174842357635498  val: loss: 2278838.0 acc: 0.5760996341705322\n",
      "step: 43350\n",
      "train: loss: 354483.59375 acc: 0.7951626777648926  val: loss: 2988639.25 acc: 0.6171720027923584\n",
      "step: 43355\n",
      "train: loss: 27656.263671875 acc: 0.9702666997909546  val: loss: 2543147.5 acc: 0.5728926658630371\n",
      "step: 43360\n",
      "train: loss: 137962.84375 acc: 0.8621283769607544  val: loss: 4719938.0 acc: 0.5980364084243774\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 43365\n",
      "train: loss: 821478.5625 acc: 0.7749696969985962  val: loss: 1541958.625 acc: 0.7226400375366211\n",
      "step: 43370\n",
      "train: loss: 1113254.75 acc: 0.822863757610321  val: loss: 1786728.25 acc: 0.6795194149017334\n",
      "step: 43375\n",
      "train: loss: 1004521.25 acc: 0.909019410610199  val: loss: 1117672.625 acc: 0.5347463488578796\n",
      "step: 43380\n",
      "train: loss: 135189.5625 acc: 0.9902424216270447  val: loss: 1935998.125 acc: 0.9035058617591858\n",
      "step: 43385\n",
      "train: loss: 283929.3125 acc: 0.9638296961784363  val: loss: 403627.0625 acc: 0.924876868724823\n",
      "step: 43390\n",
      "train: loss: 56465.96875 acc: 0.9886616468429565  val: loss: 1931701.5 acc: 0.7507240772247314\n",
      "step: 43395\n",
      "train: loss: 113654.5078125 acc: 0.9866962432861328  val: loss: 1794058.25 acc: 0.6366278529167175\n",
      "step: 43400\n",
      "train: loss: 525296.625 acc: 0.951342761516571  val: loss: 2811226.25 acc: 0.593155562877655\n",
      "step: 43405\n",
      "train: loss: 119930.75 acc: 0.9924741983413696  val: loss: 898330.0625 acc: 0.8878001570701599\n",
      "step: 43410\n",
      "train: loss: 131958.296875 acc: 0.9823698997497559  val: loss: 1752080.5 acc: 0.6948060989379883\n",
      "step: 43415\n",
      "train: loss: 677184.375 acc: 0.8647736310958862  val: loss: 1387899.375 acc: 0.38454771041870117\n",
      "step: 43420\n",
      "train: loss: 16065.8984375 acc: 0.9938513040542603  val: loss: 443978.1875 acc: 0.9069854021072388\n",
      "step: 43425\n",
      "train: loss: 11962.65625 acc: 0.9912837743759155  val: loss: 539478.3125 acc: 0.9432757496833801\n",
      "step: 43430\n",
      "train: loss: 9759.8994140625 acc: 0.9945645332336426  val: loss: 2188219.25 acc: 0.5475553274154663\n",
      "step: 43435\n",
      "train: loss: 5484.81005859375 acc: 0.9875895977020264  val: loss: 1182604.75 acc: 0.293712854385376\n",
      "step: 43440\n",
      "train: loss: 189056.015625 acc: 0.7511576414108276  val: loss: 1264044.0 acc: 0.48798495531082153\n",
      "step: 43445\n",
      "train: loss: 34317.109375 acc: 0.9754607677459717  val: loss: 697441.5 acc: 0.5642498135566711\n",
      "step: 43450\n",
      "train: loss: 13718.2421875 acc: 0.9757802486419678  val: loss: 1273123.75 acc: 0.12915098667144775\n",
      "step: 43455\n",
      "train: loss: 18426.326171875 acc: 0.9691102504730225  val: loss: 776834.25 acc: 0.8516902327537537\n",
      "step: 43460\n",
      "train: loss: 24160.115234375 acc: 0.8642969727516174  val: loss: 2444569.75 acc: 0.6821079850196838\n",
      "step: 43465\n",
      "train: loss: 15890.818359375 acc: 0.9813341498374939  val: loss: 497394.0 acc: 0.6771390438079834\n",
      "step: 43470\n",
      "train: loss: 30620.6953125 acc: 0.9729635715484619  val: loss: 1270356.375 acc: 0.32243216037750244\n",
      "step: 43475\n",
      "train: loss: 27456.025390625 acc: 0.9865776896476746  val: loss: 2341076.5 acc: 0.7056952714920044\n",
      "step: 43480\n",
      "train: loss: 23891.564453125 acc: 0.9808743000030518  val: loss: 1515576.625 acc: 0.038680315017700195\n",
      "step: 43485\n",
      "train: loss: 18037.642578125 acc: 0.990680992603302  val: loss: 894083.0625 acc: 0.8037480115890503\n",
      "step: 43490\n",
      "train: loss: 22531.244140625 acc: 0.9824991822242737  val: loss: 912930.3125 acc: 0.7774085998535156\n",
      "step: 43495\n",
      "train: loss: 17180.380859375 acc: 0.9796057939529419  val: loss: 1460873.375 acc: -0.023498892784118652\n",
      "step: 43500\n",
      "train: loss: 16014.4951171875 acc: 0.9892658591270447  val: loss: 1690388.5 acc: 0.18278318643569946\n",
      "step: 43505\n",
      "train: loss: 24965.021484375 acc: 0.9935200810432434  val: loss: 497938.625 acc: 0.899314284324646\n",
      "step: 43510\n",
      "train: loss: 44737.48046875 acc: 0.9867673516273499  val: loss: 1834768.0 acc: 0.6956942677497864\n",
      "step: 43515\n",
      "train: loss: 37436.46875 acc: 0.9909995198249817  val: loss: 716879.375 acc: 0.8299053907394409\n",
      "step: 43520\n",
      "train: loss: 31346.134765625 acc: 0.9822721481323242  val: loss: 873609.0625 acc: 0.7966781854629517\n",
      "step: 43525\n",
      "train: loss: 54975.7109375 acc: 0.9844788312911987  val: loss: 1639428.0 acc: 0.8025216460227966\n",
      "step: 43530\n",
      "train: loss: 108158.6796875 acc: 0.9703840017318726  val: loss: 321455.375 acc: 0.8981231451034546\n",
      "step: 43535\n",
      "train: loss: 290910.25 acc: 0.8985996842384338  val: loss: 128354.4375 acc: 0.9685015082359314\n",
      "step: 43540\n",
      "train: loss: 139903.546875 acc: 0.9802058935165405  val: loss: 112630.734375 acc: 0.958904504776001\n",
      "step: 43545\n",
      "train: loss: 118047.90625 acc: 0.9743509292602539  val: loss: 1652861.75 acc: 0.6775659918785095\n",
      "step: 43550\n",
      "train: loss: 71841.625 acc: 0.9910947680473328  val: loss: 1096056.625 acc: 0.645372748374939\n",
      "step: 43555\n",
      "train: loss: 52152.05859375 acc: 0.9913279414176941  val: loss: 970621.5 acc: 0.8631445169448853\n",
      "step: 43560\n",
      "train: loss: 572329.5 acc: 0.9326284527778625  val: loss: 1076612.625 acc: 0.4876754879951477\n",
      "step: 43565\n",
      "train: loss: 240879.296875 acc: 0.9624059200286865  val: loss: 369422.8125 acc: 0.9246219992637634\n",
      "step: 43570\n",
      "train: loss: 212315.40625 acc: 0.9823421239852905  val: loss: 569540.5 acc: 0.8796958923339844\n",
      "step: 43575\n",
      "train: loss: 410535.84375 acc: 0.9681535959243774  val: loss: 902095.125 acc: 0.5806277990341187\n",
      "step: 43580\n",
      "train: loss: 700735.875 acc: 0.7093612551689148  val: loss: 1067501.75 acc: 0.5948745012283325\n",
      "step: 43585\n",
      "train: loss: 1554163.75 acc: 0.9375566840171814  val: loss: 894780.875 acc: 0.5243504047393799\n",
      "step: 43590\n",
      "train: loss: 504436.875 acc: 0.9783923029899597  val: loss: 592539.875 acc: 0.852809488773346\n",
      "step: 43595\n",
      "train: loss: 1123210.25 acc: 0.962107241153717  val: loss: 710662.0 acc: 0.8532049655914307\n",
      "step: 43600\n",
      "train: loss: 2026250.5 acc: 0.8968114256858826  val: loss: 408609.3125 acc: 0.9297751784324646\n",
      "step: 43605\n",
      "train: loss: 607661.3125 acc: 0.9480528831481934  val: loss: 1075210.75 acc: 0.7660824060440063\n",
      "step: 43610\n",
      "train: loss: 263418.0625 acc: 0.9688014984130859  val: loss: 573846.4375 acc: 0.8677732348442078\n",
      "step: 43615\n",
      "train: loss: 333994.0625 acc: 0.9571707844734192  val: loss: 458377.0625 acc: 0.840846061706543\n",
      "step: 43620\n",
      "train: loss: 649708.125 acc: 0.8094288110733032  val: loss: 1100302.625 acc: 0.8088623285293579\n",
      "step: 43625\n",
      "train: loss: 458533.09375 acc: 0.851628303527832  val: loss: 2025025.0 acc: 0.4660602807998657\n",
      "step: 43630\n",
      "train: loss: 1073360.5 acc: 0.7633252739906311  val: loss: 743942.5 acc: 0.5196763873100281\n",
      "step: 43635\n",
      "train: loss: 792722.5625 acc: 0.7721731066703796  val: loss: 879991.625 acc: 0.5901731848716736\n",
      "step: 43640\n",
      "train: loss: 1155130.0 acc: 0.8133159875869751  val: loss: 789201.4375 acc: 0.9029310345649719\n",
      "step: 43645\n",
      "train: loss: 1577415.75 acc: 0.14239847660064697  val: loss: 1102845.5 acc: 0.7486339211463928\n",
      "step: 43650\n",
      "train: loss: 995028.75 acc: 0.6832221746444702  val: loss: 1807466.25 acc: 0.3655306100845337\n",
      "step: 43655\n",
      "train: loss: 408649.21875 acc: 0.7242779731750488  val: loss: 1541627.5 acc: 0.6280419826507568\n",
      "step: 43660\n",
      "train: loss: 139130.96875 acc: 0.8717239499092102  val: loss: 1824020.625 acc: 0.626468300819397\n",
      "step: 43665\n",
      "train: loss: 187411.6875 acc: 0.8204782009124756  val: loss: 4617785.5 acc: 0.46863502264022827\n",
      "step: 43670\n",
      "train: loss: 58833.87890625 acc: 0.9537688493728638  val: loss: 12584941.0 acc: 0.40068215131759644\n",
      "step: 43675\n",
      "train: loss: 14153.998046875 acc: 0.987991213798523  val: loss: 2743600.25 acc: 0.5722194910049438\n",
      "step: 43680\n",
      "train: loss: 282163.65625 acc: 0.8594174385070801  val: loss: 3542147.25 acc: 0.5772236585617065\n",
      "step: 43685\n",
      "train: loss: 48944.671875 acc: 0.9572104811668396  val: loss: 1253203.25 acc: 0.6568542718887329\n",
      "step: 43690\n",
      "train: loss: 265722.53125 acc: 0.8590717315673828  val: loss: 4029232.75 acc: 0.5307092666625977\n",
      "step: 43695\n",
      "train: loss: 350085.84375 acc: 0.7773616313934326  val: loss: 4941320.0 acc: 0.5230880975723267\n",
      "step: 43700\n",
      "train: loss: 167597.03125 acc: 0.8314622640609741  val: loss: 2171334.75 acc: 0.5694921612739563\n",
      "step: 43705\n",
      "train: loss: 113041.7890625 acc: 0.9063256978988647  val: loss: 684763.25 acc: 0.7820196151733398\n",
      "step: 43710\n",
      "train: loss: 141658.515625 acc: 0.873602032661438  val: loss: 3052937.25 acc: 0.5333020091056824\n",
      "step: 43715\n",
      "train: loss: 457871.1875 acc: 0.7347401976585388  val: loss: 1367095.125 acc: 0.7461912035942078\n",
      "step: 43720\n",
      "train: loss: 54742.86328125 acc: 0.9419242143630981  val: loss: 4692006.0 acc: 0.49643760919570923\n",
      "step: 43725\n",
      "train: loss: 486624.5 acc: 0.7444674968719482  val: loss: 2337374.75 acc: 0.5207506418228149\n",
      "step: 43730\n",
      "train: loss: 490358.28125 acc: 0.8049390912055969  val: loss: 5200991.0 acc: 0.5664322972297668\n",
      "step: 43735\n",
      "train: loss: 877077.375 acc: 0.8138614892959595  val: loss: 591329.8125 acc: 0.8130433559417725\n",
      "step: 43740\n",
      "train: loss: 1176643.125 acc: 0.8327924013137817  val: loss: 914937.9375 acc: 0.8369105458259583\n",
      "step: 43745\n",
      "train: loss: 2047507.25 acc: 0.8190364837646484  val: loss: 1877257.5 acc: 0.5545302033424377\n",
      "step: 43750\n",
      "train: loss: 598370.0 acc: 0.9252011179924011  val: loss: 1120081.125 acc: 0.7880285382270813\n",
      "step: 43755\n",
      "train: loss: 30814.4453125 acc: 0.9935857653617859  val: loss: 1942225.0 acc: -0.04138898849487305\n",
      "step: 43760\n",
      "train: loss: 72092.3671875 acc: 0.9754334092140198  val: loss: 1175420.625 acc: 0.4266577959060669\n",
      "step: 43765\n",
      "train: loss: 74695.9296875 acc: 0.9942518472671509  val: loss: 1602212.5 acc: 0.8437678217887878\n",
      "step: 43770\n",
      "train: loss: 68928.1328125 acc: 0.9952811598777771  val: loss: 2081497.0 acc: 0.6705272793769836\n",
      "step: 43775\n",
      "train: loss: 67573.8125 acc: 0.9922502040863037  val: loss: 2874271.25 acc: -0.013132452964782715\n",
      "step: 43780\n",
      "train: loss: 49174.3125 acc: 0.9897028207778931  val: loss: 2461553.0 acc: 0.33222341537475586\n",
      "step: 43785\n",
      "train: loss: 26848.5390625 acc: 0.9879034757614136  val: loss: 1411234.875 acc: 0.665800929069519\n",
      "step: 43790\n",
      "train: loss: 411326.21875 acc: 0.9390754103660583  val: loss: 752433.0625 acc: 0.8967767357826233\n",
      "step: 43795\n",
      "train: loss: 16723.044921875 acc: 0.9933549165725708  val: loss: 1570665.25 acc: 0.5100367069244385\n",
      "step: 43800\n",
      "train: loss: 19078.275390625 acc: 0.9446883201599121  val: loss: 1032223.625 acc: 0.9011625647544861\n",
      "step: 43805\n",
      "train: loss: 29129.12109375 acc: 0.9846447110176086  val: loss: 657121.125 acc: 0.9475217461585999\n",
      "step: 43810\n",
      "train: loss: 12953.9560546875 acc: 0.9737906455993652  val: loss: 1300153.875 acc: 0.8071897029876709\n",
      "step: 43815\n",
      "train: loss: 12713.9443359375 acc: 0.9475316405296326  val: loss: 1800914.25 acc: 0.5699686408042908\n",
      "step: 43820\n",
      "train: loss: 21163.556640625 acc: 0.9770464897155762  val: loss: 1178472.625 acc: 0.6189572811126709\n",
      "step: 43825\n",
      "train: loss: 15577.126953125 acc: 0.9680291414260864  val: loss: 502854.96875 acc: 0.8965955376625061\n",
      "step: 43830\n",
      "train: loss: 20725.822265625 acc: 0.9574711322784424  val: loss: 1582812.25 acc: 0.7910891175270081\n",
      "step: 43835\n",
      "train: loss: 66336.359375 acc: 0.9767774343490601  val: loss: 352225.46875 acc: 0.8986948132514954\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 43840\n",
      "train: loss: 53952.453125 acc: 0.9729444980621338  val: loss: 827008.8125 acc: 0.8997601866722107\n",
      "step: 43845\n",
      "train: loss: 12097.6669921875 acc: 0.9890276193618774  val: loss: 1493024.5 acc: -0.3270162343978882\n",
      "step: 43850\n",
      "train: loss: 33366.9609375 acc: 0.9833950400352478  val: loss: 835176.125 acc: 0.8238270878791809\n",
      "step: 43855\n",
      "train: loss: 37111.19921875 acc: 0.9772922992706299  val: loss: 632835.9375 acc: 0.8492822647094727\n",
      "step: 43860\n",
      "train: loss: 15300.4208984375 acc: 0.970107913017273  val: loss: 696911.4375 acc: 0.843046247959137\n",
      "step: 43865\n",
      "train: loss: 19328.8203125 acc: 0.9799214005470276  val: loss: 1069851.875 acc: 0.34209781885147095\n",
      "step: 43870\n",
      "train: loss: 34420.9375 acc: 0.9859598278999329  val: loss: 562987.0625 acc: 0.8993518948554993\n",
      "step: 43875\n",
      "train: loss: 23801.67578125 acc: 0.9952012896537781  val: loss: 979604.1875 acc: 0.7696723937988281\n",
      "step: 43880\n",
      "train: loss: 22491.599609375 acc: 0.9942006468772888  val: loss: 868072.1875 acc: 0.8525961637496948\n",
      "step: 43885\n",
      "train: loss: 29425.70703125 acc: 0.9809260368347168  val: loss: 1392099.75 acc: 0.43964797258377075\n",
      "step: 43890\n",
      "train: loss: 106880.453125 acc: 0.9747827649116516  val: loss: 450565.15625 acc: 0.8418258428573608\n",
      "step: 43895\n",
      "train: loss: 328194.375 acc: 0.9027923345565796  val: loss: 1281213.75 acc: 0.6811293959617615\n",
      "step: 43900\n",
      "train: loss: 236810.953125 acc: 0.9442473649978638  val: loss: 88875.40625 acc: 0.979407787322998\n",
      "step: 43905\n",
      "train: loss: 191595.890625 acc: 0.8722901344299316  val: loss: 548967.9375 acc: 0.8835127949714661\n",
      "step: 43910\n",
      "train: loss: 58923.3203125 acc: 0.9885295629501343  val: loss: 301538.21875 acc: 0.9212768077850342\n",
      "step: 43915\n",
      "train: loss: 151614.984375 acc: 0.9865053296089172  val: loss: 768660.0 acc: 0.49965548515319824\n",
      "step: 43920\n",
      "train: loss: 54762.3125 acc: 0.9933602809906006  val: loss: 350355.65625 acc: 0.901739776134491\n",
      "step: 43925\n",
      "train: loss: 314008.5625 acc: 0.9637113809585571  val: loss: 628203.125 acc: 0.7645710706710815\n",
      "step: 43930\n",
      "train: loss: 373043.875 acc: 0.9526811838150024  val: loss: 499397.3125 acc: 0.8976216912269592\n",
      "step: 43935\n",
      "train: loss: 358570.9375 acc: 0.9728904366493225  val: loss: 445966.90625 acc: 0.6820377111434937\n",
      "step: 43940\n",
      "train: loss: 285981.375 acc: 0.9799001216888428  val: loss: 758138.125 acc: 0.8420678377151489\n",
      "step: 43945\n",
      "train: loss: 858285.6875 acc: 0.9266910552978516  val: loss: 569388.625 acc: 0.835180401802063\n",
      "step: 43950\n",
      "train: loss: 744551.0625 acc: 0.9260149002075195  val: loss: 1063848.125 acc: 0.8047880530357361\n",
      "step: 43955\n",
      "train: loss: 1639539.5 acc: 0.9532660841941833  val: loss: 600267.5 acc: 0.8462291359901428\n",
      "step: 43960\n",
      "train: loss: 431152.6875 acc: 0.9863393306732178  val: loss: 361274.0625 acc: 0.9498114585876465\n",
      "step: 43965\n",
      "train: loss: 1194724.75 acc: 0.9599472880363464  val: loss: 772853.125 acc: 0.9235510230064392\n",
      "step: 43970\n",
      "train: loss: 675638.625 acc: 0.9500550627708435  val: loss: 352362.0625 acc: 0.9778344631195068\n",
      "step: 43975\n",
      "train: loss: 263985.8125 acc: 0.9849476218223572  val: loss: 1324556.875 acc: 0.667238175868988\n",
      "step: 43980\n",
      "train: loss: 412205.71875 acc: 0.9476862549781799  val: loss: 812630.375 acc: 0.9027952551841736\n",
      "step: 43985\n",
      "train: loss: 295200.40625 acc: 0.9718924760818481  val: loss: 412236.6875 acc: 0.8776593208312988\n",
      "step: 43990\n",
      "train: loss: 1902471.375 acc: 0.5964072346687317  val: loss: 1015800.3125 acc: 0.6773887872695923\n",
      "step: 43995\n",
      "train: loss: 353546.21875 acc: 0.7557802200317383  val: loss: 614929.5625 acc: 0.829490065574646\n",
      "step: 44000\n",
      "train: loss: 795160.75 acc: 0.5746372938156128  val: loss: 670656.25 acc: 0.764132022857666\n",
      "step: 44005\n",
      "train: loss: 363269.21875 acc: 0.8867138028144836  val: loss: 707914.0 acc: 0.8527161478996277\n",
      "step: 44010\n",
      "train: loss: 1031412.875 acc: 0.6092706918716431  val: loss: 294871.125 acc: 0.8144832849502563\n",
      "step: 44015\n",
      "train: loss: 123774.2734375 acc: 0.9146023392677307  val: loss: 4997093.0 acc: 0.35683006048202515\n",
      "step: 44020\n",
      "train: loss: 358142.1875 acc: 0.8123761415481567  val: loss: 1424767.5 acc: 0.6304323673248291\n",
      "step: 44025\n",
      "train: loss: 274197.9375 acc: 0.8516141772270203  val: loss: 718594.5 acc: 0.6949248313903809\n",
      "step: 44030\n",
      "train: loss: 85289.5625 acc: 0.9242722988128662  val: loss: 3159638.0 acc: 0.5197286605834961\n",
      "step: 44035\n",
      "train: loss: 43418.09375 acc: 0.9679967761039734  val: loss: 2083200.25 acc: 0.5708744525909424\n",
      "step: 44040\n",
      "train: loss: 281799.25 acc: 0.851823627948761  val: loss: 1569580.375 acc: 0.6823415756225586\n",
      "step: 44045\n",
      "train: loss: 49169.51171875 acc: 0.9627801775932312  val: loss: 1814286.5 acc: 0.6746560335159302\n",
      "step: 44050\n",
      "train: loss: 421218.5625 acc: 0.8167986869812012  val: loss: 1314166.5 acc: 0.7143105268478394\n",
      "step: 44055\n",
      "train: loss: 31666.23828125 acc: 0.9714807868003845  val: loss: 1582553.5 acc: 0.5606651306152344\n",
      "step: 44060\n",
      "train: loss: 58268.796875 acc: 0.9399118423461914  val: loss: 195684.390625 acc: 0.8757656812667847\n",
      "step: 44065\n",
      "train: loss: 23471.78515625 acc: 0.9673498272895813  val: loss: 1103294.625 acc: 0.6031814217567444\n",
      "step: 44070\n",
      "train: loss: 458790.21875 acc: 0.7532519102096558  val: loss: 7429893.0 acc: 0.4774661660194397\n",
      "step: 44075\n",
      "train: loss: 356594.6875 acc: 0.8188647627830505  val: loss: 2726550.0 acc: 0.5381902456283569\n",
      "step: 44080\n",
      "train: loss: 40021.78515625 acc: 0.9634872078895569  val: loss: 3878976.5 acc: 0.41256195306777954\n",
      "step: 44085\n",
      "train: loss: 230669.484375 acc: 0.8442651629447937  val: loss: 615656.3125 acc: 0.7794540524482727\n",
      "step: 44090\n",
      "train: loss: 186404.34375 acc: 0.8800532817840576  val: loss: 1906304.625 acc: 0.6610971689224243\n",
      "step: 44095\n",
      "train: loss: 136645.515625 acc: 0.8886715173721313  val: loss: 5039335.5 acc: 0.4210287928581238\n",
      "step: 44100\n",
      "train: loss: 919878.3125 acc: 0.8633451461791992  val: loss: 824591.25 acc: 0.7091069221496582\n",
      "step: 44105\n",
      "train: loss: 1182076.25 acc: 0.8679665327072144  val: loss: 625273.6875 acc: 0.8787320256233215\n",
      "step: 44110\n",
      "train: loss: 184166.3125 acc: 0.9854782819747925  val: loss: 1838323.5 acc: 0.09770435094833374\n",
      "step: 44115\n",
      "train: loss: 145350.0625 acc: 0.9727327227592468  val: loss: 2409410.25 acc: -0.3861790895462036\n",
      "step: 44120\n",
      "train: loss: 157321.34375 acc: 0.9822440147399902  val: loss: 414749.625 acc: 0.869711697101593\n",
      "step: 44125\n",
      "train: loss: 92346.8125 acc: 0.9768093824386597  val: loss: 1520729.375 acc: -0.15390312671661377\n",
      "step: 44130\n",
      "train: loss: 114247.2265625 acc: 0.9874212145805359  val: loss: 996253.75 acc: 0.873225748538971\n",
      "step: 44135\n",
      "train: loss: 52304.45703125 acc: 0.9962146282196045  val: loss: 1620850.75 acc: 0.8052358031272888\n",
      "step: 44140\n",
      "train: loss: 66280.7265625 acc: 0.9932234287261963  val: loss: 989112.5 acc: 0.7962192893028259\n",
      "step: 44145\n",
      "train: loss: 79513.7265625 acc: 0.9911321401596069  val: loss: 591894.9375 acc: 0.8827185034751892\n",
      "step: 44150\n",
      "train: loss: 32127.78515625 acc: 0.9909627437591553  val: loss: 1495184.125 acc: 0.4706043601036072\n",
      "step: 44155\n",
      "train: loss: 33469.01171875 acc: 0.9923421740531921  val: loss: 2218595.5 acc: 0.05620288848876953\n",
      "step: 44160\n",
      "train: loss: 29327.611328125 acc: 0.9880408644676208  val: loss: 499680.875 acc: 0.9206619262695312\n",
      "step: 44165\n",
      "train: loss: 19335.47265625 acc: 0.9932883381843567  val: loss: 1443623.5 acc: 0.722321629524231\n",
      "step: 44170\n",
      "train: loss: 25738.052734375 acc: 0.9852823615074158  val: loss: 990961.6875 acc: 0.48076963424682617\n",
      "step: 44175\n",
      "train: loss: 17658.7734375 acc: 0.9877203702926636  val: loss: 822975.125 acc: 0.8359540104866028\n",
      "step: 44180\n",
      "train: loss: 18610.876953125 acc: 0.9622275829315186  val: loss: 1680279.0 acc: 0.7605718374252319\n",
      "step: 44185\n",
      "train: loss: 158600.984375 acc: 0.647331953048706  val: loss: 987633.0 acc: 0.8347859382629395\n",
      "step: 44190\n",
      "train: loss: 24584.728515625 acc: 0.9317867755889893  val: loss: 1605933.375 acc: 0.3361397385597229\n",
      "step: 44195\n",
      "train: loss: 19261.712890625 acc: 0.9656614661216736  val: loss: 175195.09375 acc: 0.9282616972923279\n",
      "step: 44200\n",
      "train: loss: 24628.408203125 acc: 0.9712651968002319  val: loss: 763604.5 acc: 0.9122755527496338\n",
      "step: 44205\n",
      "train: loss: 57397.80078125 acc: 0.9806250333786011  val: loss: 192322.34375 acc: 0.9713845252990723\n",
      "step: 44210\n",
      "train: loss: 33804.22265625 acc: 0.9873664379119873  val: loss: 700677.5625 acc: 0.7679401636123657\n",
      "step: 44215\n",
      "train: loss: 93730.125 acc: 0.9546268582344055  val: loss: 561880.875 acc: 0.8999494314193726\n",
      "step: 44220\n",
      "train: loss: 30110.6796875 acc: 0.9732872843742371  val: loss: 389001.5 acc: 0.8581443428993225\n",
      "step: 44225\n",
      "train: loss: 20041.76171875 acc: 0.9865790605545044  val: loss: 621611.25 acc: 0.7812175154685974\n",
      "step: 44230\n",
      "train: loss: 15724.2041015625 acc: 0.9892541766166687  val: loss: 188129.125 acc: 0.9625810384750366\n",
      "step: 44235\n",
      "train: loss: 17977.453125 acc: 0.9929377436637878  val: loss: 819735.5 acc: 0.6287274956703186\n",
      "step: 44240\n",
      "train: loss: 20762.98828125 acc: 0.9939749836921692  val: loss: 505189.21875 acc: 0.9428024888038635\n",
      "step: 44245\n",
      "train: loss: 40032.56640625 acc: 0.9871048927307129  val: loss: 377441.0625 acc: 0.9121978878974915\n",
      "step: 44250\n",
      "train: loss: 33464.02734375 acc: 0.9854892492294312  val: loss: 1158134.0 acc: 0.598471999168396\n",
      "step: 44255\n",
      "train: loss: 59456.66796875 acc: 0.964545488357544  val: loss: 276089.75 acc: 0.9269434213638306\n",
      "step: 44260\n",
      "train: loss: 70147.75 acc: 0.983534574508667  val: loss: 272412.5625 acc: 0.9032813310623169\n",
      "step: 44265\n",
      "train: loss: 266032.09375 acc: 0.903880774974823  val: loss: 99495.5 acc: 0.9726404547691345\n",
      "step: 44270\n",
      "train: loss: 131304.40625 acc: 0.8892419338226318  val: loss: 80314.265625 acc: 0.9896523356437683\n",
      "step: 44275\n",
      "train: loss: 277325.0 acc: 0.9116771817207336  val: loss: 1414223.75 acc: 0.8448840379714966\n",
      "step: 44280\n",
      "train: loss: 670620.875 acc: 0.9033722877502441  val: loss: 436094.6875 acc: 0.8639636039733887\n",
      "step: 44285\n",
      "train: loss: 129155.03125 acc: 0.9838752150535583  val: loss: 544061.6875 acc: 0.8703051209449768\n",
      "step: 44290\n",
      "train: loss: 54025.4609375 acc: 0.9911522269248962  val: loss: 603115.9375 acc: 0.8455108404159546\n",
      "step: 44295\n",
      "train: loss: 118817.84375 acc: 0.9745738506317139  val: loss: 376592.1875 acc: 0.9487287402153015\n",
      "step: 44300\n",
      "train: loss: 268297.4375 acc: 0.9789608120918274  val: loss: 2282410.0 acc: 0.8427546620368958\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 44305\n",
      "train: loss: 431287.84375 acc: 0.9673779010772705  val: loss: 955130.25 acc: 0.8143113851547241\n",
      "step: 44310\n",
      "train: loss: 209864.0 acc: 0.9815141558647156  val: loss: 1341857.375 acc: 0.90160071849823\n",
      "step: 44315\n",
      "train: loss: 1332784.25 acc: 0.9229732155799866  val: loss: 1053579.875 acc: 0.8216960430145264\n",
      "step: 44320\n",
      "train: loss: 1506708.875 acc: 0.9486482739448547  val: loss: 804066.0 acc: 0.9132333397865295\n",
      "step: 44325\n",
      "train: loss: 524055.5 acc: 0.9790336489677429  val: loss: 1321889.5 acc: 0.881019651889801\n",
      "step: 44330\n",
      "train: loss: 1635054.5 acc: 0.9393535256385803  val: loss: 1833526.75 acc: 0.4289606213569641\n",
      "step: 44335\n",
      "train: loss: 292288.75 acc: 0.9787362813949585  val: loss: 706775.875 acc: 0.925585150718689\n",
      "step: 44340\n",
      "train: loss: 136578.734375 acc: 0.9535269141197205  val: loss: 3035905.5 acc: -0.4857368469238281\n",
      "step: 44345\n",
      "train: loss: 221351.703125 acc: 0.9685885906219482  val: loss: 1201137.0 acc: 0.8236724734306335\n",
      "step: 44350\n",
      "train: loss: 231804.4375 acc: 0.9316006302833557  val: loss: 435880.40625 acc: 0.9231987595558167\n",
      "step: 44355\n",
      "train: loss: 380366.5 acc: 0.8651344776153564  val: loss: 1181765.25 acc: 0.7129379510879517\n",
      "step: 44360\n",
      "train: loss: 1251844.0 acc: 0.5333216190338135  val: loss: 1356113.5 acc: 0.8185937404632568\n",
      "step: 44365\n",
      "train: loss: 396657.46875 acc: 0.8599259257316589  val: loss: 1247048.625 acc: 0.7308114171028137\n",
      "step: 44370\n",
      "train: loss: 854270.25 acc: 0.555921196937561  val: loss: 707996.3125 acc: 0.8618921637535095\n",
      "step: 44375\n",
      "train: loss: 1065578.75 acc: 0.7406129240989685  val: loss: 609728.0625 acc: 0.831283450126648\n",
      "step: 44380\n",
      "train: loss: 1710834.25 acc: 0.42439889907836914  val: loss: 731866.1875 acc: 0.7041153907775879\n",
      "step: 44385\n",
      "train: loss: 1800420.625 acc: 0.6047825813293457  val: loss: 1259005.0 acc: 0.40709882974624634\n",
      "step: 44390\n",
      "train: loss: 552593.0 acc: 0.5031155347824097  val: loss: 1382259.125 acc: 0.41274142265319824\n",
      "step: 44395\n",
      "train: loss: 721441.1875 acc: 0.6178705096244812  val: loss: 625754.25 acc: 0.6660623550415039\n",
      "step: 44400\n",
      "train: loss: 286477.4375 acc: 0.7960745096206665  val: loss: 824483.6875 acc: 0.7299443483352661\n",
      "step: 44405\n",
      "train: loss: 63673.96875 acc: 0.9537159204483032  val: loss: 2382124.25 acc: 0.5887812972068787\n",
      "step: 44410\n",
      "train: loss: 58643.51953125 acc: 0.9530789852142334  val: loss: 2505957.5 acc: 0.5271155834197998\n",
      "step: 44415\n",
      "train: loss: 49311.41796875 acc: 0.9622258543968201  val: loss: 1597244.25 acc: 0.6002676486968994\n",
      "step: 44420\n",
      "train: loss: 235969.875 acc: 0.8608789443969727  val: loss: 401504.0625 acc: 0.8117005825042725\n",
      "step: 44425\n",
      "train: loss: 233274.9375 acc: 0.8329058289527893  val: loss: 1139102.375 acc: 0.6504160165786743\n",
      "step: 44430\n",
      "train: loss: 28374.72265625 acc: 0.9683893918991089  val: loss: 1948564.0 acc: 0.6781221628189087\n",
      "step: 44435\n",
      "train: loss: 69721.375 acc: 0.9253172874450684  val: loss: 1623351.875 acc: 0.6771275997161865\n",
      "step: 44440\n",
      "train: loss: 615109.9375 acc: 0.7947050929069519  val: loss: 4631737.0 acc: 0.5629354119300842\n",
      "step: 44445\n",
      "train: loss: 200864.484375 acc: 0.8745599389076233  val: loss: 2591413.25 acc: 0.4886742830276489\n",
      "step: 44450\n",
      "train: loss: 419807.4375 acc: 0.7724191546440125  val: loss: 1242462.125 acc: 0.6994000673294067\n",
      "step: 44455\n",
      "train: loss: 29033.01171875 acc: 0.970247209072113  val: loss: 5524969.5 acc: 0.41304588317871094\n",
      "step: 44460\n",
      "train: loss: 201235.046875 acc: 0.8872483372688293  val: loss: 972119.6875 acc: 0.7154954075813293\n",
      "step: 44465\n",
      "train: loss: 477325.28125 acc: 0.7677814364433289  val: loss: 1070748.75 acc: 0.7990354895591736\n",
      "step: 44470\n",
      "train: loss: 664701.1875 acc: 0.878710925579071  val: loss: 674096.625 acc: 0.7900188565254211\n",
      "step: 44475\n",
      "train: loss: 672534.125 acc: 0.9388574957847595  val: loss: 207248.078125 acc: 0.9446448087692261\n",
      "step: 44480\n",
      "train: loss: 171291.359375 acc: 0.9844030141830444  val: loss: 794136.5 acc: 0.9013408422470093\n",
      "step: 44485\n",
      "train: loss: 82502.1875 acc: 0.9811265468597412  val: loss: 313896.28125 acc: 0.9392871260643005\n",
      "step: 44490\n",
      "train: loss: 79605.1875 acc: 0.9913168549537659  val: loss: 242163.53125 acc: 0.9015203714370728\n",
      "step: 44495\n",
      "train: loss: 96304.265625 acc: 0.9910010099411011  val: loss: 289143.46875 acc: 0.926961362361908\n",
      "step: 44500\n",
      "train: loss: 142657.0625 acc: 0.9888009428977966  val: loss: 269778.46875 acc: 0.9123117327690125\n",
      "step: 44505\n",
      "train: loss: 87460.9609375 acc: 0.9930057525634766  val: loss: 505053.3125 acc: 0.9163156747817993\n",
      "step: 44510\n",
      "train: loss: 40668.4609375 acc: 0.9934362173080444  val: loss: 816414.625 acc: 0.7742964625358582\n",
      "step: 44515\n",
      "train: loss: 39706.05859375 acc: 0.9952383637428284  val: loss: 996682.125 acc: 0.6558746099472046\n",
      "step: 44520\n",
      "train: loss: 56729.79296875 acc: 0.984093427658081  val: loss: 180009.390625 acc: 0.9437329173088074\n",
      "step: 44525\n",
      "train: loss: 15356.0166015625 acc: 0.9948191046714783  val: loss: 853108.125 acc: 0.748200535774231\n",
      "step: 44530\n",
      "train: loss: 17706.638671875 acc: 0.9661037921905518  val: loss: 623477.9375 acc: 0.7656638622283936\n",
      "step: 44535\n",
      "train: loss: 11443.9775390625 acc: 0.9928865432739258  val: loss: 492798.15625 acc: 0.896362841129303\n",
      "step: 44540\n",
      "train: loss: 23642.064453125 acc: 0.9833213090896606  val: loss: 616508.4375 acc: 0.7260544300079346\n",
      "step: 44545\n",
      "train: loss: 14108.82421875 acc: 0.9717535972595215  val: loss: 399478.875 acc: 0.9347216486930847\n",
      "step: 44550\n",
      "train: loss: 12060.615234375 acc: 0.950934112071991  val: loss: 666866.625 acc: 0.8837311863899231\n",
      "step: 44555\n",
      "train: loss: 12271.4990234375 acc: 0.9816999435424805  val: loss: 164974.171875 acc: 0.9632970094680786\n",
      "step: 44560\n",
      "train: loss: 27728.833984375 acc: 0.9680845141410828  val: loss: 795817.75 acc: 0.859087347984314\n",
      "step: 44565\n",
      "train: loss: 24471.716796875 acc: 0.9662487506866455  val: loss: 1406384.75 acc: 0.7849352955818176\n",
      "step: 44570\n",
      "train: loss: 21665.30078125 acc: 0.9876120686531067  val: loss: 1756799.125 acc: 0.5093281269073486\n",
      "step: 44575\n",
      "train: loss: 10283.9287109375 acc: 0.9908773899078369  val: loss: 1506622.0 acc: 0.7788936495780945\n",
      "step: 44580\n",
      "train: loss: 25179.869140625 acc: 0.9841051697731018  val: loss: 632376.5 acc: 0.8704384565353394\n",
      "step: 44585\n",
      "train: loss: 25408.40625 acc: 0.982145369052887  val: loss: 710585.0 acc: 0.8146941661834717\n",
      "step: 44590\n",
      "train: loss: 18222.74609375 acc: 0.9846686720848083  val: loss: 1634025.875 acc: 0.7620970010757446\n",
      "step: 44595\n",
      "train: loss: 16039.6240234375 acc: 0.9838768839836121  val: loss: 1258517.875 acc: 0.8956049680709839\n",
      "step: 44600\n",
      "train: loss: 9919.4521484375 acc: 0.9946947693824768  val: loss: 495134.6875 acc: 0.9440386891365051\n",
      "step: 44605\n",
      "train: loss: 44837.63671875 acc: 0.9844311475753784  val: loss: 2326715.25 acc: 0.8851391077041626\n",
      "step: 44610\n",
      "train: loss: 37092.3515625 acc: 0.9855121970176697  val: loss: 459301.75 acc: 0.8492130041122437\n",
      "step: 44615\n",
      "train: loss: 33003.7109375 acc: 0.9909353256225586  val: loss: 689489.25 acc: 0.9181716442108154\n",
      "step: 44620\n",
      "train: loss: 70936.1171875 acc: 0.9774622917175293  val: loss: 1130549.0 acc: 0.8239150643348694\n",
      "step: 44625\n",
      "train: loss: 157236.6875 acc: 0.9635903835296631  val: loss: 637398.9375 acc: 0.9004157781600952\n",
      "step: 44630\n",
      "train: loss: 94189.4375 acc: 0.968380331993103  val: loss: 658712.0625 acc: 0.8485785126686096\n",
      "step: 44635\n",
      "train: loss: 60660.59375 acc: 0.9387108683586121  val: loss: 1130985.625 acc: 0.883699893951416\n",
      "step: 44640\n",
      "train: loss: 849142.5 acc: 0.774893045425415  val: loss: 1508818.875 acc: 0.8018574118614197\n",
      "step: 44645\n",
      "train: loss: 97304.6796875 acc: 0.9876654148101807  val: loss: 1585489.25 acc: 0.8246592283248901\n",
      "step: 44650\n",
      "train: loss: 64462.98828125 acc: 0.9911505579948425  val: loss: 654509.625 acc: 0.8683774471282959\n",
      "step: 44655\n",
      "train: loss: 147725.46875 acc: 0.9774715900421143  val: loss: 1161105.625 acc: 0.8336865901947021\n",
      "step: 44660\n",
      "train: loss: 323167.21875 acc: 0.9602680206298828  val: loss: 874238.5 acc: 0.8574416637420654\n",
      "step: 44665\n",
      "train: loss: 291414.25 acc: 0.9425053000450134  val: loss: 253833.59375 acc: 0.927216649055481\n",
      "step: 44670\n",
      "train: loss: 394105.78125 acc: 0.966841995716095  val: loss: 1506838.0 acc: 0.2694855332374573\n",
      "step: 44675\n",
      "train: loss: 241441.890625 acc: 0.9789287447929382  val: loss: 573526.375 acc: 0.9160900115966797\n",
      "step: 44680\n",
      "train: loss: 392284.53125 acc: 0.9492661952972412  val: loss: 1087421.375 acc: 0.8887601494789124\n",
      "step: 44685\n",
      "train: loss: 915973.75 acc: 0.960158109664917  val: loss: 1802445.0 acc: 0.7617869973182678\n",
      "step: 44690\n",
      "train: loss: 1265789.0 acc: 0.9682368636131287  val: loss: 1360257.75 acc: 0.7445921897888184\n",
      "step: 44695\n",
      "train: loss: 3512841.0 acc: 0.8377406001091003  val: loss: 1465087.125 acc: 0.4760323166847229\n",
      "step: 44700\n",
      "train: loss: 647575.625 acc: 0.947979211807251  val: loss: 857385.375 acc: 0.8638180494308472\n",
      "step: 44705\n",
      "train: loss: 1166717.0 acc: 0.9037257432937622  val: loss: 1021776.9375 acc: 0.7263297438621521\n",
      "step: 44710\n",
      "train: loss: 182582.0625 acc: 0.9757968783378601  val: loss: 442806.5625 acc: 0.9375372529029846\n",
      "step: 44715\n",
      "train: loss: 390283.8125 acc: 0.8980106115341187  val: loss: 944767.875 acc: 0.45397651195526123\n",
      "step: 44720\n",
      "train: loss: 1686228.5 acc: 0.3976035714149475  val: loss: 284604.625 acc: 0.932802677154541\n",
      "step: 44725\n",
      "train: loss: 1007269.125 acc: 0.5628057718276978  val: loss: 873527.0 acc: 0.7788022756576538\n",
      "step: 44730\n",
      "train: loss: 945951.75 acc: 0.822028636932373  val: loss: 584210.375 acc: 0.6840536594390869\n",
      "step: 44735\n",
      "train: loss: 1102804.25 acc: 0.6431028842926025  val: loss: 822120.375 acc: 0.8040472269058228\n",
      "step: 44740\n",
      "train: loss: 655110.5 acc: 0.834564745426178  val: loss: 776222.0625 acc: 0.18775218725204468\n",
      "step: 44745\n",
      "train: loss: 727099.8125 acc: 0.6401011943817139  val: loss: 876794.8125 acc: 0.6942673921585083\n",
      "step: 44750\n",
      "train: loss: 330073.125 acc: 0.8042547702789307  val: loss: 1479190.0 acc: 0.6328152418136597\n",
      "step: 44755\n",
      "train: loss: 311151.625 acc: 0.8441873788833618  val: loss: 4919523.0 acc: 0.46049660444259644\n",
      "step: 44760\n",
      "train: loss: 474708.15625 acc: 0.7402153015136719  val: loss: 1343037.375 acc: 0.6714834570884705\n",
      "step: 44765\n",
      "train: loss: 75517.2734375 acc: 0.9398111701011658  val: loss: 1052045.875 acc: 0.6842612028121948\n",
      "step: 44770\n",
      "train: loss: 57575.02734375 acc: 0.9544258713722229  val: loss: 709310.4375 acc: 0.7646222710609436\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 44775\n",
      "train: loss: 103662.7734375 acc: 0.9290869235992432  val: loss: 1663120.625 acc: 0.5553224086761475\n",
      "step: 44780\n",
      "train: loss: 169733.84375 acc: 0.885982871055603  val: loss: 3137379.5 acc: 0.4522503614425659\n",
      "step: 44785\n",
      "train: loss: 127609.8671875 acc: 0.9045140743255615  val: loss: 2593266.0 acc: 0.651787519454956\n",
      "step: 44790\n",
      "train: loss: 247811.671875 acc: 0.838715672492981  val: loss: 1047023.4375 acc: 0.6675163507461548\n",
      "step: 44795\n",
      "train: loss: 38894.37109375 acc: 0.9666427969932556  val: loss: 1470353.75 acc: 0.6729550361633301\n",
      "step: 44800\n",
      "train: loss: 40221.9921875 acc: 0.9554159641265869  val: loss: 376875.34375 acc: 0.7590187191963196\n",
      "step: 44805\n",
      "train: loss: 204661.5625 acc: 0.8748208284378052  val: loss: 614568.6875 acc: 0.782485842704773\n",
      "step: 44810\n",
      "train: loss: 360312.65625 acc: 0.8063074946403503  val: loss: 2666369.0 acc: 0.4845850467681885\n",
      "step: 44815\n",
      "train: loss: 551153.0625 acc: 0.7606498003005981  val: loss: 1323187.75 acc: 0.7152650952339172\n",
      "step: 44820\n",
      "train: loss: 226783.1875 acc: 0.757392168045044  val: loss: 1321783.625 acc: 0.6225690841674805\n",
      "step: 44825\n",
      "train: loss: 434361.84375 acc: 0.802036464214325  val: loss: 809210.6875 acc: 0.7203631401062012\n",
      "step: 44830\n",
      "train: loss: 1447304.0 acc: 0.785812497138977  val: loss: 2497147.75 acc: 0.7087669968605042\n",
      "step: 44835\n",
      "train: loss: 731386.9375 acc: 0.879761815071106  val: loss: 410412.78125 acc: 0.8030860424041748\n",
      "step: 44840\n",
      "train: loss: 461229.25 acc: 0.9516331553459167  val: loss: 449569.59375 acc: 0.9014999866485596\n",
      "step: 44845\n",
      "train: loss: 120216.5078125 acc: 0.9899770617485046  val: loss: 537753.625 acc: 0.9002983570098877\n",
      "step: 44850\n",
      "train: loss: 296511.5 acc: 0.9710251688957214  val: loss: 398224.4375 acc: 0.8486807346343994\n",
      "step: 44855\n",
      "train: loss: 61362.6484375 acc: 0.9865072965621948  val: loss: 309324.96875 acc: 0.9211500883102417\n",
      "step: 44860\n",
      "train: loss: 189885.953125 acc: 0.9799529910087585  val: loss: 881591.25 acc: 0.7642205953598022\n",
      "step: 44865\n",
      "train: loss: 73305.890625 acc: 0.9944554567337036  val: loss: 957693.25 acc: 0.6250969767570496\n",
      "step: 44870\n",
      "train: loss: 107590.7734375 acc: 0.9920788407325745  val: loss: 608008.6875 acc: 0.8715832233428955\n",
      "step: 44875\n",
      "train: loss: 37804.5625 acc: 0.9944694638252258  val: loss: 393888.46875 acc: 0.911771833896637\n",
      "step: 44880\n",
      "train: loss: 52383.69140625 acc: 0.9921762943267822  val: loss: 691139.3125 acc: 0.5787161588668823\n",
      "step: 44885\n",
      "train: loss: 24791.185546875 acc: 0.9935316443443298  val: loss: 789466.375 acc: 0.9172001481056213\n",
      "step: 44890\n",
      "train: loss: 24354.6796875 acc: 0.9888413548469543  val: loss: 1076902.5 acc: 0.7512679100036621\n",
      "step: 44895\n",
      "train: loss: 14839.4306640625 acc: 0.9455150961875916  val: loss: 1398288.375 acc: 0.6924130916595459\n",
      "step: 44900\n",
      "train: loss: 16722.71875 acc: 0.9554765820503235  val: loss: 757210.8125 acc: 0.8980324864387512\n",
      "step: 44905\n",
      "train: loss: 30214.275390625 acc: 0.9818239808082581  val: loss: 2670890.25 acc: 0.6934934854507446\n",
      "step: 44910\n",
      "train: loss: 13135.8046875 acc: 0.9598434567451477  val: loss: 1672705.75 acc: 0.8695200681686401\n",
      "step: 44915\n",
      "train: loss: 13199.6591796875 acc: 0.9617165327072144  val: loss: 905357.25 acc: 0.908648669719696\n",
      "step: 44920\n",
      "train: loss: 13279.2109375 acc: 0.9697767496109009  val: loss: 191272.734375 acc: 0.9503414630889893\n",
      "step: 44925\n",
      "train: loss: 11170.5205078125 acc: 0.9822559952735901  val: loss: 422816.3125 acc: 0.9203422665596008\n",
      "step: 44930\n",
      "train: loss: 20449.693359375 acc: 0.969903826713562  val: loss: 1683750.25 acc: 0.8052101135253906\n",
      "step: 44935\n",
      "train: loss: 45667.91796875 acc: 0.9808508157730103  val: loss: 442542.46875 acc: 0.8936941027641296\n",
      "step: 44940\n",
      "train: loss: 25066.53515625 acc: 0.9820033311843872  val: loss: 708685.5 acc: 0.7554033398628235\n",
      "step: 44945\n",
      "train: loss: 26241.072265625 acc: 0.9855746030807495  val: loss: 1154630.5 acc: 0.41931581497192383\n",
      "step: 44950\n",
      "train: loss: 28145.39453125 acc: 0.9877485036849976  val: loss: 1561008.5 acc: 0.7015441060066223\n",
      "step: 44955\n",
      "train: loss: 11069.87109375 acc: 0.975127100944519  val: loss: 1637674.125 acc: 0.6915204524993896\n",
      "step: 44960\n",
      "train: loss: 22330.1953125 acc: 0.9377543330192566  val: loss: 1196394.0 acc: 0.3380594253540039\n",
      "step: 44965\n",
      "train: loss: 23479.46875 acc: 0.9913310408592224  val: loss: 744472.6875 acc: 0.8219003677368164\n",
      "step: 44970\n",
      "train: loss: 53132.00390625 acc: 0.9901942014694214  val: loss: 2063188.75 acc: 0.3781042695045471\n",
      "step: 44975\n",
      "train: loss: 48616.41015625 acc: 0.9867744445800781  val: loss: 2774168.5 acc: -0.754349946975708\n",
      "step: 44980\n",
      "train: loss: 46057.33984375 acc: 0.975114107131958  val: loss: 2998605.75 acc: 0.4316282272338867\n",
      "step: 44985\n",
      "train: loss: 34362.12109375 acc: 0.97990882396698  val: loss: 2083012.0 acc: 0.7665402293205261\n",
      "step: 44990\n",
      "train: loss: 269618.40625 acc: 0.8704237937927246  val: loss: 1076173.125 acc: 0.8275431394577026\n",
      "step: 44995\n",
      "train: loss: 220167.4375 acc: 0.921019434928894  val: loss: 429130.6875 acc: 0.9383148550987244\n",
      "step: 45000\n",
      "train: loss: 93709.4765625 acc: 0.9503865242004395  val: loss: 179686.8125 acc: 0.9617578983306885\n",
      "step: 45005\n",
      "train: loss: 579756.0 acc: 0.9227390289306641  val: loss: 2096655.125 acc: 0.5888872146606445\n",
      "step: 45010\n",
      "train: loss: 61533.00390625 acc: 0.9929470419883728  val: loss: 697608.6875 acc: 0.9368095993995667\n",
      "step: 45015\n",
      "train: loss: 54780.47265625 acc: 0.9950237274169922  val: loss: 2675418.75 acc: 0.5205122232437134\n",
      "step: 45020\n",
      "train: loss: 50866.98828125 acc: 0.9951452016830444  val: loss: 1214334.25 acc: -0.42522239685058594\n",
      "step: 45025\n",
      "train: loss: 42097.1015625 acc: 0.9877269268035889  val: loss: 1994793.625 acc: 0.663284182548523\n",
      "step: 45030\n",
      "train: loss: 218255.140625 acc: 0.9753006100654602  val: loss: 1574290.625 acc: 0.3721182942390442\n",
      "step: 45035\n",
      "train: loss: 320352.65625 acc: 0.9849623441696167  val: loss: 1437334.0 acc: 0.184617817401886\n",
      "step: 45040\n",
      "train: loss: 355197.28125 acc: 0.9560487270355225  val: loss: 1582777.0 acc: 0.6769163608551025\n",
      "step: 45045\n",
      "train: loss: 221298.875 acc: 0.9433990120887756  val: loss: 1474951.75 acc: 0.7789965271949768\n",
      "step: 45050\n",
      "train: loss: 986261.5 acc: 0.9678529500961304  val: loss: 1210424.125 acc: 0.5666006803512573\n",
      "step: 45055\n",
      "train: loss: 1887524.25 acc: 0.9432697296142578  val: loss: 287376.21875 acc: 0.8927860260009766\n",
      "step: 45060\n",
      "train: loss: 2663767.25 acc: 0.8631772398948669  val: loss: 430752.09375 acc: 0.8074366450309753\n",
      "step: 45065\n",
      "train: loss: 764593.375 acc: 0.9560773372650146  val: loss: 481817.84375 acc: 0.9016199111938477\n",
      "step: 45070\n",
      "train: loss: 176426.0625 acc: 0.9821506142616272  val: loss: 564613.4375 acc: 0.9321221113204956\n",
      "step: 45075\n",
      "train: loss: 584347.75 acc: 0.9542991518974304  val: loss: 520035.21875 acc: 0.9382883310317993\n",
      "step: 45080\n",
      "train: loss: 507404.09375 acc: 0.9145349264144897  val: loss: 753660.375 acc: 0.8306931853294373\n",
      "step: 45085\n",
      "train: loss: 1576823.5 acc: 0.792845606803894  val: loss: 763763.75 acc: 0.9064427018165588\n",
      "step: 45090\n",
      "train: loss: 670837.0 acc: 0.7089776992797852  val: loss: 1088616.75 acc: 0.8402690291404724\n",
      "step: 45095\n",
      "train: loss: 1053624.5 acc: 0.6504420042037964  val: loss: 789490.25 acc: 0.8609563708305359\n",
      "step: 45100\n",
      "train: loss: 765095.75 acc: 0.8007240295410156  val: loss: 618283.1875 acc: 0.8739667534828186\n",
      "step: 45105\n",
      "train: loss: 761407.1875 acc: 0.7963635921478271  val: loss: 1058877.75 acc: 0.7356044054031372\n",
      "step: 45110\n",
      "train: loss: 1833665.75 acc: 0.1922985315322876  val: loss: 823763.125 acc: 0.8602878451347351\n",
      "step: 45115\n",
      "train: loss: 511949.53125 acc: 0.6095860004425049  val: loss: 2344787.0 acc: 0.6368992328643799\n",
      "step: 45120\n",
      "train: loss: 331489.3125 acc: 0.7632352113723755  val: loss: 628208.3125 acc: 0.4012242555618286\n",
      "step: 45125\n",
      "train: loss: 474528.125 acc: 0.5975847244262695  val: loss: 912935.0625 acc: 0.7109750509262085\n",
      "step: 45130\n",
      "train: loss: 375996.78125 acc: 0.6988019943237305  val: loss: 962598.0 acc: 0.6927517056465149\n",
      "step: 45135\n",
      "train: loss: 342812.1875 acc: 0.7365798354148865  val: loss: 548868.625 acc: 0.8177273273468018\n",
      "step: 45140\n",
      "train: loss: 263620.90625 acc: 0.8191831111907959  val: loss: 1327929.25 acc: 0.6767969131469727\n",
      "step: 45145\n",
      "train: loss: 275056.125 acc: 0.8388025760650635  val: loss: 563638.8125 acc: 0.7560752034187317\n",
      "step: 45150\n",
      "train: loss: 325206.46875 acc: 0.7837830781936646  val: loss: 962418.625 acc: 0.8241893649101257\n",
      "step: 45155\n",
      "train: loss: 222080.5 acc: 0.8003670573234558  val: loss: 416778.5 acc: 0.7455036640167236\n",
      "step: 45160\n",
      "train: loss: 79411.2265625 acc: 0.9345157146453857  val: loss: 1844935.5 acc: 0.7639321684837341\n",
      "step: 45165\n",
      "train: loss: 95733.171875 acc: 0.8613605499267578  val: loss: 523162.78125 acc: 0.8099039793014526\n",
      "step: 45170\n",
      "train: loss: 700104.875 acc: 0.5108340382575989  val: loss: 1719213.75 acc: 0.7198750972747803\n",
      "step: 45175\n",
      "train: loss: 486152.8125 acc: 0.7383708953857422  val: loss: 874670.25 acc: 0.694902777671814\n",
      "step: 45180\n",
      "train: loss: 536973.1875 acc: 0.709996223449707  val: loss: 2991226.25 acc: 0.5919636487960815\n",
      "step: 45185\n",
      "train: loss: 273674.53125 acc: 0.8454602956771851  val: loss: 1332160.25 acc: 0.7024725079536438\n",
      "step: 45190\n",
      "train: loss: 542228.9375 acc: 0.7078200578689575  val: loss: 912912.625 acc: 0.6205036044120789\n",
      "step: 45195\n",
      "train: loss: 2008547.0 acc: 0.6560965776443481  val: loss: 2020392.75 acc: 0.6877108812332153\n",
      "step: 45200\n",
      "train: loss: 1257431.5 acc: 0.7966123223304749  val: loss: 334090.28125 acc: 0.8945725560188293\n",
      "step: 45205\n",
      "train: loss: 1192298.75 acc: 0.8772518634796143  val: loss: 439634.375 acc: 0.8841903209686279\n",
      "step: 45210\n",
      "train: loss: 591710.5 acc: 0.9292646050453186  val: loss: 875211.875 acc: 0.8822734355926514\n",
      "step: 45215\n",
      "train: loss: 691488.375 acc: 0.9361735582351685  val: loss: 1109395.75 acc: 0.8752617835998535\n",
      "step: 45220\n",
      "train: loss: 184383.109375 acc: 0.9613965749740601  val: loss: 1077135.5 acc: 0.7492274045944214\n",
      "step: 45225\n",
      "train: loss: 285294.0 acc: 0.9708877205848694  val: loss: 808053.75 acc: 0.8669073581695557\n",
      "step: 45230\n",
      "train: loss: 179146.09375 acc: 0.9856177568435669  val: loss: 1913036.25 acc: 0.8211728930473328\n",
      "step: 45235\n",
      "train: loss: 159412.921875 acc: 0.9889375567436218  val: loss: 982332.875 acc: 0.8643274307250977\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 45240\n",
      "train: loss: 152721.0625 acc: 0.9865362048149109  val: loss: 1168492.5 acc: 0.9079844951629639\n",
      "step: 45245\n",
      "train: loss: 158107.296875 acc: 0.9788828492164612  val: loss: 701353.9375 acc: 0.8832040429115295\n",
      "step: 45250\n",
      "train: loss: 77873.9609375 acc: 0.9668284058570862  val: loss: 508335.6875 acc: 0.9306350350379944\n",
      "step: 45255\n",
      "train: loss: 105521.234375 acc: 0.9312401413917542  val: loss: 156563.0625 acc: 0.9456338286399841\n",
      "step: 45260\n",
      "train: loss: 63066.0703125 acc: 0.9520221948623657  val: loss: 1523618.25 acc: 0.8730716705322266\n",
      "step: 45265\n",
      "train: loss: 26821.92578125 acc: 0.9930536150932312  val: loss: 1229555.75 acc: 0.8320662975311279\n",
      "step: 45270\n",
      "train: loss: 11024.244140625 acc: 0.9936902523040771  val: loss: 1904091.5 acc: 0.664630651473999\n",
      "step: 45275\n",
      "train: loss: 8229.9990234375 acc: 0.9831990003585815  val: loss: 292178.875 acc: 0.8518204092979431\n",
      "step: 45280\n",
      "train: loss: 9902.1025390625 acc: 0.9690521955490112  val: loss: 909534.375 acc: 0.8568389415740967\n",
      "step: 45285\n",
      "train: loss: 16001.240234375 acc: 0.9768564701080322  val: loss: 631150.75 acc: 0.9116460084915161\n",
      "step: 45290\n",
      "train: loss: 13587.4169921875 acc: 0.9830941557884216  val: loss: 2378127.0 acc: 0.5279982089996338\n",
      "step: 45295\n",
      "train: loss: 18588.859375 acc: 0.9495292901992798  val: loss: 1320518.125 acc: 0.7906128764152527\n",
      "step: 45300\n",
      "train: loss: 86168.1796875 acc: 0.9405568242073059  val: loss: 715475.625 acc: 0.887475848197937\n",
      "step: 45305\n",
      "train: loss: 50837.34765625 acc: 0.9734113812446594  val: loss: 1679740.25 acc: 0.7467769384384155\n",
      "step: 45310\n",
      "train: loss: 80596.09375 acc: 0.9024933576583862  val: loss: 2206578.25 acc: 0.6735372543334961\n",
      "step: 45315\n",
      "train: loss: 28826.41015625 acc: 0.9821800589561462  val: loss: 538007.9375 acc: 0.9267563819885254\n",
      "step: 45320\n",
      "train: loss: 30314.111328125 acc: 0.9798654317855835  val: loss: 2156093.25 acc: 0.7376008033752441\n",
      "step: 45325\n",
      "train: loss: 7175.6474609375 acc: 0.9826652407646179  val: loss: 2859517.5 acc: -0.8723989725112915\n",
      "step: 45330\n",
      "train: loss: 40074.78515625 acc: 0.9825727343559265  val: loss: 3198771.25 acc: 0.3792538642883301\n",
      "step: 45335\n",
      "train: loss: 50423.73046875 acc: 0.9703995585441589  val: loss: 2273435.0 acc: 0.5125300884246826\n",
      "step: 45340\n",
      "train: loss: 38153.0546875 acc: 0.9894298315048218  val: loss: 2458134.25 acc: 0.6443294286727905\n",
      "step: 45345\n",
      "train: loss: 48032.26953125 acc: 0.9876086711883545  val: loss: 1566516.25 acc: 0.650981068611145\n",
      "step: 45350\n",
      "train: loss: 14098.2734375 acc: 0.9952409863471985  val: loss: 702633.625 acc: 0.8204199075698853\n",
      "step: 45355\n",
      "train: loss: 114954.4609375 acc: 0.9783128499984741  val: loss: 967750.875 acc: 0.8572813272476196\n",
      "step: 45360\n",
      "train: loss: 68111.2265625 acc: 0.9677380323410034  val: loss: 1694326.375 acc: 0.6272361278533936\n",
      "step: 45365\n",
      "train: loss: 82125.75 acc: 0.9612440466880798  val: loss: 1758670.0 acc: 0.6920442581176758\n",
      "step: 45370\n",
      "train: loss: 187319.96875 acc: 0.9624699950218201  val: loss: 837133.3125 acc: 0.8782123923301697\n",
      "step: 45375\n",
      "train: loss: 85518.2734375 acc: 0.9893178939819336  val: loss: 1491802.875 acc: 0.6876780986785889\n",
      "step: 45380\n",
      "train: loss: 96607.484375 acc: 0.9897457361221313  val: loss: 1237232.5 acc: 0.804256796836853\n",
      "step: 45385\n",
      "train: loss: 63046.33984375 acc: 0.993163526058197  val: loss: 457718.21875 acc: 0.8831043243408203\n",
      "step: 45390\n",
      "train: loss: 138342.15625 acc: 0.968217670917511  val: loss: 210610.28125 acc: 0.9483328461647034\n",
      "step: 45395\n",
      "train: loss: 149348.640625 acc: 0.9786103963851929  val: loss: 1147298.0 acc: 0.8521398305892944\n",
      "step: 45400\n",
      "train: loss: 937426.1875 acc: 0.9544506072998047  val: loss: 1558909.875 acc: 0.5474756360054016\n",
      "step: 45405\n",
      "train: loss: 415592.90625 acc: 0.9511392116546631  val: loss: 740832.4375 acc: 0.575640082359314\n",
      "step: 45410\n",
      "train: loss: 195309.1875 acc: 0.9452345371246338  val: loss: 396537.28125 acc: 0.7581464052200317\n",
      "step: 45415\n",
      "train: loss: 617225.1875 acc: 0.9716601967811584  val: loss: 2323912.5 acc: 0.423636257648468\n",
      "step: 45420\n",
      "train: loss: 2875086.25 acc: 0.8975425362586975  val: loss: 191740.921875 acc: 0.9536712765693665\n",
      "step: 45425\n",
      "train: loss: 1614882.25 acc: 0.9507556557655334  val: loss: 703261.9375 acc: 0.6281123161315918\n",
      "step: 45430\n",
      "train: loss: 910434.25 acc: 0.95799320936203  val: loss: 1365811.5 acc: 0.7697727680206299\n",
      "step: 45435\n",
      "train: loss: 520773.5 acc: 0.9538346529006958  val: loss: 719922.6875 acc: 0.9154313802719116\n",
      "step: 45440\n",
      "train: loss: 248338.515625 acc: 0.9472458958625793  val: loss: 307687.71875 acc: 0.9117798805236816\n",
      "step: 45445\n",
      "train: loss: 766193.9375 acc: 0.9318149089813232  val: loss: 1691772.5 acc: 0.34612858295440674\n",
      "step: 45450\n",
      "train: loss: 1259538.75 acc: 0.8186571002006531  val: loss: 654855.5625 acc: 0.9094460606575012\n",
      "step: 45455\n",
      "train: loss: 454283.53125 acc: 0.7783593535423279  val: loss: 1309777.5 acc: 0.6971160173416138\n",
      "step: 45460\n",
      "train: loss: 792165.0 acc: 0.6805171966552734  val: loss: 1841458.25 acc: 0.6654980182647705\n",
      "step: 45465\n",
      "train: loss: 1005057.375 acc: 0.8021251559257507  val: loss: 493965.625 acc: 0.8058801889419556\n",
      "step: 45470\n",
      "train: loss: 247320.828125 acc: 0.8091457486152649  val: loss: 989061.1875 acc: 0.5385159254074097\n",
      "step: 45475\n",
      "train: loss: 843784.9375 acc: 0.7015726566314697  val: loss: 2559843.25 acc: 0.5462660789489746\n",
      "step: 45480\n",
      "train: loss: 337632.09375 acc: 0.8221250176429749  val: loss: 1532340.125 acc: 0.6008291244506836\n",
      "step: 45485\n",
      "train: loss: 102227.921875 acc: 0.9209873676300049  val: loss: 957882.1875 acc: 0.5963080525398254\n",
      "step: 45490\n",
      "train: loss: 491495.15625 acc: 0.7504485845565796  val: loss: 655412.1875 acc: 0.6252259016036987\n",
      "step: 45495\n",
      "train: loss: 43514.71875 acc: 0.962357759475708  val: loss: 6126620.5 acc: 0.46425414085388184\n",
      "step: 45500\n",
      "train: loss: 98862.8671875 acc: 0.9246155023574829  val: loss: 836686.9375 acc: 0.5815060138702393\n",
      "step: 45505\n",
      "train: loss: 160378.84375 acc: 0.9129005074501038  val: loss: 2598219.75 acc: 0.6215161085128784\n",
      "step: 45510\n",
      "train: loss: 285101.96875 acc: 0.8603494167327881  val: loss: 1313831.875 acc: 0.6808178424835205\n",
      "step: 45515\n",
      "train: loss: 405186.5 acc: 0.7508494257926941  val: loss: 654018.4375 acc: 0.7020881175994873\n",
      "step: 45520\n",
      "train: loss: 139219.25 acc: 0.862678587436676  val: loss: 800809.3125 acc: 0.7089124917984009\n",
      "step: 45525\n",
      "train: loss: 20140.890625 acc: 0.9821425676345825  val: loss: 3556583.25 acc: 0.45430850982666016\n",
      "step: 45530\n",
      "train: loss: 74573.6875 acc: 0.925851047039032  val: loss: 4264410.5 acc: 0.47386056184768677\n",
      "step: 45535\n",
      "train: loss: 125013.15625 acc: 0.9226423501968384  val: loss: 2134614.25 acc: 0.561521053314209\n",
      "step: 45540\n",
      "train: loss: 393290.53125 acc: 0.7701400518417358  val: loss: 957716.25 acc: 0.6911468505859375\n",
      "step: 45545\n",
      "train: loss: 636828.125 acc: 0.7373926639556885  val: loss: 2846948.5 acc: 0.5993579030036926\n",
      "step: 45550\n",
      "train: loss: 103911.78125 acc: 0.8684718608856201  val: loss: 1732403.75 acc: 0.724928617477417\n",
      "step: 45555\n",
      "train: loss: 163205.203125 acc: 0.9055137634277344  val: loss: 2855834.5 acc: 0.5511856079101562\n",
      "step: 45560\n",
      "train: loss: 991072.875 acc: 0.7070133686065674  val: loss: 3932198.25 acc: 0.5401912927627563\n",
      "step: 45565\n",
      "train: loss: 1100333.25 acc: 0.8185262680053711  val: loss: 1128551.0 acc: 0.8335449695587158\n",
      "step: 45570\n",
      "train: loss: 691878.75 acc: 0.9248517751693726  val: loss: 705268.375 acc: 0.868868887424469\n",
      "step: 45575\n",
      "train: loss: 100852.109375 acc: 0.9883989095687866  val: loss: 433877.03125 acc: 0.7581673264503479\n",
      "step: 45580\n",
      "train: loss: 68588.9296875 acc: 0.9926306009292603  val: loss: 576445.0625 acc: 0.8301377296447754\n",
      "step: 45585\n",
      "train: loss: 85186.3984375 acc: 0.9808319211006165  val: loss: 693216.6875 acc: 0.8992944359779358\n",
      "step: 45590\n",
      "train: loss: 105973.3046875 acc: 0.9840502142906189  val: loss: 1711076.5 acc: -0.2581787109375\n",
      "step: 45595\n",
      "train: loss: 107581.4453125 acc: 0.9905033707618713  val: loss: 1571583.375 acc: 0.6261974573135376\n",
      "step: 45600\n",
      "train: loss: 117704.0625 acc: 0.9922124147415161  val: loss: 528198.3125 acc: 0.8546530604362488\n",
      "step: 45605\n",
      "train: loss: 98748.3203125 acc: 0.9873124957084656  val: loss: 906990.6875 acc: 0.8285114765167236\n",
      "step: 45610\n",
      "train: loss: 56152.96484375 acc: 0.9912173748016357  val: loss: 1112062.875 acc: 0.29837554693222046\n",
      "step: 45615\n",
      "train: loss: 49773.23828125 acc: 0.9892551898956299  val: loss: 982519.0 acc: 0.6928919553756714\n",
      "step: 45620\n",
      "train: loss: 62459.72265625 acc: 0.9828064441680908  val: loss: 1680145.375 acc: 0.7054061889648438\n",
      "step: 45625\n",
      "train: loss: 8899.3896484375 acc: 0.9933584928512573  val: loss: 1285438.5 acc: -0.4236854314804077\n",
      "step: 45630\n",
      "train: loss: 11182.396484375 acc: 0.993849515914917  val: loss: 489738.03125 acc: 0.7009280920028687\n",
      "step: 45635\n",
      "train: loss: 27949.943359375 acc: 0.9874117374420166  val: loss: 2687824.25 acc: 0.5421332120895386\n",
      "step: 45640\n",
      "train: loss: 13879.736328125 acc: 0.9410780072212219  val: loss: 404008.1875 acc: 0.8587331771850586\n",
      "step: 45645\n",
      "train: loss: 13916.7275390625 acc: 0.9619672894477844  val: loss: 1204664.75 acc: 0.7730613946914673\n",
      "step: 45650\n",
      "train: loss: 9852.1865234375 acc: 0.9723590016365051  val: loss: 783866.25 acc: 0.630598783493042\n",
      "step: 45655\n",
      "train: loss: 6437.18896484375 acc: 0.9832663536071777  val: loss: 1996447.0 acc: 0.7328754663467407\n",
      "step: 45660\n",
      "train: loss: 28538.265625 acc: 0.9546376466751099  val: loss: 916266.8125 acc: 0.6718153953552246\n",
      "step: 45665\n",
      "train: loss: 22834.50390625 acc: 0.9887969493865967  val: loss: 1686815.375 acc: 0.8632708787918091\n",
      "step: 45670\n",
      "train: loss: 33087.953125 acc: 0.9741817712783813  val: loss: 1671919.75 acc: 0.47825300693511963\n",
      "step: 45675\n",
      "train: loss: 94034.5546875 acc: 0.9607892036437988  val: loss: 2791544.25 acc: 0.7177191376686096\n",
      "step: 45680\n",
      "train: loss: 15368.8876953125 acc: 0.9917146563529968  val: loss: 1635355.625 acc: 0.7890860438346863\n",
      "step: 45685\n",
      "train: loss: 16134.8115234375 acc: 0.9865297079086304  val: loss: 3615635.5 acc: 0.3317519426345825\n",
      "step: 45690\n",
      "train: loss: 14601.31640625 acc: 0.9741682410240173  val: loss: 2858974.75 acc: 0.8556212186813354\n",
      "step: 45695\n",
      "train: loss: 13929.5810546875 acc: 0.9925596714019775  val: loss: 441879.5625 acc: 0.909468948841095\n",
      "step: 45700\n",
      "train: loss: 38549.90625 acc: 0.9838329553604126  val: loss: 1724381.25 acc: 0.7325893044471741\n",
      "step: 45705\n",
      "train: loss: 45155.359375 acc: 0.982742190361023  val: loss: 916261.0625 acc: 0.7547292113304138\n",
      "step: 45710\n",
      "train: loss: 61381.0703125 acc: 0.9784980416297913  val: loss: 2158559.75 acc: 0.46701985597610474\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 45715\n",
      "train: loss: 44167.2578125 acc: 0.9850561022758484  val: loss: 246552.265625 acc: 0.9258849024772644\n",
      "step: 45720\n",
      "train: loss: 338422.5625 acc: 0.8709282875061035  val: loss: 2330196.25 acc: 0.39978915452957153\n",
      "step: 45725\n",
      "train: loss: 86687.7421875 acc: 0.9483706951141357  val: loss: 1078772.875 acc: 0.8597265481948853\n",
      "step: 45730\n",
      "train: loss: 191899.609375 acc: 0.8958882689476013  val: loss: 593415.25 acc: 0.8607410192489624\n",
      "step: 45735\n",
      "train: loss: 125233.0703125 acc: 0.9764305353164673  val: loss: 1392410.125 acc: 0.8448160886764526\n",
      "step: 45740\n",
      "train: loss: 103779.671875 acc: 0.9883301854133606  val: loss: 733436.1875 acc: 0.4577487111091614\n",
      "step: 45745\n",
      "train: loss: 106647.078125 acc: 0.9856430888175964  val: loss: 851624.9375 acc: 0.7901999950408936\n",
      "step: 45750\n",
      "train: loss: 69382.0703125 acc: 0.9870710372924805  val: loss: 696062.1875 acc: 0.8631835579872131\n",
      "step: 45755\n",
      "train: loss: 623884.8125 acc: 0.8993170857429504  val: loss: 256165.4375 acc: 0.9321076273918152\n",
      "step: 45760\n",
      "train: loss: 237527.1875 acc: 0.9543494582176208  val: loss: 466274.40625 acc: 0.887274444103241\n",
      "step: 45765\n",
      "train: loss: 3094251.75 acc: 0.7730629444122314  val: loss: 223885.921875 acc: 0.937683641910553\n",
      "step: 45770\n",
      "train: loss: 268712.03125 acc: 0.9522609114646912  val: loss: 3471185.75 acc: -0.3526087999343872\n",
      "step: 45775\n",
      "train: loss: 281859.875 acc: 0.8715239763259888  val: loss: 752290.4375 acc: 0.8108081817626953\n",
      "step: 45780\n",
      "train: loss: 597210.3125 acc: 0.9682965874671936  val: loss: 1044312.375 acc: 0.33066099882125854\n",
      "step: 45785\n",
      "train: loss: 2087620.5 acc: 0.9420978426933289  val: loss: 461691.28125 acc: 0.8728607892990112\n",
      "step: 45790\n",
      "train: loss: 1207515.25 acc: 0.9571068286895752  val: loss: 2233316.75 acc: -0.16584384441375732\n",
      "step: 45795\n",
      "train: loss: 752026.625 acc: 0.9622527360916138  val: loss: 1706653.25 acc: 0.1420818567276001\n",
      "step: 45800\n",
      "train: loss: 738787.375 acc: 0.953508198261261  val: loss: 1330388.5 acc: 0.6823221445083618\n",
      "step: 45805\n",
      "train: loss: 496288.5625 acc: 0.9476042985916138  val: loss: 322652.5625 acc: 0.9523699879646301\n",
      "step: 45810\n",
      "train: loss: 471862.34375 acc: 0.9465194344520569  val: loss: 230477.1875 acc: 0.9555220603942871\n",
      "step: 45815\n",
      "train: loss: 974428.125 acc: 0.8266321420669556  val: loss: 286240.625 acc: 0.899421215057373\n",
      "step: 45820\n",
      "train: loss: 1086513.25 acc: 0.44809210300445557  val: loss: 880285.4375 acc: 0.7920883297920227\n",
      "step: 45825\n",
      "train: loss: 685865.1875 acc: 0.5963906049728394  val: loss: 2067442.5 acc: 0.6357938647270203\n",
      "step: 45830\n",
      "train: loss: 674939.4375 acc: 0.7313134670257568  val: loss: 1252760.375 acc: 0.7908449769020081\n",
      "step: 45835\n",
      "train: loss: 753612.3125 acc: 0.8303996324539185  val: loss: 535107.8125 acc: 0.8056472539901733\n",
      "step: 45840\n",
      "train: loss: 798483.1875 acc: 0.6261081695556641  val: loss: 3238573.75 acc: 0.6193320751190186\n",
      "step: 45845\n",
      "train: loss: 228029.25 acc: 0.8915310502052307  val: loss: 2164952.25 acc: 0.5898548364639282\n",
      "step: 45850\n",
      "train: loss: 485796.03125 acc: 0.7777038812637329  val: loss: 4750014.0 acc: 0.4390249252319336\n",
      "step: 45855\n",
      "train: loss: 135822.9375 acc: 0.8971525430679321  val: loss: 4336421.5 acc: 0.4652738571166992\n",
      "step: 45860\n",
      "train: loss: 82061.765625 acc: 0.9289025068283081  val: loss: 5268482.5 acc: 0.41276514530181885\n",
      "step: 45865\n",
      "train: loss: 173312.9375 acc: 0.9062705039978027  val: loss: 10275944.0 acc: 0.29171228408813477\n",
      "step: 45870\n",
      "train: loss: 183720.625 acc: 0.8998256921768188  val: loss: 3225091.25 acc: 0.5060250759124756\n",
      "step: 45875\n",
      "train: loss: 125067.890625 acc: 0.9184777140617371  val: loss: 3510623.25 acc: 0.4721144437789917\n",
      "step: 45880\n",
      "train: loss: 246158.0625 acc: 0.8650439381599426  val: loss: 4781432.0 acc: 0.36800533533096313\n",
      "step: 45885\n",
      "train: loss: 113993.296875 acc: 0.9161657094955444  val: loss: 4337679.5 acc: 0.4213968515396118\n",
      "step: 45890\n",
      "train: loss: 174498.734375 acc: 0.8071969151496887  val: loss: 1101707.875 acc: 0.6656303405761719\n",
      "step: 45895\n",
      "train: loss: 40480.63671875 acc: 0.9570552110671997  val: loss: 6300577.0 acc: 0.2749291658401489\n",
      "step: 45900\n",
      "train: loss: 166329.234375 acc: 0.8964378833770752  val: loss: 6905234.0 acc: 0.37814241647720337\n",
      "step: 45905\n",
      "train: loss: 273115.03125 acc: 0.8689583539962769  val: loss: 3147565.5 acc: 0.5475658774375916\n",
      "step: 45910\n",
      "train: loss: 272543.90625 acc: 0.8176407217979431  val: loss: 4072438.75 acc: 0.36110997200012207\n",
      "step: 45915\n",
      "train: loss: 181718.25 acc: 0.8692454099655151  val: loss: 1963981.375 acc: 0.5266937017440796\n",
      "step: 45920\n",
      "train: loss: 15751.005859375 acc: 0.9793270230293274  val: loss: 1213534.625 acc: 0.6346951723098755\n",
      "step: 45925\n",
      "train: loss: 1007377.5 acc: 0.7518700957298279  val: loss: 1991779.5 acc: 0.5816894769668579\n",
      "step: 45930\n",
      "train: loss: 736196.25 acc: 0.8980783224105835  val: loss: 3062682.25 acc: 0.7824423909187317\n",
      "step: 45935\n",
      "train: loss: 462751.96875 acc: 0.9523772597312927  val: loss: 1189882.625 acc: 0.7700439691543579\n",
      "step: 45940\n",
      "train: loss: 253273.640625 acc: 0.9775983095169067  val: loss: 2716480.75 acc: -0.8367652893066406\n",
      "step: 45945\n",
      "train: loss: 95616.8671875 acc: 0.9897064566612244  val: loss: 540099.375 acc: 0.3423755168914795\n",
      "step: 45950\n",
      "train: loss: 46016.98046875 acc: 0.9882782101631165  val: loss: 1033616.625 acc: 0.7327162027359009\n",
      "step: 45955\n",
      "train: loss: 67795.875 acc: 0.990014374256134  val: loss: 1354867.125 acc: 0.10327392816543579\n",
      "step: 45960\n",
      "train: loss: 49413.390625 acc: 0.9959529042243958  val: loss: 735302.25 acc: 0.24776440858840942\n",
      "step: 45965\n",
      "train: loss: 103055.671875 acc: 0.9926785230636597  val: loss: 1253934.25 acc: 0.12606847286224365\n",
      "step: 45970\n",
      "train: loss: 77519.2890625 acc: 0.9915725588798523  val: loss: 1308959.625 acc: 0.8189557194709778\n",
      "step: 45975\n",
      "train: loss: 70754.1015625 acc: 0.9872759580612183  val: loss: 2937332.5 acc: 0.6793012022972107\n",
      "step: 45980\n",
      "train: loss: 54701.0859375 acc: 0.9893974661827087  val: loss: 1446917.5 acc: 0.6942604780197144\n",
      "step: 45985\n",
      "train: loss: 46944.33984375 acc: 0.985316812992096  val: loss: 1357585.75 acc: 0.6117106080055237\n",
      "step: 45990\n",
      "train: loss: 12527.443359375 acc: 0.9320728778839111  val: loss: 2108406.75 acc: 0.5360758304595947\n",
      "step: 45995\n",
      "train: loss: 22113.21875 acc: 0.9627682566642761  val: loss: 1933256.125 acc: 0.7973690629005432\n",
      "step: 46000\n",
      "train: loss: 17247.03515625 acc: 0.972091794013977  val: loss: 497235.375 acc: 0.8266206383705139\n",
      "step: 46005\n",
      "train: loss: 24683.447265625 acc: 0.9716439843177795  val: loss: 90915.90625 acc: 0.9846369624137878\n",
      "step: 46010\n",
      "train: loss: 13598.3701171875 acc: 0.9892950057983398  val: loss: 1926166.75 acc: 0.8611553311347961\n",
      "step: 46015\n",
      "train: loss: 23819.6640625 acc: 0.8872470855712891  val: loss: 793200.375 acc: 0.8390161991119385\n",
      "step: 46020\n",
      "train: loss: 14932.16015625 acc: 0.9700818657875061  val: loss: 1447722.25 acc: 0.7175414562225342\n",
      "step: 46025\n",
      "train: loss: 27548.56640625 acc: 0.9491865634918213  val: loss: 219349.578125 acc: 0.9529317617416382\n",
      "step: 46030\n",
      "train: loss: 47159.7734375 acc: 0.9854308366775513  val: loss: 619347.4375 acc: 0.8125826120376587\n",
      "step: 46035\n",
      "train: loss: 29677.73828125 acc: 0.9789212942123413  val: loss: 1808240.875 acc: 0.6604245901107788\n",
      "step: 46040\n",
      "train: loss: 19793.623046875 acc: 0.9842140674591064  val: loss: 1331193.875 acc: 0.7061231732368469\n",
      "step: 46045\n",
      "train: loss: 14755.400390625 acc: 0.9882931113243103  val: loss: 1068543.625 acc: 0.4729771018028259\n",
      "step: 46050\n",
      "train: loss: 20605.255859375 acc: 0.9868113398551941  val: loss: 1066889.5 acc: 0.7117882966995239\n",
      "step: 46055\n",
      "train: loss: 12741.416015625 acc: 0.989895761013031  val: loss: 1403480.375 acc: 0.6350024938583374\n",
      "step: 46060\n",
      "train: loss: 20112.845703125 acc: 0.9881483316421509  val: loss: 1242299.125 acc: 0.5013272166252136\n",
      "step: 46065\n",
      "train: loss: 43430.0078125 acc: 0.9852281808853149  val: loss: 1993279.75 acc: 0.027533352375030518\n",
      "step: 46070\n",
      "train: loss: 30859.9609375 acc: 0.9927639365196228  val: loss: 2667565.0 acc: 0.6169118881225586\n",
      "step: 46075\n",
      "train: loss: 51921.234375 acc: 0.9803471565246582  val: loss: 745242.8125 acc: 0.9003516435623169\n",
      "step: 46080\n",
      "train: loss: 31269.44921875 acc: 0.9894598722457886  val: loss: 126652.2578125 acc: 0.9650341272354126\n",
      "step: 46085\n",
      "train: loss: 40030.70703125 acc: 0.9877503514289856  val: loss: 254762.71875 acc: 0.7230123281478882\n",
      "step: 46090\n",
      "train: loss: 245262.140625 acc: 0.9345767498016357  val: loss: 1147266.75 acc: 0.7061742544174194\n",
      "step: 46095\n",
      "train: loss: 137179.0625 acc: 0.9647831916809082  val: loss: 894318.8125 acc: 0.6325641870498657\n",
      "step: 46100\n",
      "train: loss: 110443.28125 acc: 0.9713593125343323  val: loss: 594154.9375 acc: 0.8199185729026794\n",
      "step: 46105\n",
      "train: loss: 566201.3125 acc: 0.9072527289390564  val: loss: 350053.46875 acc: 0.9099550843238831\n",
      "step: 46110\n",
      "train: loss: 356609.53125 acc: 0.9629243612289429  val: loss: 1307564.0 acc: 0.7226714491844177\n",
      "step: 46115\n",
      "train: loss: 38617.59375 acc: 0.9940999746322632  val: loss: 1205206.625 acc: 0.19196516275405884\n",
      "step: 46120\n",
      "train: loss: 124146.34375 acc: 0.9868766665458679  val: loss: 193162.5625 acc: 0.9358963370323181\n",
      "step: 46125\n",
      "train: loss: 605511.0625 acc: 0.9303675293922424  val: loss: 350298.90625 acc: 0.9220341444015503\n",
      "step: 46130\n",
      "train: loss: 413862.28125 acc: 0.9717187285423279  val: loss: 223857.796875 acc: 0.96604323387146\n",
      "step: 46135\n",
      "train: loss: 509094.9375 acc: 0.9516876935958862  val: loss: 586270.875 acc: 0.7714141011238098\n",
      "step: 46140\n",
      "train: loss: 175054.046875 acc: 0.9284327030181885  val: loss: 425491.875 acc: 0.9170584678649902\n",
      "step: 46145\n",
      "train: loss: 357909.875 acc: 0.9833424687385559  val: loss: 1366831.5 acc: 0.6540705561637878\n",
      "step: 46150\n",
      "train: loss: 1230511.375 acc: 0.9493255019187927  val: loss: 1617440.75 acc: 0.7377429008483887\n",
      "step: 46155\n",
      "train: loss: 4151181.25 acc: 0.8729559779167175  val: loss: 478956.625 acc: 0.9268859624862671\n",
      "step: 46160\n",
      "train: loss: 744152.9375 acc: 0.9357448816299438  val: loss: 531604.1875 acc: 0.953140139579773\n",
      "step: 46165\n",
      "train: loss: 896110.4375 acc: 0.8825216889381409  val: loss: 849930.4375 acc: 0.8998116850852966\n",
      "step: 46170\n",
      "train: loss: 981594.6875 acc: 0.9136387705802917  val: loss: 395699.5625 acc: 0.9383012652397156\n",
      "step: 46175\n",
      "train: loss: 260217.5 acc: 0.9771329760551453  val: loss: 513396.34375 acc: 0.8024076819419861\n",
      "step: 46180\n",
      "train: loss: 437958.75 acc: 0.9316206574440002  val: loss: 419995.40625 acc: 0.9106888175010681\n",
      "step: 46185\n",
      "train: loss: 2368994.25 acc: -0.2632709741592407  val: loss: 2038706.625 acc: 0.7684276700019836\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 46190\n",
      "train: loss: 831650.0625 acc: 0.5648876428604126  val: loss: 2238053.75 acc: 0.7123861312866211\n",
      "step: 46195\n",
      "train: loss: 516273.78125 acc: 0.7742776274681091  val: loss: 2297569.75 acc: 0.6788678169250488\n",
      "step: 46200\n",
      "train: loss: 740765.3125 acc: 0.8293600678443909  val: loss: 4539397.0 acc: 0.6380252838134766\n",
      "step: 46205\n",
      "train: loss: 644179.0 acc: 0.5817512273788452  val: loss: 3238782.5 acc: 0.6246459484100342\n",
      "step: 46210\n",
      "train: loss: 184432.734375 acc: 0.8939451575279236  val: loss: 9460145.0 acc: 0.27976465225219727\n",
      "step: 46215\n",
      "train: loss: 143839.40625 acc: 0.8938855528831482  val: loss: 8183568.0 acc: 0.31268829107284546\n",
      "step: 46220\n",
      "train: loss: 111538.6484375 acc: 0.9163907766342163  val: loss: 539229.0 acc: 0.6873887777328491\n",
      "step: 46225\n",
      "train: loss: 188924.046875 acc: 0.856201708316803  val: loss: 1325866.5 acc: 0.6641286611557007\n",
      "step: 46230\n",
      "train: loss: 59315.0859375 acc: 0.9569800496101379  val: loss: 4926860.5 acc: 0.40600353479385376\n",
      "step: 46235\n",
      "train: loss: 50642.90234375 acc: 0.9628846049308777  val: loss: 284753.125 acc: 0.8439528942108154\n",
      "step: 46240\n",
      "train: loss: 11362.1015625 acc: 0.9902210235595703  val: loss: 5474475.5 acc: 0.34940505027770996\n",
      "step: 46245\n",
      "train: loss: 109224.6796875 acc: 0.9169762134552002  val: loss: 491114.15625 acc: 0.7686818242073059\n",
      "step: 46250\n",
      "train: loss: 93767.921875 acc: 0.9292697310447693  val: loss: 1999683.0 acc: 0.39841318130493164\n",
      "step: 46255\n",
      "train: loss: 47301.2890625 acc: 0.9468904137611389  val: loss: 318092.875 acc: 0.8460524082183838\n",
      "step: 46260\n",
      "train: loss: 329542.53125 acc: 0.6949703693389893  val: loss: 601926.75 acc: 0.7301664352416992\n",
      "step: 46265\n",
      "train: loss: 155750.109375 acc: 0.8907113075256348  val: loss: 4704606.0 acc: 0.2902739644050598\n",
      "step: 46270\n",
      "train: loss: 162933.46875 acc: 0.8930679559707642  val: loss: 4549776.0 acc: 0.3619653582572937\n",
      "step: 46275\n",
      "train: loss: 745653.0625 acc: 0.7053461670875549  val: loss: 862686.4375 acc: 0.7504374384880066\n",
      "step: 46280\n",
      "train: loss: 349460.8125 acc: 0.8174041509628296  val: loss: 58485.45703125 acc: 0.928367555141449\n",
      "step: 46285\n",
      "train: loss: 75341.125 acc: 0.937004804611206  val: loss: 1442145.125 acc: 0.6604212522506714\n",
      "step: 46290\n",
      "train: loss: 1231302.0 acc: 0.707431435585022  val: loss: 1149145.5 acc: 0.7357887029647827\n",
      "step: 46295\n",
      "train: loss: 804566.875 acc: 0.8346469402313232  val: loss: 594565.9375 acc: 0.8300154209136963\n",
      "step: 46300\n",
      "train: loss: 1016159.1875 acc: 0.8769028186798096  val: loss: 830160.75 acc: 0.7366907596588135\n",
      "step: 46305\n",
      "train: loss: 162311.765625 acc: 0.988762378692627  val: loss: 886646.75 acc: 0.6507300138473511\n",
      "step: 46310\n",
      "train: loss: 293426.625 acc: 0.9652776718139648  val: loss: 897029.3125 acc: 0.8192248940467834\n",
      "step: 46315\n",
      "train: loss: 533861.375 acc: 0.8842429518699646  val: loss: 1520935.875 acc: 0.5415205955505371\n",
      "step: 46320\n",
      "train: loss: 49617.0390625 acc: 0.991014301776886  val: loss: 1826005.5 acc: 0.7400949001312256\n",
      "step: 46325\n",
      "train: loss: 120097.4765625 acc: 0.9860575795173645  val: loss: 289593.84375 acc: 0.9545398950576782\n",
      "step: 46330\n",
      "train: loss: 173078.515625 acc: 0.9862784743309021  val: loss: 1533605.0 acc: 0.2823293209075928\n",
      "step: 46335\n",
      "train: loss: 316897.78125 acc: 0.9763014912605286  val: loss: 763527.9375 acc: 0.7992779612541199\n",
      "step: 46340\n",
      "train: loss: 52398.6484375 acc: 0.9866393804550171  val: loss: 1836836.0 acc: 0.6578801274299622\n",
      "step: 46345\n",
      "train: loss: 62497.21484375 acc: 0.9906312823295593  val: loss: 1303358.25 acc: 0.46870750188827515\n",
      "step: 46350\n",
      "train: loss: 61638.015625 acc: 0.9868625402450562  val: loss: 811881.9375 acc: 0.8777337670326233\n",
      "step: 46355\n",
      "train: loss: 18363.578125 acc: 0.9908091425895691  val: loss: 973238.125 acc: 0.776231050491333\n",
      "step: 46360\n",
      "train: loss: 15449.6943359375 acc: 0.9724144339561462  val: loss: 1760910.625 acc: 0.6278563737869263\n",
      "step: 46365\n",
      "train: loss: 13533.0390625 acc: 0.9846066832542419  val: loss: 1007705.1875 acc: 0.18472862243652344\n",
      "step: 46370\n",
      "train: loss: 10606.5625 acc: 0.97580486536026  val: loss: 312921.75 acc: 0.8706940412521362\n",
      "step: 46375\n",
      "train: loss: 9427.2841796875 acc: 0.9964600205421448  val: loss: 1124895.375 acc: 0.9088516235351562\n",
      "step: 46380\n",
      "train: loss: 8596.771484375 acc: 0.9839495420455933  val: loss: 87941.5546875 acc: 0.9694063663482666\n",
      "step: 46385\n",
      "train: loss: 19481.345703125 acc: 0.9549156427383423  val: loss: 1320908.125 acc: 0.3432074189186096\n",
      "step: 46390\n",
      "train: loss: 19245.1171875 acc: 0.971496045589447  val: loss: 1404776.5 acc: 0.7875159382820129\n",
      "step: 46395\n",
      "train: loss: 86409.3203125 acc: 0.9690651893615723  val: loss: 369989.96875 acc: 0.9050797820091248\n",
      "step: 46400\n",
      "train: loss: 19000.607421875 acc: 0.9861716628074646  val: loss: 40919.9140625 acc: 0.9855669140815735\n",
      "step: 46405\n",
      "train: loss: 24129.892578125 acc: 0.9824203848838806  val: loss: 1585712.25 acc: 0.24490129947662354\n",
      "step: 46410\n",
      "train: loss: 39095.6640625 acc: 0.9897052645683289  val: loss: 340924.875 acc: 0.8938652873039246\n",
      "step: 46415\n",
      "train: loss: 39264.17578125 acc: 0.9714516401290894  val: loss: 1276027.875 acc: 0.7731710076332092\n",
      "step: 46420\n",
      "train: loss: 11930.5146484375 acc: 0.9863242506980896  val: loss: 468498.4375 acc: 0.9026633501052856\n",
      "step: 46425\n",
      "train: loss: 17183.544921875 acc: 0.9672936797142029  val: loss: 327442.59375 acc: 0.8333237171173096\n",
      "step: 46430\n",
      "train: loss: 17680.09375 acc: 0.9941441416740417  val: loss: 932894.6875 acc: 0.42946863174438477\n",
      "step: 46435\n",
      "train: loss: 49127.140625 acc: 0.9835216999053955  val: loss: 118746.4765625 acc: 0.972179114818573\n",
      "step: 46440\n",
      "train: loss: 18706.5703125 acc: 0.9941020607948303  val: loss: 790283.125 acc: 0.8046393394470215\n",
      "step: 46445\n",
      "train: loss: 44364.73046875 acc: 0.9762360453605652  val: loss: 1150450.375 acc: 0.8090782165527344\n",
      "step: 46450\n",
      "train: loss: 41068.57421875 acc: 0.9860435724258423  val: loss: 1442934.625 acc: 0.3593288064002991\n",
      "step: 46455\n",
      "train: loss: 136169.53125 acc: 0.9687530398368835  val: loss: 1994117.375 acc: 0.36605244874954224\n",
      "step: 46460\n",
      "train: loss: 51538.87109375 acc: 0.9838396906852722  val: loss: 661715.1875 acc: 0.9087995290756226\n",
      "step: 46465\n",
      "train: loss: 361719.46875 acc: 0.8193573951721191  val: loss: 486397.0 acc: 0.9350607395172119\n",
      "step: 46470\n",
      "train: loss: 91410.203125 acc: 0.9848470091819763  val: loss: 971099.875 acc: 0.8607673048973083\n",
      "step: 46475\n",
      "train: loss: 62159.87890625 acc: 0.9943896532058716  val: loss: 297924.6875 acc: 0.9674787521362305\n",
      "step: 46480\n",
      "train: loss: 48908.671875 acc: 0.9929900169372559  val: loss: 400888.25 acc: 0.9508854150772095\n",
      "step: 46485\n",
      "train: loss: 197830.59375 acc: 0.979745626449585  val: loss: 338054.1875 acc: 0.947513222694397\n",
      "step: 46490\n",
      "train: loss: 492109.21875 acc: 0.9327921867370605  val: loss: 800307.375 acc: 0.8918095827102661\n",
      "step: 46495\n",
      "train: loss: 998008.8125 acc: 0.9543492794036865  val: loss: 199947.171875 acc: 0.9578092098236084\n",
      "step: 46500\n",
      "train: loss: 435321.5 acc: 0.9780268669128418  val: loss: 756943.3125 acc: 0.9003247022628784\n",
      "step: 46505\n",
      "train: loss: 307380.28125 acc: 0.9660090208053589  val: loss: 1993173.875 acc: 0.7490381002426147\n",
      "step: 46510\n",
      "train: loss: 672871.0 acc: 0.9580698013305664  val: loss: 801379.3125 acc: 0.7242391109466553\n",
      "step: 46515\n",
      "train: loss: 473948.125 acc: 0.9852351546287537  val: loss: 420297.625 acc: 0.9199733138084412\n",
      "step: 46520\n",
      "train: loss: 1479696.125 acc: 0.9557562470436096  val: loss: 1178514.5 acc: 0.9090000987052917\n",
      "step: 46525\n",
      "train: loss: 1668887.5 acc: 0.9144355058670044  val: loss: 803082.0625 acc: 0.7634961605072021\n",
      "step: 46530\n",
      "train: loss: 716389.375 acc: 0.9476790428161621  val: loss: 1236492.875 acc: 0.7867352962493896\n",
      "step: 46535\n",
      "train: loss: 123481.53125 acc: 0.9850811958312988  val: loss: 465242.34375 acc: 0.6585650444030762\n",
      "step: 46540\n",
      "train: loss: 286177.5625 acc: 0.9650874733924866  val: loss: 428244.6875 acc: 0.9343008399009705\n",
      "step: 46545\n",
      "train: loss: 425951.8125 acc: 0.9213912487030029  val: loss: 1213096.625 acc: 0.7378609776496887\n",
      "step: 46550\n",
      "train: loss: 873360.75 acc: 0.586093544960022  val: loss: 1856210.75 acc: 0.8055353164672852\n",
      "step: 46555\n",
      "train: loss: 633464.5625 acc: 0.6236649751663208  val: loss: 713204.5625 acc: 0.8438454866409302\n",
      "step: 46560\n",
      "train: loss: 786638.875 acc: 0.752859890460968  val: loss: 1698628.25 acc: 0.7600706815719604\n",
      "step: 46565\n",
      "train: loss: 450959.6875 acc: 0.6439920663833618  val: loss: 473703.71875 acc: 0.8316094279289246\n",
      "step: 46570\n",
      "train: loss: 1135918.625 acc: 0.8019373416900635  val: loss: 293722.90625 acc: 0.9044012427330017\n",
      "step: 46575\n",
      "train: loss: 1063906.0 acc: 0.6559987664222717  val: loss: 950355.625 acc: 0.6457257866859436\n",
      "step: 46580\n",
      "train: loss: 332536.84375 acc: 0.8116888999938965  val: loss: 2472664.0 acc: 0.39249688386917114\n",
      "step: 46585\n",
      "train: loss: 213788.40625 acc: 0.824851393699646  val: loss: 2509939.0 acc: 0.40636754035949707\n",
      "step: 46590\n",
      "train: loss: 129581.765625 acc: 0.8878347873687744  val: loss: 6344841.5 acc: 0.30615198612213135\n",
      "step: 46595\n",
      "train: loss: 75666.671875 acc: 0.9368258118629456  val: loss: 2709465.25 acc: 0.2849372625350952\n",
      "step: 46600\n",
      "train: loss: 68983.453125 acc: 0.9501497149467468  val: loss: 8303451.5 acc: 0.2361447811126709\n",
      "step: 46605\n",
      "train: loss: 158501.75 acc: 0.9008587598800659  val: loss: 1389699.75 acc: 0.6441006064414978\n",
      "step: 46610\n",
      "train: loss: 651286.3125 acc: 0.7401915788650513  val: loss: 2647171.5 acc: 0.5023326873779297\n",
      "step: 46615\n",
      "train: loss: 106910.859375 acc: 0.9197086095809937  val: loss: 4104269.75 acc: 0.29167455434799194\n",
      "step: 46620\n",
      "train: loss: 296116.25 acc: 0.8147050142288208  val: loss: 1788385.0 acc: 0.5661565065383911\n",
      "step: 46625\n",
      "train: loss: 31567.23046875 acc: 0.9592834115028381  val: loss: 1838592.75 acc: 0.6031924486160278\n",
      "step: 46630\n",
      "train: loss: 74383.0 acc: 0.9468790292739868  val: loss: 373941.8125 acc: 0.8033288717269897\n",
      "step: 46635\n",
      "train: loss: 463907.90625 acc: 0.7460547685623169  val: loss: 3006797.25 acc: 0.4599423408508301\n",
      "step: 46640\n",
      "train: loss: 266397.84375 acc: 0.8446425199508667  val: loss: 5402742.5 acc: 0.3834006190299988\n",
      "step: 46645\n",
      "train: loss: 103081.8671875 acc: 0.9021265506744385  val: loss: 7014765.5 acc: 0.31958436965942383\n",
      "step: 46650\n",
      "train: loss: 436613.96875 acc: 0.7377843856811523  val: loss: 2030251.5 acc: 0.6493748426437378\n",
      "step: 46655\n",
      "train: loss: 177557.390625 acc: 0.8664737343788147  val: loss: 1217617.5 acc: 0.6923994421958923\n",
      "step: 46660\n",
      "train: loss: 1299041.375 acc: 0.8696626424789429  val: loss: 530847.0 acc: 0.8364931344985962\n",
      "step: 46665\n",
      "train: loss: 382571.5 acc: 0.9486921429634094  val: loss: 694727.3125 acc: 0.8121246099472046\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 46670\n",
      "train: loss: 176711.734375 acc: 0.9752570390701294  val: loss: 1489978.0 acc: 0.48872309923171997\n",
      "step: 46675\n",
      "train: loss: 554602.3125 acc: 0.9459284543991089  val: loss: 1909912.5 acc: 0.2079821228981018\n",
      "step: 46680\n",
      "train: loss: 339575.34375 acc: 0.9517034888267517  val: loss: 428454.71875 acc: 0.9191425442695618\n",
      "step: 46685\n",
      "train: loss: 98790.0234375 acc: 0.9834846258163452  val: loss: 1161470.375 acc: 0.7879180312156677\n",
      "step: 46690\n",
      "train: loss: 170955.25 acc: 0.9815075397491455  val: loss: 860560.0 acc: 0.9157647490501404\n",
      "step: 46695\n",
      "train: loss: 66099.546875 acc: 0.9954254627227783  val: loss: 1526273.75 acc: 0.680983304977417\n",
      "step: 46700\n",
      "train: loss: 107390.1328125 acc: 0.990018367767334  val: loss: 1841213.75 acc: 0.36382973194122314\n",
      "step: 46705\n",
      "train: loss: 102425.1640625 acc: 0.9827224016189575  val: loss: 577665.125 acc: 0.7941991090774536\n",
      "step: 46710\n",
      "train: loss: 108357.1484375 acc: 0.9851574897766113  val: loss: 478486.90625 acc: 0.8601844906806946\n",
      "step: 46715\n",
      "train: loss: 42370.046875 acc: 0.9868543744087219  val: loss: 1088164.25 acc: 0.7135645151138306\n",
      "step: 46720\n",
      "train: loss: 22088.9296875 acc: 0.9815067052841187  val: loss: 398865.8125 acc: 0.9046233296394348\n",
      "step: 46725\n",
      "train: loss: 19534.75390625 acc: 0.9197453260421753  val: loss: 257944.25 acc: 0.9302841424942017\n",
      "step: 46730\n",
      "train: loss: 13463.935546875 acc: 0.9818652868270874  val: loss: 601142.75 acc: 0.9022652506828308\n",
      "step: 46735\n",
      "train: loss: 18267.646484375 acc: 0.9682773947715759  val: loss: 598416.6875 acc: 0.4329550862312317\n",
      "step: 46740\n",
      "train: loss: 8029.3515625 acc: 0.9609593152999878  val: loss: 1315423.625 acc: 0.6443767547607422\n",
      "step: 46745\n",
      "train: loss: 12257.2421875 acc: 0.9767928123474121  val: loss: 229600.15625 acc: 0.9057887196540833\n",
      "step: 46750\n",
      "train: loss: 16062.7275390625 acc: 0.9761266112327576  val: loss: 719685.6875 acc: 0.8785763382911682\n",
      "step: 46755\n",
      "train: loss: 7695.22900390625 acc: 0.9876767992973328  val: loss: 714296.5625 acc: 0.6943986415863037\n",
      "step: 46760\n",
      "train: loss: 9348.91796875 acc: 0.9613510966300964  val: loss: 403814.125 acc: 0.8285529613494873\n",
      "step: 46765\n",
      "train: loss: 19111.458984375 acc: 0.9816678762435913  val: loss: 413046.6875 acc: 0.9510838985443115\n",
      "step: 46770\n",
      "train: loss: 39340.26171875 acc: 0.9741201996803284  val: loss: 1020573.1875 acc: 0.7465993165969849\n",
      "step: 46775\n",
      "train: loss: 17469.091796875 acc: 0.9900930523872375  val: loss: 1019106.6875 acc: 0.8196710348129272\n",
      "step: 46780\n",
      "train: loss: 31329.158203125 acc: 0.9888300895690918  val: loss: 800034.75 acc: 0.909065306186676\n",
      "step: 46785\n",
      "train: loss: 16700.501953125 acc: 0.9818941354751587  val: loss: 194762.421875 acc: 0.9663036465644836\n",
      "step: 46790\n",
      "train: loss: 8264.4296875 acc: 0.9882912039756775  val: loss: 514978.28125 acc: 0.9516857862472534\n",
      "step: 46795\n",
      "train: loss: 23821.984375 acc: 0.9922232627868652  val: loss: 1476077.375 acc: 0.8406058549880981\n",
      "step: 46800\n",
      "train: loss: 42168.140625 acc: 0.9911651015281677  val: loss: 837104.3125 acc: 0.7864352464675903\n",
      "step: 46805\n",
      "train: loss: 30623.5859375 acc: 0.9893567562103271  val: loss: 1193020.75 acc: 0.8194375038146973\n",
      "step: 46810\n",
      "train: loss: 36249.2578125 acc: 0.9883087277412415  val: loss: 1404528.125 acc: 0.8729631900787354\n",
      "step: 46815\n",
      "train: loss: 28046.126953125 acc: 0.9910566806793213  val: loss: 215177.828125 acc: 0.963146448135376\n",
      "step: 46820\n",
      "train: loss: 242801.203125 acc: 0.9491283297538757  val: loss: 789222.875 acc: 0.8192441463470459\n",
      "step: 46825\n",
      "train: loss: 95385.171875 acc: 0.9490363597869873  val: loss: 834437.6875 acc: 0.8889752626419067\n",
      "step: 46830\n",
      "train: loss: 58354.24609375 acc: 0.9771549701690674  val: loss: 1496337.75 acc: 0.6331852674484253\n",
      "step: 46835\n",
      "train: loss: 892948.1875 acc: 0.8114567995071411  val: loss: 2035351.875 acc: 0.8018528819084167\n",
      "step: 46840\n",
      "train: loss: 501594.125 acc: 0.907400369644165  val: loss: 875567.8125 acc: 0.9416003227233887\n",
      "step: 46845\n",
      "train: loss: 40118.0703125 acc: 0.996347188949585  val: loss: 801566.8125 acc: 0.7867569327354431\n",
      "step: 46850\n",
      "train: loss: 28297.216796875 acc: 0.9965305328369141  val: loss: 1972522.625 acc: 0.6821783781051636\n",
      "step: 46855\n",
      "train: loss: 294330.5625 acc: 0.954765260219574  val: loss: 1870843.5 acc: 0.5536259412765503\n",
      "step: 46860\n",
      "train: loss: 674287.4375 acc: 0.9561684131622314  val: loss: 1886541.25 acc: 0.11102074384689331\n",
      "step: 46865\n",
      "train: loss: 565734.1875 acc: 0.9634836912155151  val: loss: 1031257.125 acc: 0.6343691349029541\n",
      "step: 46870\n",
      "train: loss: 2110953.75 acc: 0.8054280877113342  val: loss: 844459.0625 acc: 0.7951880097389221\n",
      "step: 46875\n",
      "train: loss: 799806.8125 acc: 0.9473158717155457  val: loss: 849791.0625 acc: 0.8280110359191895\n",
      "step: 46880\n",
      "train: loss: 854988.0625 acc: 0.9793166518211365  val: loss: 1144804.375 acc: 0.34543710947036743\n",
      "step: 46885\n",
      "train: loss: 1457163.5 acc: 0.9555639028549194  val: loss: 340648.71875 acc: 0.8894529342651367\n",
      "step: 46890\n",
      "train: loss: 921276.5625 acc: 0.9503936767578125  val: loss: 532795.0625 acc: 0.9135856032371521\n",
      "step: 46895\n",
      "train: loss: 1003695.875 acc: 0.9506134986877441  val: loss: 1929646.5 acc: -0.32562077045440674\n",
      "step: 46900\n",
      "train: loss: 392170.09375 acc: 0.9540910124778748  val: loss: 687277.5625 acc: 0.9327869415283203\n",
      "step: 46905\n",
      "train: loss: 277008.78125 acc: 0.9312462210655212  val: loss: 727907.0625 acc: 0.7991381883621216\n",
      "step: 46910\n",
      "train: loss: 567725.375 acc: 0.8755910396575928  val: loss: 2673612.75 acc: -0.4565465450286865\n",
      "step: 46915\n",
      "train: loss: 2325187.75 acc: 0.24479824304580688  val: loss: 553557.625 acc: 0.5103010535240173\n",
      "step: 46920\n",
      "train: loss: 419354.625 acc: 0.8077273964881897  val: loss: 1085115.0 acc: 0.7584472298622131\n",
      "step: 46925\n",
      "train: loss: 1659362.5 acc: 0.736005425453186  val: loss: 1721004.125 acc: 0.7122234106063843\n",
      "step: 46930\n",
      "train: loss: 169501.515625 acc: 0.9065285921096802  val: loss: 441566.3125 acc: 0.8751562237739563\n",
      "step: 46935\n",
      "train: loss: 898276.0625 acc: 0.6597420573234558  val: loss: 664103.25 acc: 0.7750644683837891\n",
      "step: 46940\n",
      "train: loss: 875790.625 acc: 0.7393946647644043  val: loss: 3474540.75 acc: 0.4310646653175354\n",
      "step: 46945\n",
      "train: loss: 137081.046875 acc: 0.901526689529419  val: loss: 3903453.75 acc: 0.5621296167373657\n",
      "step: 46950\n",
      "train: loss: 232211.1875 acc: 0.8727680444717407  val: loss: 1422685.125 acc: 0.6359128355979919\n",
      "step: 46955\n",
      "train: loss: 61055.8515625 acc: 0.9424304366111755  val: loss: 2580663.75 acc: 0.5795348882675171\n",
      "step: 46960\n",
      "train: loss: 82900.6875 acc: 0.9402124881744385  val: loss: 2078366.875 acc: 0.580748438835144\n",
      "step: 46965\n",
      "train: loss: 226883.296875 acc: 0.864600658416748  val: loss: 3290832.75 acc: 0.5692787170410156\n",
      "step: 46970\n",
      "train: loss: 613488.6875 acc: 0.7477909922599792  val: loss: 2888269.75 acc: 0.5810586214065552\n",
      "step: 46975\n",
      "train: loss: 627720.0625 acc: 0.7547531127929688  val: loss: 3904093.5 acc: 0.507613480091095\n",
      "step: 46980\n",
      "train: loss: 111273.640625 acc: 0.9062759876251221  val: loss: 2866181.25 acc: 0.541201114654541\n",
      "step: 46985\n",
      "train: loss: 37477.4140625 acc: 0.955059289932251  val: loss: 3089924.5 acc: 0.436062216758728\n",
      "step: 46990\n",
      "train: loss: 67867.1796875 acc: 0.9396287798881531  val: loss: 62390.4921875 acc: 0.9363796710968018\n",
      "step: 46995\n",
      "train: loss: 57657.12890625 acc: 0.946119487285614  val: loss: 415740.4375 acc: 0.7965801358222961\n",
      "step: 47000\n",
      "train: loss: 133013.375 acc: 0.9164023399353027  val: loss: 521467.90625 acc: 0.7518396377563477\n",
      "step: 47005\n",
      "train: loss: 254452.0625 acc: 0.8532821536064148  val: loss: 2603271.5 acc: 0.5338519811630249\n",
      "step: 47010\n",
      "train: loss: 1109593.125 acc: 0.6469384431838989  val: loss: 1702950.625 acc: 0.682255744934082\n",
      "step: 47015\n",
      "train: loss: 533189.1875 acc: 0.6912282705307007  val: loss: 181655.0625 acc: 0.8982977867126465\n",
      "step: 47020\n",
      "train: loss: 569867.875 acc: 0.7517197132110596  val: loss: 2269093.5 acc: 0.5505304336547852\n",
      "step: 47025\n",
      "train: loss: 1307362.375 acc: 0.7724725604057312  val: loss: 174708.296875 acc: 0.8838139772415161\n",
      "step: 47030\n",
      "train: loss: 616514.1875 acc: 0.796161949634552  val: loss: 460004.46875 acc: 0.7509052753448486\n",
      "step: 47035\n",
      "train: loss: 598708.75 acc: 0.9382988810539246  val: loss: 564697.875 acc: 0.9237948060035706\n",
      "step: 47040\n",
      "train: loss: 234858.234375 acc: 0.9804486036300659  val: loss: 200858.515625 acc: 0.9598128199577332\n",
      "step: 47045\n",
      "train: loss: 135324.765625 acc: 0.9880746006965637  val: loss: 586415.0 acc: 0.6950759887695312\n",
      "step: 47050\n",
      "train: loss: 43257.109375 acc: 0.9899498224258423  val: loss: 451904.25 acc: 0.8815788626670837\n",
      "step: 47055\n",
      "train: loss: 130928.734375 acc: 0.9860135316848755  val: loss: 783190.6875 acc: 0.6952075958251953\n",
      "step: 47060\n",
      "train: loss: 86449.0546875 acc: 0.9939781427383423  val: loss: 52332.10546875 acc: 0.9832989573478699\n",
      "step: 47065\n",
      "train: loss: 77255.2265625 acc: 0.9929735660552979  val: loss: 107082.21875 acc: 0.9709895849227905\n",
      "step: 47070\n",
      "train: loss: 164353.8125 acc: 0.9851193428039551  val: loss: 547091.125 acc: 0.8201611042022705\n",
      "step: 47075\n",
      "train: loss: 23527.572265625 acc: 0.994724690914154  val: loss: 312171.90625 acc: 0.9036382436752319\n",
      "step: 47080\n",
      "train: loss: 80008.171875 acc: 0.9850862622261047  val: loss: 478408.0625 acc: 0.8977694511413574\n",
      "step: 47085\n",
      "train: loss: 14458.5693359375 acc: 0.9899437427520752  val: loss: 385744.4375 acc: 0.899779200553894\n",
      "step: 47090\n",
      "train: loss: 14169.5146484375 acc: 0.9680087566375732  val: loss: 1024052.9375 acc: 0.900519609451294\n",
      "step: 47095\n",
      "train: loss: 12850.9482421875 acc: 0.9780027866363525  val: loss: 671222.9375 acc: 0.7427598237991333\n",
      "step: 47100\n",
      "train: loss: 23528.31640625 acc: 0.9681803584098816  val: loss: 646677.8125 acc: 0.8909977674484253\n",
      "step: 47105\n",
      "train: loss: 10094.1181640625 acc: 0.9782154560089111  val: loss: 770583.875 acc: 0.8878730535507202\n",
      "step: 47110\n",
      "train: loss: 31873.458984375 acc: 0.9646152853965759  val: loss: 1749027.125 acc: 0.6190845966339111\n",
      "step: 47115\n",
      "train: loss: 11174.8134765625 acc: 0.9785203337669373  val: loss: 1679708.75 acc: 0.8531442880630493\n",
      "step: 47120\n",
      "train: loss: 15023.9326171875 acc: 0.9818601012229919  val: loss: 757492.0625 acc: 0.9084191918373108\n",
      "step: 47125\n",
      "train: loss: 6624.0830078125 acc: 0.982999861240387  val: loss: 1216517.875 acc: 0.7500932216644287\n",
      "step: 47130\n",
      "train: loss: 55743.640625 acc: 0.9693557024002075  val: loss: 538015.6875 acc: 0.9275246858596802\n",
      "step: 47135\n",
      "train: loss: 31048.50390625 acc: 0.9756021499633789  val: loss: 941428.1875 acc: 0.7455804944038391\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 47140\n",
      "train: loss: 13966.4091796875 acc: 0.9895620942115784  val: loss: 2577032.0 acc: 0.6235026121139526\n",
      "step: 47145\n",
      "train: loss: 33306.54296875 acc: 0.9861405491828918  val: loss: 2806265.0 acc: 0.7090891599655151\n",
      "step: 47150\n",
      "train: loss: 6825.3916015625 acc: 0.9931067824363708  val: loss: 2370602.0 acc: 0.6103020310401917\n",
      "step: 47155\n",
      "train: loss: 14628.7314453125 acc: 0.9677729606628418  val: loss: 653041.6875 acc: 0.9371827840805054\n",
      "step: 47160\n",
      "train: loss: 30257.85546875 acc: 0.9851782917976379  val: loss: 2440777.5 acc: 0.3312968611717224\n",
      "step: 47165\n",
      "train: loss: 26834.81640625 acc: 0.9900099635124207  val: loss: 506829.28125 acc: 0.9149888753890991\n",
      "step: 47170\n",
      "train: loss: 24926.921875 acc: 0.9927735924720764  val: loss: 623873.3125 acc: 0.888278603553772\n",
      "step: 47175\n",
      "train: loss: 24975.974609375 acc: 0.9901602864265442  val: loss: 4614137.5 acc: -1.2598052024841309\n",
      "step: 47180\n",
      "train: loss: 36837.77734375 acc: 0.9863975644111633  val: loss: 1114285.75 acc: 0.7841361165046692\n",
      "step: 47185\n",
      "train: loss: 205347.140625 acc: 0.9402000308036804  val: loss: 2476657.25 acc: -0.4140348434448242\n",
      "step: 47190\n",
      "train: loss: 67989.4765625 acc: 0.98308926820755  val: loss: 386116.75 acc: 0.9296823740005493\n",
      "step: 47195\n",
      "train: loss: 114612.921875 acc: 0.9552099704742432  val: loss: 1642820.875 acc: 0.8379616737365723\n",
      "step: 47200\n",
      "train: loss: 808399.0625 acc: 0.866813063621521  val: loss: 456032.5625 acc: 0.6816233396530151\n",
      "step: 47205\n",
      "train: loss: 1117766.0 acc: 0.8394845724105835  val: loss: 744047.9375 acc: 0.7147471904754639\n",
      "step: 47210\n",
      "train: loss: 76499.8125 acc: 0.9908578991889954  val: loss: 627654.8125 acc: 0.8691875338554382\n",
      "step: 47215\n",
      "train: loss: 38038.8984375 acc: 0.9953401684761047  val: loss: 461595.0625 acc: 0.9096718430519104\n",
      "step: 47220\n",
      "train: loss: 219151.015625 acc: 0.965460479259491  val: loss: 2320051.0 acc: 0.3618682622909546\n",
      "step: 47225\n",
      "train: loss: 773838.3125 acc: 0.9329713582992554  val: loss: 1919830.875 acc: 0.5947791337966919\n",
      "step: 47230\n",
      "train: loss: 771602.0 acc: 0.9498528242111206  val: loss: 1457038.25 acc: 0.7304878830909729\n",
      "step: 47235\n",
      "train: loss: 272629.4375 acc: 0.9723153710365295  val: loss: 3533940.5 acc: 0.6260091662406921\n",
      "step: 47240\n",
      "train: loss: 667769.0625 acc: 0.9533657431602478  val: loss: 1020904.25 acc: 0.5049511194229126\n",
      "step: 47245\n",
      "train: loss: 591483.125 acc: 0.9762120842933655  val: loss: 1398666.0 acc: 0.8411540389060974\n",
      "step: 47250\n",
      "train: loss: 1796063.5 acc: 0.937626838684082  val: loss: 1064359.625 acc: 0.8149318695068359\n",
      "step: 47255\n",
      "train: loss: 2177558.75 acc: 0.9270612597465515  val: loss: 2189223.25 acc: 0.7094117403030396\n",
      "step: 47260\n",
      "train: loss: 354192.25 acc: 0.9809224605560303  val: loss: 355038.46875 acc: 0.5335407257080078\n",
      "step: 47265\n",
      "train: loss: 216941.75 acc: 0.9771550297737122  val: loss: 775462.0 acc: 0.8614325523376465\n",
      "step: 47270\n",
      "train: loss: 446123.71875 acc: 0.9466414451599121  val: loss: 245202.828125 acc: 0.8791754245758057\n",
      "step: 47275\n",
      "train: loss: 443627.0 acc: 0.9521257281303406  val: loss: 451941.3125 acc: 0.8722281455993652\n",
      "step: 47280\n",
      "train: loss: 2480328.5 acc: 0.6268230080604553  val: loss: 878400.1875 acc: 0.8519355058670044\n",
      "step: 47285\n",
      "train: loss: 560388.4375 acc: 0.5914691686630249  val: loss: 455871.125 acc: 0.8394126296043396\n",
      "step: 47290\n",
      "train: loss: 621300.0625 acc: 0.7770377397537231  val: loss: 675595.8125 acc: 0.7302873134613037\n",
      "step: 47295\n",
      "train: loss: 873336.9375 acc: 0.6564708352088928  val: loss: 4001874.75 acc: 0.5915594696998596\n",
      "step: 47300\n",
      "train: loss: 716106.1875 acc: 0.7363862991333008  val: loss: 574085.0 acc: 0.7702069282531738\n",
      "step: 47305\n",
      "train: loss: 185358.921875 acc: 0.8649779558181763  val: loss: 1061841.5 acc: 0.5240450501441956\n",
      "step: 47310\n",
      "train: loss: 530532.9375 acc: 0.8368526101112366  val: loss: 1727401.375 acc: 0.6303696632385254\n",
      "step: 47315\n",
      "train: loss: 478757.40625 acc: 0.7689599394798279  val: loss: 417298.25 acc: 0.7714219093322754\n",
      "step: 47320\n",
      "train: loss: 223847.953125 acc: 0.855143666267395  val: loss: 5515251.0 acc: 0.3217323422431946\n",
      "step: 47325\n",
      "train: loss: 83285.5546875 acc: 0.9223135113716125  val: loss: 365779.1875 acc: 0.67671138048172\n",
      "step: 47330\n",
      "train: loss: 40807.7578125 acc: 0.9655224084854126  val: loss: 2585121.5 acc: 0.46168816089630127\n",
      "step: 47335\n",
      "train: loss: 66124.9765625 acc: 0.9535359144210815  val: loss: 4122143.5 acc: 0.4201925992965698\n",
      "step: 47340\n",
      "train: loss: 453803.0625 acc: 0.7972186803817749  val: loss: 2144417.5 acc: 0.5783510208129883\n",
      "step: 47345\n",
      "train: loss: 400898.8125 acc: 0.7866104245185852  val: loss: 894273.875 acc: 0.6576447486877441\n",
      "step: 47350\n",
      "train: loss: 484858.5 acc: 0.7040567994117737  val: loss: 3413367.25 acc: 0.3833667039871216\n",
      "step: 47355\n",
      "train: loss: 102900.21875 acc: 0.8956037163734436  val: loss: 105010.546875 acc: 0.9274070262908936\n",
      "step: 47360\n",
      "train: loss: 83137.765625 acc: 0.9184206128120422  val: loss: 4327109.5 acc: 0.42797523736953735\n",
      "step: 47365\n",
      "train: loss: 190821.921875 acc: 0.8543527722358704  val: loss: 2212744.0 acc: 0.6001605987548828\n",
      "step: 47370\n",
      "train: loss: 467023.5 acc: 0.7532787919044495  val: loss: 471989.6875 acc: 0.7561667561531067\n",
      "step: 47375\n",
      "train: loss: 190283.875 acc: 0.8822359442710876  val: loss: 668616.8125 acc: 0.7905948162078857\n",
      "step: 47380\n",
      "train: loss: 252782.125 acc: 0.8281887769699097  val: loss: 532250.75 acc: 0.8035268783569336\n",
      "step: 47385\n",
      "train: loss: 414386.1875 acc: 0.818118155002594  val: loss: 1756754.25 acc: 0.7119243741035461\n",
      "step: 47390\n",
      "train: loss: 1871262.25 acc: 0.7615933418273926  val: loss: 1324741.0 acc: 0.8324951529502869\n",
      "step: 47395\n",
      "train: loss: 755874.3125 acc: 0.8544055223464966  val: loss: 380092.34375 acc: 0.8895893096923828\n",
      "step: 47400\n",
      "train: loss: 931549.6875 acc: 0.9026032090187073  val: loss: 155005.671875 acc: 0.9679076075553894\n",
      "step: 47405\n",
      "train: loss: 123817.5078125 acc: 0.9911121726036072  val: loss: 774520.4375 acc: 0.9090440273284912\n",
      "step: 47410\n",
      "train: loss: 472489.40625 acc: 0.9287866950035095  val: loss: 977221.75 acc: 0.8616523742675781\n",
      "step: 47415\n",
      "train: loss: 61284.359375 acc: 0.9897902607917786  val: loss: 1244059.875 acc: 0.8900264501571655\n",
      "step: 47420\n",
      "train: loss: 545528.375 acc: 0.9230382442474365  val: loss: 560672.75 acc: 0.887742280960083\n",
      "step: 47425\n",
      "train: loss: 49016.98046875 acc: 0.995352029800415  val: loss: 431225.9375 acc: 0.9145312309265137\n",
      "step: 47430\n",
      "train: loss: 108595.7421875 acc: 0.9913356900215149  val: loss: 398529.78125 acc: 0.9047480821609497\n",
      "step: 47435\n",
      "train: loss: 94655.546875 acc: 0.9907087683677673  val: loss: 353152.0 acc: 0.936909019947052\n",
      "step: 47440\n",
      "train: loss: 59222.35546875 acc: 0.9912151098251343  val: loss: 308475.375 acc: 0.9547314047813416\n",
      "step: 47445\n",
      "train: loss: 12783.95703125 acc: 0.9724724292755127  val: loss: 1915191.875 acc: 0.7403768301010132\n",
      "step: 47450\n",
      "train: loss: 63685.31640625 acc: 0.9698361754417419  val: loss: 1377256.375 acc: 0.8204232454299927\n",
      "step: 47455\n",
      "train: loss: 29720.087890625 acc: 0.9852882027626038  val: loss: 1816677.125 acc: 0.8311032652854919\n",
      "step: 47460\n",
      "train: loss: 16333.1572265625 acc: 0.9878668189048767  val: loss: 1724989.625 acc: 0.8312317132949829\n",
      "step: 47465\n",
      "train: loss: 28923.46484375 acc: 0.9880126714706421  val: loss: 2648860.75 acc: 0.8084025382995605\n",
      "step: 47470\n",
      "train: loss: 12823.0771484375 acc: 0.959145724773407  val: loss: 2239745.25 acc: 0.7204211354255676\n",
      "step: 47475\n",
      "train: loss: 14488.619140625 acc: 0.9595842957496643  val: loss: 1386064.375 acc: 0.280483603477478\n",
      "step: 47480\n",
      "train: loss: 6589.42431640625 acc: 0.9686829447746277  val: loss: 1462272.5 acc: 0.6134588718414307\n",
      "step: 47485\n",
      "train: loss: 14642.5693359375 acc: 0.9565708041191101  val: loss: 1514969.25 acc: 0.8593044281005859\n",
      "step: 47490\n",
      "train: loss: 29429.95703125 acc: 0.9776080846786499  val: loss: 863943.1875 acc: 0.8794309496879578\n",
      "step: 47495\n",
      "train: loss: 23587.580078125 acc: 0.9844499826431274  val: loss: 1252008.375 acc: 0.7403631210327148\n",
      "step: 47500\n",
      "train: loss: 33376.0859375 acc: 0.9771314859390259  val: loss: 1977345.375 acc: 0.6113137006759644\n",
      "step: 47505\n",
      "train: loss: 78760.7734375 acc: 0.9675958752632141  val: loss: 3130806.5 acc: 0.4162588119506836\n",
      "step: 47510\n",
      "train: loss: 22523.1796875 acc: 0.9892837405204773  val: loss: 4646385.0 acc: -1.265449047088623\n",
      "step: 47515\n",
      "train: loss: 11311.3427734375 acc: 0.9825369715690613  val: loss: 847285.875 acc: 0.8717498183250427\n",
      "step: 47520\n",
      "train: loss: 9225.1904296875 acc: 0.9756033420562744  val: loss: 2211632.75 acc: 0.2523987293243408\n",
      "step: 47525\n",
      "train: loss: 16873.841796875 acc: 0.9929894804954529  val: loss: 2716697.25 acc: -0.47105205059051514\n",
      "step: 47530\n",
      "train: loss: 35397.71875 acc: 0.9880754351615906  val: loss: 647655.625 acc: 0.9195802211761475\n",
      "step: 47535\n",
      "train: loss: 25417.103515625 acc: 0.9948426485061646  val: loss: 1380449.125 acc: 0.6481332778930664\n",
      "step: 47540\n",
      "train: loss: 24867.23828125 acc: 0.9900946021080017  val: loss: 894379.9375 acc: 0.7099407911300659\n",
      "step: 47545\n",
      "train: loss: 41797.3828125 acc: 0.9779875874519348  val: loss: 1651525.75 acc: 0.5395752191543579\n",
      "step: 47550\n",
      "train: loss: 284345.28125 acc: 0.9468954801559448  val: loss: 358599.84375 acc: 0.8341728448867798\n",
      "step: 47555\n",
      "train: loss: 109146.3359375 acc: 0.969426691532135  val: loss: 798012.6875 acc: 0.7509500980377197\n",
      "step: 47560\n",
      "train: loss: 82195.5 acc: 0.9469836354255676  val: loss: 1247143.375 acc: 0.8158857822418213\n",
      "step: 47565\n",
      "train: loss: 82654.3125 acc: 0.9657444953918457  val: loss: 905272.0 acc: 0.8421911001205444\n",
      "step: 47570\n",
      "train: loss: 171127.140625 acc: 0.9778946042060852  val: loss: 873919.125 acc: 0.7809289693832397\n",
      "step: 47575\n",
      "train: loss: 60374.98046875 acc: 0.9928818345069885  val: loss: 792656.4375 acc: 0.8903681039810181\n",
      "step: 47580\n",
      "train: loss: 313540.3125 acc: 0.9528838992118835  val: loss: 629860.0625 acc: 0.8000689744949341\n",
      "step: 47585\n",
      "train: loss: 529768.1875 acc: 0.9467523097991943  val: loss: 1144343.5 acc: 0.7256926894187927\n",
      "step: 47590\n",
      "train: loss: 256013.28125 acc: 0.9577942490577698  val: loss: 712797.375 acc: 0.90166175365448\n",
      "step: 47595\n",
      "train: loss: 802873.875 acc: 0.9587762355804443  val: loss: 530980.8125 acc: 0.9130598902702332\n",
      "step: 47600\n",
      "train: loss: 253787.921875 acc: 0.9772305488586426  val: loss: 765027.0 acc: 0.8783105611801147\n",
      "step: 47605\n",
      "train: loss: 431212.3125 acc: 0.9527040123939514  val: loss: 625166.1875 acc: 0.8463743329048157\n",
      "step: 47610\n",
      "train: loss: 2854291.0 acc: 0.8754206299781799  val: loss: 504321.375 acc: 0.4597325921058655\n",
      "step: 47615\n",
      "train: loss: 756352.8125 acc: 0.9728272557258606  val: loss: 536271.8125 acc: 0.932080864906311\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 47620\n",
      "train: loss: 2806441.5 acc: 0.9166720509529114  val: loss: 1665411.625 acc: 0.04129558801651001\n",
      "step: 47625\n",
      "train: loss: 680878.625 acc: 0.9611295461654663  val: loss: 1111480.5 acc: 0.717755138874054\n",
      "step: 47630\n",
      "train: loss: 935744.8125 acc: 0.9377038478851318  val: loss: 1145085.75 acc: 0.48337554931640625\n",
      "step: 47635\n",
      "train: loss: 728547.5625 acc: 0.9375630021095276  val: loss: 286195.5 acc: 0.9452024102210999\n",
      "step: 47640\n",
      "train: loss: 474916.0625 acc: 0.925577700138092  val: loss: 342144.53125 acc: 0.7837270498275757\n",
      "step: 47645\n",
      "train: loss: 1583231.375 acc: 0.5953365564346313  val: loss: 510042.5625 acc: 0.78532874584198\n",
      "step: 47650\n",
      "train: loss: 1325868.5 acc: 0.5800681710243225  val: loss: 1098701.25 acc: 0.7922361493110657\n",
      "step: 47655\n",
      "train: loss: 740011.0625 acc: 0.7760976552963257  val: loss: 957357.1875 acc: 0.8066548109054565\n",
      "step: 47660\n",
      "train: loss: 223953.03125 acc: 0.8833582401275635  val: loss: 384886.125 acc: 0.6949473023414612\n",
      "step: 47665\n",
      "train: loss: 709960.625 acc: 0.7886312007904053  val: loss: 693244.0625 acc: 0.690972626209259\n",
      "step: 47670\n",
      "train: loss: 184631.640625 acc: 0.8550752401351929  val: loss: 4386180.0 acc: 0.4013000726699829\n",
      "step: 47675\n",
      "train: loss: 392375.3125 acc: 0.7929998636245728  val: loss: 1407625.0 acc: 0.6552749872207642\n",
      "step: 47680\n",
      "train: loss: 471932.5625 acc: 0.7919209003448486  val: loss: 1890829.25 acc: 0.6486049890518188\n",
      "step: 47685\n",
      "train: loss: 195094.515625 acc: 0.8889479041099548  val: loss: 5071667.5 acc: 0.36521828174591064\n",
      "step: 47690\n",
      "train: loss: 45795.6875 acc: 0.9626996517181396  val: loss: 1035292.0 acc: 0.6653131246566772\n",
      "step: 47695\n",
      "train: loss: 62308.4375 acc: 0.9500168561935425  val: loss: 1075043.875 acc: 0.602594256401062\n",
      "step: 47700\n",
      "train: loss: 32766.140625 acc: 0.9692336320877075  val: loss: 1649154.25 acc: 0.5146709680557251\n",
      "step: 47705\n",
      "train: loss: 95801.6328125 acc: 0.9258651733398438  val: loss: 882098.1875 acc: 0.6793568134307861\n",
      "step: 47710\n",
      "train: loss: 69845.7421875 acc: 0.9428023099899292  val: loss: 1284400.5 acc: 0.6692616939544678\n",
      "step: 47715\n",
      "train: loss: 71735.3515625 acc: 0.9205083250999451  val: loss: 10746023.0 acc: 0.32671260833740234\n",
      "step: 47720\n",
      "train: loss: 181703.359375 acc: 0.8296172022819519  val: loss: 3226055.75 acc: 0.4323388934135437\n",
      "step: 47725\n",
      "train: loss: 204651.515625 acc: 0.8743698596954346  val: loss: 2272842.75 acc: 0.5928448438644409\n",
      "step: 47730\n",
      "train: loss: 310867.25 acc: 0.823052167892456  val: loss: 4487584.5 acc: 0.4612753987312317\n",
      "step: 47735\n",
      "train: loss: 446503.90625 acc: 0.8136126399040222  val: loss: 1280719.625 acc: 0.7471349835395813\n",
      "step: 47740\n",
      "train: loss: 13273.8486328125 acc: 0.987671971321106  val: loss: 1075924.125 acc: 0.6801493167877197\n",
      "step: 47745\n",
      "train: loss: 121547.7734375 acc: 0.8705095052719116  val: loss: 5540311.0 acc: 0.4530103802680969\n",
      "step: 47750\n",
      "train: loss: 134824.296875 acc: 0.8882277011871338  val: loss: 1184537.125 acc: 0.6797195672988892\n",
      "step: 47755\n",
      "train: loss: 1478442.0 acc: 0.7097818851470947  val: loss: 185632.484375 acc: 0.8725242614746094\n",
      "step: 47760\n",
      "train: loss: 861703.5 acc: 0.8415292501449585  val: loss: 820681.9375 acc: 0.891900897026062\n",
      "step: 47765\n",
      "train: loss: 221856.703125 acc: 0.9793246388435364  val: loss: 2327621.25 acc: 0.7924450635910034\n",
      "step: 47770\n",
      "train: loss: 202845.765625 acc: 0.9751734733581543  val: loss: 991009.125 acc: 0.8296144008636475\n",
      "step: 47775\n",
      "train: loss: 514268.53125 acc: 0.9458174109458923  val: loss: 1183738.75 acc: 0.7012016773223877\n",
      "step: 47780\n",
      "train: loss: 162960.984375 acc: 0.9496097564697266  val: loss: 2075074.25 acc: 0.7570221424102783\n",
      "step: 47785\n",
      "train: loss: 176722.0 acc: 0.9771731495857239  val: loss: 1673307.375 acc: 0.652194619178772\n",
      "step: 47790\n",
      "train: loss: 466001.5 acc: 0.9518269300460815  val: loss: 1566436.125 acc: 0.10223054885864258\n",
      "step: 47795\n",
      "train: loss: 67352.4453125 acc: 0.9944050908088684  val: loss: 699828.6875 acc: 0.834330677986145\n",
      "step: 47800\n",
      "train: loss: 60245.01953125 acc: 0.9929120540618896  val: loss: 2230105.25 acc: 0.7616096138954163\n",
      "step: 47805\n",
      "train: loss: 62935.90625 acc: 0.9902220964431763  val: loss: 2137915.0 acc: 0.2894354462623596\n",
      "step: 47810\n",
      "train: loss: 54501.51953125 acc: 0.9813591241836548  val: loss: 497030.25 acc: 0.8405699729919434\n",
      "step: 47815\n",
      "train: loss: 6770.744140625 acc: 0.9771554470062256  val: loss: 842575.4375 acc: 0.6282919049263\n",
      "step: 47820\n",
      "train: loss: 13222.2587890625 acc: 0.9354091882705688  val: loss: 1582819.75 acc: 0.6915010213851929\n",
      "step: 47825\n",
      "train: loss: 13512.005859375 acc: 0.991460919380188  val: loss: 1743602.0 acc: 0.8395957946777344\n",
      "step: 47830\n",
      "train: loss: 20604.44140625 acc: 0.9680461287498474  val: loss: 2641286.75 acc: -1.0438032150268555\n",
      "step: 47835\n",
      "train: loss: 24176.4453125 acc: 0.9573982954025269  val: loss: 592528.5 acc: 0.8706889748573303\n",
      "step: 47840\n",
      "train: loss: 8343.4423828125 acc: 0.9776796102523804  val: loss: 750509.75 acc: 0.8681126832962036\n",
      "step: 47845\n",
      "train: loss: 170106.203125 acc: 0.7970314621925354  val: loss: 2571752.5 acc: 0.7373952865600586\n",
      "step: 47850\n",
      "train: loss: 7865.3046875 acc: 0.9656902551651001  val: loss: 1245062.0 acc: -0.17461097240447998\n",
      "step: 47855\n",
      "train: loss: 19194.0234375 acc: 0.9802845120429993  val: loss: 1440824.625 acc: 0.6633161902427673\n",
      "step: 47860\n",
      "train: loss: 41894.37109375 acc: 0.9757080078125  val: loss: 1530208.125 acc: 0.7138034105300903\n",
      "step: 47865\n",
      "train: loss: 41631.9453125 acc: 0.9788935780525208  val: loss: 911527.9375 acc: 0.8449480533599854\n",
      "step: 47870\n",
      "train: loss: 24977.72265625 acc: 0.9784078001976013  val: loss: 2319287.75 acc: 0.7111787796020508\n",
      "step: 47875\n",
      "train: loss: 20840.05078125 acc: 0.9885776042938232  val: loss: 1169275.125 acc: 0.8250454664230347\n",
      "step: 47880\n",
      "train: loss: 15699.0517578125 acc: 0.9874916672706604  val: loss: 2227322.0 acc: 0.7469077110290527\n",
      "step: 47885\n",
      "train: loss: 13129.2021484375 acc: 0.9829914569854736  val: loss: 2074502.625 acc: 0.6731262803077698\n",
      "step: 47890\n",
      "train: loss: 8211.6572265625 acc: 0.9908407926559448  val: loss: 1113117.375 acc: 0.8227794170379639\n",
      "step: 47895\n",
      "train: loss: 19842.349609375 acc: 0.9940518736839294  val: loss: 537619.0625 acc: 0.8375206589698792\n",
      "step: 47900\n",
      "train: loss: 33087.9453125 acc: 0.9921935796737671  val: loss: 473991.65625 acc: 0.8891921639442444\n",
      "step: 47905\n",
      "train: loss: 42616.34765625 acc: 0.9859707951545715  val: loss: 1361725.875 acc: 0.6308333873748779\n",
      "step: 47910\n",
      "train: loss: 38229.01953125 acc: 0.9837226867675781  val: loss: 744878.75 acc: 0.876629650592804\n",
      "step: 47915\n",
      "train: loss: 84896.984375 acc: 0.9710400104522705  val: loss: 147494.9375 acc: 0.9651620388031006\n",
      "step: 47920\n",
      "train: loss: 138653.078125 acc: 0.9568942785263062  val: loss: 929785.125 acc: 0.5256220102310181\n",
      "step: 47925\n",
      "train: loss: 84975.625 acc: 0.9588308930397034  val: loss: 1976686.875 acc: 0.30637508630752563\n",
      "step: 47930\n",
      "train: loss: 767639.5625 acc: 0.7484824657440186  val: loss: 609805.875 acc: 0.8034052848815918\n",
      "step: 47935\n",
      "train: loss: 660873.0625 acc: 0.9110748767852783  val: loss: 409600.625 acc: 0.8200024962425232\n",
      "step: 47940\n",
      "train: loss: 49846.58984375 acc: 0.9963130950927734  val: loss: 1066078.5 acc: 0.6327146291732788\n",
      "step: 47945\n",
      "train: loss: 41786.18359375 acc: 0.9939141869544983  val: loss: 1430988.625 acc: 0.8067786693572998\n",
      "step: 47950\n",
      "train: loss: 38368.8671875 acc: 0.9950705170631409  val: loss: 859932.5625 acc: 0.6875786781311035\n",
      "step: 47955\n",
      "train: loss: 431610.90625 acc: 0.9673189520835876  val: loss: 377921.5625 acc: 0.9038779735565186\n",
      "step: 47960\n",
      "train: loss: 687568.0 acc: 0.9137042760848999  val: loss: 625188.6875 acc: 0.870959997177124\n",
      "step: 47965\n",
      "train: loss: 298365.90625 acc: 0.9633967280387878  val: loss: 1184050.625 acc: 0.8061175346374512\n",
      "step: 47970\n",
      "train: loss: 923447.5625 acc: 0.8377304077148438  val: loss: 833730.375 acc: 0.6081629991531372\n",
      "step: 47975\n",
      "train: loss: 642061.6875 acc: 0.9723150134086609  val: loss: 604162.3125 acc: 0.7767641544342041\n",
      "step: 47980\n",
      "train: loss: 2498052.0 acc: 0.9337190389633179  val: loss: 396876.59375 acc: 0.89759761095047\n",
      "step: 47985\n",
      "train: loss: 1226932.125 acc: 0.9502634406089783  val: loss: 153462.125 acc: 0.9744029641151428\n",
      "step: 47990\n",
      "train: loss: 829984.6875 acc: 0.9683515429496765  val: loss: 792592.0 acc: 0.8109086751937866\n",
      "step: 47995\n",
      "train: loss: 357958.8125 acc: 0.9538214802742004  val: loss: 1636604.75 acc: 0.7625529170036316\n",
      "step: 48000\n",
      "train: loss: 232821.1875 acc: 0.9717645645141602  val: loss: 396916.09375 acc: 0.89997398853302\n",
      "step: 48005\n",
      "train: loss: 292819.96875 acc: 0.9377543330192566  val: loss: 775885.6875 acc: 0.8725802302360535\n",
      "step: 48010\n",
      "train: loss: 1396413.375 acc: 0.8603230714797974  val: loss: 556027.6875 acc: 0.8896418809890747\n",
      "step: 48015\n",
      "train: loss: 2260447.5 acc: 0.632350742816925  val: loss: 1141369.625 acc: 0.7913905382156372\n",
      "step: 48020\n",
      "train: loss: 403243.21875 acc: 0.8754693865776062  val: loss: 1824851.125 acc: 0.619645893573761\n",
      "step: 48025\n",
      "train: loss: 437651.40625 acc: 0.7943646311759949  val: loss: 4764400.0 acc: 0.564932107925415\n",
      "step: 48030\n",
      "train: loss: 962150.0625 acc: 0.8175787329673767  val: loss: 2068819.125 acc: 0.7365179061889648\n",
      "step: 48035\n",
      "train: loss: 554902.9375 acc: 0.7986704111099243  val: loss: 6798164.5 acc: 0.43361157178878784\n",
      "step: 48040\n",
      "train: loss: 82374.6171875 acc: 0.9443888664245605  val: loss: 5075683.5 acc: 0.3088541030883789\n",
      "step: 48045\n",
      "train: loss: 86272.328125 acc: 0.9527201056480408  val: loss: 3289174.75 acc: 0.4900869131088257\n",
      "step: 48050\n",
      "train: loss: 22492.203125 acc: 0.983210563659668  val: loss: 5904721.5 acc: 0.3422781229019165\n",
      "step: 48055\n",
      "train: loss: 33521.9765625 acc: 0.9738340973854065  val: loss: 820229.25 acc: 0.631638765335083\n",
      "step: 48060\n",
      "train: loss: 490434.28125 acc: 0.7666285037994385  val: loss: 5524678.0 acc: 0.2873985171318054\n",
      "step: 48065\n",
      "train: loss: 193337.8125 acc: 0.8734744787216187  val: loss: 1951715.5 acc: 0.5240598917007446\n",
      "step: 48070\n",
      "train: loss: 121436.28125 acc: 0.9243831038475037  val: loss: 1048541.5625 acc: 0.6910288333892822\n",
      "step: 48075\n",
      "train: loss: 95990.671875 acc: 0.9279617071151733  val: loss: 6881737.5 acc: 0.3301037549972534\n",
      "step: 48080\n",
      "train: loss: 256341.859375 acc: 0.8402764201164246  val: loss: 2453077.5 acc: 0.5148062705993652\n",
      "step: 48085\n",
      "train: loss: 18172.9453125 acc: 0.9829351902008057  val: loss: 685267.0 acc: 0.7374897003173828\n",
      "step: 48090\n",
      "train: loss: 31786.40625 acc: 0.9583302140235901  val: loss: 2484579.75 acc: 0.5296502709388733\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 48095\n",
      "train: loss: 45477.13671875 acc: 0.9578261375427246  val: loss: 3354665.75 acc: 0.48081767559051514\n",
      "step: 48100\n",
      "train: loss: 509759.3125 acc: 0.6999608874320984  val: loss: 210291.046875 acc: 0.8132927417755127\n",
      "step: 48105\n",
      "train: loss: 262349.0625 acc: 0.8264302015304565  val: loss: 4749673.0 acc: 0.35008513927459717\n",
      "step: 48110\n",
      "train: loss: 119898.140625 acc: 0.9048857688903809  val: loss: 637030.1875 acc: 0.5715528726577759\n",
      "step: 48115\n",
      "train: loss: 143994.8125 acc: 0.9018825888633728  val: loss: 1501748.375 acc: 0.6423383951187134\n",
      "step: 48120\n",
      "train: loss: 1387228.25 acc: 0.742444634437561  val: loss: 1355218.375 acc: 0.6592402458190918\n",
      "step: 48125\n",
      "train: loss: 1059694.125 acc: 0.6617741584777832  val: loss: 2679061.75 acc: 0.7401236295700073\n",
      "step: 48130\n",
      "train: loss: 1074085.625 acc: 0.8859548568725586  val: loss: 1215196.125 acc: 0.37416213750839233\n",
      "step: 48135\n",
      "train: loss: 105486.3984375 acc: 0.9919976592063904  val: loss: 2450022.75 acc: 0.5178511142730713\n",
      "step: 48140\n",
      "train: loss: 144734.109375 acc: 0.9874171018600464  val: loss: 1050507.125 acc: 0.6825584769248962\n",
      "step: 48145\n",
      "train: loss: 394094.0625 acc: 0.9528112411499023  val: loss: 1845149.125 acc: 0.8365819454193115\n",
      "step: 48150\n",
      "train: loss: 125738.5 acc: 0.978000819683075  val: loss: 1068947.875 acc: 0.7599248290061951\n",
      "step: 48155\n",
      "train: loss: 80162.5078125 acc: 0.9926627278327942  val: loss: 1392761.625 acc: 0.7469578981399536\n",
      "step: 48160\n",
      "train: loss: 101477.109375 acc: 0.9915880560874939  val: loss: 1495460.0 acc: 0.7382141351699829\n",
      "step: 48165\n",
      "train: loss: 54223.4140625 acc: 0.9904706478118896  val: loss: 1177305.375 acc: 0.38924461603164673\n",
      "step: 48170\n",
      "train: loss: 39873.63671875 acc: 0.9944274425506592  val: loss: 1271842.25 acc: 0.704742968082428\n",
      "step: 48175\n",
      "train: loss: 28749.734375 acc: 0.9953762888908386  val: loss: 1261269.625 acc: 0.43888258934020996\n",
      "step: 48180\n",
      "train: loss: 21711.83984375 acc: 0.9936095476150513  val: loss: 2162872.0 acc: -0.6363568305969238\n",
      "step: 48185\n",
      "train: loss: 19419.40625 acc: 0.9903255701065063  val: loss: 2725314.25 acc: 0.16710257530212402\n",
      "step: 48190\n",
      "train: loss: 12934.4423828125 acc: 0.9814935326576233  val: loss: 1983330.375 acc: 0.466417133808136\n",
      "step: 48195\n",
      "train: loss: 43608.08203125 acc: 0.9852222800254822  val: loss: 370587.65625 acc: 0.9405933022499084\n",
      "step: 48200\n",
      "train: loss: 183964.4375 acc: 0.6482658386230469  val: loss: 1518965.5 acc: 0.6751956939697266\n",
      "step: 48205\n",
      "train: loss: 15738.5302734375 acc: 0.9696154594421387  val: loss: 1149459.625 acc: 0.7300190329551697\n",
      "step: 48210\n",
      "train: loss: 15444.220703125 acc: 0.9813292622566223  val: loss: 536200.25 acc: 0.8467348217964172\n",
      "step: 48215\n",
      "train: loss: 7641.26416015625 acc: 0.9760897159576416  val: loss: 334564.875 acc: 0.9456635117530823\n",
      "step: 48220\n",
      "train: loss: 23127.14453125 acc: 0.9609740376472473  val: loss: 1279382.875 acc: 0.6867973804473877\n",
      "step: 48225\n",
      "train: loss: 59653.35546875 acc: 0.9815901517868042  val: loss: 1311975.625 acc: 0.8016939759254456\n",
      "step: 48230\n",
      "train: loss: 36706.75 acc: 0.9791931509971619  val: loss: 2200466.75 acc: 0.3304126262664795\n",
      "step: 48235\n",
      "train: loss: 16146.625 acc: 0.9887655377388  val: loss: 927019.25 acc: 0.8906087875366211\n",
      "step: 48240\n",
      "train: loss: 43963.3359375 acc: 0.9862444996833801  val: loss: 1368853.875 acc: 0.7558589577674866\n",
      "step: 48245\n",
      "train: loss: 15579.630859375 acc: 0.9887155890464783  val: loss: 562734.5625 acc: 0.8770865201950073\n",
      "step: 48250\n",
      "train: loss: 6657.59130859375 acc: 0.99046790599823  val: loss: 955360.4375 acc: 0.8904037475585938\n",
      "step: 48255\n",
      "train: loss: 19363.23046875 acc: 0.9877331852912903  val: loss: 1055998.125 acc: 0.8834624290466309\n",
      "step: 48260\n",
      "train: loss: 29602.708984375 acc: 0.9888719916343689  val: loss: 953576.625 acc: 0.6702151298522949\n",
      "step: 48265\n",
      "train: loss: 36216.91015625 acc: 0.9873186349868774  val: loss: 554766.625 acc: 0.76288241147995\n",
      "step: 48270\n",
      "train: loss: 34456.58984375 acc: 0.9899314045906067  val: loss: 1851013.0 acc: 0.26115483045578003\n",
      "step: 48275\n",
      "train: loss: 66858.140625 acc: 0.9840829968452454  val: loss: 1364384.125 acc: 0.619704008102417\n",
      "step: 48280\n",
      "train: loss: 132297.65625 acc: 0.9791429042816162  val: loss: 71299.453125 acc: 0.9813346862792969\n",
      "step: 48285\n",
      "train: loss: 242092.203125 acc: 0.9283074140548706  val: loss: 1434071.875 acc: 0.3998679518699646\n",
      "step: 48290\n",
      "train: loss: 104544.4140625 acc: 0.9579960107803345  val: loss: 2847486.0 acc: 0.29041093587875366\n",
      "step: 48295\n",
      "train: loss: 1015823.0 acc: 0.610410749912262  val: loss: 309425.28125 acc: 0.5997871160507202\n",
      "step: 48300\n",
      "train: loss: 679913.25 acc: 0.8964074850082397  val: loss: 99831.6796875 acc: 0.9658488035202026\n",
      "step: 48305\n",
      "train: loss: 89898.453125 acc: 0.9888901710510254  val: loss: 678253.375 acc: 0.8354945182800293\n",
      "step: 48310\n",
      "train: loss: 141156.75 acc: 0.9719673991203308  val: loss: 830071.875 acc: 0.6107556819915771\n",
      "step: 48315\n",
      "train: loss: 111474.3359375 acc: 0.9823879599571228  val: loss: 231231.8125 acc: 0.9252504706382751\n",
      "step: 48320\n",
      "train: loss: 387090.09375 acc: 0.9511075019836426  val: loss: 845986.9375 acc: 0.724755048751831\n",
      "step: 48325\n",
      "train: loss: 2278807.0 acc: 0.8860440850257874  val: loss: 950161.25 acc: 0.8368616104125977\n",
      "step: 48330\n",
      "train: loss: 860077.1875 acc: 0.9627332091331482  val: loss: 1477131.25 acc: 0.8452975153923035\n",
      "step: 48335\n",
      "train: loss: 186829.1875 acc: 0.9602940082550049  val: loss: 2802135.75 acc: 0.1502077579498291\n",
      "step: 48340\n",
      "train: loss: 258623.859375 acc: 0.9832988977432251  val: loss: 1576726.875 acc: 0.7283413410186768\n",
      "step: 48345\n",
      "train: loss: 738362.125 acc: 0.9706125259399414  val: loss: 1613468.125 acc: 0.43995481729507446\n",
      "step: 48350\n",
      "train: loss: 1628390.875 acc: 0.9401941895484924  val: loss: 707246.8125 acc: 0.9168139696121216\n",
      "step: 48355\n",
      "train: loss: 706213.4375 acc: 0.965527355670929  val: loss: 321551.40625 acc: 0.9233748316764832\n",
      "step: 48360\n",
      "train: loss: 603205.0625 acc: 0.9475271701812744  val: loss: 594608.5 acc: 0.8847178220748901\n",
      "step: 48365\n",
      "train: loss: 893488.875 acc: 0.9313290119171143  val: loss: 327623.46875 acc: 0.9603099226951599\n",
      "step: 48370\n",
      "train: loss: 412846.21875 acc: 0.9490638971328735  val: loss: 700117.6875 acc: 0.9153451919555664\n",
      "step: 48375\n",
      "train: loss: 709722.625 acc: 0.7728811502456665  val: loss: 1032690.5625 acc: 0.8037561178207397\n",
      "step: 48380\n",
      "train: loss: 1001476.6875 acc: 0.7103848457336426  val: loss: 859182.6875 acc: 0.7833005785942078\n",
      "step: 48385\n",
      "train: loss: 820485.0625 acc: 0.7778871655464172  val: loss: 1010470.0625 acc: 0.6858453750610352\n",
      "step: 48390\n",
      "train: loss: 1320418.875 acc: 0.5825896263122559  val: loss: 1840342.375 acc: 0.7753292918205261\n",
      "step: 48395\n",
      "train: loss: 681816.0625 acc: 0.8080256581306458  val: loss: 2793497.25 acc: 0.6925996541976929\n",
      "step: 48400\n",
      "train: loss: 1391601.25 acc: 0.3014554977416992  val: loss: 2382458.75 acc: 0.6281477212905884\n",
      "step: 48405\n",
      "train: loss: 136747.890625 acc: 0.8942986130714417  val: loss: 7571590.0 acc: 0.20150166749954224\n",
      "step: 48410\n",
      "train: loss: 528395.5625 acc: 0.7718360424041748  val: loss: 5651931.0 acc: 0.2842867970466614\n",
      "step: 48415\n",
      "train: loss: 212246.3125 acc: 0.8475646376609802  val: loss: 10982851.0 acc: 0.2705267667770386\n",
      "step: 48420\n",
      "train: loss: 168853.828125 acc: 0.8995692729949951  val: loss: 774393.5625 acc: 0.6936858296394348\n",
      "step: 48425\n",
      "train: loss: 55241.3984375 acc: 0.9546399116516113  val: loss: 226840.375 acc: 0.8507363200187683\n",
      "step: 48430\n",
      "train: loss: 75463.9453125 acc: 0.9474099278450012  val: loss: 1791370.25 acc: 0.527915358543396\n",
      "step: 48435\n",
      "train: loss: 147212.859375 acc: 0.8831164240837097  val: loss: 3893346.5 acc: 0.5099503993988037\n",
      "step: 48440\n",
      "train: loss: 435833.40625 acc: 0.8073268532752991  val: loss: 6568255.0 acc: 0.4084826111793518\n",
      "step: 48445\n",
      "train: loss: 307316.75 acc: 0.780698835849762  val: loss: 1969221.625 acc: 0.4958575367927551\n",
      "step: 48450\n",
      "train: loss: 150721.28125 acc: 0.8625575304031372  val: loss: 1297405.0 acc: 0.67469322681427\n",
      "step: 48455\n",
      "train: loss: 80968.234375 acc: 0.9285399913787842  val: loss: 3017722.25 acc: 0.5438876152038574\n",
      "step: 48460\n",
      "train: loss: 151178.09375 acc: 0.8807722926139832  val: loss: 2865565.0 acc: 0.3285975456237793\n",
      "step: 48465\n",
      "train: loss: 178617.390625 acc: 0.8706773519515991  val: loss: 1741770.75 acc: 0.6232565641403198\n",
      "step: 48470\n",
      "train: loss: 1750751.875 acc: 0.5754773616790771  val: loss: 747232.0625 acc: 0.7927968502044678\n",
      "step: 48475\n",
      "train: loss: 854532.125 acc: 0.7495739459991455  val: loss: 1241985.125 acc: 0.7359369993209839\n",
      "step: 48480\n",
      "train: loss: 422970.0625 acc: 0.7839996814727783  val: loss: 4195354.5 acc: 0.4501553773880005\n",
      "step: 48485\n",
      "train: loss: 69218.203125 acc: 0.937355637550354  val: loss: 1091400.0 acc: 0.632479190826416\n",
      "step: 48490\n",
      "train: loss: 1136538.875 acc: 0.8109001517295837  val: loss: 750636.8125 acc: 0.8468663692474365\n",
      "step: 48495\n",
      "train: loss: 973706.5625 acc: 0.9021031856536865  val: loss: 1619456.25 acc: 0.45784878730773926\n",
      "step: 48500\n",
      "train: loss: 741029.875 acc: 0.9355710744857788  val: loss: 1145415.125 acc: 0.8169256448745728\n",
      "step: 48505\n",
      "train: loss: 130885.0859375 acc: 0.9841431379318237  val: loss: 738674.8125 acc: 0.8644523620605469\n",
      "step: 48510\n",
      "train: loss: 131872.640625 acc: 0.9804314374923706  val: loss: 757017.6875 acc: 0.8816531300544739\n",
      "step: 48515\n",
      "train: loss: 133983.046875 acc: 0.9820480346679688  val: loss: 887591.875 acc: 0.814103901386261\n",
      "step: 48520\n",
      "train: loss: 74042.328125 acc: 0.9931214451789856  val: loss: 1235919.0 acc: 0.7329699397087097\n",
      "step: 48525\n",
      "train: loss: 91215.9375 acc: 0.99268639087677  val: loss: 649320.5625 acc: 0.7806707620620728\n",
      "step: 48530\n",
      "train: loss: 82223.5546875 acc: 0.992108166217804  val: loss: 1068186.375 acc: 0.8569756746292114\n",
      "step: 48535\n",
      "train: loss: 48566.76171875 acc: 0.9922765493392944  val: loss: 677721.0625 acc: 0.9108108282089233\n",
      "step: 48540\n",
      "train: loss: 116485.1015625 acc: 0.9763938188552856  val: loss: 422104.28125 acc: 0.966419517993927\n",
      "step: 48545\n",
      "train: loss: 32202.15625 acc: 0.9893640279769897  val: loss: 1038452.5625 acc: 0.6261124610900879\n",
      "step: 48550\n",
      "train: loss: 23864.34765625 acc: 0.9891729354858398  val: loss: 1113538.375 acc: 0.8572190999984741\n",
      "step: 48555\n",
      "train: loss: 17805.455078125 acc: 0.9919969439506531  val: loss: 902184.375 acc: 0.886456310749054\n",
      "step: 48560\n",
      "train: loss: 12737.255859375 acc: 0.9712563753128052  val: loss: 1638198.875 acc: 0.8453718423843384\n",
      "step: 48565\n",
      "train: loss: 12164.21875 acc: 0.9867644309997559  val: loss: 1245769.0 acc: 0.7946109771728516\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 48570\n",
      "train: loss: 9441.720703125 acc: 0.9555243253707886  val: loss: 672553.8125 acc: 0.9111570119857788\n",
      "step: 48575\n",
      "train: loss: 8297.8662109375 acc: 0.9816473126411438  val: loss: 1050287.375 acc: 0.7094697952270508\n",
      "step: 48580\n",
      "train: loss: 6592.23681640625 acc: 0.9668929576873779  val: loss: 2014001.875 acc: 0.26697057485580444\n",
      "step: 48585\n",
      "train: loss: 26394.431640625 acc: 0.965757429599762  val: loss: 1325213.75 acc: 0.6742355823516846\n",
      "step: 48590\n",
      "train: loss: 45786.796875 acc: 0.9684613347053528  val: loss: 653774.875 acc: 0.7487035989761353\n",
      "step: 48595\n",
      "train: loss: 35248.31640625 acc: 0.9842274188995361  val: loss: 397334.75 acc: 0.9035443067550659\n",
      "step: 48600\n",
      "train: loss: 16090.806640625 acc: 0.985318660736084  val: loss: 395262.3125 acc: 0.6309888362884521\n",
      "step: 48605\n",
      "train: loss: 55501.12890625 acc: 0.9775724411010742  val: loss: 215587.484375 acc: 0.9422951936721802\n",
      "step: 48610\n",
      "train: loss: 13822.32421875 acc: 0.9900500178337097  val: loss: 1120078.375 acc: 0.6981462240219116\n",
      "step: 48615\n",
      "train: loss: 9626.8935546875 acc: 0.9829438924789429  val: loss: 460103.9375 acc: 0.7771376371383667\n",
      "step: 48620\n",
      "train: loss: 22009.806640625 acc: 0.9845035076141357  val: loss: 381100.625 acc: 0.7806463837623596\n",
      "step: 48625\n",
      "train: loss: 18719.392578125 acc: 0.9932853579521179  val: loss: 880492.0 acc: 0.8148513436317444\n",
      "step: 48630\n",
      "train: loss: 23318.97265625 acc: 0.993281364440918  val: loss: 420239.5625 acc: 0.9285608530044556\n",
      "step: 48635\n",
      "train: loss: 25500.859375 acc: 0.9892258048057556  val: loss: 124642.625 acc: 0.9147722721099854\n",
      "step: 48640\n",
      "train: loss: 21551.4140625 acc: 0.9915047287940979  val: loss: 902010.75 acc: 0.6692463159561157\n",
      "step: 48645\n",
      "train: loss: 50939.421875 acc: 0.9878604412078857  val: loss: 1528822.875 acc: 0.4954279661178589\n",
      "step: 48650\n",
      "train: loss: 162513.71875 acc: 0.9193785190582275  val: loss: 451938.6875 acc: 0.7446184158325195\n",
      "step: 48655\n",
      "train: loss: 127746.234375 acc: 0.9527883529663086  val: loss: 172693.203125 acc: 0.9630423784255981\n",
      "step: 48660\n",
      "train: loss: 95036.6875 acc: 0.9758619070053101  val: loss: 338356.0625 acc: 0.9539164304733276\n",
      "step: 48665\n",
      "train: loss: 340574.5 acc: 0.9633913040161133  val: loss: 680670.375 acc: 0.8985307216644287\n",
      "step: 48670\n",
      "train: loss: 241354.734375 acc: 0.9808194637298584  val: loss: 1797645.875 acc: 0.8453682661056519\n",
      "step: 48675\n",
      "train: loss: 76569.4140625 acc: 0.9882344603538513  val: loss: 2313868.5 acc: 0.5896363258361816\n",
      "step: 48680\n",
      "train: loss: 196295.578125 acc: 0.9533697962760925  val: loss: 1524410.375 acc: 0.8293818235397339\n",
      "step: 48685\n",
      "train: loss: 128538.4453125 acc: 0.955295205116272  val: loss: 866167.8125 acc: 0.8102506995201111\n",
      "step: 48690\n",
      "train: loss: 848354.5 acc: 0.9651468992233276  val: loss: 780782.5 acc: 0.8929641246795654\n",
      "step: 48695\n",
      "train: loss: 248285.828125 acc: 0.9745944738388062  val: loss: 351986.59375 acc: 0.9056005477905273\n",
      "step: 48700\n",
      "train: loss: 256716.703125 acc: 0.978899359703064  val: loss: 1388754.125 acc: 0.7928155660629272\n",
      "step: 48705\n",
      "train: loss: 256546.921875 acc: 0.9802843332290649  val: loss: 3439500.75 acc: 0.6786452531814575\n",
      "step: 48710\n",
      "train: loss: 434245.03125 acc: 0.9857292771339417  val: loss: 1015801.1875 acc: 0.7371597290039062\n",
      "step: 48715\n",
      "train: loss: 929869.6875 acc: 0.9726927280426025  val: loss: 1629052.375 acc: 0.8491643667221069\n",
      "step: 48720\n",
      "train: loss: 229745.34375 acc: 0.9873635172843933  val: loss: 447157.46875 acc: 0.594630241394043\n",
      "step: 48725\n",
      "train: loss: 616083.0 acc: 0.9576997756958008  val: loss: 755448.0 acc: 0.8369436264038086\n",
      "step: 48730\n",
      "train: loss: 1005685.1875 acc: 0.8881663084030151  val: loss: 1162718.125 acc: 0.630528450012207\n",
      "step: 48735\n",
      "train: loss: 551315.0625 acc: 0.8598020076751709  val: loss: 839299.125 acc: 0.7410493493080139\n",
      "step: 48740\n",
      "train: loss: 203768.0 acc: 0.9389990568161011  val: loss: 2491843.25 acc: 0.6740442514419556\n",
      "step: 48745\n",
      "train: loss: 928763.125 acc: 0.6952816247940063  val: loss: 552370.1875 acc: 0.6683212518692017\n",
      "step: 48750\n",
      "train: loss: 574982.6875 acc: 0.7020179033279419  val: loss: 882991.0 acc: 0.7974365949630737\n",
      "step: 48755\n",
      "train: loss: 646502.6875 acc: 0.7297549247741699  val: loss: 3332411.25 acc: 0.6456925272941589\n",
      "step: 48760\n",
      "train: loss: 415453.90625 acc: 0.8537642955780029  val: loss: 2617904.25 acc: 0.6677674055099487\n",
      "step: 48765\n",
      "train: loss: 1034141.1875 acc: 0.4435141086578369  val: loss: 3400239.75 acc: 0.6021192073822021\n",
      "step: 48770\n",
      "train: loss: 556599.875 acc: 0.7904306650161743  val: loss: 7087713.0 acc: 0.2575594186782837\n",
      "step: 48775\n",
      "train: loss: 217335.453125 acc: 0.8753594160079956  val: loss: 1778362.375 acc: 0.5032939910888672\n",
      "step: 48780\n",
      "train: loss: 72855.0390625 acc: 0.9432493448257446  val: loss: 1630324.375 acc: 0.616642415523529\n",
      "step: 48785\n",
      "train: loss: 134953.859375 acc: 0.9099959135055542  val: loss: 559496.0 acc: 0.6997983455657959\n",
      "step: 48790\n",
      "train: loss: 32717.740234375 acc: 0.9725034236907959  val: loss: 7097158.5 acc: 0.21935266256332397\n",
      "step: 48795\n",
      "train: loss: 140856.015625 acc: 0.914051353931427  val: loss: 94084.03125 acc: 0.9187861680984497\n",
      "step: 48800\n",
      "train: loss: 198395.046875 acc: 0.882163941860199  val: loss: 1197068.625 acc: 0.590946614742279\n",
      "step: 48805\n",
      "train: loss: 457988.53125 acc: 0.8025103211402893  val: loss: 2366063.0 acc: 0.6445869207382202\n",
      "step: 48810\n",
      "train: loss: 358838.78125 acc: 0.7493597269058228  val: loss: 1499814.125 acc: 0.5775976181030273\n",
      "step: 48815\n",
      "train: loss: 28298.541015625 acc: 0.9743194580078125  val: loss: 561427.1875 acc: 0.7624483704566956\n",
      "step: 48820\n",
      "train: loss: 156731.953125 acc: 0.8775593638420105  val: loss: 2249712.0 acc: 0.6312850713729858\n",
      "step: 48825\n",
      "train: loss: 182284.265625 acc: 0.8766244053840637  val: loss: 83716.640625 acc: 0.9611611366271973\n",
      "step: 48830\n",
      "train: loss: 653410.3125 acc: 0.7350189685821533  val: loss: 1276078.625 acc: 0.7042526602745056\n",
      "step: 48835\n",
      "train: loss: 209312.96875 acc: 0.85597825050354  val: loss: 936979.875 acc: 0.6686868667602539\n",
      "step: 48840\n",
      "train: loss: 336083.875 acc: 0.8431288003921509  val: loss: 618084.625 acc: 0.7464256882667542\n",
      "step: 48845\n",
      "train: loss: 43948.70703125 acc: 0.960521936416626  val: loss: 2991574.0 acc: 0.3473556637763977\n",
      "step: 48850\n",
      "train: loss: 111439.0703125 acc: 0.9316966533660889  val: loss: 1113981.625 acc: 0.743094801902771\n",
      "step: 48855\n",
      "train: loss: 624584.75 acc: 0.8606887459754944  val: loss: 2227235.5 acc: 0.7557827234268188\n",
      "step: 48860\n",
      "train: loss: 1397183.625 acc: 0.8348304033279419  val: loss: 884803.1875 acc: 0.683890700340271\n",
      "step: 48865\n",
      "train: loss: 48196.5546875 acc: 0.9948854446411133  val: loss: 249354.109375 acc: 0.9506392478942871\n",
      "step: 48870\n",
      "train: loss: 180597.96875 acc: 0.9790908098220825  val: loss: 411684.09375 acc: 0.918502688407898\n",
      "step: 48875\n",
      "train: loss: 89009.03125 acc: 0.9792803525924683  val: loss: 730380.9375 acc: 0.6271854639053345\n",
      "step: 48880\n",
      "train: loss: 242759.171875 acc: 0.9515103697776794  val: loss: 1237662.25 acc: 0.5815771818161011\n",
      "step: 48885\n",
      "train: loss: 128040.25 acc: 0.9889090061187744  val: loss: 1759577.0 acc: 0.42857736349105835\n",
      "step: 48890\n",
      "train: loss: 102139.515625 acc: 0.9912807941436768  val: loss: 183244.265625 acc: 0.9311116933822632\n",
      "step: 48895\n",
      "train: loss: 62061.53125 acc: 0.9934462904930115  val: loss: 356925.40625 acc: 0.817031741142273\n",
      "step: 48900\n",
      "train: loss: 67758.1796875 acc: 0.9890024662017822  val: loss: 875163.5625 acc: 0.7507433295249939\n",
      "step: 48905\n",
      "train: loss: 16710.9609375 acc: 0.9809034466743469  val: loss: 240819.4375 acc: 0.9231991171836853\n",
      "step: 48910\n",
      "train: loss: 95330.7578125 acc: 0.9819962382316589  val: loss: 276675.25 acc: 0.9255489110946655\n",
      "step: 48915\n",
      "train: loss: 24953.857421875 acc: 0.9832852482795715  val: loss: 1211433.625 acc: 0.6537637114524841\n",
      "step: 48920\n",
      "train: loss: 13167.056640625 acc: 0.976872980594635  val: loss: 817276.875 acc: 0.6564068794250488\n",
      "step: 48925\n",
      "train: loss: 12514.337890625 acc: 0.9955518245697021  val: loss: 1116856.875 acc: 0.7139469981193542\n",
      "step: 48930\n",
      "train: loss: 8963.22265625 acc: 0.9542237520217896  val: loss: 78968.4453125 acc: 0.987689733505249\n",
      "step: 48935\n",
      "train: loss: 7540.1455078125 acc: 0.9574885368347168  val: loss: 90704.5859375 acc: 0.9811134338378906\n",
      "step: 48940\n",
      "train: loss: 23212.3515625 acc: 0.934169590473175  val: loss: 966704.125 acc: 0.6647976040840149\n",
      "step: 48945\n",
      "train: loss: 45378.484375 acc: 0.9655502438545227  val: loss: 216066.1875 acc: 0.916549563407898\n",
      "step: 48950\n",
      "train: loss: 18707.990234375 acc: 0.9771881103515625  val: loss: 1834492.875 acc: 0.7987945079803467\n",
      "step: 48955\n",
      "train: loss: 30799.91796875 acc: 0.9761176705360413  val: loss: 691307.875 acc: 0.8334715366363525\n",
      "step: 48960\n",
      "train: loss: 47603.48828125 acc: 0.9763876795768738  val: loss: 695772.0 acc: 0.6471089720726013\n",
      "step: 48965\n",
      "train: loss: 17035.595703125 acc: 0.9910714030265808  val: loss: 301240.3125 acc: 0.9047995805740356\n",
      "step: 48970\n",
      "train: loss: 38533.92578125 acc: 0.9823766946792603  val: loss: 1381867.875 acc: 0.8467548489570618\n",
      "step: 48975\n",
      "train: loss: 95455.9296875 acc: 0.9374011158943176  val: loss: 150360.796875 acc: 0.9753975868225098\n",
      "step: 48980\n",
      "train: loss: 28907.373046875 acc: 0.9828208088874817  val: loss: 916269.875 acc: 0.8381568789482117\n",
      "step: 48985\n",
      "train: loss: 14611.4384765625 acc: 0.9806368947029114  val: loss: 962131.5625 acc: 0.8936775326728821\n",
      "step: 48990\n",
      "train: loss: 20856.52734375 acc: 0.9933752417564392  val: loss: 1172028.25 acc: 0.9183107614517212\n",
      "step: 48995\n",
      "train: loss: 37651.75390625 acc: 0.990206241607666  val: loss: 2034214.125 acc: 0.7773863077163696\n",
      "step: 49000\n",
      "train: loss: 19166.548828125 acc: 0.9954683780670166  val: loss: 1850460.875 acc: 0.7131168842315674\n",
      "step: 49005\n",
      "train: loss: 57947.1640625 acc: 0.9849428534507751  val: loss: 965400.0 acc: 0.8452168703079224\n",
      "step: 49010\n",
      "train: loss: 51201.765625 acc: 0.9879664182662964  val: loss: 1304082.625 acc: 0.840193510055542\n",
      "step: 49015\n",
      "train: loss: 105430.953125 acc: 0.9582982659339905  val: loss: 1549623.25 acc: 0.83908611536026\n",
      "step: 49020\n",
      "train: loss: 112365.265625 acc: 0.9604737162590027  val: loss: 1040717.875 acc: 0.6568653583526611\n",
      "step: 49025\n",
      "train: loss: 266307.40625 acc: 0.8964524865150452  val: loss: 1782560.625 acc: 0.7796725034713745\n",
      "step: 49030\n",
      "train: loss: 592444.625 acc: 0.9025917053222656  val: loss: 268421.71875 acc: 0.8775826096534729\n",
      "step: 49035\n",
      "train: loss: 825183.75 acc: 0.9160453081130981  val: loss: 1179177.625 acc: 0.7855619788169861\n",
      "step: 49040\n",
      "train: loss: 36767.03515625 acc: 0.995997965335846  val: loss: 941956.0 acc: 0.6968507766723633\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 49045\n",
      "train: loss: 73590.6328125 acc: 0.9873536825180054  val: loss: 986979.875 acc: 0.8659175634384155\n",
      "step: 49050\n",
      "train: loss: 207905.75 acc: 0.9668378829956055  val: loss: 644888.9375 acc: 0.9145179986953735\n",
      "step: 49055\n",
      "train: loss: 426143.53125 acc: 0.9661587476730347  val: loss: 1896065.5 acc: -0.3816763162612915\n",
      "step: 49060\n",
      "train: loss: 625407.4375 acc: 0.9650444388389587  val: loss: 611719.75 acc: 0.9431138038635254\n",
      "step: 49065\n",
      "train: loss: 181149.578125 acc: 0.9637241959571838  val: loss: 1994467.125 acc: -0.08198809623718262\n",
      "step: 49070\n",
      "train: loss: 715732.1875 acc: 0.9528629779815674  val: loss: 2097939.0 acc: 0.6727315187454224\n",
      "step: 49075\n",
      "train: loss: 567407.0625 acc: 0.9775909781455994  val: loss: 669091.4375 acc: 0.9121337532997131\n",
      "step: 49080\n",
      "train: loss: 900122.6875 acc: 0.9707679152488708  val: loss: 719494.75 acc: 0.3829955458641052\n",
      "step: 49085\n",
      "train: loss: 812416.375 acc: 0.9524650573730469  val: loss: 1227023.125 acc: 0.8823916912078857\n",
      "step: 49090\n",
      "train: loss: 1184660.75 acc: 0.9383244514465332  val: loss: 2126328.75 acc: -0.18639934062957764\n",
      "step: 49095\n",
      "train: loss: 318233.9375 acc: 0.9481678605079651  val: loss: 452775.71875 acc: 0.8559677600860596\n",
      "step: 49100\n",
      "train: loss: 474361.15625 acc: 0.9564108848571777  val: loss: 1174139.5 acc: 0.8101657629013062\n",
      "step: 49105\n",
      "train: loss: 727546.1875 acc: 0.8334388732910156  val: loss: 1214204.875 acc: 0.4163271188735962\n",
      "step: 49110\n",
      "train: loss: 1642133.125 acc: 0.6750324964523315  val: loss: 798398.9375 acc: 0.824272871017456\n",
      "step: 49115\n",
      "train: loss: 486527.90625 acc: 0.7605071663856506  val: loss: 908290.0625 acc: 0.7096764445304871\n",
      "step: 49120\n",
      "train: loss: 375917.15625 acc: 0.8426905274391174  val: loss: 831619.6875 acc: 0.7574878931045532\n",
      "step: 49125\n",
      "train: loss: 459276.375 acc: 0.8710747957229614  val: loss: 1167814.125 acc: 0.8099448084831238\n",
      "step: 49130\n",
      "train: loss: 585751.6875 acc: 0.775994598865509  val: loss: 1645268.0 acc: 0.7299469709396362\n",
      "step: 49135\n",
      "train: loss: 380710.5625 acc: 0.8586446642875671  val: loss: 2663377.75 acc: 0.5371763706207275\n",
      "step: 49140\n",
      "train: loss: 79205.6328125 acc: 0.944618821144104  val: loss: 3938559.75 acc: 0.4885483980178833\n",
      "step: 49145\n",
      "train: loss: 438216.71875 acc: 0.780505359172821  val: loss: 1363627.125 acc: 0.6671425700187683\n",
      "step: 49150\n",
      "train: loss: 103743.3984375 acc: 0.8937763571739197  val: loss: 3136814.25 acc: 0.5008479952812195\n",
      "step: 49155\n",
      "train: loss: 536954.25 acc: 0.7304424047470093  val: loss: 3027662.5 acc: 0.37801676988601685\n",
      "step: 49160\n",
      "train: loss: 85806.734375 acc: 0.9362843632698059  val: loss: 3848585.75 acc: 0.3770989179611206\n",
      "step: 49165\n",
      "train: loss: 214921.078125 acc: 0.8712396621704102  val: loss: 2657095.75 acc: 0.5699676275253296\n",
      "step: 49170\n",
      "train: loss: 180136.875 acc: 0.8930720686912537  val: loss: 6434747.5 acc: 0.3665987253189087\n",
      "step: 49175\n",
      "train: loss: 187177.0 acc: 0.8677529692649841  val: loss: 1282982.25 acc: 0.6296977400779724\n",
      "step: 49180\n",
      "train: loss: 61739.68359375 acc: 0.9332433938980103  val: loss: 877574.5625 acc: 0.7535648345947266\n",
      "step: 49185\n",
      "train: loss: 48894.3203125 acc: 0.9384803771972656  val: loss: 651164.3125 acc: 0.753852903842926\n",
      "step: 49190\n",
      "train: loss: 78343.1953125 acc: 0.9492441415786743  val: loss: 972330.0625 acc: 0.6716926097869873\n",
      "step: 49195\n",
      "train: loss: 895420.5625 acc: 0.7183501124382019  val: loss: 117935.1484375 acc: 0.8911630511283875\n",
      "step: 49200\n",
      "train: loss: 441265.96875 acc: 0.741258978843689  val: loss: 316993.09375 acc: 0.8429444432258606\n",
      "step: 49205\n",
      "train: loss: 193090.859375 acc: 0.8407821655273438  val: loss: 2751537.25 acc: 0.3393022418022156\n",
      "step: 49210\n",
      "train: loss: 87374.6484375 acc: 0.9123966693878174  val: loss: 1147623.75 acc: 0.7322629690170288\n",
      "step: 49215\n",
      "train: loss: 572456.0625 acc: 0.7851674556732178  val: loss: 1244621.25 acc: 0.7304775714874268\n",
      "step: 49220\n",
      "train: loss: 801406.5 acc: 0.8314934372901917  val: loss: 498326.5625 acc: 0.9319684505462646\n",
      "step: 49225\n",
      "train: loss: 788490.0625 acc: 0.8575701713562012  val: loss: 656070.0 acc: 0.8647826313972473\n",
      "step: 49230\n",
      "train: loss: 239157.328125 acc: 0.9726261496543884  val: loss: 516109.875 acc: 0.8842465877532959\n",
      "step: 49235\n",
      "train: loss: 416026.125 acc: 0.9529250860214233  val: loss: 46284.44921875 acc: 0.9790699481964111\n",
      "step: 49240\n",
      "train: loss: 91066.4765625 acc: 0.9888486266136169  val: loss: 291901.5 acc: 0.8888022899627686\n",
      "step: 49245\n",
      "train: loss: 66148.1484375 acc: 0.9881319999694824  val: loss: 1048340.375 acc: 0.8728885650634766\n",
      "step: 49250\n",
      "train: loss: 131982.203125 acc: 0.9884880781173706  val: loss: 141040.078125 acc: 0.9728042483329773\n",
      "step: 49255\n",
      "train: loss: 61921.91015625 acc: 0.995201587677002  val: loss: 709250.3125 acc: 0.7932766675949097\n",
      "step: 49260\n",
      "train: loss: 79360.75 acc: 0.9934672117233276  val: loss: 650903.0625 acc: 0.9330816864967346\n",
      "step: 49265\n",
      "train: loss: 46762.33203125 acc: 0.9936292171478271  val: loss: 577760.6875 acc: 0.8914813995361328\n",
      "step: 49270\n",
      "train: loss: 63558.51953125 acc: 0.989842414855957  val: loss: 498830.71875 acc: 0.803236722946167\n",
      "step: 49275\n",
      "train: loss: 11612.419921875 acc: 0.9949019551277161  val: loss: 938142.3125 acc: 0.8428089022636414\n",
      "step: 49280\n",
      "train: loss: 13979.93359375 acc: 0.9938010573387146  val: loss: 722644.9375 acc: 0.8695331811904907\n",
      "step: 49285\n",
      "train: loss: 22433.443359375 acc: 0.9839353561401367  val: loss: 1445039.625 acc: 0.8645778298377991\n",
      "step: 49290\n",
      "train: loss: 18679.5546875 acc: 0.9655289649963379  val: loss: 1076094.75 acc: 0.013607919216156006\n",
      "step: 49295\n",
      "train: loss: 18031.361328125 acc: 0.9674878120422363  val: loss: 842355.25 acc: 0.8609957098960876\n",
      "step: 49300\n",
      "train: loss: 5875.1044921875 acc: 0.973309338092804  val: loss: 1069131.75 acc: 0.8527950644493103\n",
      "step: 49305\n",
      "train: loss: 6879.83984375 acc: 0.9808999300003052  val: loss: 1622218.5 acc: 0.7382352352142334\n",
      "step: 49310\n",
      "train: loss: 6726.77099609375 acc: 0.9792765974998474  val: loss: 1526108.75 acc: 0.8395177721977234\n",
      "step: 49315\n",
      "train: loss: 13065.0703125 acc: 0.9534568190574646  val: loss: 902154.875 acc: 0.787534236907959\n",
      "step: 49320\n",
      "train: loss: 25135.166015625 acc: 0.9802804589271545  val: loss: 1565834.125 acc: 0.8209121227264404\n",
      "step: 49325\n",
      "train: loss: 34252.33984375 acc: 0.9789315462112427  val: loss: 1854397.875 acc: 0.8042702078819275\n",
      "step: 49330\n",
      "train: loss: 41541.6015625 acc: 0.9808653593063354  val: loss: 2912141.75 acc: 0.759864091873169\n",
      "step: 49335\n",
      "train: loss: 14953.6748046875 acc: 0.9886983036994934  val: loss: 1924966.125 acc: 0.16513562202453613\n",
      "step: 49340\n",
      "train: loss: 107503.53125 acc: 0.9679867625236511  val: loss: 1610729.125 acc: 0.4056973457336426\n",
      "step: 49345\n",
      "train: loss: 19876.939453125 acc: 0.9875186681747437  val: loss: 1687281.0 acc: 0.650302529335022\n",
      "step: 49350\n",
      "train: loss: 10447.181640625 acc: 0.9905182719230652  val: loss: 2638297.75 acc: 0.7823353409767151\n",
      "step: 49355\n",
      "train: loss: 7829.53173828125 acc: 0.9954038858413696  val: loss: 1528491.375 acc: 0.718848705291748\n",
      "step: 49360\n",
      "train: loss: 12270.28125 acc: 0.9966831207275391  val: loss: 1066548.125 acc: 0.9095214605331421\n",
      "step: 49365\n",
      "train: loss: 22239.19140625 acc: 0.9943797588348389  val: loss: 1917079.625 acc: 0.496518611907959\n",
      "step: 49370\n",
      "train: loss: 19661.904296875 acc: 0.9922181963920593  val: loss: 1623085.125 acc: 0.09574532508850098\n",
      "step: 49375\n",
      "train: loss: 25083.21484375 acc: 0.9901983141899109  val: loss: 1099385.0 acc: 0.756293535232544\n",
      "step: 49380\n",
      "train: loss: 190928.140625 acc: 0.9556171894073486  val: loss: 1432117.5 acc: 0.7944889664649963\n",
      "step: 49385\n",
      "train: loss: 77448.21875 acc: 0.9811537265777588  val: loss: 818702.25 acc: 0.854941725730896\n",
      "step: 49390\n",
      "train: loss: 61300.02734375 acc: 0.9617105722427368  val: loss: 1427433.125 acc: 0.7213551998138428\n",
      "step: 49395\n",
      "train: loss: 289252.0625 acc: 0.9644879102706909  val: loss: 1913220.625 acc: 0.4735729694366455\n",
      "step: 49400\n",
      "train: loss: 86044.65625 acc: 0.9934844970703125  val: loss: 2014789.875 acc: -0.6958298683166504\n",
      "step: 49405\n",
      "train: loss: 79734.1171875 acc: 0.9891340732574463  val: loss: 2179971.25 acc: 0.4999814033508301\n",
      "step: 49410\n",
      "train: loss: 40393.73046875 acc: 0.9937761425971985  val: loss: 781196.6875 acc: 0.7199975252151489\n",
      "step: 49415\n",
      "train: loss: 805877.8125 acc: 0.9224915504455566  val: loss: 1967756.25 acc: 0.59397292137146\n",
      "step: 49420\n",
      "train: loss: 721786.9375 acc: 0.9655872583389282  val: loss: 493041.59375 acc: 0.6772683262825012\n",
      "step: 49425\n",
      "train: loss: 564339.625 acc: 0.8995879292488098  val: loss: 690480.75 acc: 0.9123097658157349\n",
      "step: 49430\n",
      "train: loss: 448749.46875 acc: 0.967518150806427  val: loss: 969533.375 acc: 0.6981604695320129\n",
      "step: 49435\n",
      "train: loss: 475736.71875 acc: 0.9733045697212219  val: loss: 3636963.75 acc: -0.21580767631530762\n",
      "step: 49440\n",
      "train: loss: 848164.5 acc: 0.9619441032409668  val: loss: 839880.3125 acc: 0.8930661082267761\n",
      "step: 49445\n",
      "train: loss: 791373.875 acc: 0.9691159129142761  val: loss: 995522.75 acc: 0.8942313194274902\n",
      "step: 49450\n",
      "train: loss: 991064.0 acc: 0.9623855352401733  val: loss: 567841.0 acc: 0.8793392181396484\n",
      "step: 49455\n",
      "train: loss: 377437.65625 acc: 0.9664290547370911  val: loss: 515639.96875 acc: 0.9319354295730591\n",
      "step: 49460\n",
      "train: loss: 212189.34375 acc: 0.9774669408798218  val: loss: 287892.15625 acc: 0.9248395562171936\n",
      "step: 49465\n",
      "train: loss: 718241.1875 acc: 0.9062703251838684  val: loss: 398483.71875 acc: 0.9056938886642456\n",
      "step: 49470\n",
      "train: loss: 1065037.375 acc: 0.8755049705505371  val: loss: 1154690.25 acc: 0.6878581047058105\n",
      "step: 49475\n",
      "train: loss: 2766509.25 acc: 0.16997474431991577  val: loss: 1286751.875 acc: 0.6643176078796387\n",
      "step: 49480\n",
      "train: loss: 793236.8125 acc: 0.7020453214645386  val: loss: 728524.125 acc: 0.7635847330093384\n",
      "step: 49485\n",
      "train: loss: 505422.3125 acc: 0.8336353302001953  val: loss: 770487.5 acc: 0.6856335401535034\n",
      "step: 49490\n",
      "train: loss: 372633.78125 acc: 0.857525110244751  val: loss: 2114874.5 acc: 0.6805515289306641\n",
      "step: 49495\n",
      "train: loss: 1059645.125 acc: 0.5271733999252319  val: loss: 911529.8125 acc: 0.839885413646698\n",
      "step: 49500\n",
      "train: loss: 291616.84375 acc: 0.8317365646362305  val: loss: 1059112.125 acc: 0.515385627746582\n",
      "step: 49505\n",
      "train: loss: 104728.984375 acc: 0.937142014503479  val: loss: 1142944.25 acc: 0.5982479453086853\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 49510\n",
      "train: loss: 506312.4375 acc: 0.7689269781112671  val: loss: 3224009.5 acc: 0.4991534948348999\n",
      "step: 49515\n",
      "train: loss: 117320.6640625 acc: 0.9033676981925964  val: loss: 2657293.75 acc: 0.434295117855072\n",
      "step: 49520\n",
      "train: loss: 116794.203125 acc: 0.913696825504303  val: loss: 2115218.25 acc: 0.6019682884216309\n",
      "step: 49525\n",
      "train: loss: 63823.72265625 acc: 0.9451017379760742  val: loss: 1537460.375 acc: 0.545351505279541\n",
      "step: 49530\n",
      "train: loss: 201350.203125 acc: 0.882332444190979  val: loss: 7017842.5 acc: 0.21188223361968994\n",
      "step: 49535\n",
      "train: loss: 301876.8125 acc: 0.843914270401001  val: loss: 1823425.75 acc: 0.5861752033233643\n",
      "step: 49540\n",
      "train: loss: 400736.21875 acc: 0.7869415879249573  val: loss: 848184.1875 acc: 0.6807624101638794\n",
      "step: 49545\n",
      "train: loss: 316036.4375 acc: 0.8264784812927246  val: loss: 1065501.25 acc: 0.5834380388259888\n",
      "step: 49550\n",
      "train: loss: 207507.046875 acc: 0.7788923978805542  val: loss: 1646373.125 acc: 0.5856924057006836\n",
      "step: 49555\n",
      "train: loss: 103303.765625 acc: 0.907765805721283  val: loss: 634136.375 acc: 0.8100136518478394\n",
      "step: 49560\n",
      "train: loss: 50288.51953125 acc: 0.959417998790741  val: loss: 229249.859375 acc: 0.8662717342376709\n",
      "step: 49565\n",
      "train: loss: 552398.9375 acc: 0.7929593324661255  val: loss: 1599522.375 acc: 0.6912423968315125\n",
      "step: 49570\n",
      "train: loss: 552100.4375 acc: 0.7314630746841431  val: loss: 226519.328125 acc: 0.8494377732276917\n",
      "step: 49575\n",
      "train: loss: 568023.5625 acc: 0.725861668586731  val: loss: 5088984.5 acc: 0.44083380699157715\n",
      "step: 49580\n",
      "train: loss: 409752.65625 acc: 0.7457864284515381  val: loss: 585870.5625 acc: 0.7377808094024658\n",
      "step: 49585\n",
      "train: loss: 1798319.125 acc: 0.7202260494232178  val: loss: 1903291.75 acc: 0.7608471512794495\n",
      "step: 49590\n",
      "train: loss: 865396.6875 acc: 0.8741591572761536  val: loss: 1815855.625 acc: 0.7235973477363586\n",
      "step: 49595\n",
      "train: loss: 361602.09375 acc: 0.960436999797821  val: loss: 974536.125 acc: 0.8992879390716553\n",
      "step: 49600\n",
      "train: loss: 132153.9375 acc: 0.9889287352561951  val: loss: 1651836.125 acc: 0.899483859539032\n",
      "step: 49605\n",
      "train: loss: 200352.53125 acc: 0.9732002019882202  val: loss: 502204.375 acc: 0.8488336801528931\n",
      "step: 49610\n",
      "train: loss: 74560.53125 acc: 0.9883278012275696  val: loss: 1045718.875 acc: 0.9144303798675537\n",
      "step: 49615\n",
      "train: loss: 55932.22265625 acc: 0.9950502514839172  val: loss: 239788.953125 acc: 0.9452863931655884\n",
      "step: 49620\n",
      "train: loss: 544004.1875 acc: 0.9591367840766907  val: loss: 2052558.375 acc: 0.7807162404060364\n",
      "step: 49625\n",
      "train: loss: 113975.5859375 acc: 0.990066409111023  val: loss: 547779.0625 acc: 0.9046674966812134\n",
      "step: 49630\n",
      "train: loss: 271130.625 acc: 0.9661446809768677  val: loss: 1195023.375 acc: 0.6461886167526245\n",
      "step: 49635\n",
      "train: loss: 39748.5234375 acc: 0.9904446601867676  val: loss: 1758459.375 acc: 0.7628811001777649\n",
      "step: 49640\n",
      "train: loss: 17674.376953125 acc: 0.995303213596344  val: loss: 933168.625 acc: 0.8639257550239563\n",
      "step: 49645\n",
      "train: loss: 24298.654296875 acc: 0.9787201285362244  val: loss: 2526380.75 acc: 0.6070247888565063\n",
      "step: 49650\n",
      "train: loss: 16971.82421875 acc: 0.9852743744850159  val: loss: 1177401.375 acc: 0.3071204423904419\n",
      "step: 49655\n",
      "train: loss: 3771.4755859375 acc: 0.992185652256012  val: loss: 1433349.875 acc: 0.34329044818878174\n",
      "step: 49660\n",
      "train: loss: 167729.890625 acc: 0.7458850145339966  val: loss: 1184661.875 acc: 0.5896053314208984\n",
      "step: 49665\n",
      "train: loss: 11506.732421875 acc: 0.9814497828483582  val: loss: 1177570.625 acc: 0.8842160701751709\n",
      "step: 49670\n",
      "train: loss: 8290.1220703125 acc: 0.9792373776435852  val: loss: 835864.875 acc: 0.6519295573234558\n",
      "step: 49675\n",
      "train: loss: 16747.31640625 acc: 0.9591161012649536  val: loss: 693977.0625 acc: 0.8815102577209473\n",
      "step: 49680\n",
      "train: loss: 10186.4169921875 acc: 0.9808363318443298  val: loss: 656205.0 acc: 0.4839804172515869\n",
      "step: 49685\n",
      "train: loss: 22723.806640625 acc: 0.9712119102478027  val: loss: 1488988.125 acc: 0.8417908549308777\n",
      "step: 49690\n",
      "train: loss: 26235.21875 acc: 0.9803908467292786  val: loss: 1071953.125 acc: 0.8694627285003662\n",
      "step: 49695\n",
      "train: loss: 13086.82421875 acc: 0.9871123433113098  val: loss: 350161.28125 acc: 0.8940041065216064\n",
      "step: 49700\n",
      "train: loss: 19120.455078125 acc: 0.986089289188385  val: loss: 2066599.875 acc: 0.6271958947181702\n",
      "step: 49705\n",
      "train: loss: 35826.77734375 acc: 0.9768990278244019  val: loss: 2485144.5 acc: -0.80037522315979\n",
      "step: 49710\n",
      "train: loss: 12979.4150390625 acc: 0.9763970971107483  val: loss: 3303901.75 acc: -0.26105356216430664\n",
      "step: 49715\n",
      "train: loss: 21030.740234375 acc: 0.9765070676803589  val: loss: 2399963.25 acc: 0.21125787496566772\n",
      "step: 49720\n",
      "train: loss: 25767.453125 acc: 0.9856263399124146  val: loss: 1667985.375 acc: 0.7788724899291992\n",
      "step: 49725\n",
      "train: loss: 32635.388671875 acc: 0.9902791976928711  val: loss: 1810213.125 acc: 0.5725176334381104\n",
      "step: 49730\n",
      "train: loss: 17462.376953125 acc: 0.9937654733657837  val: loss: 3080023.75 acc: 0.6695581674575806\n",
      "step: 49735\n",
      "train: loss: 28553.447265625 acc: 0.989291787147522  val: loss: 1822290.375 acc: 0.6935498118400574\n",
      "step: 49740\n",
      "train: loss: 35591.64453125 acc: 0.9832301735877991  val: loss: 3096202.25 acc: 0.2118011713027954\n",
      "step: 49745\n",
      "train: loss: 224079.578125 acc: 0.9640100598335266  val: loss: 455981.625 acc: 0.9119963049888611\n",
      "step: 49750\n",
      "train: loss: 112323.71875 acc: 0.9764343500137329  val: loss: 499668.25 acc: 0.8636070489883423\n",
      "step: 49755\n",
      "train: loss: 119374.703125 acc: 0.9262056946754456  val: loss: 1529926.875 acc: 0.6283312439918518\n",
      "step: 49760\n",
      "train: loss: 61123.77734375 acc: 0.9856509566307068  val: loss: 1644926.625 acc: 0.41552627086639404\n",
      "step: 49765\n",
      "train: loss: 49790.7578125 acc: 0.9958147406578064  val: loss: 233919.921875 acc: 0.9379338622093201\n",
      "step: 49770\n",
      "train: loss: 620365.5625 acc: 0.9311584234237671  val: loss: 579040.875 acc: 0.8510792851448059\n",
      "step: 49775\n",
      "train: loss: 119963.1796875 acc: 0.9841137528419495  val: loss: 473103.0625 acc: 0.865564227104187\n",
      "step: 49780\n",
      "train: loss: 498643.875 acc: 0.9441961646080017  val: loss: 579946.8125 acc: 0.7910170555114746\n",
      "step: 49785\n",
      "train: loss: 555289.9375 acc: 0.9234743714332581  val: loss: 799662.625 acc: 0.7667602896690369\n",
      "step: 49790\n",
      "train: loss: 549869.0 acc: 0.9457768797874451  val: loss: 818288.5 acc: -0.05629932880401611\n",
      "step: 49795\n",
      "train: loss: 385342.875 acc: 0.9306470155715942  val: loss: 1010170.0625 acc: 0.8674541711807251\n",
      "step: 49800\n",
      "train: loss: 553488.125 acc: 0.968045711517334  val: loss: 1371149.125 acc: 0.702332079410553\n",
      "step: 49805\n",
      "train: loss: 134225.296875 acc: 0.987205982208252  val: loss: 1672550.375 acc: 0.6690552234649658\n",
      "step: 49810\n",
      "train: loss: 2297029.75 acc: 0.9415676593780518  val: loss: 367377.96875 acc: 0.6290441751480103\n",
      "step: 49815\n",
      "train: loss: 1022237.75 acc: 0.9654065370559692  val: loss: 264945.40625 acc: 0.8814943432807922\n",
      "step: 49820\n",
      "train: loss: 194973.734375 acc: 0.9793413877487183  val: loss: 517716.875 acc: 0.9404820203781128\n",
      "step: 49825\n",
      "train: loss: 424405.78125 acc: 0.9559549689292908  val: loss: 854412.75 acc: -0.006814360618591309\n",
      "step: 49830\n",
      "train: loss: 266496.40625 acc: 0.9775840044021606  val: loss: 1429476.125 acc: 0.48106199502944946\n",
      "step: 49835\n",
      "train: loss: 1209866.125 acc: 0.8400381803512573  val: loss: 226176.640625 acc: 0.8994867205619812\n",
      "step: 49840\n",
      "train: loss: 1002128.125 acc: 0.6552058458328247  val: loss: 604842.875 acc: 0.7569080591201782\n",
      "step: 49845\n",
      "train: loss: 989895.9375 acc: 0.33568471670150757  val: loss: 770050.75 acc: 0.7966691255569458\n",
      "step: 49850\n",
      "train: loss: 978563.0625 acc: 0.3464922308921814  val: loss: 1564989.625 acc: 0.8100863695144653\n",
      "step: 49855\n",
      "train: loss: 518242.21875 acc: 0.890203058719635  val: loss: 1595197.5 acc: 0.6767591238021851\n",
      "step: 49860\n",
      "train: loss: 909182.5625 acc: 0.658901035785675  val: loss: 388715.0 acc: 0.913449764251709\n",
      "step: 49865\n",
      "train: loss: 1628833.625 acc: 0.2767106294631958  val: loss: 976121.1875 acc: 0.6742656826972961\n",
      "step: 49870\n",
      "train: loss: 440552.75 acc: 0.7466345429420471  val: loss: 3848846.75 acc: 0.6362416744232178\n",
      "step: 49875\n",
      "train: loss: 403733.71875 acc: 0.6034083366394043  val: loss: 2965579.75 acc: 0.4504416584968567\n",
      "step: 49880\n",
      "train: loss: 417216.65625 acc: 0.6215319633483887  val: loss: 1933282.375 acc: 0.6082570552825928\n",
      "step: 49885\n",
      "train: loss: 407980.1875 acc: 0.7464601993560791  val: loss: 830966.4375 acc: 0.5655583143234253\n",
      "step: 49890\n",
      "train: loss: 186428.546875 acc: 0.8711706399917603  val: loss: 622581.9375 acc: 0.78151535987854\n",
      "step: 49895\n",
      "train: loss: 193015.453125 acc: 0.8423037528991699  val: loss: 1114039.0 acc: 0.7236816883087158\n",
      "step: 49900\n",
      "train: loss: 96303.3828125 acc: 0.9197013974189758  val: loss: 2419820.25 acc: 0.7342952489852905\n",
      "step: 49905\n",
      "train: loss: 216135.890625 acc: 0.827292263507843  val: loss: 2333820.0 acc: 0.7572158575057983\n",
      "step: 49910\n",
      "train: loss: 81563.8671875 acc: 0.9260618090629578  val: loss: 2424714.75 acc: 0.7794002294540405\n",
      "step: 49915\n",
      "train: loss: 205070.859375 acc: 0.7973235845565796  val: loss: 3097488.75 acc: 0.750766396522522\n",
      "step: 49920\n",
      "train: loss: 187773.859375 acc: 0.8632290959358215  val: loss: 1335571.875 acc: 0.8375670313835144\n",
      "step: 49925\n",
      "train: loss: 451134.21875 acc: 0.7796995639801025  val: loss: 4215013.0 acc: 0.7002882957458496\n",
      "step: 49930\n",
      "train: loss: 262553.3125 acc: 0.7252820134162903  val: loss: 1687694.25 acc: 0.6759066581726074\n",
      "step: 49935\n",
      "train: loss: 509824.84375 acc: 0.7330533266067505  val: loss: 2745351.0 acc: 0.45122987031936646\n",
      "step: 49940\n",
      "train: loss: 507011.125 acc: 0.7498593330383301  val: loss: 2102072.5 acc: 0.6894207000732422\n",
      "step: 49945\n",
      "train: loss: 364282.28125 acc: 0.8017432689666748  val: loss: 3290079.5 acc: 0.6459130048751831\n",
      "step: 49950\n",
      "train: loss: 2222654.25 acc: 0.6910117268562317  val: loss: 881586.8125 acc: 0.7820626497268677\n",
      "step: 49955\n",
      "train: loss: 1071855.625 acc: 0.7583296895027161  val: loss: 4698783.5 acc: 0.7018656730651855\n",
      "step: 49960\n",
      "train: loss: 744851.6875 acc: 0.928265392780304  val: loss: 1964225.625 acc: 0.5751774311065674\n",
      "step: 49965\n",
      "train: loss: 317331.125 acc: 0.9673689007759094  val: loss: 1797284.875 acc: 0.8512500524520874\n",
      "step: 49970\n",
      "train: loss: 146563.859375 acc: 0.9793001413345337  val: loss: 745802.3125 acc: 0.8525391221046448\n",
      "step: 49975\n",
      "train: loss: 122183.796875 acc: 0.9702258110046387  val: loss: 1360538.375 acc: 0.6142600774765015\n",
      "step: 49980\n",
      "train: loss: 217791.125 acc: 0.9783731698989868  val: loss: 1292112.875 acc: 0.5850602388381958\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 49985\n",
      "train: loss: 66492.1171875 acc: 0.9952086210250854  val: loss: 755852.6875 acc: 0.857893705368042\n",
      "step: 49990\n",
      "train: loss: 209477.53125 acc: 0.9834766983985901  val: loss: 676006.0625 acc: 0.8963378667831421\n",
      "step: 49995\n",
      "train: loss: 175150.0625 acc: 0.9783071875572205  val: loss: 1280114.75 acc: 0.6899206638336182\n",
      "INFO:tensorflow:Error reported to Coordinator: <class 'tensorflow.python.framework.errors_impl.CancelledError'>, Run call was cancelled\n"
     ]
    }
   ],
   "source": [
    "min_loss=10000\n",
    "sess=tf.Session()\n",
    "train_writer=tf.summary.FileWriter('C:/graph/fitted/train/',sess.graph)\n",
    "test_writer = tf.summary.FileWriter('C:/graph/fitted/test/', sess.graph)\n",
    "saver=tf.train.Saver(max_to_keep=3)\n",
    "\n",
    "init_op = tf.global_variables_initializer()\n",
    "sess.run(init_op)\n",
    "coord = tf.train.Coordinator()\n",
    "threads = tf.train.start_queue_runners(sess, coord)\n",
    "\n",
    "for training_itr in range(50000):\n",
    "    x1, y1 = sess.run([batch_xt,batch_yt])\n",
    "    feed_dict1 = {x:x1,y:y1}\n",
    "    _, loss1,acc1,summaries1 = sess.run([train_op, loss,acc,merged_summary], feed_dict1)\n",
    "\n",
    "    train_writer.add_summary(summaries1, training_itr)\n",
    "    if training_itr %5==0:\n",
    "#             saver.save(sess=sess, save_path='model/hand_landmark_v6.1_model/model.ckpt',global_step=(global_step + 1))\n",
    "        mean_val_loss = 0\n",
    "\n",
    "        x2,y2=sess.run([batch_xv,batch_yv])\n",
    "        feed_dict2 = {x:x2,y:y2}\n",
    "        loss2,acc2,summaries2 = sess.run([loss,acc,merged_summary], feed_dict2)\n",
    "\n",
    "        print('step: {}'.format(training_itr))\n",
    "        print('train: loss: {} acc: {}  val: loss: {} acc: {}'.format(loss1,acc1,loss2,acc2))\n",
    "        test_writer.add_summary(summaries2, training_itr)\n",
    "        if loss1 < min_loss:\n",
    "            min_loss=loss1\n",
    "            saver.save(sess=sess, save_path='D:/model/fitted/model.ckpt',global_step=(training_itr + 1))\n",
    "sess.close()\n",
    "coord.request_stop()\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
