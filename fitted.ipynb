{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import h5py\n",
    "import random\n",
    "import time\n",
    "slim=tf.contrib.slim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[18.08,  2.49],\n",
       "       [18.14,  2.48]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f=h5py.File('data/fitt.h5','r')\n",
    "data_x=f['x']\n",
    "data_y=f['y']\n",
    "data_x[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#读取数据\n",
    "f=h5py.File('data/fitt.h5','r')\n",
    "data_x=f['x']\n",
    "data_y=f['y']\n",
    "\n",
    "\n",
    "#分配训练测试集\n",
    "num=list(range(data_x.shape[0]))\n",
    "num1=random.sample(num,1000)\n",
    "num2=set(num)-set(num1)\n",
    "num2=list(num2)\n",
    "num1.sort()\n",
    "\n",
    "\n",
    "x_train=data_x[num2]\n",
    "y_train=data_y[num2]\n",
    "\n",
    "x_test=data_x[num1]\n",
    "y_test=data_y[num1]\n",
    "#生成批次\n",
    "train_queue = tf.train.slice_input_producer([x_train,y_train],shuffle=None)\n",
    "val_queue = tf.train.slice_input_producer([x_test,y_test],shuffle=None)\n",
    "batch_xt,batch_yt=tf.train.shuffle_batch(train_queue,batch_size=16,capacity=500,min_after_dequeue=150)\n",
    "batch_xv,batch_yv=tf.train.shuffle_batch(val_queue,batch_size=16,capacity=500,min_after_dequeue=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 搭建模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 超参数设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optmizer: \n",
    "learning_rate = 0.007  # Small lr helps not to diverge during training. \n",
    "nb_iters = 10000  # How many times we perform a training step (therefore how many times we show a batch). \n",
    "lr_decay = 0.92  # default: 0.9 . Simulated annealing.\n",
    "momentum = 0.5  # default: 0.0 . Momentum technique in weights update\n",
    "lambda_l2_reg = 0.003  # L2 regularization of weights - avoids overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 搭建神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=tf.placeholder(tf.float32,[None,2])\n",
    "y=tf.placeholder(tf.float32,[None,1])\n",
    "with slim.arg_scope([slim.layers.fully_connected],\n",
    "                    weights_initializer=slim.xavier_initializer(),\n",
    "                    activation_fn=tf.nn.relu):\n",
    "    net=slim.fully_connected(x,4,scope='fc1')\n",
    "#     net=slim.fully_connected(net,8,scope='fc2')\n",
    "    net=slim.fully_connected(net,4,scope='fc3')\n",
    "    output=slim.fully_connected(net,1,activation_fn=None,scope='fc4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 搭建损失函数 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loss and optimizer\n",
    "\n",
    "with tf.variable_scope('Loss'):\n",
    "    # L2 loss\n",
    "   \n",
    "    output_loss = tf.reduce_mean(tf.nn.l2_loss(output - y))\n",
    "        \n",
    "    # L2 regularization (to avoid overfitting and to have a  better generalization capacity)\n",
    "    reg_loss = 0\n",
    "    for tf_var in tf.trainable_variables():\n",
    "        if not (\"Bias\" in tf_var.name or \"Output_\" in tf_var.name):\n",
    "            reg_loss += tf.reduce_mean(tf.nn.l2_loss(tf_var))\n",
    "            \n",
    "    loss = output_loss + lambda_l2_reg * reg_loss\n",
    "    tf.summary.scalar('loss',loss)\n",
    "\n",
    "with tf.variable_scope('acc'):\n",
    "    acc = tf.reduce_mean(tf.nn.l2_loss(y-output))/tf.reduce_mean(tf.nn.l2_loss(y-0))\n",
    "    \n",
    "    acc=1-acc\n",
    "    tf.summary.scalar('acc',acc)\n",
    "\n",
    "with tf.variable_scope('Optimizer'):\n",
    "    optimizer = tf.train.RMSPropOptimizer(learning_rate, decay=lr_decay, momentum=momentum)\n",
    "    train_op = optimizer.minimize(loss)\n",
    "    merged_summary = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0 , time : 0.018008947372436523\n",
      "train: loss: 2398496.0 acc: -0.021650314331054688  val: loss: 11493932.0 acc: -0.008323311805725098\n",
      "step: 5 , time : 0.0\n",
      "train: loss: 5536794.0 acc: 0.0029897093772888184  val: loss: 10574709.0 acc: 0.002350449562072754\n",
      "step: 10 , time : 0.0\n",
      "train: loss: 4038916.25 acc: 0.019552290439605713  val: loss: 6597804.0 acc: 0.025935590267181396\n",
      "step: 15 , time : 0.0010018348693847656\n",
      "train: loss: 5282892.5 acc: 0.05013144016265869  val: loss: 6773632.0 acc: 0.06888723373413086\n",
      "step: 20 , time : 0.0\n",
      "train: loss: 5894154.0 acc: 0.10273385047912598  val: loss: 1518776.625 acc: 0.24219155311584473\n",
      "step: 25 , time : 0.0\n",
      "train: loss: 4544944.0 acc: 0.16228020191192627  val: loss: 7731868.5 acc: 0.1614583134651184\n",
      "step: 30 , time : 0.0010018348693847656\n",
      "train: loss: 3159933.0 acc: 0.2504119277000427  val: loss: 5395313.0 acc: 0.31889504194259644\n",
      "step: 35 , time : 0.0\n",
      "train: loss: 3243807.25 acc: 0.3505120277404785  val: loss: 3445324.25 acc: 0.4094967246055603\n",
      "step: 40 , time : 0.0\n",
      "train: loss: 6076586.0 acc: 0.42359066009521484  val: loss: 3238803.0 acc: 0.4952375292778015\n",
      "step: 45 , time : 0.0\n",
      "train: loss: 11588947.0 acc: 0.40298783779144287  val: loss: 990790.4375 acc: 0.6933534741401672\n",
      "step: 50 , time : 0.0\n",
      "train: loss: 6603146.0 acc: 0.5062199234962463  val: loss: 1946932.75 acc: 0.5416313409805298\n",
      "step: 55 , time : 0.0010004043579101562\n",
      "train: loss: 1122748.875 acc: 0.510498046875  val: loss: 1078602.625 acc: 0.5971087217330933\n",
      "step: 60 , time : 0.0\n",
      "train: loss: 7530552.5 acc: 0.6534433960914612  val: loss: 3836428.75 acc: 0.6981642842292786\n",
      "step: 65 , time : 0.0\n",
      "train: loss: 5465605.0 acc: 0.781537652015686  val: loss: 1471527.5 acc: 0.7031239867210388\n",
      "step: 70 , time : 0.0\n",
      "train: loss: 3665172.0 acc: 0.8278826475143433  val: loss: 1734776.625 acc: 0.7517874836921692\n",
      "step: 75 , time : 0.0010006427764892578\n",
      "train: loss: 2453356.25 acc: 0.846027135848999  val: loss: 2014425.375 acc: 0.5651987791061401\n",
      "step: 80 , time : 0.0\n",
      "train: loss: 2392690.5 acc: 0.8529872298240662  val: loss: 2252038.75 acc: 0.7877621054649353\n",
      "step: 85 , time : 0.0\n",
      "train: loss: 2758674.25 acc: 0.8598657250404358  val: loss: 2371211.75 acc: 0.6907641887664795\n",
      "step: 90 , time : 0.0010008811950683594\n",
      "train: loss: 897089.1875 acc: 0.8301401138305664  val: loss: 2600594.0 acc: 0.7410184144973755\n",
      "step: 95 , time : 0.0010004043579101562\n",
      "train: loss: 1476937.25 acc: 0.6143288016319275  val: loss: 2183403.5 acc: 0.5982686281204224\n",
      "step: 100 , time : 0.0\n",
      "train: loss: 977026.8125 acc: 0.6381640434265137  val: loss: 1314278.625 acc: 0.6028730869293213\n",
      "step: 105 , time : 0.0\n",
      "train: loss: 1574452.25 acc: 0.7353775501251221  val: loss: 1805625.625 acc: 0.6938749551773071\n",
      "step: 110 , time : 0.0010008811950683594\n",
      "train: loss: 394457.8125 acc: 0.8370905518531799  val: loss: 1138572.25 acc: 0.6812134981155396\n",
      "step: 115 , time : 0.0\n",
      "train: loss: 990404.0625 acc: 0.795180082321167  val: loss: 1556912.875 acc: 0.6712164878845215\n",
      "step: 120 , time : 0.0010008811950683594\n",
      "train: loss: 1000634.0 acc: 0.5527017116546631  val: loss: 1389028.5 acc: 0.6644689440727234\n",
      "step: 125 , time : 0.0010004043579101562\n",
      "train: loss: 312729.625 acc: 0.7679495811462402  val: loss: 3162217.0 acc: 0.7075599431991577\n",
      "step: 130 , time : 0.0\n",
      "train: loss: 185122.703125 acc: 0.8441950082778931  val: loss: 2227249.75 acc: 0.5655882358551025\n",
      "step: 135 , time : 0.0010006427764892578\n",
      "train: loss: 356836.84375 acc: 0.7763695120811462  val: loss: 2875686.5 acc: 0.547385036945343\n",
      "step: 140 , time : 0.0\n",
      "train: loss: 2661556.25 acc: 0.580527663230896  val: loss: 1045412.4375 acc: 0.6082820892333984\n",
      "step: 145 , time : 0.0\n",
      "train: loss: 44424.64453125 acc: 0.9606621265411377  val: loss: 2138216.0 acc: 0.6464409828186035\n",
      "step: 150 , time : 0.0010023117065429688\n",
      "train: loss: 42190.30859375 acc: 0.9705232977867126  val: loss: 1862377.625 acc: 0.5617356896400452\n",
      "step: 155 , time : 0.0\n",
      "train: loss: 41003.41796875 acc: 0.9675310254096985  val: loss: 1508475.0 acc: 0.5771713256835938\n",
      "step: 160 , time : 0.0\n",
      "train: loss: 61879.25390625 acc: 0.9565073847770691  val: loss: 872935.5625 acc: 0.6332310438156128\n",
      "step: 165 , time : 0.0010008811950683594\n",
      "train: loss: 239628.96875 acc: 0.8564502596855164  val: loss: 637803.25 acc: 0.7413274645805359\n",
      "step: 170 , time : 0.0010004043579101562\n",
      "train: loss: 62411.89453125 acc: 0.9408316612243652  val: loss: 2069915.125 acc: 0.6058561205863953\n",
      "step: 175 , time : 0.0010008811950683594\n",
      "train: loss: 107598.171875 acc: 0.8321618437767029  val: loss: 1609148.25 acc: 0.6615681648254395\n",
      "step: 180 , time : 0.0\n",
      "train: loss: 615471.0 acc: 0.7200114727020264  val: loss: 1316307.75 acc: 0.5857408046722412\n",
      "step: 185 , time : 0.0010004043579101562\n",
      "train: loss: 1101063.625 acc: 0.6932452917098999  val: loss: 713147.6875 acc: 0.713582456111908\n",
      "step: 190 , time : 0.0\n",
      "train: loss: 70443.6640625 acc: 0.9359338283538818  val: loss: 3403821.75 acc: 0.5587223768234253\n",
      "step: 195 , time : 0.0\n",
      "train: loss: 281085.125 acc: 0.7752869129180908  val: loss: 3211507.0 acc: 0.572754442691803\n",
      "step: 200 , time : 0.0\n",
      "train: loss: 1059890.75 acc: 0.6430574655532837  val: loss: 1477241.0 acc: 0.5916938781738281\n",
      "step: 205 , time : 0.0\n",
      "train: loss: 815449.375 acc: 0.6504018306732178  val: loss: 2923546.75 acc: 0.6239898204803467\n",
      "step: 210 , time : 0.0\n",
      "train: loss: 2391500.5 acc: 0.6530425548553467  val: loss: 1358411.625 acc: 0.713117241859436\n",
      "step: 215 , time : 0.0\n",
      "train: loss: 1684550.125 acc: 0.6976191401481628  val: loss: 1330728.375 acc: 0.6890698671340942\n",
      "step: 220 , time : 0.0\n",
      "train: loss: 1910557.875 acc: 0.8316258192062378  val: loss: 1194447.75 acc: 0.7531670331954956\n",
      "step: 225 , time : 0.0\n",
      "train: loss: 1213098.625 acc: 0.8822264671325684  val: loss: 2784957.0 acc: 0.7991722822189331\n",
      "step: 230 , time : 0.0\n",
      "train: loss: 1591470.75 acc: 0.8164922595024109  val: loss: 1904899.75 acc: 0.8432235717773438\n",
      "step: 235 , time : 0.0\n",
      "train: loss: 742025.8125 acc: 0.8956407308578491  val: loss: 1633195.5 acc: 0.6233319044113159\n",
      "step: 240 , time : 0.0010004043579101562\n",
      "train: loss: 1231309.875 acc: 0.9044985771179199  val: loss: 1503828.375 acc: 0.755272388458252\n",
      "step: 245 , time : 0.001001119613647461\n",
      "train: loss: 497809.84375 acc: 0.9686809778213501  val: loss: 2489387.5 acc: 0.23183482885360718\n",
      "step: 250 , time : 0.0010006427764892578\n",
      "train: loss: 568924.375 acc: 0.9592265486717224  val: loss: 2397873.5 acc: 0.7900118827819824\n",
      "step: 255 , time : 0.0010006427764892578\n",
      "train: loss: 1304838.875 acc: 0.787459135055542  val: loss: 2062669.125 acc: 0.5108650326728821\n",
      "step: 260 , time : 0.0\n",
      "train: loss: 1339232.125 acc: 0.7769646048545837  val: loss: 2754631.5 acc: 0.8359735608100891\n",
      "step: 265 , time : 0.0\n",
      "train: loss: 694803.3125 acc: -0.12143898010253906  val: loss: 475199.28125 acc: 0.7837473154067993\n",
      "step: 270 , time : 0.0\n",
      "train: loss: 572084.25 acc: 0.6684681177139282  val: loss: 3165470.25 acc: 0.7441791296005249\n",
      "step: 275 , time : 0.0\n",
      "train: loss: 505060.84375 acc: 0.5270484089851379  val: loss: 2215136.75 acc: 0.7334907054901123\n",
      "step: 280 , time : 0.0010008811950683594\n",
      "train: loss: 236466.171875 acc: 0.5234559178352356  val: loss: 1506250.375 acc: 0.7564477920532227\n",
      "step: 285 , time : 0.001001119613647461\n",
      "train: loss: 85234.2265625 acc: 0.8768284916877747  val: loss: 1414600.75 acc: 0.7898895144462585\n",
      "step: 290 , time : 0.0\n",
      "train: loss: 60438.45703125 acc: 0.7288075089454651  val: loss: 390618.09375 acc: 0.7706364393234253\n",
      "step: 295 , time : 0.0\n",
      "train: loss: 46355.5546875 acc: 0.7848200798034668  val: loss: 3147119.0 acc: 0.6964262127876282\n",
      "step: 300 , time : 0.0010004043579101562\n",
      "train: loss: 47868.47265625 acc: 0.917216420173645  val: loss: 2614064.75 acc: 0.651384711265564\n",
      "step: 305 , time : 0.0010001659393310547\n",
      "train: loss: 114240.2734375 acc: 0.7844120860099792  val: loss: 2402486.25 acc: 0.6595584154129028\n",
      "step: 310 , time : 0.0010008811950683594\n",
      "train: loss: 349234.40625 acc: 0.7627532482147217  val: loss: 916755.1875 acc: 0.7395980954170227\n",
      "step: 315 , time : 0.0\n",
      "train: loss: 372785.84375 acc: 0.8080032467842102  val: loss: 491759.8125 acc: 0.8891119360923767\n",
      "step: 320 , time : 0.0010020732879638672\n",
      "train: loss: 274670.5 acc: 0.8653323650360107  val: loss: 298071.46875 acc: 0.7840073108673096\n",
      "step: 325 , time : 0.001001119613647461\n",
      "train: loss: 254166.890625 acc: 0.8673076629638672  val: loss: 1596789.125 acc: 0.8257077932357788\n",
      "step: 330 , time : 0.0010004043579101562\n",
      "train: loss: 347563.3125 acc: 0.8216143846511841  val: loss: 282546.1875 acc: 0.6980352401733398\n",
      "step: 335 , time : 0.0010008811950683594\n",
      "train: loss: 183775.71875 acc: 0.8304906487464905  val: loss: 904883.875 acc: 0.7907096743583679\n",
      "step: 340 , time : 0.0\n",
      "train: loss: 198250.5625 acc: 0.8546007871627808  val: loss: 627251.75 acc: 0.602450966835022\n",
      "step: 345 , time : 0.001003265380859375\n",
      "train: loss: 278887.40625 acc: 0.8430202603340149  val: loss: 1771093.25 acc: 0.7827130556106567\n",
      "step: 350 , time : 0.0\n",
      "train: loss: 262641.34375 acc: 0.9363159537315369  val: loss: 1277769.125 acc: 0.8508872389793396\n",
      "step: 355 , time : 0.003001689910888672\n",
      "train: loss: 345352.71875 acc: 0.9065451622009277  val: loss: 1010648.625 acc: 0.7843019962310791\n",
      "step: 360 , time : 0.0\n",
      "train: loss: 186654.96875 acc: 0.9321051836013794  val: loss: 706711.25 acc: 0.8563790917396545\n",
      "step: 365 , time : 0.0010004043579101562\n",
      "train: loss: 531954.1875 acc: 0.8944591879844666  val: loss: 341415.6875 acc: 0.8677196502685547\n",
      "step: 370 , time : 0.0\n",
      "train: loss: 432329.625 acc: 0.8293299078941345  val: loss: 1680000.125 acc: 0.823578417301178\n",
      "step: 375 , time : 0.0\n",
      "train: loss: 267176.5625 acc: 0.8730316162109375  val: loss: 1782078.75 acc: 0.6826130151748657\n",
      "step: 380 , time : 0.0\n",
      "train: loss: 293608.65625 acc: 0.88108891248703  val: loss: 1011163.375 acc: 0.8108621835708618\n",
      "step: 385 , time : 0.0\n",
      "train: loss: 332345.53125 acc: 0.9468406438827515  val: loss: 1326503.25 acc: 0.8453754782676697\n",
      "step: 390 , time : 0.0\n",
      "train: loss: 197502.796875 acc: 0.977957010269165  val: loss: 2106818.25 acc: 0.7136054039001465\n",
      "step: 395 , time : 0.0\n",
      "train: loss: 275979.125 acc: 0.9605472683906555  val: loss: 1480094.875 acc: 0.7406919002532959\n",
      "step: 400 , time : 0.0\n",
      "train: loss: 189245.03125 acc: 0.9548183083534241  val: loss: 2456784.0 acc: 0.5053263902664185\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 405 , time : 0.0\n",
      "train: loss: 481424.3125 acc: 0.9223803281784058  val: loss: 290830.8125 acc: 0.9559406042098999\n",
      "step: 410 , time : 0.0\n",
      "train: loss: 833183.5 acc: 0.9446377754211426  val: loss: 521007.34375 acc: 0.7078371644020081\n",
      "step: 415 , time : 0.0010008811950683594\n",
      "train: loss: 564037.875 acc: 0.9538982510566711  val: loss: 653002.0625 acc: 0.6656532287597656\n",
      "step: 420 , time : 0.0\n",
      "train: loss: 181913.453125 acc: 0.9517530798912048  val: loss: 1240253.25 acc: 0.6879330277442932\n",
      "step: 425 , time : 0.001001119613647461\n",
      "train: loss: 713342.5625 acc: 0.9771745800971985  val: loss: 848204.875 acc: 0.9100331664085388\n",
      "step: 430 , time : 0.0\n",
      "train: loss: 3227378.25 acc: 0.8707790374755859  val: loss: 195611.359375 acc: 0.9346972107887268\n",
      "step: 435 , time : 0.0\n",
      "train: loss: 1990656.875 acc: 0.932315468788147  val: loss: 1131318.625 acc: 0.7775947451591492\n",
      "step: 440 , time : 0.0010004043579101562\n",
      "train: loss: 2404147.5 acc: 0.8410935401916504  val: loss: 133235.4375 acc: 0.9739951491355896\n",
      "step: 445 , time : 0.0010004043579101562\n",
      "train: loss: 808488.125 acc: 0.950316846370697  val: loss: 804957.0625 acc: 0.7815656661987305\n",
      "step: 450 , time : 0.0\n",
      "train: loss: 1196564.75 acc: 0.9030282497406006  val: loss: 164502.046875 acc: 0.9346749186515808\n",
      "step: 455 , time : 0.0\n",
      "train: loss: 734693.75 acc: 0.9174850583076477  val: loss: 387401.25 acc: 0.9237077236175537\n",
      "step: 460 , time : 0.0\n",
      "train: loss: 1138346.75 acc: 0.8513122200965881  val: loss: 3455794.75 acc: 0.3830164074897766\n",
      "step: 465 , time : 0.0\n",
      "train: loss: 1265005.5 acc: 0.524088978767395  val: loss: 540882.3125 acc: 0.831297755241394\n",
      "step: 470 , time : 0.0\n",
      "train: loss: 359233.84375 acc: 0.8374010324478149  val: loss: 1471989.25 acc: 0.7840319275856018\n",
      "step: 475 , time : 0.0\n",
      "train: loss: 620691.625 acc: 0.7628482580184937  val: loss: 880379.4375 acc: 0.8440722823143005\n",
      "step: 480 , time : 0.0010006427764892578\n",
      "train: loss: 1278682.875 acc: 0.4653943181037903  val: loss: 211024.765625 acc: 0.8462291955947876\n",
      "step: 485 , time : 0.0\n",
      "train: loss: 1120163.5 acc: 0.5326589941978455  val: loss: 572300.3125 acc: 0.7260143160820007\n",
      "step: 490 , time : 0.0\n",
      "train: loss: 495816.1875 acc: 0.6601789593696594  val: loss: 861456.3125 acc: 0.2909999489784241\n",
      "step: 495 , time : 0.0010006427764892578\n",
      "train: loss: 480479.5 acc: 0.6246474981307983  val: loss: 2096606.5 acc: 0.745249330997467\n",
      "step: 500 , time : 0.0010008811950683594\n",
      "train: loss: 780063.25 acc: 0.6368023157119751  val: loss: 1357302.5 acc: 0.5860123634338379\n",
      "step: 505 , time : 0.0010006427764892578\n",
      "train: loss: 423468.375 acc: 0.6583057045936584  val: loss: 1163556.75 acc: 0.7448374032974243\n",
      "step: 510 , time : 0.0\n",
      "train: loss: 348594.53125 acc: 0.720703125  val: loss: 802875.1875 acc: 0.7618705034255981\n",
      "step: 515 , time : 0.001001119613647461\n",
      "train: loss: 167867.09375 acc: 0.8954950571060181  val: loss: 870040.4375 acc: 0.8083993792533875\n",
      "step: 520 , time : 0.0010006427764892578\n",
      "train: loss: 92319.1875 acc: 0.921707808971405  val: loss: 1421739.0 acc: 0.8283053040504456\n",
      "step: 525 , time : 0.0010001659393310547\n",
      "train: loss: 144373.53125 acc: 0.9029362201690674  val: loss: 895241.3125 acc: 0.7818917036056519\n",
      "step: 530 , time : 0.0\n",
      "train: loss: 71712.9921875 acc: 0.9335206151008606  val: loss: 522132.28125 acc: 0.7603709697723389\n",
      "step: 535 , time : 0.0010004043579101562\n",
      "train: loss: 453327.9375 acc: 0.8134483695030212  val: loss: 1183104.0 acc: 0.7962523102760315\n",
      "step: 540 , time : 0.0\n",
      "train: loss: 126986.265625 acc: 0.8650208711624146  val: loss: 1232991.75 acc: 0.8893306851387024\n",
      "step: 545 , time : 0.0\n",
      "train: loss: 179305.125 acc: 0.8635132908821106  val: loss: 1949717.375 acc: 0.7267001867294312\n",
      "step: 550 , time : 0.0\n",
      "train: loss: 438619.09375 acc: 0.7628718018531799  val: loss: 1601560.375 acc: 0.7316412925720215\n",
      "step: 555 , time : 0.0010013580322265625\n",
      "train: loss: 1066024.375 acc: 0.08654135465621948  val: loss: 1339480.0 acc: 0.7008641958236694\n",
      "step: 560 , time : 0.0010008811950683594\n",
      "train: loss: 651714.625 acc: 0.6674193739891052  val: loss: 1464186.375 acc: 0.6546877026557922\n",
      "step: 565 , time : 0.0010006427764892578\n",
      "train: loss: 438709.875 acc: 0.7516148090362549  val: loss: 2048677.0 acc: 0.8113494515419006\n",
      "step: 570 , time : 0.0\n",
      "train: loss: 763383.25 acc: 0.699369490146637  val: loss: 1237341.625 acc: 0.8731569051742554\n",
      "step: 575 , time : 0.0\n",
      "train: loss: 1440710.875 acc: 0.7598923444747925  val: loss: 1808232.375 acc: 0.8123571872711182\n",
      "step: 580 , time : 0.0\n",
      "train: loss: 1068204.625 acc: 0.8205254673957825  val: loss: 3416086.25 acc: 0.6992026567459106\n",
      "step: 585 , time : 0.0\n",
      "train: loss: 814546.0 acc: 0.942376971244812  val: loss: 433287.625 acc: 0.803436279296875\n",
      "step: 590 , time : 0.0\n",
      "train: loss: 559010.5625 acc: 0.9426470398902893  val: loss: 1062777.625 acc: 0.8642179369926453\n",
      "step: 595 , time : 0.0010008811950683594\n",
      "train: loss: 432934.03125 acc: 0.934930682182312  val: loss: 1204546.875 acc: 0.8876509070396423\n",
      "step: 600 , time : 0.0\n",
      "train: loss: 461299.5625 acc: 0.9195569753646851  val: loss: 1713793.125 acc: -0.6026939153671265\n",
      "step: 605 , time : 0.0\n",
      "train: loss: 745238.5625 acc: 0.9467577934265137  val: loss: 792767.25 acc: 0.875177264213562\n",
      "step: 610 , time : 0.0\n",
      "train: loss: 224669.3125 acc: 0.9801238775253296  val: loss: 1982719.75 acc: 0.6662645936012268\n",
      "step: 615 , time : 0.0\n",
      "train: loss: 361985.8125 acc: 0.970756471157074  val: loss: 1524846.75 acc: 0.8198041319847107\n",
      "step: 620 , time : 0.0\n",
      "train: loss: 246615.53125 acc: 0.9718700647354126  val: loss: 541536.625 acc: 0.9015189409255981\n",
      "step: 625 , time : 0.0010001659393310547\n",
      "train: loss: 197532.765625 acc: 0.951492190361023  val: loss: 1240215.5 acc: 0.8328205943107605\n",
      "step: 630 , time : 0.0\n",
      "train: loss: 67842.1953125 acc: 0.9748644828796387  val: loss: 696683.8125 acc: 0.8767447471618652\n",
      "step: 635 , time : 0.001001119613647461\n",
      "train: loss: 216223.140625 acc: 0.9235663414001465  val: loss: 528100.25 acc: 0.864683210849762\n",
      "step: 640 , time : 0.0\n",
      "train: loss: 129588.3203125 acc: 0.8907147645950317  val: loss: 250465.078125 acc: 0.9608054161071777\n",
      "step: 645 , time : 0.0\n",
      "train: loss: 23211.853515625 acc: 0.9605218768119812  val: loss: 880539.5 acc: 0.8778668642044067\n",
      "step: 650 , time : 0.0\n",
      "train: loss: 28700.205078125 acc: 0.9469336271286011  val: loss: 174871.0625 acc: 0.9698438048362732\n",
      "step: 655 , time : 0.0\n",
      "train: loss: 36148.87109375 acc: 0.9251165986061096  val: loss: 1219320.75 acc: 0.7549794316291809\n",
      "step: 660 , time : 0.0\n",
      "train: loss: 11453.4970703125 acc: 0.9538955092430115  val: loss: 560506.125 acc: 0.7478721737861633\n",
      "step: 665 , time : 0.0\n",
      "train: loss: 19011.876953125 acc: 0.9679566025733948  val: loss: 1275934.125 acc: 0.49978941679000854\n",
      "step: 670 , time : 0.0\n",
      "train: loss: 10695.9951171875 acc: 0.9849699139595032  val: loss: 1264797.375 acc: 0.4580434560775757\n",
      "step: 675 , time : 0.0\n",
      "train: loss: 89241.5859375 acc: 0.9252474308013916  val: loss: 2127570.75 acc: 0.7778777480125427\n",
      "step: 680 , time : 0.0\n",
      "train: loss: 103637.828125 acc: 0.9335762858390808  val: loss: 815470.375 acc: 0.807888925075531\n",
      "step: 685 , time : 0.0\n",
      "train: loss: 168897.140625 acc: 0.929672122001648  val: loss: 1368979.625 acc: 0.6698068976402283\n",
      "step: 690 , time : 0.0\n",
      "train: loss: 72988.6875 acc: 0.9607802629470825  val: loss: 1767974.375 acc: -0.20058393478393555\n",
      "step: 695 , time : 0.0\n",
      "train: loss: 38335.36328125 acc: 0.9765938520431519  val: loss: 2151608.25 acc: 0.5810176134109497\n",
      "step: 700 , time : 0.0\n",
      "train: loss: 36390.48046875 acc: 0.9569679498672485  val: loss: 250638.734375 acc: 0.9499664902687073\n",
      "step: 705 , time : 0.0\n",
      "train: loss: 8300.38671875 acc: 0.9857321381568909  val: loss: 1157969.375 acc: 0.7258647680282593\n",
      "step: 710 , time : 0.0\n",
      "train: loss: 49147.94140625 acc: 0.9798370003700256  val: loss: 1271883.625 acc: 0.848780632019043\n",
      "step: 715 , time : 0.0\n",
      "train: loss: 73554.4296875 acc: 0.9807415008544922  val: loss: 358971.75 acc: 0.9144647717475891\n",
      "step: 720 , time : 0.0\n",
      "train: loss: 51298.66015625 acc: 0.983301043510437  val: loss: 1183400.25 acc: 0.5523372888565063\n",
      "step: 725 , time : 0.00099945068359375\n",
      "train: loss: 45504.34375 acc: 0.980769157409668  val: loss: 3669505.0 acc: 0.47049373388290405\n",
      "step: 730 , time : 0.0010008811950683594\n",
      "train: loss: 57527.2578125 acc: 0.9794666171073914  val: loss: 2951017.75 acc: 0.3285319209098816\n",
      "step: 735 , time : 0.0010008811950683594\n",
      "train: loss: 332664.78125 acc: 0.8973638415336609  val: loss: 2275360.5 acc: 0.22884893417358398\n",
      "step: 740 , time : 0.0010008811950683594\n",
      "train: loss: 261625.3125 acc: 0.8755456209182739  val: loss: 1835815.125 acc: 0.7558997869491577\n",
      "step: 745 , time : 0.0\n",
      "train: loss: 94256.375 acc: 0.9534570574760437  val: loss: 789635.5625 acc: 0.6376684308052063\n",
      "step: 750 , time : 0.0010006427764892578\n",
      "train: loss: 1331517.125 acc: 0.631991982460022  val: loss: 1134032.875 acc: 0.8626316785812378\n",
      "step: 755 , time : 0.0010008811950683594\n",
      "train: loss: 157302.75 acc: 0.9874788522720337  val: loss: 764188.0625 acc: 0.8892004489898682\n",
      "step: 760 , time : 0.0\n",
      "train: loss: 546369.8125 acc: 0.9018326997756958  val: loss: 751196.6875 acc: 0.7779622673988342\n",
      "step: 765 , time : 0.0\n",
      "train: loss: 66455.6953125 acc: 0.9872412085533142  val: loss: 368964.5 acc: 0.8850471377372742\n",
      "step: 770 , time : 0.0\n",
      "train: loss: 242658.40625 acc: 0.9526058435440063  val: loss: 413510.8125 acc: 0.8737194538116455\n",
      "step: 775 , time : 0.0010004043579101562\n",
      "train: loss: 1799350.375 acc: 0.8611080646514893  val: loss: 2166785.25 acc: 0.1898462176322937\n",
      "step: 780 , time : 0.0009999275207519531\n",
      "train: loss: 999723.8125 acc: 0.8403581380844116  val: loss: 630990.25 acc: 0.8369850516319275\n",
      "step: 785 , time : 0.0010004043579101562\n",
      "train: loss: 116572.65625 acc: 0.9865830540657043  val: loss: 528175.125 acc: 0.6724682450294495\n",
      "step: 790 , time : 0.0\n",
      "train: loss: 781831.75 acc: 0.9408608078956604  val: loss: 583483.9375 acc: 0.8045318126678467\n",
      "step: 795 , time : 0.0010004043579101562\n",
      "train: loss: 2969649.75 acc: 0.9098276495933533  val: loss: 588187.25 acc: 0.8771064877510071\n",
      "step: 800 , time : 0.0010008811950683594\n",
      "train: loss: 1660602.875 acc: 0.9451841711997986  val: loss: 1600136.125 acc: 0.622856616973877\n",
      "step: 805 , time : 0.0\n",
      "train: loss: 2352271.0 acc: 0.8848683834075928  val: loss: 140937.25 acc: 0.9136022925376892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 810 , time : 0.0010006427764892578\n",
      "train: loss: 784286.75 acc: 0.9434651136398315  val: loss: 430360.40625 acc: 0.9539437294006348\n",
      "step: 815 , time : 0.0\n",
      "train: loss: 869721.9375 acc: 0.927031934261322  val: loss: 192146.578125 acc: 0.9507090449333191\n",
      "step: 820 , time : 0.0\n",
      "train: loss: 223473.421875 acc: 0.9419749975204468  val: loss: 355806.90625 acc: 0.9373776912689209\n",
      "step: 825 , time : 0.0\n",
      "train: loss: 509070.875 acc: 0.908772885799408  val: loss: 1753887.375 acc: 0.7117499113082886\n",
      "step: 830 , time : 0.0010006427764892578\n",
      "train: loss: 951728.0625 acc: 0.8937259912490845  val: loss: 698718.875 acc: 0.8620364665985107\n",
      "step: 835 , time : 0.0\n",
      "train: loss: 789149.8125 acc: 0.8852525353431702  val: loss: 734750.8125 acc: 0.8397188186645508\n",
      "step: 840 , time : 0.0010008811950683594\n",
      "train: loss: 616630.8125 acc: 0.8101705312728882  val: loss: 412855.96875 acc: 0.8440320491790771\n",
      "step: 845 , time : 0.0010006427764892578\n",
      "train: loss: 542118.625 acc: 0.6531134247779846  val: loss: 438594.21875 acc: 0.8398926854133606\n",
      "step: 850 , time : 0.0\n",
      "train: loss: 1662842.375 acc: 0.4933949112892151  val: loss: 1204043.125 acc: 0.855089008808136\n",
      "step: 855 , time : 0.0\n",
      "train: loss: 1104925.25 acc: 0.4855797290802002  val: loss: 1048665.25 acc: 0.7550815939903259\n",
      "step: 860 , time : 0.0010004043579101562\n",
      "train: loss: 1257283.625 acc: 0.7758257389068604  val: loss: 1694540.0 acc: 0.5718580484390259\n",
      "step: 865 , time : 0.0\n",
      "train: loss: 375411.125 acc: 0.7305670976638794  val: loss: 2158749.25 acc: 0.6453050374984741\n",
      "step: 870 , time : 0.0\n",
      "train: loss: 279361.15625 acc: 0.7793233394622803  val: loss: 1411472.125 acc: 0.8057588934898376\n",
      "step: 875 , time : 0.0\n",
      "train: loss: 287245.15625 acc: 0.823060929775238  val: loss: 3661071.5 acc: 0.7398225665092468\n",
      "step: 880 , time : 0.0010004043579101562\n",
      "train: loss: 121110.9140625 acc: 0.9021010994911194  val: loss: 1801549.875 acc: 0.7892076969146729\n",
      "step: 885 , time : 0.0010004043579101562\n",
      "train: loss: 513261.9375 acc: 0.7824801206588745  val: loss: 1303091.125 acc: 0.8171350359916687\n",
      "step: 890 , time : 0.0\n",
      "train: loss: 173991.5625 acc: 0.8905037045478821  val: loss: 1491711.375 acc: 0.7808845043182373\n",
      "step: 895 , time : 0.0\n",
      "train: loss: 371555.75 acc: 0.8156063556671143  val: loss: 689591.0 acc: 0.907514750957489\n",
      "step: 900 , time : 0.0\n",
      "train: loss: 355333.0625 acc: 0.7601602077484131  val: loss: 1762628.625 acc: 0.7498248815536499\n",
      "step: 905 , time : 0.0\n",
      "train: loss: 41402.63671875 acc: 0.9535208940505981  val: loss: 3455611.5 acc: 0.7601577639579773\n",
      "step: 910 , time : 0.0\n",
      "train: loss: 375035.625 acc: 0.7145057916641235  val: loss: 1686062.125 acc: 0.8521590232849121\n",
      "step: 915 , time : 0.0\n",
      "train: loss: 1130043.5 acc: 0.680729329586029  val: loss: 2296600.75 acc: 0.6854128241539001\n",
      "step: 920 , time : 0.0\n",
      "train: loss: 797680.5 acc: 0.6875864863395691  val: loss: 1784193.75 acc: 0.6709389686584473\n",
      "step: 925 , time : 0.0010006427764892578\n",
      "train: loss: 600960.25 acc: 0.6345652937889099  val: loss: 4124651.25 acc: 0.7051796317100525\n",
      "step: 930 , time : 0.0\n",
      "train: loss: 262355.4375 acc: 0.731910228729248  val: loss: 1649051.25 acc: 0.6938206553459167\n",
      "step: 935 , time : 0.0\n",
      "train: loss: 400052.25 acc: 0.7903881072998047  val: loss: 1011359.6875 acc: 0.8067561388015747\n",
      "step: 940 , time : 0.0010006427764892578\n",
      "train: loss: 1027880.5 acc: 0.7784765362739563  val: loss: 922634.6875 acc: 0.8393054604530334\n",
      "step: 945 , time : 0.0\n",
      "train: loss: 1328979.625 acc: 0.8343102931976318  val: loss: 907617.5 acc: 0.8270004391670227\n",
      "step: 950 , time : 0.0\n",
      "train: loss: 831817.5 acc: 0.9192373752593994  val: loss: 633196.125 acc: 0.76296067237854\n",
      "step: 955 , time : 0.0\n",
      "train: loss: 458555.71875 acc: 0.9551730155944824  val: loss: 1897127.25 acc: 0.6735231876373291\n",
      "step: 960 , time : 0.0\n",
      "train: loss: 323318.75 acc: 0.9489633440971375  val: loss: 403239.21875 acc: 0.949762761592865\n",
      "step: 965 , time : 0.0\n",
      "train: loss: 312998.875 acc: 0.8708484172821045  val: loss: 1651438.25 acc: 0.8379719257354736\n",
      "step: 970 , time : 0.0\n",
      "train: loss: 527786.75 acc: 0.9461609721183777  val: loss: 1204360.875 acc: 0.8081374168395996\n",
      "step: 975 , time : 0.0\n",
      "train: loss: 319801.6875 acc: 0.9764365553855896  val: loss: 1287380.625 acc: 0.593345046043396\n",
      "step: 980 , time : 0.0\n",
      "train: loss: 277300.34375 acc: 0.9828038811683655  val: loss: 1063452.875 acc: 0.8873381614685059\n",
      "step: 985 , time : 0.0\n",
      "train: loss: 305683.90625 acc: 0.9687010645866394  val: loss: 2607705.5 acc: 0.29172050952911377\n",
      "step: 990 , time : 0.0\n",
      "train: loss: 190405.71875 acc: 0.93318110704422  val: loss: 1073699.125 acc: 0.5284236669540405\n",
      "step: 995 , time : 0.0\n",
      "train: loss: 100591.609375 acc: 0.955981433391571  val: loss: 1166265.125 acc: 0.5075134038925171\n",
      "step: 1000 , time : 0.0\n",
      "train: loss: 13163.2626953125 acc: 0.9692827463150024  val: loss: 1364829.625 acc: 0.3702346682548523\n",
      "step: 1005 , time : 0.0\n",
      "train: loss: 67450.5703125 acc: 0.9521445035934448  val: loss: 677600.875 acc: 0.9014050960540771\n",
      "step: 1010 , time : 0.0\n",
      "train: loss: 5390.28857421875 acc: 0.9909153580665588  val: loss: 1753036.875 acc: 0.3004949688911438\n",
      "step: 1015 , time : 0.0\n",
      "train: loss: 17097.74609375 acc: 0.9901731014251709  val: loss: 768390.3125 acc: 0.764643132686615\n",
      "step: 1020 , time : 0.0\n",
      "train: loss: 4714.16796875 acc: 0.9901587963104248  val: loss: 762456.875 acc: 0.8376850485801697\n",
      "step: 1025 , time : 0.0\n",
      "train: loss: 10437.5625 acc: 0.9751618504524231  val: loss: 211098.765625 acc: 0.9011989831924438\n",
      "step: 1030 , time : 0.0010006427764892578\n",
      "train: loss: 8778.5712890625 acc: 0.9795543551445007  val: loss: 1251504.75 acc: 0.7377957105636597\n",
      "step: 1035 , time : 0.0\n",
      "train: loss: 9133.2041015625 acc: 0.9753975868225098  val: loss: 1098567.375 acc: 0.8630926609039307\n",
      "step: 1040 , time : 0.0010008811950683594\n",
      "train: loss: 37492.36328125 acc: 0.9661511182785034  val: loss: 1039322.8125 acc: 0.5160223245620728\n",
      "step: 1045 , time : 0.0\n",
      "train: loss: 66418.9921875 acc: 0.9657315611839294  val: loss: 904179.8125 acc: 0.7881142497062683\n",
      "step: 1050 , time : 0.0\n",
      "train: loss: 55069.453125 acc: 0.9657115340232849  val: loss: 1247635.25 acc: 0.649773359298706\n",
      "step: 1055 , time : 0.0\n",
      "train: loss: 28712.787109375 acc: 0.9797751307487488  val: loss: 1155682.625 acc: 0.4813830256462097\n",
      "step: 1060 , time : 0.0\n",
      "train: loss: 60541.91796875 acc: 0.9585322141647339  val: loss: 2632145.75 acc: 0.5355257987976074\n",
      "step: 1065 , time : 0.0\n",
      "train: loss: 33793.03515625 acc: 0.9579023122787476  val: loss: 788556.125 acc: 0.6960033774375916\n",
      "step: 1070 , time : 0.0\n",
      "train: loss: 5941.8310546875 acc: 0.9815019369125366  val: loss: 313371.5625 acc: 0.9512709975242615\n",
      "step: 1075 , time : 0.0010004043579101562\n",
      "train: loss: 39617.95703125 acc: 0.97064608335495  val: loss: 1571337.25 acc: 0.19281744956970215\n",
      "step: 1080 , time : 0.0\n",
      "train: loss: 53481.984375 acc: 0.9863426685333252  val: loss: 396497.1875 acc: 0.9110380411148071\n",
      "step: 1085 , time : 0.0\n",
      "train: loss: 54751.63671875 acc: 0.9664936065673828  val: loss: 655737.0625 acc: 0.8045834302902222\n",
      "step: 1090 , time : 0.0\n",
      "train: loss: 59547.88671875 acc: 0.9721824526786804  val: loss: 80948.203125 acc: 0.969157874584198\n",
      "step: 1095 , time : 0.015625476837158203\n",
      "train: loss: 62191.4140625 acc: 0.973613440990448  val: loss: 1113208.875 acc: 0.6667666435241699\n",
      "step: 1100 , time : 0.0\n",
      "train: loss: 75908.5546875 acc: 0.971132218837738  val: loss: 261180.890625 acc: 0.8424935936927795\n",
      "step: 1105 , time : 0.015626192092895508\n",
      "train: loss: 61221.00390625 acc: 0.9775962233543396  val: loss: 4135297.25 acc: 0.36268454790115356\n",
      "step: 1110 , time : 0.0\n",
      "train: loss: 105204.3828125 acc: 0.9535989761352539  val: loss: 189892.21875 acc: 0.947306752204895\n",
      "step: 1115 , time : 0.0\n",
      "train: loss: 213983.40625 acc: 0.9385839104652405  val: loss: 251001.125 acc: 0.931139349937439\n",
      "step: 1120 , time : 0.0\n",
      "train: loss: 98294.765625 acc: 0.9912635684013367  val: loss: 420674.65625 acc: 0.8851747512817383\n",
      "step: 1125 , time : 0.0\n",
      "train: loss: 81597.4296875 acc: 0.9900978207588196  val: loss: 950739.5625 acc: 0.6561223268508911\n",
      "step: 1130 , time : 0.0\n",
      "train: loss: 185814.234375 acc: 0.975448489189148  val: loss: 929170.0625 acc: 0.5016014575958252\n",
      "step: 1135 , time : 0.0\n",
      "train: loss: 229264.234375 acc: 0.9737834930419922  val: loss: 1140741.375 acc: 0.7067343592643738\n",
      "step: 1140 , time : 0.0\n",
      "train: loss: 486123.125 acc: 0.9814530611038208  val: loss: 419649.125 acc: 0.9180272221565247\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 1145 , time : 0.0\n",
      "train: loss: 937387.1875 acc: 0.942436933517456  val: loss: 383144.125 acc: 0.9378708600997925\n",
      "step: 1150 , time : 0.0010006427764892578\n",
      "train: loss: 262225.71875 acc: 0.9491804838180542  val: loss: 587671.0 acc: 0.8488461375236511\n",
      "step: 1155 , time : 0.001001119613647461\n",
      "train: loss: 1520521.75 acc: 0.9003580808639526  val: loss: 324197.125 acc: 0.9481358528137207\n",
      "step: 1160 , time : 0.0\n",
      "train: loss: 977008.625 acc: 0.9581920504570007  val: loss: 1489318.75 acc: 0.8416479229927063\n",
      "step: 1165 , time : 0.0\n",
      "train: loss: 1779586.125 acc: 0.9330569505691528  val: loss: 889454.1875 acc: 0.9067158102989197\n",
      "step: 1170 , time : 0.0010004043579101562\n",
      "train: loss: 1797355.375 acc: 0.8919078707695007  val: loss: 612997.0 acc: 0.927200973033905\n",
      "step: 1175 , time : 0.0\n",
      "train: loss: 382033.25 acc: 0.9756484627723694  val: loss: 513372.5 acc: 0.9387797713279724\n",
      "step: 1180 , time : 0.0\n",
      "train: loss: 600847.375 acc: 0.92554771900177  val: loss: 504255.6875 acc: 0.6989320516586304\n",
      "step: 1185 , time : 0.0010004043579101562\n",
      "train: loss: 644872.1875 acc: 0.8955644369125366  val: loss: 782548.5625 acc: 0.9187964200973511\n",
      "step: 1190 , time : 0.001001119613647461\n",
      "train: loss: 588854.1875 acc: 0.932942271232605  val: loss: 562586.8125 acc: 0.9416190385818481\n",
      "step: 1195 , time : 0.0\n",
      "train: loss: 3686878.25 acc: 0.5863618850708008  val: loss: 1666630.75 acc: 0.8047272562980652\n",
      "step: 1200 , time : 0.0010006427764892578\n",
      "train: loss: 513222.5625 acc: 0.7347496151924133  val: loss: 546762.75 acc: 0.5457569360733032\n",
      "step: 1205 , time : 0.0\n",
      "train: loss: 647903.25 acc: 0.7467689514160156  val: loss: 935496.3125 acc: 0.8674672245979309\n",
      "step: 1210 , time : 0.0\n",
      "train: loss: 620777.6875 acc: 0.8727259635925293  val: loss: 760405.375 acc: 0.8670666813850403\n",
      "step: 1215 , time : 0.0\n",
      "train: loss: 778421.75 acc: 0.8127159476280212  val: loss: 1547048.375 acc: 0.7807193994522095\n",
      "step: 1220 , time : 0.0\n",
      "train: loss: 781225.125 acc: 0.5748082995414734  val: loss: 545077.0 acc: 0.6640093922615051\n",
      "step: 1225 , time : 0.0\n",
      "train: loss: 1483565.75 acc: 0.4686615467071533  val: loss: 530583.75 acc: 0.6579965353012085\n",
      "step: 1230 , time : 0.0\n",
      "train: loss: 1188634.625 acc: 0.6119726896286011  val: loss: 1754905.25 acc: 0.7672505378723145\n",
      "step: 1235 , time : 0.0\n",
      "train: loss: 383779.78125 acc: 0.7229934334754944  val: loss: 3347054.5 acc: 0.7647698521614075\n",
      "step: 1240 , time : 0.0\n",
      "train: loss: 223266.015625 acc: 0.8695561289787292  val: loss: 337706.96875 acc: 0.8438203930854797\n",
      "step: 1245 , time : 0.0\n",
      "train: loss: 179404.484375 acc: 0.8808044195175171  val: loss: 2712857.75 acc: 0.7067365050315857\n",
      "step: 1250 , time : 0.0010008811950683594\n",
      "train: loss: 119189.8125 acc: 0.9078162312507629  val: loss: 654716.375 acc: 0.7405010461807251\n",
      "step: 1255 , time : 0.0010004043579101562\n",
      "train: loss: 202297.609375 acc: 0.8754717111587524  val: loss: 1883296.0 acc: 0.8041093349456787\n",
      "step: 1260 , time : 0.0\n",
      "train: loss: 394722.71875 acc: 0.8249800801277161  val: loss: 604617.75 acc: 0.7499275207519531\n",
      "step: 1265 , time : 0.0\n",
      "train: loss: 112886.6328125 acc: 0.8605822920799255  val: loss: 896131.25 acc: 0.8413742780685425\n",
      "step: 1270 , time : 0.0010004043579101562\n",
      "train: loss: 82975.7734375 acc: 0.9003718495368958  val: loss: 2071382.875 acc: 0.819109320640564\n",
      "step: 1275 , time : 0.0\n",
      "train: loss: 137792.71875 acc: 0.9144847989082336  val: loss: 919534.625 acc: 0.599031925201416\n",
      "step: 1280 , time : 0.0\n",
      "train: loss: 640922.5 acc: 0.4270291328430176  val: loss: 346281.21875 acc: 0.778942346572876\n",
      "step: 1285 , time : 0.0\n",
      "train: loss: 346998.03125 acc: 0.6731122732162476  val: loss: 375409.0 acc: 0.826884388923645\n",
      "step: 1290 , time : 0.0\n",
      "train: loss: 311250.625 acc: 0.7593969106674194  val: loss: 2925668.75 acc: 0.596523642539978\n",
      "step: 1295 , time : 0.0\n",
      "train: loss: 660468.4375 acc: 0.505468487739563  val: loss: 3149909.25 acc: 0.6385814547538757\n",
      "step: 1300 , time : 0.0\n",
      "train: loss: 819231.875 acc: 0.7066377401351929  val: loss: 1179273.625 acc: 0.7877283096313477\n",
      "step: 1305 , time : 0.0\n",
      "train: loss: 1292210.75 acc: 0.7227351665496826  val: loss: 1189812.625 acc: 0.7910595536231995\n",
      "step: 1310 , time : 0.0\n",
      "train: loss: 1340056.0 acc: 0.7493364810943604  val: loss: 612452.5625 acc: 0.8775985240936279\n",
      "step: 1315 , time : 0.0\n",
      "train: loss: 661184.5625 acc: 0.9212356805801392  val: loss: 1081249.125 acc: 0.8526703119277954\n",
      "step: 1320 , time : 0.0\n",
      "train: loss: 512225.78125 acc: 0.9526714086532593  val: loss: 1619375.375 acc: 0.799037754535675\n",
      "step: 1325 , time : 0.0\n",
      "train: loss: 348764.40625 acc: 0.9341580867767334  val: loss: 773940.5 acc: 0.8769708275794983\n",
      "step: 1330 , time : 0.0\n",
      "train: loss: 341945.03125 acc: 0.959679365158081  val: loss: 428338.4375 acc: 0.9256047606468201\n",
      "step: 1335 , time : 0.0\n",
      "train: loss: 303973.5 acc: 0.9651862978935242  val: loss: 1309495.5 acc: 0.8028848171234131\n",
      "step: 1340 , time : 0.0\n",
      "train: loss: 221720.546875 acc: 0.9825174808502197  val: loss: 1034675.3125 acc: 0.7055760622024536\n",
      "step: 1345 , time : 0.0\n",
      "train: loss: 499565.4375 acc: 0.9684723615646362  val: loss: 537142.6875 acc: 0.9122039079666138\n",
      "step: 1350 , time : 0.0\n",
      "train: loss: 251386.828125 acc: 0.9661334753036499  val: loss: 1541436.5 acc: 0.6722785830497742\n",
      "step: 1355 , time : 0.0\n",
      "train: loss: 200154.125 acc: 0.9660179615020752  val: loss: 916355.0625 acc: 0.525849461555481\n",
      "step: 1360 , time : 0.0\n",
      "train: loss: 67721.0703125 acc: 0.9753042459487915  val: loss: 815697.8125 acc: 0.6825225949287415\n",
      "step: 1365 , time : 0.0\n",
      "train: loss: 3940.133056640625 acc: 0.9723681211471558  val: loss: 378076.125 acc: 0.9598802328109741\n",
      "step: 1370 , time : 0.0\n",
      "train: loss: 62689.3515625 acc: 0.942000687122345  val: loss: 628811.6875 acc: 0.7345929741859436\n",
      "step: 1375 , time : 0.0\n",
      "train: loss: 136356.9375 acc: 0.9575409889221191  val: loss: 274583.875 acc: 0.9436488151550293\n",
      "step: 1380 , time : 0.0\n",
      "train: loss: 58056.0234375 acc: 0.944957435131073  val: loss: 847016.9375 acc: 0.8753618001937866\n",
      "step: 1385 , time : 0.0\n",
      "train: loss: 29647.904296875 acc: 0.982725977897644  val: loss: 1487511.625 acc: 0.5928050875663757\n",
      "step: 1390 , time : 0.0\n",
      "train: loss: 13886.9677734375 acc: 0.9720373153686523  val: loss: 869559.3125 acc: 0.8158468008041382\n",
      "step: 1395 , time : 0.0010008811950683594\n",
      "train: loss: 7262.216796875 acc: 0.9852270483970642  val: loss: 77030.8046875 acc: 0.9831932783126831\n",
      "step: 1400 , time : 0.0010006427764892578\n",
      "train: loss: 8122.26025390625 acc: 0.9902321696281433  val: loss: 420902.5 acc: 0.9110051989555359\n",
      "step: 1405 , time : 0.0\n",
      "train: loss: 14676.6611328125 acc: 0.9830628633499146  val: loss: 846910.125 acc: 0.41805362701416016\n",
      "step: 1410 , time : 0.0\n",
      "train: loss: 41152.81640625 acc: 0.9529402256011963  val: loss: 2498057.75 acc: 0.4832723140716553\n",
      "step: 1415 , time : 0.0\n",
      "train: loss: 46113.671875 acc: 0.9732142686843872  val: loss: 814175.1875 acc: 0.703720211982727\n",
      "step: 1420 , time : 0.0\n",
      "train: loss: 31117.955078125 acc: 0.9782541990280151  val: loss: 2867189.25 acc: 0.43452566862106323\n",
      "step: 1425 , time : 0.0\n",
      "train: loss: 57054.4609375 acc: 0.9716381430625916  val: loss: 507607.5 acc: 0.8791285753250122\n",
      "step: 1430 , time : 0.0\n",
      "train: loss: 32509.373046875 acc: 0.9749042391777039  val: loss: 971218.5625 acc: 0.7668293714523315\n",
      "step: 1435 , time : 0.0\n",
      "train: loss: 10773.326171875 acc: 0.9830066561698914  val: loss: 554093.6875 acc: 0.927282989025116\n",
      "step: 1440 , time : 0.0\n",
      "train: loss: 35019.88671875 acc: 0.9799324870109558  val: loss: 88007.1875 acc: 0.9552060961723328\n",
      "step: 1445 , time : 0.0\n",
      "train: loss: 30847.26171875 acc: 0.9901933670043945  val: loss: 2424362.75 acc: 0.5878245234489441\n",
      "step: 1450 , time : 0.0005464553833007812\n",
      "train: loss: 51050.70703125 acc: 0.9890682101249695  val: loss: 795914.1875 acc: 0.9050562381744385\n",
      "step: 1455 , time : 0.0\n",
      "train: loss: 102823.6640625 acc: 0.9704071879386902  val: loss: 1007773.75 acc: 0.8716249465942383\n",
      "step: 1460 , time : 0.0\n",
      "train: loss: 75907.4453125 acc: 0.9798822402954102  val: loss: 1517524.125 acc: 0.7582992911338806\n",
      "step: 1465 , time : 0.0\n",
      "train: loss: 326895.6875 acc: 0.919273316860199  val: loss: 1834795.625 acc: 0.5468220114707947\n",
      "step: 1470 , time : 0.0\n",
      "train: loss: 251247.84375 acc: 0.9408989548683167  val: loss: 642013.1875 acc: 0.9424610733985901\n",
      "step: 1475 , time : 0.0\n",
      "train: loss: 87271.53125 acc: 0.9546191692352295  val: loss: 211442.109375 acc: 0.9793529510498047\n",
      "step: 1480 , time : 0.0\n",
      "train: loss: 579444.125 acc: 0.8786457777023315  val: loss: 1474622.75 acc: 0.7577865123748779\n",
      "step: 1485 , time : 0.0\n",
      "train: loss: 203152.484375 acc: 0.9740779995918274  val: loss: 1150116.0 acc: 0.8509883284568787\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 1490 , time : 0.0\n",
      "train: loss: 417969.3125 acc: 0.9516592621803284  val: loss: 2906035.25 acc: 0.5900057554244995\n",
      "step: 1495 , time : 0.0\n",
      "train: loss: 103317.046875 acc: 0.9865365624427795  val: loss: 811593.875 acc: 0.9027464985847473\n",
      "step: 1500 , time : 0.0\n",
      "train: loss: 256214.875 acc: 0.9706386923789978  val: loss: 501702.46875 acc: 0.9467088580131531\n",
      "step: 1505 , time : 0.0\n",
      "train: loss: 300878.40625 acc: 0.9772555828094482  val: loss: 715607.4375 acc: 0.9494674205780029\n",
      "step: 1510 , time : 0.0010008811950683594\n",
      "train: loss: 473288.46875 acc: 0.9693734645843506  val: loss: 1033060.0625 acc: 0.5786436200141907\n",
      "step: 1515 , time : 0.0010008811950683594\n",
      "train: loss: 281598.21875 acc: 0.9484043121337891  val: loss: 1269889.0 acc: 0.40341025590896606\n",
      "step: 1520 , time : 0.0010008811950683594\n",
      "train: loss: 1750671.375 acc: 0.8074809312820435  val: loss: 285556.34375 acc: 0.9716757535934448\n",
      "step: 1525 , time : 0.0\n",
      "train: loss: 703452.3125 acc: 0.9690589904785156  val: loss: 860441.1875 acc: 0.894761860370636\n",
      "step: 1530 , time : 0.0\n",
      "train: loss: 3292464.0 acc: 0.8904268145561218  val: loss: 649622.4375 acc: 0.9499576687812805\n",
      "step: 1535 , time : 0.0\n",
      "train: loss: 675145.5625 acc: 0.9604759812355042  val: loss: 988233.875 acc: 0.8388885855674744\n",
      "step: 1540 , time : 0.0\n",
      "train: loss: 591614.25 acc: 0.9547763466835022  val: loss: 1903638.125 acc: 0.6873159408569336\n",
      "step: 1545 , time : 0.0\n",
      "train: loss: 493888.3125 acc: 0.9534416198730469  val: loss: 414426.0625 acc: 0.8686487674713135\n",
      "step: 1550 , time : 0.0\n",
      "train: loss: 605450.1875 acc: 0.9660291075706482  val: loss: 1145195.125 acc: 0.459131121635437\n",
      "step: 1555 , time : 0.0\n",
      "train: loss: 242126.375 acc: 0.9673827886581421  val: loss: 4124861.5 acc: 0.4942108988761902\n",
      "step: 1560 , time : 0.0\n",
      "train: loss: 1283765.625 acc: 0.7917577624320984  val: loss: 912698.0625 acc: 0.8900095224380493\n",
      "step: 1565 , time : 0.0\n",
      "train: loss: 543694.1875 acc: 0.727372944355011  val: loss: 1139569.875 acc: 0.6776533126831055\n",
      "step: 1570 , time : 0.0\n",
      "train: loss: 566352.4375 acc: 0.6438108682632446  val: loss: 1782899.5 acc: 0.6833765506744385\n",
      "step: 1575 , time : 0.0\n",
      "train: loss: 636316.125 acc: 0.7618553638458252  val: loss: 1288407.875 acc: 0.7477095127105713\n",
      "step: 1580 , time : 0.0\n",
      "train: loss: 698918.5 acc: 0.7542591691017151  val: loss: 441831.125 acc: 0.7320840358734131\n",
      "step: 1585 , time : 0.0\n",
      "train: loss: 1274860.25 acc: 0.5088508129119873  val: loss: 757113.6875 acc: 0.8512517213821411\n",
      "step: 1590 , time : 0.0\n",
      "train: loss: 1040645.6875 acc: 0.49148935079574585  val: loss: 982707.3125 acc: 0.8227805495262146\n",
      "step: 1595 , time : 0.0\n",
      "train: loss: 403908.75 acc: 0.739172101020813  val: loss: 1050558.75 acc: 0.7841213345527649\n",
      "step: 1600 , time : 0.0\n",
      "train: loss: 316387.5625 acc: 0.7940313816070557  val: loss: 1793615.125 acc: 0.6388399004936218\n",
      "step: 1605 , time : 0.0\n",
      "train: loss: 320567.84375 acc: 0.8079710006713867  val: loss: 1806584.375 acc: 0.7535305619239807\n",
      "step: 1610 , time : 0.0\n",
      "train: loss: 403964.625 acc: 0.796904444694519  val: loss: 750791.25 acc: 0.7819013595581055\n",
      "step: 1615 , time : 0.001001119613647461\n",
      "train: loss: 162242.5 acc: 0.890331506729126  val: loss: 787320.5625 acc: 0.8395280838012695\n",
      "step: 1620 , time : 0.0\n",
      "train: loss: 101955.6015625 acc: 0.9203830361366272  val: loss: 1187030.625 acc: 0.773077666759491\n",
      "step: 1625 , time : 0.0010008811950683594\n",
      "train: loss: 85740.6484375 acc: 0.9227054119110107  val: loss: 1542839.875 acc: 0.7322220802307129\n",
      "step: 1630 , time : 0.0\n",
      "train: loss: 120257.390625 acc: 0.9039434790611267  val: loss: 2343214.75 acc: 0.738563060760498\n",
      "step: 1635 , time : 0.0\n",
      "train: loss: 96631.109375 acc: 0.8940519094467163  val: loss: 1002784.0 acc: 0.8390491604804993\n",
      "step: 1640 , time : 0.0\n",
      "train: loss: 145561.203125 acc: 0.9052320122718811  val: loss: 952504.625 acc: 0.8034723401069641\n",
      "step: 1645 , time : 0.0\n",
      "train: loss: 427206.71875 acc: 0.6503276228904724  val: loss: 2515864.0 acc: 0.7292165756225586\n",
      "step: 1650 , time : 0.0010004043579101562\n",
      "train: loss: 143423.375 acc: 0.8626486659049988  val: loss: 1263343.5 acc: 0.7087981700897217\n",
      "step: 1655 , time : 0.0\n",
      "train: loss: 1210487.25 acc: 0.5544594526290894  val: loss: 473430.25 acc: 0.7675980925559998\n",
      "step: 1660 , time : 0.015624761581420898\n",
      "train: loss: 391024.0625 acc: 0.7475754022598267  val: loss: 1641259.5 acc: 0.7631478309631348\n",
      "step: 1665 , time : 0.0\n",
      "train: loss: 351194.5625 acc: 0.8086096048355103  val: loss: 1554091.125 acc: 0.6554789543151855\n",
      "step: 1670 , time : 0.0\n",
      "train: loss: 2330413.0 acc: 0.6952941417694092  val: loss: 562321.0 acc: 0.707351565361023\n",
      "step: 1675 , time : 0.0\n",
      "train: loss: 1490895.375 acc: 0.7823941707611084  val: loss: 180294.9375 acc: 0.9349395036697388\n",
      "step: 1680 , time : 0.0\n",
      "train: loss: 1300052.625 acc: 0.8760701417922974  val: loss: 609605.0 acc: 0.8008460998535156\n",
      "step: 1685 , time : 0.0\n",
      "train: loss: 579087.5625 acc: 0.9501843452453613  val: loss: 1302867.25 acc: 0.4825837016105652\n",
      "step: 1690 , time : 0.0\n",
      "train: loss: 442786.84375 acc: 0.9418312311172485  val: loss: 725076.8125 acc: 0.7655515074729919\n",
      "step: 1695 , time : 0.0\n",
      "train: loss: 249908.546875 acc: 0.9688854217529297  val: loss: 923431.0625 acc: 0.7953084111213684\n",
      "step: 1700 , time : 0.0\n",
      "train: loss: 442334.9375 acc: 0.9432340860366821  val: loss: 1213087.5 acc: 0.8299736380577087\n",
      "step: 1705 , time : 0.0\n",
      "train: loss: 199087.265625 acc: 0.9858980178833008  val: loss: 308948.21875 acc: 0.9157482981681824\n",
      "step: 1710 , time : 0.0\n",
      "train: loss: 262202.46875 acc: 0.9785957932472229  val: loss: 473060.40625 acc: 0.8693190217018127\n",
      "step: 1715 , time : 0.0\n",
      "train: loss: 514972.0 acc: 0.9435913562774658  val: loss: 1025820.5 acc: 0.21534180641174316\n",
      "step: 1720 , time : 0.0010008811950683594\n",
      "train: loss: 170202.5 acc: 0.9766543507575989  val: loss: 565934.75 acc: 0.9031562209129333\n",
      "step: 1725 , time : 0.0\n",
      "train: loss: 33813.6796875 acc: 0.9671960473060608  val: loss: 547828.25 acc: 0.9011276364326477\n",
      "step: 1730 , time : 0.0010006427764892578\n",
      "train: loss: 62107.2109375 acc: 0.9861547946929932  val: loss: 1037266.75 acc: 0.598394513130188\n",
      "step: 1735 , time : 0.0010008811950683594\n",
      "train: loss: 45763.09765625 acc: 0.9821321964263916  val: loss: 117553.28125 acc: 0.9762125611305237\n",
      "step: 1740 , time : 0.0\n",
      "train: loss: 27990.4453125 acc: 0.9910069704055786  val: loss: 579962.25 acc: 0.5932161211967468\n",
      "step: 1745 , time : 0.0\n",
      "train: loss: 22235.2109375 acc: 0.9781821966171265  val: loss: 305488.375 acc: 0.8242870569229126\n",
      "step: 1750 , time : 0.0\n",
      "train: loss: 63508.89453125 acc: 0.9157630801200867  val: loss: 49881.87109375 acc: 0.9786669015884399\n",
      "step: 1755 , time : 0.0\n",
      "train: loss: 11280.52734375 acc: 0.9815770387649536  val: loss: 112765.1015625 acc: 0.9774242639541626\n",
      "step: 1760 , time : 0.0\n",
      "train: loss: 10601.2890625 acc: 0.961829662322998  val: loss: 936250.1875 acc: 0.26438796520233154\n",
      "step: 1765 , time : 0.0\n",
      "train: loss: 4148.53662109375 acc: 0.9862068891525269  val: loss: 881536.1875 acc: 0.7575076222419739\n",
      "step: 1770 , time : 0.0\n",
      "train: loss: 11663.396484375 acc: 0.9532963633537292  val: loss: 438033.875 acc: 0.927237868309021\n",
      "step: 1775 , time : 0.0\n",
      "train: loss: 24861.7265625 acc: 0.9740201234817505  val: loss: 536599.0 acc: 0.8700121641159058\n",
      "step: 1780 , time : 0.0\n",
      "train: loss: 32756.98046875 acc: 0.9809532761573792  val: loss: 1542749.375 acc: 0.5348457098007202\n",
      "step: 1785 , time : 0.0\n",
      "train: loss: 43397.0078125 acc: 0.9757078289985657  val: loss: 462533.9375 acc: 0.9565690755844116\n",
      "step: 1790 , time : 0.0\n",
      "train: loss: 33415.125 acc: 0.9786601662635803  val: loss: 595004.5625 acc: 0.92490553855896\n",
      "step: 1795 , time : 0.0\n",
      "train: loss: 16264.3037109375 acc: 0.9854968190193176  val: loss: 987904.0625 acc: 0.8877854347229004\n",
      "step: 1800 , time : 0.0\n",
      "train: loss: 23146.599609375 acc: 0.9781437516212463  val: loss: 348750.78125 acc: 0.9526556134223938\n",
      "step: 1805 , time : 0.0\n",
      "train: loss: 17552.103515625 acc: 0.9898627400398254  val: loss: 639927.0625 acc: 0.5941890478134155\n",
      "step: 1810 , time : 0.0\n",
      "train: loss: 55853.83984375 acc: 0.9798240065574646  val: loss: 1500810.625 acc: 0.8680233359336853\n",
      "step: 1815 , time : 0.0\n",
      "train: loss: 67476.7890625 acc: 0.9852862358093262  val: loss: 337190.71875 acc: 0.9481706023216248\n",
      "step: 1820 , time : 0.0\n",
      "train: loss: 39040.81640625 acc: 0.9877652525901794  val: loss: 1774210.25 acc: 0.7210433483123779\n",
      "step: 1825 , time : 0.0\n",
      "train: loss: 56608.76953125 acc: 0.9742591381072998  val: loss: 643373.375 acc: 0.6007871627807617\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 1830 , time : 0.0\n",
      "train: loss: 424205.625 acc: 0.8139967918395996  val: loss: 1510679.0 acc: 0.3473818898200989\n",
      "step: 1835 , time : 0.0\n",
      "train: loss: 145998.390625 acc: 0.9664628505706787  val: loss: 572238.3125 acc: 0.9186475872993469\n",
      "step: 1840 , time : 0.0\n",
      "train: loss: 100748.140625 acc: 0.9286493062973022  val: loss: 2196526.75 acc: 0.5519282817840576\n",
      "step: 1845 , time : 0.0\n",
      "train: loss: 836299.5625 acc: 0.6398642063140869  val: loss: 2145462.75 acc: 0.6965521574020386\n",
      "step: 1850 , time : 0.0\n",
      "train: loss: 132712.203125 acc: 0.9845572113990784  val: loss: 2939974.0 acc: 0.5435644388198853\n",
      "step: 1855 , time : 0.0\n",
      "train: loss: 126732.609375 acc: 0.9886378049850464  val: loss: 2957576.75 acc: 0.5107908248901367\n",
      "step: 1860 , time : 0.0\n",
      "train: loss: 139631.6875 acc: 0.9800811409950256  val: loss: 360976.34375 acc: 0.95797199010849\n",
      "step: 1865 , time : 0.0\n",
      "train: loss: 230299.65625 acc: 0.9729652404785156  val: loss: 1196181.625 acc: -0.01861560344696045\n",
      "step: 1870 , time : 0.0\n",
      "train: loss: 222894.546875 acc: 0.9822247624397278  val: loss: 696194.25 acc: 0.8629668354988098\n",
      "step: 1875 , time : 0.0\n",
      "train: loss: 483508.84375 acc: 0.9329102039337158  val: loss: 1646570.0 acc: 0.739778995513916\n",
      "step: 1880 , time : 0.0\n",
      "train: loss: 223751.609375 acc: 0.979134202003479  val: loss: 1552402.0 acc: 0.7464154958724976\n",
      "step: 1885 , time : 0.0\n",
      "train: loss: 424088.28125 acc: 0.933430552482605  val: loss: 2532195.75 acc: 0.5596613883972168\n",
      "step: 1890 , time : 0.0\n",
      "train: loss: 499589.46875 acc: 0.9783971309661865  val: loss: 1595461.125 acc: 0.7256993055343628\n",
      "step: 1895 , time : 0.0\n",
      "train: loss: 797236.75 acc: 0.9751015901565552  val: loss: 2330756.25 acc: 0.7056530714035034\n",
      "step: 1900 , time : 0.0\n",
      "train: loss: 400696.09375 acc: 0.9797385334968567  val: loss: 923618.5 acc: 0.8247923851013184\n",
      "step: 1905 , time : 0.0\n",
      "train: loss: 1604086.75 acc: 0.9161220192909241  val: loss: 1313207.25 acc: 0.777675449848175\n",
      "step: 1910 , time : 0.0\n",
      "train: loss: 392406.5 acc: 0.9658191204071045  val: loss: 1670855.625 acc: 0.5012701749801636\n",
      "step: 1915 , time : 0.0\n",
      "train: loss: 788544.6875 acc: 0.9170538783073425  val: loss: 1924751.75 acc: 0.7071437835693359\n",
      "step: 1920 , time : 0.0\n",
      "train: loss: 657010.9375 acc: 0.942173421382904  val: loss: 1930266.625 acc: 0.5964635014533997\n",
      "step: 1925 , time : 0.0\n",
      "train: loss: 2011714.25 acc: 0.3262491226196289  val: loss: 1194979.75 acc: 0.5132707357406616\n",
      "step: 1930 , time : 0.0\n",
      "train: loss: 513513.5 acc: 0.7049260139465332  val: loss: 3880613.5 acc: 0.5073286294937134\n",
      "step: 1935 , time : 0.0\n",
      "train: loss: 1340410.75 acc: 0.7012255191802979  val: loss: 1030472.9375 acc: 0.7726444602012634\n",
      "step: 1940 , time : 0.015625476837158203\n",
      "train: loss: 347813.9375 acc: 0.8404284715652466  val: loss: 636670.1875 acc: 0.9235457181930542\n",
      "step: 1945 , time : 0.0\n",
      "train: loss: 902671.1875 acc: 0.677585244178772  val: loss: 565639.0625 acc: 0.9079711437225342\n",
      "step: 1950 , time : 0.0010008811950683594\n",
      "train: loss: 1365248.875 acc: 0.5384101271629333  val: loss: 1608185.625 acc: 0.729006290435791\n",
      "step: 1955 , time : 0.0\n",
      "train: loss: 861312.5625 acc: 0.25544655323028564  val: loss: 1131263.875 acc: 0.8163943290710449\n",
      "step: 1960 , time : 0.0\n",
      "train: loss: 593392.375 acc: 0.5425432920455933  val: loss: 1689226.75 acc: 0.7382165193557739\n",
      "step: 1965 , time : 0.0\n",
      "train: loss: 272492.59375 acc: 0.7589647769927979  val: loss: 298665.15625 acc: 0.7955136895179749\n",
      "step: 1970 , time : 0.0\n",
      "train: loss: 44490.2265625 acc: 0.9619349241256714  val: loss: 4335999.0 acc: 0.6982532739639282\n",
      "step: 1975 , time : 0.0\n",
      "train: loss: 163462.96875 acc: 0.8586059808731079  val: loss: 835721.1875 acc: 0.6500470638275146\n",
      "step: 1980 , time : 0.0\n",
      "train: loss: 184345.921875 acc: 0.8589123487472534  val: loss: 689883.75 acc: 0.721955418586731\n",
      "step: 1985 , time : 0.0\n",
      "train: loss: 341749.65625 acc: 0.8409229516983032  val: loss: 1158241.75 acc: 0.7642766237258911\n",
      "step: 1990 , time : 0.0\n",
      "train: loss: 104210.7578125 acc: 0.9149645566940308  val: loss: 827260.4375 acc: 0.7863366603851318\n",
      "step: 1995 , time : 0.0\n",
      "train: loss: 400353.46875 acc: 0.6973665952682495  val: loss: 2606522.75 acc: 0.7484010457992554\n",
      "step: 2000 , time : 0.015625953674316406\n",
      "train: loss: 51953.76953125 acc: 0.9479914307594299  val: loss: 725381.5625 acc: 0.7338804006576538\n",
      "step: 2005 , time : 0.0\n",
      "train: loss: 122377.734375 acc: 0.8604193925857544  val: loss: 735060.0 acc: 0.7646399736404419\n",
      "step: 2010 , time : 0.0\n",
      "train: loss: 222814.1875 acc: 0.8441949486732483  val: loss: 1099040.625 acc: 0.6938623189926147\n",
      "step: 2015 , time : 0.0\n",
      "train: loss: 231350.125 acc: 0.8445397615432739  val: loss: 1733211.125 acc: 0.7278285026550293\n",
      "step: 2020 , time : 0.0\n",
      "train: loss: 211321.515625 acc: 0.7919222116470337  val: loss: 1465963.875 acc: 0.7064716815948486\n",
      "step: 2025 , time : 0.0\n",
      "train: loss: 424560.40625 acc: 0.7258690595626831  val: loss: 1052101.0 acc: 0.6929821968078613\n",
      "step: 2030 , time : 0.0\n",
      "train: loss: 674410.6875 acc: 0.7079590559005737  val: loss: 2682772.75 acc: 0.633161187171936\n",
      "step: 2035 , time : 0.0\n",
      "train: loss: 1339392.625 acc: 0.7168062925338745  val: loss: 1158292.125 acc: 0.7198258638381958\n",
      "step: 2040 , time : 0.0\n",
      "train: loss: 2178235.0 acc: 0.7091847658157349  val: loss: 413766.625 acc: 0.7287143468856812\n",
      "step: 2045 , time : 0.0\n",
      "train: loss: 787658.6875 acc: 0.878326416015625  val: loss: 352044.03125 acc: 0.8421350717544556\n",
      "step: 2050 , time : 0.0\n",
      "train: loss: 611717.9375 acc: 0.956268310546875  val: loss: 648323.4375 acc: 0.8540485501289368\n",
      "step: 2055 , time : 0.0\n",
      "train: loss: 955078.0625 acc: 0.8411974906921387  val: loss: 258496.0 acc: 0.9391719102859497\n",
      "step: 2060 , time : 0.0010004043579101562\n",
      "train: loss: 178906.71875 acc: 0.9638625979423523  val: loss: 332171.8125 acc: 0.9328742027282715\n",
      "step: 2065 , time : 0.0\n",
      "train: loss: 228754.71875 acc: 0.9700984358787537  val: loss: 422525.5625 acc: 0.9146249294281006\n",
      "step: 2070 , time : 0.0\n",
      "train: loss: 183238.515625 acc: 0.9838269352912903  val: loss: 1822152.375 acc: 0.5067317485809326\n",
      "step: 2075 , time : 0.0\n",
      "train: loss: 335419.0625 acc: 0.9769319295883179  val: loss: 243417.375 acc: 0.9419098496437073\n",
      "step: 2080 , time : 0.0\n",
      "train: loss: 210281.328125 acc: 0.9803714752197266  val: loss: 542789.5625 acc: 0.778327226638794\n",
      "step: 2085 , time : 0.0010006427764892578\n",
      "train: loss: 129347.6796875 acc: 0.9726662039756775  val: loss: 576379.5625 acc: 0.9204794764518738\n",
      "step: 2090 , time : 0.0010008811950683594\n",
      "train: loss: 34608.66015625 acc: 0.9835296273231506  val: loss: 799031.6875 acc: 0.8321627378463745\n",
      "step: 2095 , time : 0.0\n",
      "train: loss: 10859.78125 acc: 0.9784473776817322  val: loss: 646652.4375 acc: 0.9223199486732483\n",
      "step: 2100 , time : 0.015624523162841797\n",
      "train: loss: 9347.35546875 acc: 0.985706627368927  val: loss: 460367.78125 acc: 0.7397432923316956\n",
      "step: 2105 , time : 0.0\n",
      "train: loss: 56609.59765625 acc: 0.9781152009963989  val: loss: 3424032.5 acc: 0.6806880235671997\n",
      "step: 2110 , time : 0.0\n",
      "train: loss: 25662.615234375 acc: 0.974227249622345  val: loss: 507873.875 acc: 0.9320247769355774\n",
      "step: 2115 , time : 0.0\n",
      "train: loss: 32482.361328125 acc: 0.9775399565696716  val: loss: 804979.625 acc: 0.9280148148536682\n",
      "step: 2120 , time : 0.0\n",
      "train: loss: 8125.4150390625 acc: 0.9741563200950623  val: loss: 484247.90625 acc: 0.9491246938705444\n",
      "step: 2125 , time : 0.0\n",
      "train: loss: 18235.734375 acc: 0.9537098407745361  val: loss: 776362.875 acc: 0.9480858445167542\n",
      "step: 2130 , time : 0.0\n",
      "train: loss: 5683.4228515625 acc: 0.9750152230262756  val: loss: 1071100.75 acc: 0.9325566291809082\n",
      "step: 2135 , time : 0.0\n",
      "train: loss: 8985.25 acc: 0.9753782749176025  val: loss: 801557.5625 acc: 0.9131668210029602\n",
      "step: 2140 , time : 0.0\n",
      "train: loss: 25171.287109375 acc: 0.9744083881378174  val: loss: 583324.5 acc: 0.8735879063606262\n",
      "step: 2145 , time : 0.0\n",
      "train: loss: 31728.1015625 acc: 0.9789581894874573  val: loss: 895472.625 acc: 0.9278284907341003\n",
      "step: 2150 , time : 0.0\n",
      "train: loss: 106819.765625 acc: 0.935894787311554  val: loss: 2048037.0 acc: 0.730498194694519\n",
      "step: 2155 , time : 0.0\n",
      "train: loss: 31341.939453125 acc: 0.983351469039917  val: loss: 1697681.75 acc: 0.7602109313011169\n",
      "step: 2160 , time : 0.0\n",
      "train: loss: 38085.4375 acc: 0.9807619452476501  val: loss: 2699830.75 acc: 0.6214770078659058\n",
      "step: 2165 , time : 0.0\n",
      "train: loss: 6059.29541015625 acc: 0.9863439202308655  val: loss: 1737727.125 acc: 0.5983110070228577\n",
      "step: 2170 , time : 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: loss: 18589.859375 acc: 0.9898062348365784  val: loss: 2422943.25 acc: 0.45908093452453613\n",
      "step: 2175 , time : 0.0\n",
      "train: loss: 32064.9453125 acc: 0.9857553243637085  val: loss: 1268479.875 acc: 0.6650487780570984\n",
      "step: 2180 , time : 0.0010008811950683594\n",
      "train: loss: 41147.03515625 acc: 0.9844111800193787  val: loss: 1493565.375 acc: -0.23527419567108154\n",
      "step: 2185 , time : 0.0010004043579101562\n",
      "train: loss: 27923.255859375 acc: 0.9891065359115601  val: loss: 2936129.5 acc: 0.02257150411605835\n",
      "step: 2190 , time : 0.0010006427764892578\n",
      "train: loss: 95851.3125 acc: 0.9717835187911987  val: loss: 1810383.25 acc: 0.7034955620765686\n",
      "step: 2195 , time : 0.0\n",
      "train: loss: 86152.375 acc: 0.9733353853225708  val: loss: 1364566.125 acc: 0.8005804419517517\n",
      "step: 2200 , time : 0.0\n",
      "train: loss: 82221.890625 acc: 0.9576689004898071  val: loss: 1150142.125 acc: 0.8981599807739258\n",
      "step: 2205 , time : 0.0\n",
      "train: loss: 235791.921875 acc: 0.8561684489250183  val: loss: 782183.75 acc: 0.863792896270752\n",
      "step: 2210 , time : 0.0\n",
      "train: loss: 366008.375 acc: 0.9411874413490295  val: loss: 2085316.25 acc: -0.2568777799606323\n",
      "step: 2215 , time : 0.0\n",
      "train: loss: 159431.640625 acc: 0.9774702191352844  val: loss: 1848248.25 acc: 0.7418090105056763\n",
      "step: 2220 , time : 0.0\n",
      "train: loss: 519520.125 acc: 0.9208333492279053  val: loss: 1378904.25 acc: 0.8021472692489624\n",
      "step: 2225 , time : 0.0\n",
      "train: loss: 385924.4375 acc: 0.9587399363517761  val: loss: 1051942.25 acc: 0.5443981885910034\n",
      "step: 2230 , time : 0.0\n",
      "train: loss: 173632.015625 acc: 0.9794816374778748  val: loss: 1264634.5 acc: 0.3136359453201294\n",
      "step: 2235 , time : 0.0010006427764892578\n",
      "train: loss: 172570.71875 acc: 0.9796642661094666  val: loss: 1957779.75 acc: 0.40424013137817383\n",
      "step: 2240 , time : 0.0\n",
      "train: loss: 458097.65625 acc: 0.9650410413742065  val: loss: 1420362.875 acc: 0.5994143486022949\n",
      "step: 2245 , time : 0.0\n",
      "train: loss: 580339.75 acc: 0.947472095489502  val: loss: 922707.25 acc: 0.32102054357528687\n",
      "step: 2250 , time : 0.0\n",
      "train: loss: 262337.25 acc: 0.9783735871315002  val: loss: 1872204.5 acc: 0.816351592540741\n",
      "step: 2255 , time : 0.0\n",
      "train: loss: 289087.21875 acc: 0.9640970230102539  val: loss: 1349725.25 acc: 0.8520023822784424\n",
      "step: 2260 , time : 0.0\n",
      "train: loss: 2622162.25 acc: 0.9295555949211121  val: loss: 472687.40625 acc: 0.929214596748352\n",
      "step: 2265 , time : 0.0\n",
      "train: loss: 1608942.75 acc: 0.9526658654212952  val: loss: 1818544.25 acc: 0.6712162494659424\n",
      "step: 2270 , time : 0.0010008811950683594\n",
      "train: loss: 562974.1875 acc: 0.9636180996894836  val: loss: 822929.3125 acc: 0.8263441324234009\n",
      "step: 2275 , time : 0.0\n",
      "train: loss: 516631.28125 acc: 0.9545897245407104  val: loss: 1530734.25 acc: 0.6215558052062988\n",
      "step: 2280 , time : 0.0\n",
      "train: loss: 327673.03125 acc: 0.9315131902694702  val: loss: 865926.0 acc: 0.5865478515625\n",
      "step: 2285 , time : 0.0\n",
      "train: loss: 166395.96875 acc: 0.9754456281661987  val: loss: 1722545.125 acc: 0.28610605001449585\n",
      "step: 2290 , time : 0.0\n",
      "train: loss: 1000930.9375 acc: 0.727782130241394  val: loss: 539896.25 acc: 0.8725206255912781\n",
      "step: 2295 , time : 0.0010004043579101562\n",
      "train: loss: 546603.4375 acc: 0.6953337788581848  val: loss: 1001123.9375 acc: 0.42116814851760864\n",
      "step: 2300 , time : 0.0010008811950683594\n",
      "train: loss: 809897.0625 acc: 0.563355565071106  val: loss: 1210230.625 acc: 0.48893266916275024\n",
      "step: 2305 , time : 0.0010006427764892578\n",
      "train: loss: 466906.65625 acc: 0.8136908411979675  val: loss: 880719.5625 acc: 0.8452835083007812\n",
      "step: 2310 , time : 0.0\n",
      "train: loss: 742787.375 acc: 0.8262438774108887  val: loss: 480378.84375 acc: 0.8231126070022583\n",
      "step: 2315 , time : 0.0\n",
      "train: loss: 2660405.25 acc: 0.03776824474334717  val: loss: 942240.75 acc: 0.8342405557632446\n",
      "step: 2320 , time : 0.0\n",
      "train: loss: 1016164.625 acc: 0.4853699207305908  val: loss: 991110.25 acc: 0.7408492565155029\n",
      "step: 2325 , time : 0.0\n",
      "train: loss: 782764.625 acc: 0.662335991859436  val: loss: 1085569.0 acc: 0.6678214073181152\n",
      "step: 2330 , time : 0.0\n",
      "train: loss: 266230.46875 acc: 0.7340313792228699  val: loss: 1256761.625 acc: 0.7125732898712158\n",
      "step: 2335 , time : 0.0\n",
      "train: loss: 92018.6640625 acc: 0.9243525862693787  val: loss: 745146.3125 acc: 0.838807225227356\n",
      "step: 2340 , time : 0.0\n",
      "train: loss: 90890.3359375 acc: 0.9258077144622803  val: loss: 682905.1875 acc: 0.7652180194854736\n",
      "step: 2345 , time : 0.0\n",
      "train: loss: 143530.796875 acc: 0.903546929359436  val: loss: 1152578.875 acc: 0.7547721266746521\n",
      "step: 2350 , time : 0.0\n",
      "train: loss: 214977.390625 acc: 0.8859530687332153  val: loss: 479619.625 acc: 0.781701922416687\n",
      "step: 2355 , time : 0.0\n",
      "train: loss: 191814.984375 acc: 0.8948329091072083  val: loss: 1608601.875 acc: 0.7095357179641724\n",
      "step: 2360 , time : 0.0\n",
      "train: loss: 156541.203125 acc: 0.8139662146568298  val: loss: 1186895.875 acc: 0.7713544964790344\n",
      "step: 2365 , time : 0.0\n",
      "train: loss: 88500.265625 acc: 0.8948087692260742  val: loss: 852410.75 acc: 0.7087198495864868\n",
      "step: 2370 , time : 0.0010006427764892578\n",
      "train: loss: 21039.9609375 acc: 0.977088212966919  val: loss: 366899.6875 acc: 0.8463605642318726\n",
      "step: 2375 , time : 0.0010006427764892578\n",
      "train: loss: 539671.5625 acc: 0.7149393558502197  val: loss: 1624250.375 acc: 0.7159427404403687\n",
      "step: 2380 , time : 0.0\n",
      "train: loss: 231065.453125 acc: 0.8026717305183411  val: loss: 1269468.5 acc: 0.6931722164154053\n",
      "step: 2385 , time : 0.0\n",
      "train: loss: 1017002.3125 acc: 0.6612944602966309  val: loss: 487702.96875 acc: 0.7144509553909302\n",
      "step: 2390 , time : 0.0\n",
      "train: loss: 482157.625 acc: 0.6860462427139282  val: loss: 2030230.0 acc: 0.7006057500839233\n",
      "step: 2395 , time : 0.0\n",
      "train: loss: 82388.6640625 acc: 0.883123517036438  val: loss: 1271174.5 acc: 0.6594610214233398\n",
      "step: 2400 , time : 0.0\n",
      "train: loss: 754636.0625 acc: 0.7151073217391968  val: loss: 1950149.625 acc: 0.6285910606384277\n",
      "step: 2405 , time : 0.0\n",
      "train: loss: 1618551.25 acc: 0.7639473080635071  val: loss: 1965328.375 acc: 0.7785203456878662\n",
      "step: 2410 , time : 0.0\n",
      "train: loss: 1190525.75 acc: 0.8343964219093323  val: loss: 1036097.625 acc: 0.7588598132133484\n",
      "step: 2415 , time : 0.0010004043579101562\n",
      "train: loss: 666859.0 acc: 0.9442064762115479  val: loss: 506066.65625 acc: 0.8004469871520996\n",
      "step: 2420 , time : 0.0\n",
      "train: loss: 369035.21875 acc: 0.955851137638092  val: loss: 468160.96875 acc: 0.9473634958267212\n",
      "step: 2425 , time : 0.0\n",
      "train: loss: 180339.265625 acc: 0.9639027118682861  val: loss: 569542.9375 acc: 0.949955940246582\n",
      "step: 2430 , time : 0.0\n",
      "train: loss: 178730.78125 acc: 0.9797216057777405  val: loss: 312405.59375 acc: 0.9417382478713989\n",
      "step: 2435 , time : 0.0\n",
      "train: loss: 225987.21875 acc: 0.9794735312461853  val: loss: 738060.9375 acc: 0.8458409309387207\n",
      "step: 2440 , time : 0.0\n",
      "train: loss: 186822.953125 acc: 0.9842237830162048  val: loss: 1240832.0 acc: 0.5716109275817871\n",
      "step: 2445 , time : 0.0\n",
      "train: loss: 221156.75 acc: 0.982844889163971  val: loss: 790599.25 acc: 0.8464012742042542\n",
      "step: 2450 , time : 0.0\n",
      "train: loss: 243147.75 acc: 0.9606484174728394  val: loss: 254030.171875 acc: 0.9606581330299377\n",
      "step: 2455 , time : 0.0\n",
      "train: loss: 104127.0234375 acc: 0.9854022860527039  val: loss: 2151476.5 acc: 0.5976787805557251\n",
      "step: 2460 , time : 0.0\n",
      "train: loss: 54398.30078125 acc: 0.9845585823059082  val: loss: 1760863.5 acc: 0.4236043095588684\n",
      "step: 2465 , time : 0.0\n",
      "train: loss: 36236.2421875 acc: 0.9754510521888733  val: loss: 1055033.375 acc: 0.8133387565612793\n",
      "step: 2470 , time : 0.0\n",
      "train: loss: 38918.953125 acc: 0.9860284924507141  val: loss: 1358241.75 acc: 0.9090310335159302\n",
      "step: 2475 , time : 0.0\n",
      "train: loss: 9246.4814453125 acc: 0.9869421720504761  val: loss: 1145798.25 acc: 0.78251051902771\n",
      "step: 2480 , time : 0.0\n",
      "train: loss: 35879.8046875 acc: 0.9665099382400513  val: loss: 1301845.375 acc: 0.5521258115768433\n",
      "step: 2485 , time : 0.0\n",
      "train: loss: 25777.3515625 acc: 0.9489816427230835  val: loss: 1081968.25 acc: 0.7824582457542419\n",
      "step: 2490 , time : 0.0\n",
      "train: loss: 19542.107421875 acc: 0.9735180139541626  val: loss: 1344685.25 acc: 0.4535672664642334\n",
      "step: 2495 , time : 0.0\n",
      "train: loss: 6059.85693359375 acc: 0.9541824460029602  val: loss: 573595.625 acc: 0.6602028608322144\n",
      "step: 2500 , time : 0.0\n",
      "train: loss: 8351.3876953125 acc: 0.9877092838287354  val: loss: 899739.8125 acc: 0.770990788936615\n",
      "step: 2505 , time : 0.0\n",
      "train: loss: 30585.2578125 acc: 0.9836799502372742  val: loss: 1517046.5 acc: 0.7943187355995178\n",
      "step: 2510 , time : 0.0\n",
      "train: loss: 34586.5625 acc: 0.9597962498664856  val: loss: 507011.09375 acc: 0.9273452758789062\n",
      "step: 2515 , time : 0.0\n",
      "train: loss: 44076.96875 acc: 0.9726036190986633  val: loss: 1434302.5 acc: 0.6402838230133057\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 2520 , time : 0.0\n",
      "train: loss: 31213.677734375 acc: 0.9836024045944214  val: loss: 1414181.625 acc: 0.40167444944381714\n",
      "step: 2525 , time : 0.0\n",
      "train: loss: 18432.240234375 acc: 0.9871680736541748  val: loss: 1376516.25 acc: 0.8009517192840576\n",
      "step: 2530 , time : 0.0\n",
      "train: loss: 31329.658203125 acc: 0.9752601981163025  val: loss: 1368346.0 acc: 0.635346531867981\n",
      "step: 2535 , time : 0.0010004043579101562\n",
      "train: loss: 14373.8642578125 acc: 0.9847437143325806  val: loss: 3144583.25 acc: 0.4115936756134033\n",
      "step: 2540 , time : 0.0010004043579101562\n",
      "train: loss: 31190.125 acc: 0.9888572692871094  val: loss: 1439143.5 acc: 0.8134581446647644\n",
      "step: 2545 , time : 0.0\n",
      "train: loss: 32519.533203125 acc: 0.990837037563324  val: loss: 3178409.5 acc: 0.28583192825317383\n",
      "step: 2550 , time : 0.0\n",
      "train: loss: 59857.10546875 acc: 0.9865645170211792  val: loss: 1292813.625 acc: 0.5809400081634521\n",
      "step: 2555 , time : 0.0\n",
      "train: loss: 32077.759765625 acc: 0.9614467620849609  val: loss: 1735737.625 acc: 0.47945529222488403\n",
      "step: 2560 , time : 0.0\n",
      "train: loss: 48288.6171875 acc: 0.9880921244621277  val: loss: 1008697.0625 acc: 0.6554064154624939\n",
      "step: 2565 , time : 0.0\n",
      "train: loss: 170527.046875 acc: 0.9707387089729309  val: loss: 2033371.125 acc: 0.6034708023071289\n",
      "step: 2570 , time : 0.0\n",
      "train: loss: 147688.15625 acc: 0.9533592462539673  val: loss: 993671.3125 acc: 0.6732319593429565\n",
      "step: 2575 , time : 0.0\n",
      "train: loss: 173787.59375 acc: 0.9586402177810669  val: loss: 386360.875 acc: 0.9413900375366211\n",
      "step: 2580 , time : 0.0\n",
      "train: loss: 245939.75 acc: 0.963532567024231  val: loss: 1269771.0 acc: 0.8606247305870056\n",
      "step: 2585 , time : 0.0\n",
      "train: loss: 262346.0 acc: 0.9752569794654846  val: loss: 268302.4375 acc: 0.9821161031723022\n",
      "step: 2590 , time : 0.0\n",
      "train: loss: 159739.0 acc: 0.9811519384384155  val: loss: 1900907.5 acc: 0.8229860663414001\n",
      "step: 2595 , time : 0.0\n",
      "train: loss: 251118.375 acc: 0.9077482223510742  val: loss: 1246201.375 acc: 0.8283237218856812\n",
      "step: 2600 , time : 0.0\n",
      "train: loss: 201032.3125 acc: 0.9794267416000366  val: loss: 2277424.75 acc: -0.06598222255706787\n",
      "step: 2605 , time : 0.0\n",
      "train: loss: 546659.0 acc: 0.9709463119506836  val: loss: 739023.125 acc: 0.8990954756736755\n",
      "step: 2610 , time : 0.0\n",
      "train: loss: 289951.5 acc: 0.9722572565078735  val: loss: 1575473.375 acc: 0.34463202953338623\n",
      "step: 2615 , time : 0.0\n",
      "train: loss: 219224.796875 acc: 0.9776081442832947  val: loss: 1278560.5 acc: 0.8259978294372559\n",
      "step: 2620 , time : 0.0\n",
      "train: loss: 548479.8125 acc: 0.9727135300636292  val: loss: 386896.59375 acc: 0.9357865452766418\n",
      "step: 2625 , time : 0.0\n",
      "train: loss: 1969445.25 acc: 0.9245929718017578  val: loss: 3502140.75 acc: 0.6550360321998596\n",
      "step: 2630 , time : 0.0\n",
      "train: loss: 1344284.0 acc: 0.9563377499580383  val: loss: 724347.375 acc: 0.692803144454956\n",
      "step: 2635 , time : 0.0\n",
      "train: loss: 684314.9375 acc: 0.9160699844360352  val: loss: 177925.046875 acc: 0.9715470671653748\n",
      "step: 2640 , time : 0.0\n",
      "train: loss: 1132949.25 acc: 0.9389487504959106  val: loss: 1503695.125 acc: 0.6451189517974854\n",
      "step: 2645 , time : 0.0\n",
      "train: loss: 609281.875 acc: 0.935230016708374  val: loss: 809029.8125 acc: 0.5322996377944946\n",
      "step: 2650 , time : 0.0\n",
      "train: loss: 89404.5703125 acc: 0.982387900352478  val: loss: 595987.1875 acc: 0.8825560212135315\n",
      "step: 2655 , time : 0.0010004043579101562\n",
      "train: loss: 3037524.75 acc: -0.43023836612701416  val: loss: 409303.78125 acc: 0.8013476729393005\n",
      "step: 2660 , time : 0.0\n",
      "train: loss: 1104741.5 acc: 0.7695006728172302  val: loss: 1199306.75 acc: 0.7343704700469971\n",
      "step: 2665 , time : 0.0\n",
      "train: loss: 582449.875 acc: 0.7462830543518066  val: loss: 510446.21875 acc: 0.6885842084884644\n",
      "step: 2670 , time : 0.0\n",
      "train: loss: 769837.375 acc: 0.6498770117759705  val: loss: 1011563.5 acc: 0.8285442590713501\n",
      "step: 2675 , time : 0.0\n",
      "train: loss: 642778.0 acc: 0.8555350303649902  val: loss: 307953.6875 acc: 0.8430867195129395\n",
      "step: 2680 , time : 0.0\n",
      "train: loss: 2266470.75 acc: 0.241521418094635  val: loss: 693074.6875 acc: 0.39155274629592896\n",
      "step: 2685 , time : 0.0\n",
      "train: loss: 2044287.25 acc: 0.32542848587036133  val: loss: 1067375.125 acc: 0.7834833860397339\n",
      "step: 2690 , time : 0.0\n",
      "train: loss: 1465443.75 acc: 0.3251745104789734  val: loss: 2946177.0 acc: 0.5653125047683716\n",
      "step: 2695 , time : 0.0\n",
      "train: loss: 779052.5625 acc: 0.4058763384819031  val: loss: 1572987.5 acc: 0.5497195720672607\n",
      "step: 2700 , time : 0.0\n",
      "train: loss: 450334.625 acc: 0.6205108761787415  val: loss: 1820621.125 acc: 0.6264785528182983\n",
      "step: 2705 , time : 0.0\n",
      "train: loss: 80643.15625 acc: 0.9360177516937256  val: loss: 1769565.875 acc: 0.7836141586303711\n",
      "step: 2710 , time : 0.0\n",
      "train: loss: 527096.3125 acc: 0.7789978981018066  val: loss: 807387.25 acc: 0.799960732460022\n",
      "step: 2715 , time : 0.0\n",
      "train: loss: 376010.5 acc: 0.8405349254608154  val: loss: 1336749.125 acc: 0.7996501326560974\n",
      "step: 2720 , time : 0.015626192092895508\n",
      "train: loss: 253660.71875 acc: 0.8694676160812378  val: loss: 1684299.75 acc: 0.7867494225502014\n",
      "step: 2725 , time : 0.0\n",
      "train: loss: 156130.421875 acc: 0.8432791829109192  val: loss: 934610.9375 acc: 0.863088846206665\n",
      "step: 2730 , time : 0.0\n",
      "train: loss: 204395.265625 acc: 0.7640452980995178  val: loss: 2413695.75 acc: 0.8271085023880005\n",
      "step: 2735 , time : 0.0\n",
      "train: loss: 31503.142578125 acc: 0.9618664979934692  val: loss: 1567886.25 acc: 0.693318247795105\n",
      "step: 2740 , time : 0.0\n",
      "train: loss: 436720.25 acc: 0.7150280475616455  val: loss: 1145874.125 acc: 0.8219603300094604\n",
      "step: 2745 , time : 0.0\n",
      "train: loss: 561097.25 acc: 0.6622503995895386  val: loss: 1310641.0 acc: 0.5805609226226807\n",
      "step: 2750 , time : 0.0\n",
      "train: loss: 553809.0625 acc: 0.49957913160324097  val: loss: 1278128.0 acc: 0.8892736434936523\n",
      "step: 2755 , time : 0.0\n",
      "train: loss: 391296.21875 acc: 0.5352524518966675  val: loss: 2121000.75 acc: 0.7157803773880005\n",
      "step: 2760 , time : 0.0\n",
      "train: loss: 250088.0625 acc: 0.8178005814552307  val: loss: 2166226.0 acc: 0.6758373975753784\n",
      "step: 2765 , time : 0.0\n",
      "train: loss: 782959.125 acc: 0.6278496384620667  val: loss: 2276554.25 acc: 0.7887880802154541\n",
      "step: 2770 , time : 0.0010008811950683594\n",
      "train: loss: 1611011.875 acc: 0.7488503456115723  val: loss: 2003213.75 acc: 0.7743895649909973\n",
      "step: 2775 , time : 0.0\n",
      "train: loss: 1634058.0 acc: 0.8306267261505127  val: loss: 358238.21875 acc: 0.843579888343811\n",
      "step: 2780 , time : 0.0\n",
      "train: loss: 599717.5625 acc: 0.9607595205307007  val: loss: 1497453.25 acc: 0.7186633348464966\n",
      "step: 2785 , time : 0.0\n",
      "train: loss: 396661.875 acc: 0.9605334997177124  val: loss: 786453.25 acc: 0.8758456707000732\n",
      "step: 2790 , time : 0.0\n",
      "train: loss: 505560.59375 acc: 0.9232893586158752  val: loss: 1444115.75 acc: 0.5436955690383911\n",
      "step: 2795 , time : 0.0\n",
      "train: loss: 573846.4375 acc: 0.922126054763794  val: loss: 1752212.5 acc: 0.6861973404884338\n",
      "step: 2800 , time : 0.0\n",
      "train: loss: 382476.71875 acc: 0.9694464802742004  val: loss: 389100.125 acc: 0.8837591409683228\n",
      "step: 2805 , time : 0.0\n",
      "train: loss: 273703.375 acc: 0.9816169738769531  val: loss: 793336.625 acc: 0.726687490940094\n",
      "step: 2810 , time : 0.0\n",
      "train: loss: 236210.1875 acc: 0.9707584977149963  val: loss: 3481504.5 acc: 0.4178975820541382\n",
      "step: 2815 , time : 0.0\n",
      "train: loss: 249935.4375 acc: 0.9733849167823792  val: loss: 2515199.0 acc: 0.7576770782470703\n",
      "step: 2820 , time : 0.0\n",
      "train: loss: 487418.3125 acc: 0.9376239776611328  val: loss: 1568329.25 acc: 0.552143931388855\n",
      "step: 2825 , time : 0.0\n",
      "train: loss: 71820.4453125 acc: 0.8115805387496948  val: loss: 897529.9375 acc: 0.8466591835021973\n",
      "step: 2830 , time : 0.0\n",
      "train: loss: 14071.240234375 acc: 0.9374017119407654  val: loss: 782741.1875 acc: 0.718794584274292\n",
      "step: 2835 , time : 0.0\n",
      "train: loss: 28662.451171875 acc: 0.9559341669082642  val: loss: 1557505.625 acc: 0.290416955947876\n",
      "step: 2840 , time : 0.0\n",
      "train: loss: 24054.8828125 acc: 0.9575326442718506  val: loss: 1046656.875 acc: 0.5231518149375916\n",
      "step: 2845 , time : 0.0\n",
      "train: loss: 14469.173828125 acc: 0.9555144309997559  val: loss: 323931.65625 acc: 0.9109293222427368\n",
      "step: 2850 , time : 0.0\n",
      "train: loss: 14609.310546875 acc: 0.9629194736480713  val: loss: 1747854.25 acc: 0.8080427646636963\n",
      "step: 2855 , time : 0.0\n",
      "train: loss: 11501.1669921875 acc: 0.9748206734657288  val: loss: 1581521.375 acc: 0.5786051154136658\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 2860 , time : 0.015625715255737305\n",
      "train: loss: 17661.548828125 acc: 0.9543543457984924  val: loss: 1515737.0 acc: 0.3573377728462219\n",
      "step: 2865 , time : 0.0\n",
      "train: loss: 13035.267578125 acc: 0.9800072312355042  val: loss: 3536581.5 acc: 0.5934851169586182\n",
      "step: 2870 , time : 0.0\n",
      "train: loss: 25835.41796875 acc: 0.9770110845565796  val: loss: 854799.25 acc: 0.8535031080245972\n",
      "step: 2875 , time : 0.0010008811950683594\n",
      "train: loss: 36011.4296875 acc: 0.973050057888031  val: loss: 2510918.0 acc: -0.5662362575531006\n",
      "step: 2880 , time : 0.001001119613647461\n",
      "train: loss: 39607.57421875 acc: 0.9711301922798157  val: loss: 906523.3125 acc: 0.8832330703735352\n",
      "step: 2885 , time : 0.0\n",
      "train: loss: 33371.32421875 acc: 0.9826061725616455  val: loss: 1135910.75 acc: 0.8888132572174072\n",
      "step: 2890 , time : 0.0\n",
      "train: loss: 11984.744140625 acc: 0.9904017448425293  val: loss: 1724894.375 acc: 0.6696560382843018\n",
      "step: 2895 , time : 0.0\n",
      "train: loss: 28540.986328125 acc: 0.9638754725456238  val: loss: 1192588.375 acc: 0.7468664646148682\n",
      "step: 2900 , time : 0.0\n",
      "train: loss: 19542.892578125 acc: 0.9853326082229614  val: loss: 1074090.5 acc: 0.6982187032699585\n",
      "step: 2905 , time : 0.0\n",
      "train: loss: 22313.56640625 acc: 0.9884681701660156  val: loss: 267535.125 acc: 0.9772340655326843\n",
      "step: 2910 , time : 0.0\n",
      "train: loss: 41630.484375 acc: 0.989115297794342  val: loss: 668998.5 acc: 0.8167809247970581\n",
      "step: 2915 , time : 0.0\n",
      "train: loss: 66099.5390625 acc: 0.9842332005500793  val: loss: 1921908.375 acc: 0.7850409150123596\n",
      "step: 2920 , time : 0.0\n",
      "train: loss: 30904.130859375 acc: 0.9871450662612915  val: loss: 769126.1875 acc: 0.8876099586486816\n",
      "step: 2925 , time : 0.0\n",
      "train: loss: 76998.625 acc: 0.9833409786224365  val: loss: 166445.28125 acc: 0.9160698652267456\n",
      "step: 2930 , time : 0.0\n",
      "train: loss: 143698.84375 acc: 0.9644256830215454  val: loss: 680360.8125 acc: 0.9423801302909851\n",
      "step: 2935 , time : 0.0\n",
      "train: loss: 117263.4375 acc: 0.9373649954795837  val: loss: 1257940.875 acc: 0.8075774312019348\n",
      "step: 2940 , time : 0.0\n",
      "train: loss: 90788.546875 acc: 0.9577242732048035  val: loss: 477533.9375 acc: 0.918123185634613\n",
      "step: 2945 , time : 0.0\n",
      "train: loss: 590221.25 acc: 0.8333780169487  val: loss: 140866.8125 acc: 0.9696379899978638\n",
      "step: 2950 , time : 0.0\n",
      "train: loss: 94928.578125 acc: 0.9878686666488647  val: loss: 319059.0 acc: 0.8658174276351929\n",
      "step: 2955 , time : 0.0\n",
      "train: loss: 100395.4453125 acc: 0.9876912832260132  val: loss: 174365.484375 acc: 0.9388351440429688\n",
      "step: 2960 , time : 0.0\n",
      "train: loss: 65051.1953125 acc: 0.991304337978363  val: loss: 464682.5625 acc: 0.8651280403137207\n",
      "step: 2965 , time : 0.0\n",
      "train: loss: 179904.640625 acc: 0.9798887968063354  val: loss: 1071167.25 acc: 0.6128538846969604\n",
      "step: 2970 , time : 0.0\n",
      "train: loss: 378465.53125 acc: 0.9675858020782471  val: loss: 1807604.0 acc: 0.3333509564399719\n",
      "step: 2975 , time : 0.0\n",
      "train: loss: 393268.34375 acc: 0.9733893275260925  val: loss: 1225564.5 acc: 0.7894555330276489\n",
      "step: 2980 , time : 0.0\n",
      "train: loss: 300453.4375 acc: 0.9747663140296936  val: loss: 522763.46875 acc: 0.7029514908790588\n",
      "step: 2985 , time : 0.0\n",
      "train: loss: 829865.75 acc: 0.9656442403793335  val: loss: 1756530.875 acc: 0.30801886320114136\n",
      "step: 2990 , time : 0.0\n",
      "train: loss: 1810668.75 acc: 0.9483075737953186  val: loss: 239782.0625 acc: 0.9028626084327698\n",
      "step: 2995 , time : 0.0\n",
      "train: loss: 1167088.875 acc: 0.9558289051055908  val: loss: 327925.1875 acc: 0.9189867973327637\n",
      "step: 3000 , time : 0.001001119613647461\n",
      "train: loss: 831644.125 acc: 0.9561303853988647  val: loss: 363007.4375 acc: 0.9308382868766785\n",
      "step: 3005 , time : 0.0\n",
      "train: loss: 618064.375 acc: 0.9634525775909424  val: loss: 787146.75 acc: 0.7955735921859741\n",
      "step: 3010 , time : 0.0010008811950683594\n",
      "train: loss: 453737.4375 acc: 0.955287516117096  val: loss: 354789.875 acc: 0.9551829099655151\n",
      "step: 3015 , time : 0.0\n",
      "train: loss: 490225.8125 acc: 0.8864986300468445  val: loss: 211699.046875 acc: 0.9381985664367676\n",
      "step: 3020 , time : 0.0010004043579101562\n",
      "train: loss: 441414.78125 acc: 0.846121609210968  val: loss: 442283.71875 acc: 0.9183814525604248\n",
      "step: 3025 , time : 0.0\n",
      "train: loss: 762491.875 acc: 0.6990576386451721  val: loss: 1608770.25 acc: 0.7085591554641724\n",
      "step: 3030 , time : 0.0\n",
      "train: loss: 888066.875 acc: 0.7829314470291138  val: loss: 973583.25 acc: 0.8046590685844421\n",
      "step: 3035 , time : 0.0\n",
      "train: loss: 1157547.0 acc: 0.3977571725845337  val: loss: 964848.4375 acc: 0.7190976142883301\n",
      "step: 3040 , time : 0.0\n",
      "train: loss: 452512.09375 acc: 0.9128423929214478  val: loss: 239251.71875 acc: 0.9283714294433594\n",
      "step: 3045 , time : 0.0\n",
      "train: loss: 1718916.5 acc: 0.47473716735839844  val: loss: 476057.25 acc: 0.8602196574211121\n",
      "step: 3050 , time : 0.0\n",
      "train: loss: 1266322.75 acc: 0.5211955308914185  val: loss: 3711658.25 acc: 0.7861987352371216\n",
      "step: 3055 , time : 0.0\n",
      "train: loss: 622274.5 acc: 0.6420530080795288  val: loss: 1982221.0 acc: 0.8031336665153503\n",
      "step: 3060 , time : 0.0156252384185791\n",
      "train: loss: 269482.90625 acc: 0.7718257904052734  val: loss: 889381.9375 acc: 0.7158870697021484\n",
      "step: 3065 , time : 0.0\n",
      "train: loss: 116392.3671875 acc: 0.9055793285369873  val: loss: 1072260.125 acc: 0.7231550216674805\n",
      "step: 3070 , time : 0.0\n",
      "train: loss: 53262.421875 acc: 0.9563566446304321  val: loss: 790846.25 acc: 0.8472774624824524\n",
      "step: 3075 , time : 0.0\n",
      "train: loss: 125754.5 acc: 0.9142262935638428  val: loss: 777519.4375 acc: 0.7537509202957153\n",
      "step: 3080 , time : 0.0\n",
      "train: loss: 76662.3828125 acc: 0.9352260828018188  val: loss: 1480114.75 acc: 0.7812515497207642\n",
      "step: 3085 , time : 0.0\n",
      "train: loss: 40868.3671875 acc: 0.9660399556159973  val: loss: 498799.875 acc: 0.7279687523841858\n",
      "step: 3090 , time : 0.0\n",
      "train: loss: 319320.4375 acc: 0.8411440253257751  val: loss: 2502240.75 acc: 0.7157325744628906\n",
      "step: 3095 , time : 0.0010008811950683594\n",
      "train: loss: 51762.8515625 acc: 0.9523634910583496  val: loss: 3230030.0 acc: 0.7480303645133972\n",
      "step: 3100 , time : 0.0\n",
      "train: loss: 129931.78125 acc: 0.8387783169746399  val: loss: 1956883.25 acc: 0.7677472829818726\n",
      "step: 3105 , time : 0.0010004043579101562\n",
      "train: loss: 133554.6875 acc: 0.8777155876159668  val: loss: 2821443.75 acc: 0.46780794858932495\n",
      "step: 3110 , time : 0.0\n",
      "train: loss: 268977.9375 acc: 0.8474169373512268  val: loss: 2442992.25 acc: 0.6742165088653564\n",
      "step: 3115 , time : 0.0\n",
      "train: loss: 270982.96875 acc: 0.7869489192962646  val: loss: 3740594.25 acc: 0.6975870132446289\n",
      "step: 3120 , time : 0.001001119613647461\n",
      "train: loss: 346011.84375 acc: 0.6506831049919128  val: loss: 1703158.5 acc: 0.7526824474334717\n",
      "step: 3125 , time : 0.0010008811950683594\n",
      "train: loss: 257182.4375 acc: 0.8002251386642456  val: loss: 2470133.5 acc: 0.6356330513954163\n",
      "step: 3130 , time : 0.0\n",
      "train: loss: 649433.5625 acc: 0.7171764969825745  val: loss: 3728588.75 acc: 0.6034777760505676\n",
      "step: 3135 , time : 0.0\n",
      "train: loss: 1521617.625 acc: 0.7066606283187866  val: loss: 609001.3125 acc: 0.7267283201217651\n",
      "step: 3140 , time : 0.0\n",
      "train: loss: 1766325.875 acc: 0.7826842069625854  val: loss: 435818.9375 acc: 0.8647966980934143\n",
      "step: 3145 , time : 0.0\n",
      "train: loss: 1722880.75 acc: 0.8487522006034851  val: loss: 1256052.0 acc: 0.7144392728805542\n",
      "step: 3150 , time : 0.0\n",
      "train: loss: 340931.625 acc: 0.927587628364563  val: loss: 855400.75 acc: 0.8615831136703491\n",
      "step: 3155 , time : 0.0\n",
      "train: loss: 722408.1875 acc: 0.8194807767868042  val: loss: 645023.0 acc: 0.7929176092147827\n",
      "step: 3160 , time : 0.0\n",
      "train: loss: 262847.875 acc: 0.9388558864593506  val: loss: 345963.625 acc: 0.7794015407562256\n",
      "step: 3165 , time : 0.0\n",
      "train: loss: 212986.859375 acc: 0.9781544208526611  val: loss: 983920.25 acc: 0.8031527996063232\n",
      "step: 3170 , time : 0.0\n",
      "train: loss: 226263.046875 acc: 0.9852822422981262  val: loss: 1767045.25 acc: 0.5404496192932129\n",
      "step: 3175 , time : 0.0\n",
      "train: loss: 260260.234375 acc: 0.9797788858413696  val: loss: 2049435.375 acc: -0.27546703815460205\n",
      "step: 3180 , time : 0.0\n",
      "train: loss: 179466.8125 acc: 0.9841980934143066  val: loss: 1043309.625 acc: 0.8807681798934937\n",
      "step: 3185 , time : 0.0\n",
      "train: loss: 83398.6875 acc: 0.9775059819221497  val: loss: 1445813.375 acc: 0.8091278076171875\n",
      "step: 3190 , time : 0.0\n",
      "train: loss: 171170.78125 acc: 0.9644107222557068  val: loss: 248712.40625 acc: 0.9489750266075134\n",
      "step: 3195 , time : 0.0\n",
      "train: loss: 57239.203125 acc: 0.9571332931518555  val: loss: 2035129.625 acc: 0.6322543621063232\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 3200 , time : 0.0\n",
      "train: loss: 44211.70703125 acc: 0.9674695730209351  val: loss: 356302.5 acc: 0.9270592927932739\n",
      "step: 3205 , time : 0.0\n",
      "train: loss: 13546.916015625 acc: 0.9715627431869507  val: loss: 2005667.375 acc: 0.6286190152168274\n",
      "step: 3210 , time : 0.0\n",
      "train: loss: 19536.759765625 acc: 0.9751809239387512  val: loss: 2563298.75 acc: -0.6508002281188965\n",
      "step: 3215 , time : 0.0\n",
      "train: loss: 10799.9169921875 acc: 0.9815267324447632  val: loss: 1574028.0 acc: 0.6319769620895386\n",
      "step: 3220 , time : 0.0010004043579101562\n",
      "train: loss: 12451.7099609375 acc: 0.9697570204734802  val: loss: 810225.1875 acc: 0.7432055473327637\n",
      "step: 3225 , time : 0.0010006427764892578\n",
      "train: loss: 15702.6943359375 acc: 0.9707748889923096  val: loss: 1273146.0 acc: 0.7391051054000854\n",
      "step: 3230 , time : 0.0\n",
      "train: loss: 11483.7373046875 acc: 0.9746460914611816  val: loss: 193693.484375 acc: 0.9740545749664307\n",
      "step: 3235 , time : 0.0\n",
      "train: loss: 17512.634765625 acc: 0.9851014018058777  val: loss: 1018427.0625 acc: 0.7387533783912659\n",
      "step: 3240 , time : 0.0\n",
      "train: loss: 35391.609375 acc: 0.9737405776977539  val: loss: 532804.375 acc: 0.8076081275939941\n",
      "step: 3245 , time : 0.0\n",
      "train: loss: 41993.68359375 acc: 0.977297306060791  val: loss: 1283912.875 acc: 0.8037363290786743\n",
      "step: 3250 , time : 0.0\n",
      "train: loss: 30914.240234375 acc: 0.973327100276947  val: loss: 189866.65625 acc: 0.9757296442985535\n",
      "step: 3255 , time : 0.0\n",
      "train: loss: 99625.21875 acc: 0.9471496939659119  val: loss: 404631.4375 acc: 0.8365594148635864\n",
      "step: 3260 , time : 0.0\n",
      "train: loss: 5591.64013671875 acc: 0.9881189465522766  val: loss: 172213.28125 acc: 0.9648123979568481\n",
      "step: 3265 , time : 0.0\n",
      "train: loss: 13962.9658203125 acc: 0.9828923344612122  val: loss: 1413201.25 acc: 0.44366705417633057\n",
      "step: 3270 , time : 0.0\n",
      "train: loss: 23154.662109375 acc: 0.9902663230895996  val: loss: 1015184.5 acc: 0.7857033014297485\n",
      "step: 3275 , time : 0.0\n",
      "train: loss: 33377.79296875 acc: 0.9924554228782654  val: loss: 508860.8125 acc: 0.9201040267944336\n",
      "step: 3280 , time : 0.0\n",
      "train: loss: 35174.95703125 acc: 0.9889265894889832  val: loss: 1277746.0 acc: 0.8199122548103333\n",
      "step: 3285 , time : 0.0\n",
      "train: loss: 33347.640625 acc: 0.9908064007759094  val: loss: 1228587.625 acc: 0.6379736661911011\n",
      "step: 3290 , time : 0.0\n",
      "train: loss: 21006.060546875 acc: 0.9878451824188232  val: loss: 1328554.375 acc: 0.7960214614868164\n",
      "step: 3295 , time : 0.0\n",
      "train: loss: 145694.109375 acc: 0.9628857374191284  val: loss: 1392892.5 acc: 0.6011190414428711\n",
      "step: 3300 , time : 0.0\n",
      "train: loss: 140978.796875 acc: 0.9576824903488159  val: loss: 1610080.875 acc: 0.6277243494987488\n",
      "step: 3305 , time : 0.0\n",
      "train: loss: 153830.84375 acc: 0.9522660374641418  val: loss: 520663.15625 acc: 0.9449676871299744\n",
      "step: 3310 , time : 0.0\n",
      "train: loss: 102983.46875 acc: 0.9806468486785889  val: loss: 778196.875 acc: 0.897189736366272\n",
      "step: 3315 , time : 0.0\n",
      "train: loss: 998049.75 acc: 0.9004855751991272  val: loss: 1494266.5 acc: 0.5283322334289551\n",
      "step: 3320 , time : 0.0\n",
      "train: loss: 78508.734375 acc: 0.9884698987007141  val: loss: 2724315.5 acc: 0.30391865968704224\n",
      "step: 3325 , time : 0.0\n",
      "train: loss: 322376.0625 acc: 0.9287122488021851  val: loss: 845997.0625 acc: 0.5838337540626526\n",
      "step: 3330 , time : 0.0010008811950683594\n",
      "train: loss: 403309.96875 acc: 0.9748784303665161  val: loss: 454418.71875 acc: 0.8719499707221985\n",
      "step: 3335 , time : 0.001001119613647461\n",
      "train: loss: 203655.90625 acc: 0.9891141653060913  val: loss: 982604.25 acc: 0.1505909562110901\n",
      "step: 3340 , time : 0.0\n",
      "train: loss: 337314.65625 acc: 0.9659007787704468  val: loss: 755813.3125 acc: 0.7144854068756104\n",
      "step: 3345 , time : 0.0\n",
      "train: loss: 388218.71875 acc: 0.968370795249939  val: loss: 542355.75 acc: 0.9512202143669128\n",
      "step: 3350 , time : 0.0\n",
      "train: loss: 1125337.75 acc: 0.9543264508247375  val: loss: 560626.9375 acc: 0.9208421111106873\n",
      "step: 3355 , time : 0.0\n",
      "train: loss: 1202266.0 acc: 0.9562287926673889  val: loss: 453817.8125 acc: 0.9328733086585999\n",
      "step: 3360 , time : 0.0\n",
      "train: loss: 1213791.75 acc: 0.9507306218147278  val: loss: 470036.5625 acc: 0.9354802966117859\n",
      "step: 3365 , time : 0.0\n",
      "train: loss: 1431376.5 acc: 0.9257166385650635  val: loss: 326026.25 acc: 0.9085415601730347\n",
      "step: 3370 , time : 0.0\n",
      "train: loss: 392962.0625 acc: 0.94585120677948  val: loss: 1529212.125 acc: 0.8019866943359375\n",
      "step: 3375 , time : 0.0\n",
      "train: loss: 216494.1875 acc: 0.9702473878860474  val: loss: 639759.375 acc: 0.8535546660423279\n",
      "step: 3380 , time : 0.0\n",
      "train: loss: 821633.0 acc: 0.9041388034820557  val: loss: 935594.125 acc: 0.8850398659706116\n",
      "step: 3385 , time : 0.0\n",
      "train: loss: 1289464.875 acc: 0.7598012089729309  val: loss: 777290.75 acc: 0.9197942018508911\n",
      "step: 3390 , time : 0.0\n",
      "train: loss: 946295.0 acc: 0.5573384761810303  val: loss: 1406103.75 acc: 0.8501114845275879\n",
      "step: 3395 , time : 0.0\n",
      "train: loss: 1087724.5 acc: 0.7996790409088135  val: loss: 1313753.375 acc: 0.7997111082077026\n",
      "step: 3400 , time : 0.0\n",
      "train: loss: 930270.5 acc: 0.47136640548706055  val: loss: 351210.875 acc: 0.8978893756866455\n",
      "step: 3405 , time : 0.0\n",
      "train: loss: 604498.5 acc: 0.810605525970459  val: loss: 676210.3125 acc: 0.8354854583740234\n",
      "step: 3410 , time : 0.0\n",
      "train: loss: 1505234.25 acc: 0.5791016221046448  val: loss: 1029928.5625 acc: 0.7504553198814392\n",
      "step: 3415 , time : 0.0\n",
      "train: loss: 1361590.75 acc: 0.4001927375793457  val: loss: 2712776.25 acc: 0.6407855749130249\n",
      "step: 3420 , time : 0.0\n",
      "train: loss: 438088.15625 acc: 0.7001743912696838  val: loss: 923697.9375 acc: 0.790402889251709\n",
      "step: 3425 , time : 0.0\n",
      "train: loss: 432870.40625 acc: 0.6686770915985107  val: loss: 2677558.75 acc: 0.7567773461341858\n",
      "step: 3430 , time : 0.0\n",
      "train: loss: 397096.0 acc: 0.6459215879440308  val: loss: 1505138.875 acc: 0.6785756945610046\n",
      "step: 3435 , time : 0.0\n",
      "train: loss: 157851.0625 acc: 0.8951340317726135  val: loss: 1572634.625 acc: 0.8131715655326843\n",
      "step: 3440 , time : 0.0\n",
      "train: loss: 43528.59375 acc: 0.9661837220191956  val: loss: 710789.375 acc: 0.7054437398910522\n",
      "step: 3445 , time : 0.0\n",
      "train: loss: 318942.90625 acc: 0.7866742014884949  val: loss: 2375606.0 acc: 0.7544858455657959\n",
      "step: 3450 , time : 0.0010008811950683594\n",
      "train: loss: 13267.849609375 acc: 0.989534318447113  val: loss: 2279602.25 acc: 0.7519130706787109\n",
      "step: 3455 , time : 0.0010008811950683594\n",
      "train: loss: 156132.484375 acc: 0.8844763040542603  val: loss: 1109906.0 acc: 0.7711743116378784\n",
      "step: 3460 , time : 0.0010006427764892578\n",
      "train: loss: 57287.5078125 acc: 0.9450021982192993  val: loss: 1562923.0 acc: 0.7548381686210632\n",
      "step: 3465 , time : 0.0010006427764892578\n",
      "train: loss: 83874.3828125 acc: 0.9096387624740601  val: loss: 583249.5625 acc: 0.8601903915405273\n",
      "step: 3470 , time : 0.0\n",
      "train: loss: 163237.5 acc: 0.867424726486206  val: loss: 419259.28125 acc: 0.7722469568252563\n",
      "step: 3475 , time : 0.0\n",
      "train: loss: 588636.5625 acc: 0.7253496050834656  val: loss: 1512574.5 acc: 0.7848770022392273\n",
      "step: 3480 , time : 0.0\n",
      "train: loss: 360634.0625 acc: 0.7130833268165588  val: loss: 1527414.5 acc: 0.7524359226226807\n",
      "step: 3485 , time : 0.015625953674316406\n",
      "train: loss: 335822.15625 acc: 0.7721508741378784  val: loss: 876341.4375 acc: 0.6872282028198242\n",
      "step: 3490 , time : 0.0\n",
      "train: loss: 260088.828125 acc: 0.7155469655990601  val: loss: 2719550.0 acc: 0.6509422063827515\n",
      "step: 3495 , time : 0.0\n",
      "train: loss: 444983.75 acc: 0.7880300283432007  val: loss: 1850890.625 acc: 0.6380198001861572\n",
      "step: 3500 , time : 0.0\n",
      "train: loss: 2118973.5 acc: 0.6689161062240601  val: loss: 1651417.375 acc: 0.7706215381622314\n",
      "step: 3505 , time : 0.0\n",
      "train: loss: 1378965.0 acc: 0.706667423248291  val: loss: 995491.4375 acc: 0.7679523825645447\n",
      "step: 3510 , time : 0.0\n",
      "train: loss: 1163059.0 acc: 0.883793830871582  val: loss: 1109274.0 acc: 0.8574105501174927\n",
      "step: 3515 , time : 0.0\n",
      "train: loss: 826357.125 acc: 0.9023908376693726  val: loss: 988197.5 acc: 0.77363121509552\n",
      "step: 3520 , time : 0.0\n",
      "train: loss: 209286.171875 acc: 0.9598357677459717  val: loss: 1324996.625 acc: 0.805778980255127\n",
      "step: 3525 , time : 0.0\n",
      "train: loss: 195779.609375 acc: 0.9582703113555908  val: loss: 2229188.75 acc: 0.46605080366134644\n",
      "step: 3530 , time : 0.0\n",
      "train: loss: 199380.421875 acc: 0.9750162363052368  val: loss: 490396.8125 acc: 0.8428043127059937\n",
      "step: 3535 , time : 0.0\n",
      "train: loss: 172525.625 acc: 0.9864201545715332  val: loss: 740454.125 acc: 0.8624793291091919\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 3540 , time : 0.0\n",
      "train: loss: 168806.484375 acc: 0.9852861166000366  val: loss: 2335349.5 acc: 0.3600809574127197\n",
      "step: 3545 , time : 0.0\n",
      "train: loss: 282368.3125 acc: 0.9745607972145081  val: loss: 710695.125 acc: 0.8105589151382446\n",
      "step: 3550 , time : 0.0010004043579101562\n",
      "train: loss: 64065.7421875 acc: 0.9841875433921814  val: loss: 1641361.125 acc: 0.7125736474990845\n",
      "step: 3555 , time : 0.0\n",
      "train: loss: 74679.28125 acc: 0.9823891520500183  val: loss: 935727.5625 acc: 0.5651537775993347\n",
      "step: 3560 , time : 0.0\n",
      "train: loss: 48930.4140625 acc: 0.9647223353385925  val: loss: 1999374.75 acc: 0.07189518213272095\n",
      "step: 3565 , time : 0.001001119613647461\n",
      "train: loss: 5087.0732421875 acc: 0.9744788408279419  val: loss: 1178886.375 acc: 0.7330209016799927\n",
      "step: 3570 , time : 0.0\n",
      "train: loss: 5576.7646484375 acc: 0.9847837090492249  val: loss: 153182.421875 acc: 0.9705867171287537\n",
      "step: 3575 , time : 0.0\n",
      "train: loss: 214013.015625 acc: 0.9139801859855652  val: loss: 933772.25 acc: 0.655188798904419\n",
      "step: 3580 , time : 0.0\n",
      "train: loss: 20518.333984375 acc: 0.9272663593292236  val: loss: 321738.34375 acc: 0.8339072465896606\n",
      "step: 3585 , time : 0.0\n",
      "train: loss: 14265.9345703125 acc: 0.963932991027832  val: loss: 2162511.5 acc: 0.6514945030212402\n",
      "step: 3590 , time : 0.00023436546325683594\n",
      "train: loss: 5053.958984375 acc: 0.9874678254127502  val: loss: 261327.078125 acc: 0.9575952887535095\n",
      "step: 3595 , time : 0.0\n",
      "train: loss: 19498.70703125 acc: 0.9837269186973572  val: loss: 1427271.5 acc: 0.46877801418304443\n",
      "step: 3600 , time : 0.0\n",
      "train: loss: 6274.15478515625 acc: 0.9897240400314331  val: loss: 1523821.0 acc: 0.134813129901886\n",
      "step: 3605 , time : 0.0\n",
      "train: loss: 16465.541015625 acc: 0.9917277693748474  val: loss: 516532.03125 acc: 0.8914575576782227\n",
      "step: 3610 , time : 0.0\n",
      "train: loss: 47646.125 acc: 0.9722471237182617  val: loss: 415052.96875 acc: 0.8877346515655518\n",
      "step: 3615 , time : 0.0\n",
      "train: loss: 22342.62109375 acc: 0.9796544313430786  val: loss: 2434793.0 acc: 0.4863464832305908\n",
      "step: 3620 , time : 0.0\n",
      "train: loss: 18838.158203125 acc: 0.9894957542419434  val: loss: 233218.0625 acc: 0.9708598256111145\n",
      "step: 3625 , time : 0.0\n",
      "train: loss: 8907.9267578125 acc: 0.981427788734436  val: loss: 1656899.25 acc: 0.551053524017334\n",
      "step: 3630 , time : 0.0\n",
      "train: loss: 14157.8798828125 acc: 0.9869211316108704  val: loss: 412186.59375 acc: 0.8681799173355103\n",
      "step: 3635 , time : 0.0\n",
      "train: loss: 23236.0234375 acc: 0.9885135889053345  val: loss: 276367.34375 acc: 0.980506956577301\n",
      "step: 3640 , time : 0.0\n",
      "train: loss: 34044.27734375 acc: 0.9904298186302185  val: loss: 243624.046875 acc: 0.8576221466064453\n",
      "step: 3645 , time : 0.0\n",
      "train: loss: 73297.7890625 acc: 0.9837684631347656  val: loss: 364916.875 acc: 0.909951388835907\n",
      "step: 3650 , time : 0.0\n",
      "train: loss: 49425.7265625 acc: 0.9805877208709717  val: loss: 1211681.0 acc: 0.6667652726173401\n",
      "step: 3655 , time : 0.0\n",
      "train: loss: 59762.40625 acc: 0.9788587093353271  val: loss: 690765.25 acc: 0.8082785606384277\n",
      "step: 3660 , time : 0.0\n",
      "train: loss: 196030.078125 acc: 0.9428108930587769  val: loss: 239454.03125 acc: 0.9426738023757935\n",
      "step: 3665 , time : 0.0\n",
      "train: loss: 86373.1640625 acc: 0.959218442440033  val: loss: 542488.5625 acc: 0.908759355545044\n",
      "step: 3670 , time : 0.0\n",
      "train: loss: 212142.9375 acc: 0.9171617031097412  val: loss: 452427.375 acc: 0.9454728960990906\n",
      "step: 3675 , time : 0.0\n",
      "train: loss: 127372.5546875 acc: 0.9702239632606506  val: loss: 150116.1875 acc: 0.9859557151794434\n",
      "step: 3680 , time : 0.0\n",
      "train: loss: 115157.0859375 acc: 0.9867115020751953  val: loss: 685800.25 acc: 0.8801668882369995\n",
      "step: 3685 , time : 0.0\n",
      "train: loss: 152959.625 acc: 0.9840118288993835  val: loss: 1706255.0 acc: 0.7434929013252258\n",
      "step: 3690 , time : 0.0\n",
      "train: loss: 72693.6328125 acc: 0.9911611080169678  val: loss: 337279.78125 acc: 0.9310775399208069\n",
      "step: 3695 , time : 0.0\n",
      "train: loss: 169723.09375 acc: 0.9818375706672668  val: loss: 478573.28125 acc: 0.962892472743988\n",
      "step: 3700 , time : 0.0\n",
      "train: loss: 1895090.25 acc: 0.9072731733322144  val: loss: 3034609.25 acc: 0.5821162462234497\n",
      "step: 3705 , time : 0.0\n",
      "train: loss: 513822.34375 acc: 0.9581502676010132  val: loss: 2336180.0 acc: 0.588697075843811\n",
      "step: 3710 , time : 0.0\n",
      "train: loss: 366346.59375 acc: 0.7941794395446777  val: loss: 870853.9375 acc: 0.6348711252212524\n",
      "step: 3715 , time : 0.0\n",
      "train: loss: 1915936.75 acc: 0.916586697101593  val: loss: 801338.3125 acc: 0.8449974060058594\n",
      "step: 3720 , time : 0.0\n",
      "train: loss: 997013.125 acc: 0.970446765422821  val: loss: 1077966.75 acc: 0.8947098255157471\n",
      "step: 3725 , time : 0.0\n",
      "train: loss: 1304927.5 acc: 0.94580078125  val: loss: 2068860.125 acc: 0.7310854196548462\n",
      "step: 3730 , time : 0.0\n",
      "train: loss: 1749086.375 acc: 0.9230620265007019  val: loss: 875654.25 acc: 0.7272361516952515\n",
      "step: 3735 , time : 0.0\n",
      "train: loss: 523997.9375 acc: 0.9695822596549988  val: loss: 1106864.5 acc: 0.9178691506385803\n",
      "step: 3740 , time : 0.0\n",
      "train: loss: 185956.328125 acc: 0.9483413100242615  val: loss: 1611392.875 acc: 0.6566013097763062\n",
      "step: 3745 , time : 0.0\n",
      "train: loss: 660991.6875 acc: 0.936138391494751  val: loss: 2784479.25 acc: 0.4686660170555115\n",
      "step: 3750 , time : 0.015625953674316406\n",
      "train: loss: 896663.125 acc: 0.903490424156189  val: loss: 1761928.75 acc: 0.7418931722640991\n",
      "step: 3755 , time : 0.0\n",
      "train: loss: 2244297.75 acc: 0.6389089226722717  val: loss: 1042146.25 acc: 0.850893497467041\n",
      "step: 3760 , time : 0.0\n",
      "train: loss: 796140.75 acc: 0.57135409116745  val: loss: 1541605.0 acc: 0.7167246341705322\n",
      "step: 3765 , time : 0.0\n",
      "train: loss: 500599.9375 acc: 0.8279380202293396  val: loss: 652076.875 acc: 0.7298953533172607\n",
      "step: 3770 , time : 0.0\n",
      "train: loss: 132676.4375 acc: 0.9095368981361389  val: loss: 247996.90625 acc: 0.8799427151679993\n",
      "step: 3775 , time : 0.0\n",
      "train: loss: 763570.1875 acc: 0.5406060218811035  val: loss: 1186430.5 acc: 0.7552967071533203\n",
      "step: 3780 , time : 0.001001119613647461\n",
      "train: loss: 1432283.625 acc: -0.15346384048461914  val: loss: 1098743.0 acc: 0.7043328881263733\n",
      "step: 3785 , time : 0.0\n",
      "train: loss: 812680.375 acc: 0.6549708843231201  val: loss: 1590833.5 acc: 0.7213765382766724\n",
      "step: 3790 , time : 0.0\n",
      "train: loss: 433829.78125 acc: 0.700468897819519  val: loss: 1885908.0 acc: 0.7354164123535156\n",
      "step: 3795 , time : 0.001001119613647461\n",
      "train: loss: 205137.3125 acc: 0.8023244142532349  val: loss: 1464397.375 acc: 0.7454204559326172\n",
      "step: 3800 , time : 0.0\n",
      "train: loss: 392439.28125 acc: 0.840688943862915  val: loss: 145882.84375 acc: 0.8418054580688477\n",
      "step: 3805 , time : 0.0\n",
      "train: loss: 134531.015625 acc: 0.909996509552002  val: loss: 882897.0625 acc: 0.7403237819671631\n",
      "step: 3810 , time : 0.0\n",
      "train: loss: 340488.46875 acc: 0.8493012189865112  val: loss: 2115556.0 acc: 0.7016952037811279\n",
      "step: 3815 , time : 0.0\n",
      "train: loss: 74221.71875 acc: 0.9536721706390381  val: loss: 1736063.75 acc: 0.7378675937652588\n",
      "step: 3820 , time : 0.0\n",
      "train: loss: 210700.34375 acc: 0.8708110451698303  val: loss: 1540708.75 acc: 0.7103750705718994\n",
      "step: 3825 , time : 0.0\n",
      "train: loss: 177290.734375 acc: 0.823088526725769  val: loss: 2457807.5 acc: 0.6996443271636963\n",
      "step: 3830 , time : 0.0\n",
      "train: loss: 84835.7109375 acc: 0.9123979806900024  val: loss: 1287625.625 acc: 0.7940418720245361\n",
      "step: 3835 , time : 0.0\n",
      "train: loss: 115675.71875 acc: 0.8902173042297363  val: loss: 1534068.5 acc: 0.7436954975128174\n",
      "step: 3840 , time : 0.0\n",
      "train: loss: 568358.875 acc: 0.7282015085220337  val: loss: 1402865.625 acc: 0.6749471426010132\n",
      "step: 3845 , time : 0.0\n",
      "train: loss: 179402.953125 acc: 0.8718815445899963  val: loss: 1099169.375 acc: 0.7574506998062134\n",
      "step: 3850 , time : 0.0\n",
      "train: loss: 942681.0625 acc: 0.711005449295044  val: loss: 365267.21875 acc: 0.8003466725349426\n",
      "step: 3855 , time : 0.0\n",
      "train: loss: 32241.796875 acc: 0.965327799320221  val: loss: 891093.4375 acc: 0.7095987200737\n",
      "step: 3860 , time : 0.0\n",
      "train: loss: 1123881.125 acc: 0.6814239621162415  val: loss: 4292199.5 acc: 0.45320552587509155\n",
      "step: 3865 , time : 0.0\n",
      "train: loss: 2778128.25 acc: 0.63014817237854  val: loss: 2145023.75 acc: 0.6663594245910645\n",
      "step: 3870 , time : 0.0\n",
      "train: loss: 1466317.375 acc: 0.6997564435005188  val: loss: 1290577.75 acc: 0.6494100689888\n",
      "step: 3875 , time : 0.0\n",
      "train: loss: 1636949.875 acc: 0.8600767254829407  val: loss: 575622.75 acc: 0.5677334666252136\n",
      "step: 3880 , time : 0.015625715255737305\n",
      "train: loss: 561240.8125 acc: 0.9433762431144714  val: loss: 1272906.625 acc: 0.3500000834465027\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 3885 , time : 0.0\n",
      "train: loss: 278865.4375 acc: 0.9615930914878845  val: loss: 543635.625 acc: 0.7693533301353455\n",
      "step: 3890 , time : 0.0\n",
      "train: loss: 295116.0 acc: 0.9423555731773376  val: loss: 590762.0 acc: 0.7179338932037354\n",
      "step: 3895 , time : 0.001001119613647461\n",
      "train: loss: 902820.125 acc: 0.8934462666511536  val: loss: 686440.75 acc: 0.8155996799468994\n",
      "step: 3900 , time : 0.0010008811950683594\n",
      "train: loss: 106389.765625 acc: 0.9921876192092896  val: loss: 1715072.625 acc: 0.7818933725357056\n",
      "step: 3905 , time : 0.0\n",
      "train: loss: 272221.0625 acc: 0.9792125821113586  val: loss: 120673.421875 acc: 0.9588791728019714\n",
      "step: 3910 , time : 0.0\n",
      "train: loss: 158358.78125 acc: 0.9875713586807251  val: loss: 568286.125 acc: 0.8466035723686218\n",
      "step: 3915 , time : 0.0\n",
      "train: loss: 128053.6640625 acc: 0.9815096259117126  val: loss: 255210.265625 acc: 0.9294596314430237\n",
      "step: 3920 , time : 0.0\n",
      "train: loss: 58977.64453125 acc: 0.9751508235931396  val: loss: 470293.25 acc: 0.929529070854187\n",
      "step: 3925 , time : 0.0\n",
      "train: loss: 124595.90625 acc: 0.9731132388114929  val: loss: 203051.1875 acc: 0.9447551369667053\n",
      "step: 3930 , time : 0.0\n",
      "train: loss: 35741.5 acc: 0.9785386323928833  val: loss: 788641.375 acc: 0.772152304649353\n",
      "step: 3935 , time : 0.0\n",
      "train: loss: 35455.015625 acc: 0.981432318687439  val: loss: 179672.984375 acc: 0.9630427956581116\n",
      "step: 3940 , time : 0.0\n",
      "train: loss: 13840.9453125 acc: 0.9876814484596252  val: loss: 496111.375 acc: 0.8175931572914124\n",
      "step: 3945 , time : 0.0\n",
      "train: loss: 8071.63232421875 acc: 0.9747340679168701  val: loss: 345518.71875 acc: 0.970124363899231\n",
      "step: 3950 , time : 0.0\n",
      "train: loss: 5312.82763671875 acc: 0.9754949808120728  val: loss: 201472.3125 acc: 0.955314040184021\n",
      "step: 3955 , time : 0.0\n",
      "train: loss: 16100.298828125 acc: 0.9588695168495178  val: loss: 1219655.25 acc: 0.9079847931861877\n",
      "step: 3960 , time : 0.0\n",
      "train: loss: 9541.41015625 acc: 0.9787005186080933  val: loss: 230017.25 acc: 0.9312798976898193\n",
      "step: 3965 , time : 0.0\n",
      "train: loss: 16915.556640625 acc: 0.9667102694511414  val: loss: 346560.21875 acc: 0.9625692367553711\n",
      "step: 3970 , time : 0.0\n",
      "train: loss: 12593.0712890625 acc: 0.9789116978645325  val: loss: 447705.03125 acc: 0.8697199821472168\n",
      "step: 3975 , time : 0.0\n",
      "train: loss: 32683.966796875 acc: 0.9865633249282837  val: loss: 600977.375 acc: 0.9579914212226868\n",
      "step: 3980 , time : 0.0\n",
      "train: loss: 17676.203125 acc: 0.9909266233444214  val: loss: 347679.96875 acc: 0.9295738339424133\n",
      "step: 3985 , time : 0.0\n",
      "train: loss: 13108.9619140625 acc: 0.9940155148506165  val: loss: 1246963.5 acc: 0.7491422891616821\n",
      "step: 3990 , time : 0.0\n",
      "train: loss: 28151.505859375 acc: 0.9845553636550903  val: loss: 747674.5 acc: 0.9270870685577393\n",
      "step: 3995 , time : 0.0\n",
      "train: loss: 10347.7412109375 acc: 0.988584041595459  val: loss: 4403966.5 acc: 0.5316279530525208\n",
      "step: 4000 , time : 0.0\n",
      "train: loss: 25080.310546875 acc: 0.9868969321250916  val: loss: 703120.1875 acc: 0.641185998916626\n",
      "step: 4005 , time : 0.0\n",
      "train: loss: 66751.796875 acc: 0.9820392727851868  val: loss: 1963932.375 acc: 0.2670028805732727\n",
      "step: 4010 , time : 0.0\n",
      "train: loss: 67803.765625 acc: 0.9819357991218567  val: loss: 1729739.375 acc: 0.6911672949790955\n",
      "step: 4015 , time : 0.0\n",
      "train: loss: 16511.287109375 acc: 0.9949919581413269  val: loss: 2626280.0 acc: 0.406821608543396\n",
      "step: 4020 , time : 0.0\n",
      "train: loss: 22793.185546875 acc: 0.9922001361846924  val: loss: 1629420.75 acc: 0.8685547709465027\n",
      "step: 4025 , time : 0.0\n",
      "train: loss: 122244.7265625 acc: 0.9540382027626038  val: loss: 1705073.75 acc: 0.3210963010787964\n",
      "step: 4030 , time : 0.0\n",
      "train: loss: 92288.9296875 acc: 0.9797701835632324  val: loss: 2678486.25 acc: -0.08009004592895508\n",
      "step: 4035 , time : 0.0\n",
      "train: loss: 138333.75 acc: 0.623507022857666  val: loss: 2925792.75 acc: -0.3801300525665283\n",
      "step: 4040 , time : 0.0\n",
      "train: loss: 508304.71875 acc: 0.9255215525627136  val: loss: 1878956.125 acc: 0.7648407220840454\n",
      "step: 4045 , time : 0.0\n",
      "train: loss: 253257.03125 acc: 0.9644733667373657  val: loss: 1955400.625 acc: -0.8640999794006348\n",
      "step: 4050 , time : 0.0\n",
      "train: loss: 552456.25 acc: 0.9212380647659302  val: loss: 2525422.75 acc: 0.6184933185577393\n",
      "step: 4055 , time : 0.015625953674316406\n",
      "train: loss: 87698.953125 acc: 0.9852777123451233  val: loss: 1116896.625 acc: 0.7804712057113647\n",
      "step: 4060 , time : 0.0\n",
      "train: loss: 284627.34375 acc: 0.9736276865005493  val: loss: 561471.375 acc: 0.9153950214385986\n",
      "step: 4065 , time : 0.0\n",
      "train: loss: 1672106.5 acc: 0.7965424656867981  val: loss: 1478438.75 acc: 0.5650817155838013\n",
      "step: 4070 , time : 0.0\n",
      "train: loss: 772061.125 acc: 0.9489822387695312  val: loss: 936161.6875 acc: 0.5970063209533691\n",
      "step: 4075 , time : 0.0\n",
      "train: loss: 300356.625 acc: 0.9818712472915649  val: loss: 857282.5625 acc: 0.7372182011604309\n",
      "step: 4080 , time : 0.0\n",
      "train: loss: 345413.5625 acc: 0.9536539316177368  val: loss: 2217402.5 acc: 0.7541324496269226\n",
      "step: 4085 , time : 0.0\n",
      "train: loss: 909405.625 acc: 0.9685823917388916  val: loss: 1856959.375 acc: 0.6626453399658203\n",
      "step: 4090 , time : 0.0\n",
      "train: loss: 3383709.25 acc: 0.8776592016220093  val: loss: 421606.15625 acc: 0.9191092848777771\n",
      "step: 4095 , time : 0.0\n",
      "train: loss: 1759821.125 acc: 0.8973430395126343  val: loss: 3201677.0 acc: 0.6324518918991089\n",
      "step: 4100 , time : 0.0\n",
      "train: loss: 451393.46875 acc: 0.9647665619850159  val: loss: 944730.0 acc: 0.8346171975135803\n",
      "step: 4105 , time : 0.0\n",
      "train: loss: 1212787.125 acc: 0.9453946948051453  val: loss: 1029318.0625 acc: 0.7318145632743835\n",
      "step: 4110 , time : 0.0\n",
      "train: loss: 314102.78125 acc: 0.9362727999687195  val: loss: 483184.3125 acc: 0.9252476692199707\n",
      "step: 4115 , time : 0.0\n",
      "train: loss: 648247.0 acc: 0.92880779504776  val: loss: 661094.25 acc: 0.9169031977653503\n",
      "step: 4120 , time : 0.0\n",
      "train: loss: 1050020.125 acc: 0.7704364061355591  val: loss: 674639.75 acc: 0.7962331771850586\n",
      "step: 4125 , time : 0.0010004043579101562\n",
      "train: loss: 1201767.375 acc: 0.2584648132324219  val: loss: 1132544.875 acc: 0.7642418742179871\n",
      "step: 4130 , time : 0.0010008811950683594\n",
      "train: loss: 1874516.25 acc: 0.7639674544334412  val: loss: 1795790.75 acc: 0.5951054096221924\n",
      "step: 4135 , time : 0.0\n",
      "train: loss: 435373.875 acc: 0.8571277856826782  val: loss: 1848380.0 acc: 0.8694407939910889\n",
      "step: 4140 , time : 0.0\n",
      "train: loss: 1278475.75 acc: 0.5480177402496338  val: loss: 492592.1875 acc: 0.855742335319519\n",
      "step: 4145 , time : 0.0\n",
      "train: loss: 1804422.75 acc: 0.3883928060531616  val: loss: 1093239.5 acc: 0.7781404256820679\n",
      "step: 4150 , time : 0.0\n",
      "train: loss: 781240.0 acc: 0.4866233468055725  val: loss: 2174993.25 acc: 0.7178066372871399\n",
      "step: 4155 , time : 0.0\n",
      "train: loss: 527986.5 acc: 0.6531376838684082  val: loss: 1626463.875 acc: 0.5974853038787842\n",
      "step: 4160 , time : 0.0\n",
      "train: loss: 425094.1875 acc: 0.6759185791015625  val: loss: 1760829.25 acc: 0.7846617102622986\n",
      "step: 4165 , time : 0.0\n",
      "train: loss: 375899.0 acc: 0.7793653607368469  val: loss: 2733783.0 acc: 0.66047203540802\n",
      "step: 4170 , time : 0.0\n",
      "train: loss: 371174.3125 acc: 0.8272848725318909  val: loss: 1215321.5 acc: 0.6759243011474609\n",
      "step: 4175 , time : 0.0\n",
      "train: loss: 221493.078125 acc: 0.8664692640304565  val: loss: 2701845.25 acc: 0.7470968961715698\n",
      "step: 4180 , time : 0.0\n",
      "train: loss: 84582.890625 acc: 0.9379391670227051  val: loss: 1598930.75 acc: 0.6910869479179382\n",
      "step: 4185 , time : 0.0\n",
      "train: loss: 126316.75 acc: 0.9125741720199585  val: loss: 477122.03125 acc: 0.7331187725067139\n",
      "step: 4190 , time : 0.0\n",
      "train: loss: 250787.875 acc: 0.835885763168335  val: loss: 96349.984375 acc: 0.9169192314147949\n",
      "step: 4195 , time : 0.0\n",
      "train: loss: 88810.4609375 acc: 0.9196467399597168  val: loss: 1542874.75 acc: 0.8288158178329468\n",
      "step: 4200 , time : 0.0010008811950683594\n",
      "train: loss: 138928.28125 acc: 0.8834136128425598  val: loss: 843769.0 acc: 0.8095260262489319\n",
      "step: 4205 , time : 0.0\n",
      "train: loss: 312146.0 acc: 0.6710944771766663  val: loss: 760991.875 acc: 0.7477077841758728\n",
      "step: 4210 , time : 0.0\n",
      "train: loss: 666084.4375 acc: 0.7487047910690308  val: loss: 373521.5625 acc: 0.8179025650024414\n",
      "step: 4215 , time : 0.0\n",
      "train: loss: 751910.1875 acc: 0.5584604740142822  val: loss: 351511.78125 acc: 0.7509843111038208\n",
      "step: 4220 , time : 0.0\n",
      "train: loss: 357732.125 acc: 0.7982515692710876  val: loss: 696454.375 acc: 0.7466583847999573\n",
      "step: 4225 , time : 0.0\n",
      "train: loss: 212884.5625 acc: 0.8234610557556152  val: loss: 885202.4375 acc: 0.7094215154647827\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 4230 , time : 0.0\n",
      "train: loss: 1053525.75 acc: 0.7268751263618469  val: loss: 1561515.375 acc: 0.7122539281845093\n",
      "step: 4235 , time : 0.0\n",
      "train: loss: 1737302.125 acc: 0.7407429218292236  val: loss: 378089.75 acc: 0.8688223361968994\n",
      "step: 4240 , time : 0.0\n",
      "train: loss: 1413437.25 acc: 0.854253351688385  val: loss: 984285.875 acc: 0.5252312421798706\n",
      "step: 4245 , time : 0.0010004043579101562\n",
      "train: loss: 688788.8125 acc: 0.9168417453765869  val: loss: 775622.75 acc: 0.825671374797821\n",
      "step: 4250 , time : 0.0\n",
      "train: loss: 401537.625 acc: 0.958375096321106  val: loss: 424898.9375 acc: 0.8774034976959229\n",
      "step: 4255 , time : 0.0\n",
      "train: loss: 359298.75 acc: 0.9499097466468811  val: loss: 655916.625 acc: 0.8767722249031067\n",
      "step: 4260 , time : 0.0010006427764892578\n",
      "train: loss: 451741.59375 acc: 0.9467663168907166  val: loss: 365495.21875 acc: 0.8450242280960083\n",
      "step: 4265 , time : 0.0\n",
      "train: loss: 271808.125 acc: 0.9782414436340332  val: loss: 611046.1875 acc: 0.8652865290641785\n",
      "step: 4270 , time : 0.0\n",
      "train: loss: 161077.078125 acc: 0.9850366711616516  val: loss: 1368004.25 acc: 0.8275591135025024\n",
      "step: 4275 , time : 0.0\n",
      "train: loss: 319077.78125 acc: 0.976448118686676  val: loss: 502098.15625 acc: 0.9161213040351868\n",
      "step: 4280 , time : 0.0\n",
      "train: loss: 187079.421875 acc: 0.9690565466880798  val: loss: 885067.4375 acc: 0.8456961512565613\n",
      "step: 4285 , time : 0.0\n",
      "train: loss: 84612.5390625 acc: 0.989684522151947  val: loss: 416465.28125 acc: 0.9455108046531677\n",
      "step: 4290 , time : 0.0\n",
      "train: loss: 51445.6328125 acc: 0.9661582112312317  val: loss: 559928.75 acc: 0.9485124945640564\n",
      "step: 4295 , time : 0.0\n",
      "train: loss: 22610.185546875 acc: 0.9798035025596619  val: loss: 1239283.625 acc: 0.890021562576294\n",
      "step: 4300 , time : 0.0\n",
      "train: loss: 30839.3359375 acc: 0.9782019853591919  val: loss: 293193.40625 acc: 0.9570644497871399\n",
      "step: 4305 , time : 0.0\n",
      "train: loss: 18961.099609375 acc: 0.9663302302360535  val: loss: 596523.8125 acc: 0.8250299692153931\n",
      "step: 4310 , time : 0.0010008811950683594\n",
      "train: loss: 15006.1484375 acc: 0.9843108654022217  val: loss: 256482.28125 acc: 0.9725390076637268\n",
      "step: 4315 , time : 0.0010013580322265625\n",
      "train: loss: 13092.125 acc: 0.9677062630653381  val: loss: 1481668.25 acc: 0.8983344435691833\n",
      "step: 4320 , time : 0.0\n",
      "train: loss: 186257.703125 acc: 0.6428859233856201  val: loss: 506875.46875 acc: 0.8551932573318481\n",
      "step: 4325 , time : 0.0\n",
      "train: loss: 8136.205078125 acc: 0.9778376221656799  val: loss: 497645.09375 acc: 0.9464995265007019\n",
      "step: 4330 , time : 0.0010004043579101562\n",
      "train: loss: 4868.279296875 acc: 0.9895403385162354  val: loss: 345162.21875 acc: 0.8796361684799194\n",
      "step: 4335 , time : 0.001001119613647461\n",
      "train: loss: 34554.3828125 acc: 0.983081579208374  val: loss: 669585.375 acc: 0.9244914650917053\n",
      "step: 4340 , time : 0.0\n",
      "train: loss: 20248.826171875 acc: 0.991301953792572  val: loss: 918061.25 acc: 0.9146629571914673\n",
      "step: 4345 , time : 0.0\n",
      "train: loss: 16039.0556640625 acc: 0.9913970828056335  val: loss: 1892007.375 acc: 0.8586493730545044\n",
      "step: 4350 , time : 0.0010006427764892578\n",
      "train: loss: 41709.5390625 acc: 0.9732823371887207  val: loss: 1721044.625 acc: 0.826023280620575\n",
      "step: 4355 , time : 0.0\n",
      "train: loss: 16950.802734375 acc: 0.983886182308197  val: loss: 2094876.125 acc: 0.6429225206375122\n",
      "step: 4360 , time : 0.0010004043579101562\n",
      "train: loss: 31792.40234375 acc: 0.9689329862594604  val: loss: 1484742.875 acc: 0.6728310585021973\n",
      "step: 4365 , time : 0.0010004043579101562\n",
      "train: loss: 35036.28515625 acc: 0.9882053732872009  val: loss: 611350.9375 acc: 0.9261934757232666\n",
      "step: 4370 , time : 0.0010006427764892578\n",
      "train: loss: 24457.169921875 acc: 0.9893296957015991  val: loss: 1581232.0 acc: 0.6091873049736023\n",
      "step: 4375 , time : 0.0\n",
      "train: loss: 52206.87890625 acc: 0.9845984578132629  val: loss: 1809882.0 acc: 0.761029064655304\n",
      "step: 4380 , time : 0.0\n",
      "train: loss: 32720.40625 acc: 0.9894338846206665  val: loss: 1138014.5 acc: 0.6024975180625916\n",
      "step: 4385 , time : 0.0\n",
      "train: loss: 29655.66015625 acc: 0.9919547438621521  val: loss: 1984034.125 acc: -0.5760295391082764\n",
      "step: 4390 , time : 0.0\n",
      "train: loss: 144183.3125 acc: 0.9314553141593933  val: loss: 1480569.875 acc: 0.7011798620223999\n",
      "step: 4395 , time : 0.0\n",
      "train: loss: 246130.671875 acc: 0.8837300539016724  val: loss: 1489402.5 acc: 0.8571531772613525\n",
      "step: 4400 , time : 0.0\n",
      "train: loss: 52765.96875 acc: 0.9825490713119507  val: loss: 1669816.125 acc: 0.4443511962890625\n",
      "step: 4405 , time : 0.0\n",
      "train: loss: 269179.71875 acc: 0.9297532439231873  val: loss: 808524.9375 acc: 0.9269623756408691\n",
      "step: 4410 , time : 0.0\n",
      "train: loss: 143707.078125 acc: 0.9828075170516968  val: loss: 3875731.75 acc: 0.19211798906326294\n",
      "step: 4415 , time : 0.0\n",
      "train: loss: 108154.9609375 acc: 0.9891394376754761  val: loss: 672492.3125 acc: 0.7121935486793518\n",
      "step: 4420 , time : 0.0\n",
      "train: loss: 107863.3828125 acc: 0.986760675907135  val: loss: 3331442.25 acc: 0.019308507442474365\n",
      "step: 4425 , time : 0.0010004043579101562\n",
      "train: loss: 266827.34375 acc: 0.9840667843818665  val: loss: 842164.5625 acc: 0.8846098184585571\n",
      "step: 4430 , time : 0.0\n",
      "train: loss: 128897.0546875 acc: 0.9750466346740723  val: loss: 738260.3125 acc: 0.841397225856781\n",
      "step: 4435 , time : 0.0\n",
      "train: loss: 423514.25 acc: 0.9570356607437134  val: loss: 977796.375 acc: 0.3969775438308716\n",
      "step: 4440 , time : 0.0\n",
      "train: loss: 217845.234375 acc: 0.9572640061378479  val: loss: 1458502.5 acc: 0.15490835905075073\n",
      "step: 4445 , time : 0.0010004043579101562\n",
      "train: loss: 236146.34375 acc: 0.9624099731445312  val: loss: 1733038.375 acc: 0.6871997117996216\n",
      "step: 4450 , time : 0.0010004043579101562\n",
      "train: loss: 523447.28125 acc: 0.9697427749633789  val: loss: 1315215.125 acc: 0.6375666856765747\n",
      "step: 4455 , time : 0.0\n",
      "train: loss: 1073845.25 acc: 0.9658730030059814  val: loss: 665191.8125 acc: 0.8698118925094604\n",
      "step: 4460 , time : 0.0010008811950683594\n",
      "train: loss: 880381.125 acc: 0.9644966721534729  val: loss: 1075499.75 acc: 0.7824037075042725\n",
      "step: 4465 , time : 0.0\n",
      "train: loss: 2017831.75 acc: 0.8764574527740479  val: loss: 982047.9375 acc: 0.8157837986946106\n",
      "step: 4470 , time : 0.0010008811950683594\n",
      "train: loss: 211768.203125 acc: 0.9736296534538269  val: loss: 1221599.625 acc: 0.6969947218894958\n",
      "step: 4475 , time : 0.0\n",
      "train: loss: 254935.34375 acc: 0.9734053611755371  val: loss: 1842814.25 acc: 0.780394971370697\n",
      "step: 4480 , time : 0.0\n",
      "train: loss: 499276.53125 acc: 0.856982946395874  val: loss: 199499.796875 acc: 0.9671248197555542\n",
      "step: 4485 , time : 0.0010008811950683594\n",
      "train: loss: 2402965.25 acc: 0.2706833481788635  val: loss: 456296.375 acc: 0.8730645775794983\n",
      "step: 4490 , time : 0.0\n",
      "train: loss: 553136.75 acc: 0.8695868253707886  val: loss: 1175875.25 acc: 0.7135730385780334\n",
      "step: 4495 , time : 0.0\n",
      "train: loss: 544691.0625 acc: 0.8211954832077026  val: loss: 628534.5 acc: 0.9034120440483093\n",
      "step: 4500 , time : 0.0010006427764892578\n",
      "train: loss: 312967.625 acc: 0.8896082639694214  val: loss: 1382229.25 acc: 0.5544331073760986\n",
      "step: 4505 , time : 0.001001119613647461\n",
      "train: loss: 864352.8125 acc: 0.6674257516860962  val: loss: 2974240.5 acc: 0.3622128963470459\n",
      "step: 4510 , time : 0.0\n",
      "train: loss: 1117154.875 acc: 0.7240190505981445  val: loss: 1755607.75 acc: 0.8486634492874146\n",
      "step: 4515 , time : 0.0\n",
      "train: loss: 855917.3125 acc: 0.34456759691238403  val: loss: 1496983.5 acc: 0.4765200614929199\n",
      "step: 4520 , time : 0.0010006427764892578\n",
      "train: loss: 583480.125 acc: 0.6202310919761658  val: loss: 991463.875 acc: 0.48912739753723145\n",
      "step: 4525 , time : 0.0\n",
      "train: loss: 938153.25 acc: 0.6473751068115234  val: loss: 721981.375 acc: 0.664700984954834\n",
      "step: 4530 , time : 0.0010006427764892578\n",
      "train: loss: 46850.41015625 acc: 0.9584460854530334  val: loss: 705011.25 acc: 0.8116464614868164\n",
      "step: 4535 , time : 0.0010004043579101562\n",
      "train: loss: 113997.40625 acc: 0.8925120830535889  val: loss: 1698271.25 acc: 0.6927651166915894\n",
      "step: 4540 , time : 0.0\n",
      "train: loss: 173433.53125 acc: 0.8728532791137695  val: loss: 996279.0 acc: 0.7274389266967773\n",
      "step: 4545 , time : 0.0\n",
      "train: loss: 287951.78125 acc: 0.8394631147384644  val: loss: 726697.75 acc: 0.7485518455505371\n",
      "step: 4550 , time : 0.0\n",
      "train: loss: 354046.21875 acc: 0.7615951299667358  val: loss: 552239.125 acc: 0.8025636672973633\n",
      "step: 4555 , time : 0.0\n",
      "train: loss: 140025.046875 acc: 0.8891682028770447  val: loss: 1280605.25 acc: 0.7207907438278198\n",
      "step: 4560 , time : 0.0\n",
      "train: loss: 147296.953125 acc: 0.8491635322570801  val: loss: 727111.9375 acc: 0.7796332240104675\n",
      "step: 4565 , time : 0.0\n",
      "train: loss: 157865.5625 acc: 0.8502238392829895  val: loss: 1968007.625 acc: 0.7593986392021179\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 4570 , time : 0.0\n",
      "train: loss: 545861.875 acc: 0.7297701835632324  val: loss: 794235.375 acc: 0.716010570526123\n",
      "step: 4575 , time : 0.0\n",
      "train: loss: 802180.125 acc: 0.7236645221710205  val: loss: 2700224.25 acc: 0.7499645948410034\n",
      "step: 4580 , time : 0.0010004043579101562\n",
      "train: loss: 638055.875 acc: 0.7130579948425293  val: loss: 1879821.0 acc: 0.631721019744873\n",
      "step: 4585 , time : 0.001001119613647461\n",
      "train: loss: 709254.4375 acc: 0.7317994832992554  val: loss: 1421977.25 acc: 0.6824238896369934\n",
      "step: 4590 , time : 0.0\n",
      "train: loss: 332436.59375 acc: 0.6989784240722656  val: loss: 509125.1875 acc: 0.759221613407135\n",
      "step: 4595 , time : 0.0\n",
      "train: loss: 1824301.5 acc: 0.5993798971176147  val: loss: 945778.25 acc: 0.7124021053314209\n",
      "step: 4600 , time : 0.0\n",
      "train: loss: 1899307.25 acc: 0.7072261571884155  val: loss: 3077976.75 acc: 0.7127124071121216\n",
      "step: 4605 , time : 0.0\n",
      "train: loss: 1706607.5 acc: 0.8203200101852417  val: loss: 995731.0625 acc: 0.8928960561752319\n",
      "step: 4610 , time : 0.0\n",
      "train: loss: 640302.625 acc: 0.9481061697006226  val: loss: 1224742.75 acc: 0.8257752656936646\n",
      "step: 4615 , time : 0.0\n",
      "train: loss: 532467.375 acc: 0.9279155731201172  val: loss: 1268096.0 acc: 0.9330608248710632\n",
      "step: 4620 , time : 0.0\n",
      "train: loss: 281642.75 acc: 0.960935652256012  val: loss: 866313.3125 acc: 0.9352167844772339\n",
      "step: 4625 , time : 0.0\n",
      "train: loss: 251805.671875 acc: 0.9669445753097534  val: loss: 570917.8125 acc: 0.9345777034759521\n",
      "step: 4630 , time : 0.0\n",
      "train: loss: 176039.109375 acc: 0.9822589159011841  val: loss: 2213907.75 acc: 0.7580939531326294\n",
      "step: 4635 , time : 0.0\n",
      "train: loss: 277136.21875 acc: 0.9779890179634094  val: loss: 822819.25 acc: 0.6797017455101013\n",
      "step: 4640 , time : 0.0\n",
      "train: loss: 167135.46875 acc: 0.9875397682189941  val: loss: 1462810.5 acc: 0.8337519764900208\n",
      "step: 4645 , time : 0.0\n",
      "train: loss: 122541.7265625 acc: 0.9786163568496704  val: loss: 599920.625 acc: 0.9442392587661743\n",
      "step: 4650 , time : 0.0\n",
      "train: loss: 38987.6875 acc: 0.989901602268219  val: loss: 1459182.5 acc: 0.7971090078353882\n",
      "step: 4655 , time : 0.0\n",
      "train: loss: 43467.5859375 acc: 0.9804723858833313  val: loss: 2013247.75 acc: 0.6783150434494019\n",
      "step: 4660 , time : 0.0\n",
      "train: loss: 6132.95068359375 acc: 0.9766665697097778  val: loss: 1916087.625 acc: -1.0196130275726318\n",
      "step: 4665 , time : 0.015625476837158203\n",
      "train: loss: 12393.4716796875 acc: 0.9927671551704407  val: loss: 748158.9375 acc: 0.6861875057220459\n",
      "step: 4670 , time : 0.0\n",
      "train: loss: 23870.259765625 acc: 0.9736691117286682  val: loss: 782184.0 acc: 0.5397138595581055\n",
      "step: 4675 , time : 0.0\n",
      "train: loss: 12620.6435546875 acc: 0.9912630915641785  val: loss: 1681549.625 acc: 0.5512452125549316\n",
      "step: 4680 , time : 0.0\n",
      "train: loss: 28837.7109375 acc: 0.8968410491943359  val: loss: 1545499.875 acc: 0.7446836233139038\n",
      "step: 4685 , time : 0.0\n",
      "train: loss: 18297.181640625 acc: 0.9664315581321716  val: loss: 1860924.375 acc: 0.6549863219261169\n",
      "step: 4690 , time : 0.0010004043579101562\n",
      "train: loss: 11100.060546875 acc: 0.9866176247596741  val: loss: 2387404.25 acc: 0.7154317498207092\n",
      "step: 4695 , time : 0.0\n",
      "train: loss: 19738.376953125 acc: 0.9723250865936279  val: loss: 2009982.625 acc: 0.6654583215713501\n",
      "step: 4700 , time : 0.0\n",
      "train: loss: 28646.76171875 acc: 0.9884350299835205  val: loss: 1288186.0 acc: 0.8126427531242371\n",
      "step: 4705 , time : 0.0010006427764892578\n",
      "train: loss: 35024.20703125 acc: 0.9815777540206909  val: loss: 708216.25 acc: 0.8559494614601135\n",
      "step: 4710 , time : 0.0\n",
      "train: loss: 10471.5244140625 acc: 0.9905163645744324  val: loss: 2101082.25 acc: -0.7155702114105225\n",
      "step: 4715 , time : 0.0\n",
      "train: loss: 36592.140625 acc: 0.982211709022522  val: loss: 2535433.5 acc: 0.247036874294281\n",
      "step: 4720 , time : 0.0\n",
      "train: loss: 19559.32421875 acc: 0.9854945540428162  val: loss: 1305651.75 acc: 0.819700300693512\n",
      "step: 4725 , time : 0.0\n",
      "train: loss: 17454.6875 acc: 0.9725346565246582  val: loss: 1877477.0 acc: 0.3037228584289551\n",
      "step: 4730 , time : 0.0\n",
      "train: loss: 9457.2529296875 acc: 0.9935773611068726  val: loss: 4734379.5 acc: 0.2640211582183838\n",
      "step: 4735 , time : 0.0\n",
      "train: loss: 17771.724609375 acc: 0.99386066198349  val: loss: 1782414.0 acc: 0.7359769344329834\n",
      "step: 4740 , time : 0.0\n",
      "train: loss: 26980.828125 acc: 0.9933619499206543  val: loss: 1055849.75 acc: 0.7832365036010742\n",
      "step: 4745 , time : 0.0\n",
      "train: loss: 39592.78515625 acc: 0.9918271899223328  val: loss: 2378188.0 acc: -0.31233739852905273\n",
      "step: 4750 , time : 0.0\n",
      "train: loss: 21992.142578125 acc: 0.9866166710853577  val: loss: 793378.0 acc: 0.9080677032470703\n",
      "step: 4755 , time : 0.0\n",
      "train: loss: 40416.9609375 acc: 0.9862514138221741  val: loss: 805820.6875 acc: 0.9031127691268921\n",
      "step: 4760 , time : 0.0\n",
      "train: loss: 107193.6796875 acc: 0.9611734747886658  val: loss: 371911.4375 acc: 0.9141364693641663\n",
      "step: 4765 , time : 0.0\n",
      "train: loss: 289491.875 acc: 0.8932658433914185  val: loss: 1225393.0 acc: 0.7818799018859863\n",
      "step: 4770 , time : 0.0\n",
      "train: loss: 564371.8125 acc: 0.7780080437660217  val: loss: 2018894.125 acc: 0.5813214182853699\n",
      "step: 4775 , time : 0.0\n",
      "train: loss: 611509.4375 acc: 0.904233992099762  val: loss: 1991216.75 acc: 0.2979440689086914\n",
      "step: 4780 , time : 0.0\n",
      "train: loss: 105671.6015625 acc: 0.9915874600410461  val: loss: 988570.4375 acc: 0.8393968343734741\n",
      "step: 4785 , time : 0.0\n",
      "train: loss: 109966.40625 acc: 0.9898874163627625  val: loss: 312139.375 acc: 0.9394349455833435\n",
      "step: 4790 , time : 0.0\n",
      "train: loss: 164085.59375 acc: 0.9781607389450073  val: loss: 481491.875 acc: 0.7287231683731079\n",
      "step: 4795 , time : 0.0\n",
      "train: loss: 156978.734375 acc: 0.9755957126617432  val: loss: 268087.3125 acc: 0.9578119516372681\n",
      "step: 4800 , time : 0.0\n",
      "train: loss: 398961.1875 acc: 0.9792124032974243  val: loss: 1225878.375 acc: 0.2668417692184448\n",
      "step: 4805 , time : 0.0\n",
      "train: loss: 344144.1875 acc: 0.9742849469184875  val: loss: 1190964.375 acc: 0.7849259376525879\n",
      "step: 4810 , time : 0.0\n",
      "train: loss: 315382.1875 acc: 0.9156032204627991  val: loss: 336524.375 acc: 0.9089126586914062\n",
      "step: 4815 , time : 0.0010006427764892578\n",
      "train: loss: 1316315.375 acc: 0.9223110675811768  val: loss: 732384.8125 acc: 0.7454955577850342\n",
      "step: 4820 , time : 0.0\n",
      "train: loss: 1369854.625 acc: 0.9551448822021484  val: loss: 835041.6875 acc: 0.5990586280822754\n",
      "step: 4825 , time : 0.0\n",
      "train: loss: 492458.59375 acc: 0.9760048389434814  val: loss: 134016.578125 acc: 0.9412251114845276\n",
      "step: 4830 , time : 0.0\n",
      "train: loss: 749136.0625 acc: 0.9516259431838989  val: loss: 1222550.375 acc: 0.6795676350593567\n",
      "step: 4835 , time : 0.0\n",
      "train: loss: 535791.125 acc: 0.8918996453285217  val: loss: 258226.265625 acc: 0.9551657438278198\n",
      "step: 4840 , time : 0.0\n",
      "train: loss: 371977.9375 acc: 0.9444374442100525  val: loss: 635167.625 acc: 0.906382143497467\n",
      "step: 4845 , time : 0.0\n",
      "train: loss: 565539.75 acc: 0.9297099113464355  val: loss: 1619890.25 acc: 0.5122876167297363\n",
      "step: 4850 , time : 0.0\n",
      "train: loss: 2352543.0 acc: -0.06111466884613037  val: loss: 746270.6875 acc: 0.7260805368423462\n",
      "step: 4855 , time : 0.0\n",
      "train: loss: 964513.4375 acc: 0.7975379228591919  val: loss: 1149792.5 acc: 0.6734116077423096\n",
      "step: 4860 , time : 0.0\n",
      "train: loss: 737728.125 acc: 0.7578135132789612  val: loss: 1036263.0 acc: 0.6996011137962341\n",
      "step: 4865 , time : 0.0\n",
      "train: loss: 373854.28125 acc: 0.8164374232292175  val: loss: 663854.375 acc: 0.6923256516456604\n",
      "step: 4870 , time : 0.0\n",
      "train: loss: 786093.125 acc: 0.7039860486984253  val: loss: 747017.5625 acc: 0.8131058216094971\n",
      "step: 4875 , time : 0.0\n",
      "train: loss: 2366671.5 acc: 0.04500240087509155  val: loss: 778179.125 acc: 0.7370624542236328\n",
      "step: 4880 , time : 0.0\n",
      "train: loss: 1137120.625 acc: 0.14409148693084717  val: loss: 1186457.25 acc: 0.4466296434402466\n",
      "step: 4885 , time : 0.0\n",
      "train: loss: 428478.3125 acc: 0.6940405368804932  val: loss: 1185533.25 acc: 0.7709731459617615\n",
      "step: 4890 , time : 0.0\n",
      "train: loss: 331936.0625 acc: 0.8061206340789795  val: loss: 551557.5 acc: 0.8261054754257202\n",
      "step: 4895 , time : 0.0\n",
      "train: loss: 462603.875 acc: 0.694798469543457  val: loss: 583919.4375 acc: 0.7736047506332397\n",
      "step: 4900 , time : 0.0\n",
      "train: loss: 79992.59375 acc: 0.9359229207038879  val: loss: 1323668.5 acc: 0.8067413568496704\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 4905 , time : 0.0010008811950683594\n",
      "train: loss: 236233.0625 acc: 0.8756306767463684  val: loss: 530516.75 acc: 0.7145101428031921\n",
      "step: 4910 , time : 0.0010004043579101562\n",
      "train: loss: 258151.140625 acc: 0.7718144655227661  val: loss: 881799.5 acc: 0.7566766738891602\n",
      "step: 4915 , time : 0.0\n",
      "train: loss: 48319.61328125 acc: 0.9554229378700256  val: loss: 830319.5625 acc: 0.8614851832389832\n",
      "step: 4920 , time : 0.0\n",
      "train: loss: 105668.109375 acc: 0.9079375267028809  val: loss: 1881872.5 acc: 0.8219903707504272\n",
      "step: 4925 , time : 0.0\n",
      "train: loss: 248540.4375 acc: 0.7933734059333801  val: loss: 1237379.25 acc: 0.8072804808616638\n",
      "step: 4930 , time : 0.0010004043579101562\n",
      "train: loss: 56966.453125 acc: 0.9231299757957458  val: loss: 1995462.25 acc: 0.7563093900680542\n",
      "step: 4935 , time : 0.0\n",
      "train: loss: 509122.6875 acc: 0.7633672952651978  val: loss: 3988097.25 acc: 0.7504233121871948\n",
      "step: 4940 , time : 0.0\n",
      "train: loss: 366762.96875 acc: 0.6859477758407593  val: loss: 1253537.25 acc: 0.7018333673477173\n",
      "step: 4945 , time : 0.0\n",
      "train: loss: 1073297.625 acc: 0.6274680495262146  val: loss: 1817261.75 acc: 0.7677387595176697\n",
      "step: 4950 , time : 0.0010006427764892578\n",
      "train: loss: 397005.84375 acc: 0.7476788759231567  val: loss: 755475.625 acc: 0.747481107711792\n",
      "step: 4955 , time : 0.0010008811950683594\n",
      "train: loss: 487216.21875 acc: 0.6744697093963623  val: loss: 632960.875 acc: 0.7913697957992554\n",
      "step: 4960 , time : 0.0\n",
      "train: loss: 453224.40625 acc: 0.7457737922668457  val: loss: 947651.75 acc: 0.7453032732009888\n",
      "step: 4965 , time : 0.0\n",
      "train: loss: 1344248.5 acc: 0.7775578498840332  val: loss: 1937786.875 acc: 0.7768677473068237\n",
      "step: 4970 , time : 0.0010006427764892578\n",
      "train: loss: 1172255.0 acc: 0.787199854850769  val: loss: 1389325.375 acc: 0.7329569458961487\n",
      "step: 4975 , time : 0.0\n",
      "train: loss: 1183163.625 acc: 0.8888307809829712  val: loss: 1022682.4375 acc: 0.860821008682251\n",
      "step: 4980 , time : 0.0\n",
      "train: loss: 830633.625 acc: 0.9159660935401917  val: loss: 1351984.5 acc: 0.44701147079467773\n",
      "step: 4985 , time : 0.0010006427764892578\n",
      "train: loss: 388688.375 acc: 0.8841162919998169  val: loss: 903220.25 acc: 0.863722562789917\n",
      "step: 4990 , time : 0.0\n",
      "train: loss: 628477.375 acc: 0.9155304431915283  val: loss: 2245678.5 acc: 0.5072258710861206\n",
      "step: 4995 , time : 0.0\n",
      "train: loss: 250124.875 acc: 0.9746020436286926  val: loss: 2243217.75 acc: 0.5748748779296875\n",
      "step: 5000 , time : 0.0010008811950683594\n",
      "train: loss: 284127.53125 acc: 0.9784401655197144  val: loss: 2322242.5 acc: 0.7074079513549805\n",
      "step: 5005 , time : 0.0010004043579101562\n",
      "train: loss: 219989.4375 acc: 0.9847850799560547  val: loss: 739066.6875 acc: 0.9083326458930969\n",
      "step: 5010 , time : 0.0\n",
      "train: loss: 217831.75 acc: 0.9600509405136108  val: loss: 686737.4375 acc: 0.9157102108001709\n",
      "step: 5015 , time : 0.008005380630493164\n",
      "train: loss: 53271.57421875 acc: 0.9779089093208313  val: loss: 1294363.5 acc: 0.7831945419311523\n",
      "step: 5020 , time : 0.0010004043579101562\n",
      "train: loss: 37990.1171875 acc: 0.9867609739303589  val: loss: 1425335.5 acc: 0.7179372310638428\n",
      "step: 5025 , time : 0.0\n",
      "train: loss: 4423.12646484375 acc: 0.9880974292755127  val: loss: 1324439.75 acc: 0.7880234718322754\n",
      "step: 5030 , time : 0.0\n",
      "train: loss: 4703.5888671875 acc: 0.9832560420036316  val: loss: 3896890.75 acc: 0.4024038314819336\n",
      "step: 5035 , time : 0.0\n",
      "train: loss: 14151.033203125 acc: 0.960623025894165  val: loss: 2563458.75 acc: 0.3494625687599182\n",
      "step: 5040 , time : 0.0010008811950683594\n",
      "train: loss: 224699.171875 acc: 0.9021598696708679  val: loss: 371817.5625 acc: 0.8906009793281555\n",
      "step: 5045 , time : 0.0010004043579101562\n",
      "train: loss: 24530.599609375 acc: 0.9794884920120239  val: loss: 656949.625 acc: 0.8265984058380127\n",
      "step: 5050 , time : 0.0\n",
      "train: loss: 8367.4453125 acc: 0.9882280826568604  val: loss: 1556625.25 acc: 0.8351320028305054\n",
      "step: 5055 , time : 0.0\n",
      "train: loss: 3089.12939453125 acc: 0.9948005676269531  val: loss: 864584.1875 acc: 0.885104775428772\n",
      "step: 5060 , time : 0.0\n",
      "train: loss: 16310.0283203125 acc: 0.9345986843109131  val: loss: 735468.8125 acc: 0.2396361231803894\n",
      "step: 5065 , time : 0.0\n",
      "train: loss: 21406.8984375 acc: 0.9819545149803162  val: loss: 1814592.75 acc: 0.6613266468048096\n",
      "step: 5070 , time : 0.0010004043579101562\n",
      "train: loss: 33214.45703125 acc: 0.9763875007629395  val: loss: 1081294.625 acc: 0.7951626777648926\n",
      "step: 5075 , time : 0.0\n",
      "train: loss: 34441.49609375 acc: 0.9830902814865112  val: loss: 2544671.75 acc: 0.6304858922958374\n",
      "step: 5080 , time : 0.0\n",
      "train: loss: 27282.091796875 acc: 0.9891217947006226  val: loss: 248353.171875 acc: 0.9740745425224304\n",
      "step: 5085 , time : 0.001001119613647461\n",
      "train: loss: 35362.796875 acc: 0.962708592414856  val: loss: 1614339.5 acc: 0.6101686954498291\n",
      "step: 5090 , time : 0.001001119613647461\n",
      "train: loss: 4569.32275390625 acc: 0.980362057685852  val: loss: 4356348.0 acc: 0.193966805934906\n",
      "step: 5095 , time : 0.0\n",
      "train: loss: 8030.59228515625 acc: 0.9921509623527527  val: loss: 1905067.75 acc: 0.8510242700576782\n",
      "step: 5100 , time : 0.0156252384185791\n",
      "train: loss: 41674.2421875 acc: 0.9865772724151611  val: loss: 1996021.375 acc: 0.3875870108604431\n",
      "step: 5105 , time : 0.0\n",
      "train: loss: 25306.919921875 acc: 0.9940012693405151  val: loss: 695984.1875 acc: 0.832262396812439\n",
      "step: 5110 , time : 0.0010004043579101562\n",
      "train: loss: 21793.162109375 acc: 0.9962378740310669  val: loss: 2146818.75 acc: 0.05140465497970581\n",
      "step: 5115 , time : 0.0\n",
      "train: loss: 41391.23046875 acc: 0.9872543811798096  val: loss: 1505734.5 acc: 0.7303575873374939\n",
      "step: 5120 , time : 0.0\n",
      "train: loss: 31933.255859375 acc: 0.9886359572410583  val: loss: 1084323.125 acc: 0.8206002712249756\n",
      "step: 5125 , time : 0.0\n",
      "train: loss: 82176.4375 acc: 0.9830699563026428  val: loss: 710953.625 acc: 0.5321276187896729\n",
      "step: 5130 , time : 0.0\n",
      "train: loss: 233859.53125 acc: 0.8163168430328369  val: loss: 111795.6796875 acc: 0.9029121398925781\n",
      "step: 5135 , time : 0.0\n",
      "train: loss: 139849.96875 acc: 0.9528325796127319  val: loss: 564296.6875 acc: 0.892210841178894\n",
      "step: 5140 , time : 0.0\n",
      "train: loss: 206631.515625 acc: 0.9687726497650146  val: loss: 168759.921875 acc: 0.9642063975334167\n",
      "step: 5145 , time : 0.0\n",
      "train: loss: 38107.33203125 acc: 0.9951080679893494  val: loss: 1583739.875 acc: 0.4239805340766907\n",
      "step: 5150 , time : 0.0\n",
      "train: loss: 116733.125 acc: 0.9870701432228088  val: loss: 912033.0625 acc: 0.8921365737915039\n",
      "step: 5155 , time : 0.0\n",
      "train: loss: 270171.09375 acc: 0.9453964233398438  val: loss: 341906.96875 acc: 0.6450479030609131\n",
      "step: 5160 , time : 0.0\n",
      "train: loss: 166559.890625 acc: 0.9864845275878906  val: loss: 981273.8125 acc: 0.82512366771698\n",
      "step: 5165 , time : 0.0\n",
      "train: loss: 377280.71875 acc: 0.9683682322502136  val: loss: 296079.9375 acc: 0.800104558467865\n",
      "step: 5170 , time : 0.0\n",
      "train: loss: 287034.375 acc: 0.9661069512367249  val: loss: 665547.4375 acc: 0.9193910956382751\n",
      "step: 5175 , time : 0.0\n",
      "train: loss: 286855.5 acc: 0.9621970057487488  val: loss: 892801.6875 acc: 0.5334722995758057\n",
      "step: 5180 , time : 0.0\n",
      "train: loss: 1329862.625 acc: 0.9550566077232361  val: loss: 415988.8125 acc: 0.9074537754058838\n",
      "step: 5185 , time : 0.0\n",
      "train: loss: 455565.84375 acc: 0.9855673313140869  val: loss: 233928.34375 acc: 0.9349490404129028\n",
      "step: 5190 , time : 0.0\n",
      "train: loss: 1045266.1875 acc: 0.9572903513908386  val: loss: 258680.78125 acc: 0.9322434067726135\n",
      "step: 5195 , time : 0.0\n",
      "train: loss: 1182109.5 acc: 0.8927013874053955  val: loss: 737904.125 acc: 0.8443000912666321\n",
      "step: 5200 , time : 0.0\n",
      "train: loss: 543483.6875 acc: 0.9536939859390259  val: loss: 171348.203125 acc: 0.9239122271537781\n",
      "step: 5205 , time : 0.0\n",
      "train: loss: 296283.875 acc: 0.9548723101615906  val: loss: 359749.46875 acc: 0.9567139148712158\n",
      "step: 5210 , time : 0.0\n",
      "train: loss: 363805.15625 acc: 0.9484053254127502  val: loss: 1372977.5 acc: 0.45661574602127075\n",
      "step: 5215 , time : 0.001001119613647461\n",
      "train: loss: 2793087.0 acc: 0.47476208209991455  val: loss: 303349.15625 acc: 0.8843809366226196\n",
      "step: 5220 , time : 0.0010004043579101562\n",
      "train: loss: 1198112.75 acc: 0.4380239248275757  val: loss: 1676980.75 acc: 0.6449171900749207\n",
      "step: 5225 , time : 0.0\n",
      "train: loss: 516588.4375 acc: 0.8179359436035156  val: loss: 1603620.75 acc: 0.7975797653198242\n",
      "step: 5230 , time : 0.0\n",
      "train: loss: 1908594.5 acc: -0.15864956378936768  val: loss: 1515399.0 acc: 0.8532245755195618\n",
      "step: 5235 , time : 0.0010006427764892578\n",
      "train: loss: 797272.875 acc: 0.7670811414718628  val: loss: 337668.53125 acc: 0.9227322936058044\n",
      "step: 5240 , time : 0.0\n",
      "train: loss: 1906821.0 acc: 0.44086605310440063  val: loss: 835105.3125 acc: 0.7965633273124695\n",
      "step: 5245 , time : 0.0\n",
      "train: loss: 1012393.125 acc: 0.29400867223739624  val: loss: 2359029.75 acc: 0.746574342250824\n",
      "step: 5250 , time : 0.0\n",
      "train: loss: 1115436.125 acc: 0.39789170026779175  val: loss: 3863466.5 acc: 0.5611487627029419\n",
      "step: 5255 , time : 0.0\n",
      "train: loss: 557681.8125 acc: 0.5587056875228882  val: loss: 1556208.625 acc: 0.6897109150886536\n",
      "step: 5260 , time : 0.0\n",
      "train: loss: 617906.9375 acc: 0.6918869614601135  val: loss: 1436513.625 acc: 0.7897855639457703\n",
      "step: 5265 , time : 0.0\n",
      "train: loss: 63390.3515625 acc: 0.9509652853012085  val: loss: 1069575.25 acc: 0.851887583732605\n",
      "step: 5270 , time : 0.0\n",
      "train: loss: 53232.109375 acc: 0.9589670896530151  val: loss: 1174453.875 acc: 0.8292526006698608\n",
      "step: 5275 , time : 0.0\n",
      "train: loss: 195959.03125 acc: 0.8527805209159851  val: loss: 4519615.5 acc: 0.6254482269287109\n",
      "step: 5280 , time : 0.0\n",
      "train: loss: 205383.5625 acc: 0.8275142908096313  val: loss: 2416838.25 acc: 0.7975382208824158\n",
      "step: 5285 , time : 0.0\n",
      "train: loss: 19539.63671875 acc: 0.9831234216690063  val: loss: 1276297.375 acc: 0.8236292600631714\n",
      "step: 5290 , time : 0.0\n",
      "train: loss: 57881.2578125 acc: 0.9434746503829956  val: loss: 1458309.75 acc: 0.7693171501159668\n",
      "step: 5295 , time : 0.0\n",
      "train: loss: 272470.0625 acc: 0.7935768365859985  val: loss: 705118.875 acc: 0.8226069808006287\n",
      "step: 5300 , time : 0.0\n",
      "train: loss: 172713.65625 acc: 0.8411672115325928  val: loss: 1228334.75 acc: 0.7542670369148254\n",
      "step: 5305 , time : 0.0\n",
      "train: loss: 374397.625 acc: 0.7783588171005249  val: loss: 1462022.125 acc: 0.7412096858024597\n",
      "step: 5310 , time : 0.0\n",
      "train: loss: 441412.625 acc: 0.7592820525169373  val: loss: 2650989.0 acc: 0.7029921412467957\n",
      "step: 5315 , time : 0.0\n",
      "train: loss: 742797.625 acc: 0.5540684461593628  val: loss: 771796.6875 acc: 0.7120367288589478\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 5320 , time : 0.0\n",
      "train: loss: 67610.28125 acc: 0.9202722311019897  val: loss: 1721145.25 acc: 0.8280725479125977\n",
      "step: 5325 , time : 0.0\n",
      "train: loss: 243770.65625 acc: 0.559963583946228  val: loss: 319320.375 acc: 0.8017513155937195\n",
      "step: 5330 , time : 0.0\n",
      "train: loss: 1610318.375 acc: 0.7374237775802612  val: loss: 690695.5 acc: 0.7581617832183838\n",
      "step: 5335 , time : 0.0010013580322265625\n",
      "train: loss: 1397087.875 acc: 0.7234410047531128  val: loss: 1284147.5 acc: 0.64549720287323\n",
      "step: 5340 , time : 0.0\n",
      "train: loss: 1003641.4375 acc: 0.8854363560676575  val: loss: 1802077.625 acc: 0.6781575679779053\n",
      "step: 5345 , time : 0.0\n",
      "train: loss: 561756.0 acc: 0.9513667821884155  val: loss: 677420.625 acc: 0.6849436163902283\n",
      "step: 5350 , time : 0.0\n",
      "train: loss: 1120616.375 acc: 0.7551650404930115  val: loss: 1355084.5 acc: 0.5969693660736084\n",
      "step: 5355 , time : 0.0010004043579101562\n",
      "train: loss: 396049.28125 acc: 0.8843011260032654  val: loss: 1269344.0 acc: 0.5281292200088501\n",
      "step: 5360 , time : 0.0\n",
      "train: loss: 176038.03125 acc: 0.9837867021560669  val: loss: 2407716.25 acc: 0.18685603141784668\n",
      "step: 5365 , time : 0.0010008811950683594\n",
      "train: loss: 119894.484375 acc: 0.991046130657196  val: loss: 2877130.0 acc: 0.7549904584884644\n",
      "step: 5370 , time : 0.0\n",
      "train: loss: 176165.109375 acc: 0.9856211543083191  val: loss: 696294.625 acc: 0.9323107004165649\n",
      "step: 5375 , time : 0.0\n",
      "train: loss: 166990.1875 acc: 0.9657741785049438  val: loss: 967423.0 acc: 0.8010355234146118\n",
      "step: 5380 , time : 0.0\n",
      "train: loss: 63075.671875 acc: 0.9775536060333252  val: loss: 1857545.5 acc: 0.7802479267120361\n",
      "step: 5385 , time : 0.0\n",
      "train: loss: 89787.2421875 acc: 0.9832556247711182  val: loss: 1203410.0 acc: 0.8312267065048218\n",
      "step: 5390 , time : 0.0\n",
      "train: loss: 19152.435546875 acc: 0.9855382442474365  val: loss: 105567.5078125 acc: 0.9673721194267273\n",
      "step: 5395 , time : 0.0\n",
      "train: loss: 56309.97265625 acc: 0.9740551710128784  val: loss: 764228.75 acc: 0.8726029992103577\n",
      "step: 5400 , time : 0.0\n",
      "train: loss: 8522.4306640625 acc: 0.9868919849395752  val: loss: 712326.8125 acc: 0.8848865032196045\n",
      "step: 5405 , time : 0.0\n",
      "train: loss: 201923.515625 acc: 0.7842913866043091  val: loss: 1842290.0 acc: 0.6299079656600952\n",
      "step: 5410 , time : 0.0\n",
      "train: loss: 8144.14892578125 acc: 0.9701631665229797  val: loss: 1352620.75 acc: 0.7152409553527832\n",
      "step: 5415 , time : 0.0\n",
      "train: loss: 11543.43359375 acc: 0.9454635381698608  val: loss: 1297255.0 acc: 0.8445833921432495\n",
      "step: 5420 , time : 0.0\n",
      "train: loss: 6253.4658203125 acc: 0.9803013205528259  val: loss: 904664.3125 acc: 0.7981371283531189\n",
      "step: 5425 , time : 0.0\n",
      "train: loss: 11537.37109375 acc: 0.9790124297142029  val: loss: 231641.28125 acc: 0.7430925369262695\n",
      "step: 5430 , time : 0.0\n",
      "train: loss: 8260.921875 acc: 0.9944729804992676  val: loss: 1412766.125 acc: 0.5905532836914062\n",
      "step: 5435 , time : 0.0\n",
      "train: loss: 12944.0126953125 acc: 0.9848524928092957  val: loss: 1033934.3125 acc: 0.7348024845123291\n",
      "step: 5440 , time : 0.0\n",
      "train: loss: 10580.537109375 acc: 0.9872646927833557  val: loss: 1098957.25 acc: 0.412405788898468\n",
      "step: 5445 , time : 0.0\n",
      "train: loss: 111815.0234375 acc: 0.9365864396095276  val: loss: 419874.28125 acc: 0.8053057193756104\n",
      "step: 5450 , time : 0.0\n",
      "train: loss: 36318.10546875 acc: 0.9804874658584595  val: loss: 2119405.75 acc: 0.21752113103866577\n",
      "step: 5455 , time : 0.0010001659393310547\n",
      "train: loss: 25459.48828125 acc: 0.9790810942649841  val: loss: 1744317.5 acc: 0.06674277782440186\n",
      "step: 5460 , time : 0.0\n",
      "train: loss: 8182.9052734375 acc: 0.981806218624115  val: loss: 276346.90625 acc: 0.929428219795227\n",
      "step: 5465 , time : 0.0\n",
      "train: loss: 7908.74169921875 acc: 0.9960774183273315  val: loss: 2629265.25 acc: 0.7199082374572754\n",
      "step: 5470 , time : 0.0010008811950683594\n",
      "train: loss: 21769.537109375 acc: 0.9947945475578308  val: loss: 731980.0625 acc: 0.7673956155776978\n",
      "step: 5475 , time : 0.0\n",
      "train: loss: 60951.3515625 acc: 0.9882267713546753  val: loss: 1146612.625 acc: 0.7775543928146362\n",
      "step: 5480 , time : 0.0010006427764892578\n",
      "train: loss: 32930.29296875 acc: 0.9902439117431641  val: loss: 1068059.75 acc: 0.7805194854736328\n",
      "step: 5485 , time : 0.0\n",
      "train: loss: 22669.234375 acc: 0.9873929619789124  val: loss: 40042.42578125 acc: 0.9850801229476929\n",
      "step: 5490 , time : 0.0\n",
      "train: loss: 172282.890625 acc: 0.9563515186309814  val: loss: 306303.125 acc: 0.8658210039138794\n",
      "step: 5495 , time : 0.0010006427764892578\n",
      "train: loss: 172337.828125 acc: 0.9366543889045715  val: loss: 656521.75 acc: 0.7097163200378418\n",
      "step: 5500 , time : 0.0\n",
      "train: loss: 215404.1875 acc: 0.8752456307411194  val: loss: 226523.921875 acc: 0.9521202445030212\n",
      "step: 5505 , time : 0.0010013580322265625\n",
      "train: loss: 100376.8828125 acc: 0.9800257682800293  val: loss: 1121776.125 acc: 0.6932541131973267\n",
      "step: 5510 , time : 0.001001119613647461\n",
      "train: loss: 58092.5390625 acc: 0.9940277934074402  val: loss: 1155786.25 acc: -0.20218932628631592\n",
      "step: 5515 , time : 0.0\n",
      "train: loss: 578183.875 acc: 0.8978406190872192  val: loss: 490899.375 acc: 0.5261716842651367\n",
      "step: 5520 , time : 0.0010013580322265625\n",
      "train: loss: 222087.4375 acc: 0.9556470513343811  val: loss: 1558092.625 acc: 0.18071109056472778\n",
      "step: 5525 , time : 0.0\n",
      "train: loss: 204491.53125 acc: 0.9677240252494812  val: loss: 434988.375 acc: 0.9466752409934998\n",
      "step: 5530 , time : 0.0010006427764892578\n",
      "train: loss: 545377.3125 acc: 0.9589547514915466  val: loss: 207192.265625 acc: 0.9552642107009888\n",
      "step: 5535 , time : 0.0\n",
      "train: loss: 272854.5625 acc: 0.9721540212631226  val: loss: 1225962.0 acc: 0.7646313309669495\n",
      "step: 5540 , time : 0.0\n",
      "train: loss: 152988.078125 acc: 0.9693127870559692  val: loss: 339993.28125 acc: 0.9269598126411438\n",
      "step: 5545 , time : 0.0\n",
      "train: loss: 2405076.75 acc: 0.9252064228057861  val: loss: 358521.53125 acc: 0.8599988222122192\n",
      "step: 5550 , time : 0.0010006427764892578\n",
      "train: loss: 966533.3125 acc: 0.9678521752357483  val: loss: 866561.75 acc: 0.8817136287689209\n",
      "step: 5555 , time : 0.0\n",
      "train: loss: 1210082.375 acc: 0.9459585547447205  val: loss: 249180.96875 acc: 0.9659726023674011\n",
      "step: 5560 , time : 0.0010008811950683594\n",
      "train: loss: 1416002.25 acc: 0.9402924180030823  val: loss: 2152880.25 acc: 0.8545321226119995\n",
      "step: 5565 , time : 0.0010001659393310547\n",
      "train: loss: 776880.375 acc: 0.9373874664306641  val: loss: 379468.15625 acc: 0.9757421612739563\n",
      "step: 5570 , time : 0.0\n",
      "train: loss: 170624.453125 acc: 0.9686180353164673  val: loss: 1968227.25 acc: 0.8725141286849976\n",
      "step: 5575 , time : 0.0010004043579101562\n",
      "train: loss: 583597.3125 acc: 0.9232648611068726  val: loss: 537789.4375 acc: 0.9407743215560913\n",
      "step: 5580 , time : 0.0010006427764892578\n",
      "train: loss: 802539.4375 acc: 0.8591406345367432  val: loss: 1360537.0 acc: 0.754896342754364\n",
      "step: 5585 , time : 0.0\n",
      "train: loss: 1325122.5 acc: 0.8425877094268799  val: loss: 1640065.875 acc: 0.6628497838973999\n",
      "step: 5590 , time : 0.0\n",
      "train: loss: 1638569.75 acc: 0.18991929292678833  val: loss: 940402.5 acc: 0.8794114589691162\n",
      "step: 5595 , time : 0.0\n",
      "train: loss: 386376.53125 acc: 0.7998660206794739  val: loss: 1910662.0 acc: 0.607202410697937\n",
      "step: 5600 , time : 0.0\n",
      "train: loss: 295544.28125 acc: 0.8688669204711914  val: loss: 450491.21875 acc: 0.9192929267883301\n",
      "step: 5605 , time : 0.0\n",
      "train: loss: 1450920.5 acc: 0.6380172371864319  val: loss: 942773.0 acc: 0.8655740022659302\n",
      "step: 5610 , time : 0.0\n",
      "train: loss: 1136229.125 acc: 0.4123070240020752  val: loss: 1308793.0 acc: 0.444934606552124\n",
      "step: 5615 , time : 0.0010008811950683594\n",
      "train: loss: 612926.5 acc: 0.6647988557815552  val: loss: 1051919.25 acc: 0.5180302858352661\n",
      "step: 5620 , time : 0.0\n",
      "train: loss: 583043.125 acc: 0.4881709814071655  val: loss: 676988.625 acc: 0.5745761394500732\n",
      "step: 5625 , time : 0.0\n",
      "train: loss: 178925.0 acc: 0.8692247271537781  val: loss: 759381.625 acc: 0.7422293424606323\n",
      "step: 5630 , time : 0.0\n",
      "train: loss: 100968.015625 acc: 0.9326618909835815  val: loss: 869326.5 acc: 0.40693938732147217\n",
      "step: 5635 , time : 0.0\n",
      "train: loss: 115344.484375 acc: 0.9166810512542725  val: loss: 1271736.5 acc: 0.7005383968353271\n",
      "step: 5640 , time : 0.0\n",
      "train: loss: 231804.953125 acc: 0.8623984456062317  val: loss: 400116.5625 acc: 0.9172449111938477\n",
      "step: 5645 , time : 0.0010008811950683594\n",
      "train: loss: 146407.109375 acc: 0.8811701536178589  val: loss: 399883.5625 acc: 0.7647954225540161\n",
      "step: 5650 , time : 0.0\n",
      "train: loss: 55037.01953125 acc: 0.9600496888160706  val: loss: 494057.1875 acc: 0.789466917514801\n",
      "step: 5655 , time : 0.0\n",
      "train: loss: 34492.609375 acc: 0.9714464545249939  val: loss: 872091.75 acc: 0.8805242776870728\n",
      "step: 5660 , time : 0.0\n",
      "train: loss: 132669.5625 acc: 0.8072364330291748  val: loss: 1617095.0 acc: 0.7114343643188477\n",
      "step: 5665 , time : 0.0\n",
      "train: loss: 261582.625 acc: 0.7617132067680359  val: loss: 669451.375 acc: 0.7910592555999756\n",
      "step: 5670 , time : 0.001001119613647461\n",
      "train: loss: 689815.375 acc: 0.6698226928710938  val: loss: 3062475.0 acc: 0.6322686672210693\n",
      "step: 5675 , time : 0.0\n",
      "train: loss: 887131.25 acc: 0.1406337022781372  val: loss: 3894579.25 acc: 0.6679972410202026\n",
      "step: 5680 , time : 0.0\n",
      "train: loss: 575084.8125 acc: 0.7272945642471313  val: loss: 1320048.25 acc: 0.7420499324798584\n",
      "step: 5685 , time : 0.0\n",
      "train: loss: 256662.078125 acc: 0.8162297010421753  val: loss: 534260.625 acc: 0.7720383405685425\n",
      "step: 5690 , time : 0.0\n",
      "train: loss: 1263908.5 acc: 0.6778309345245361  val: loss: 1990984.5 acc: 0.7015725374221802\n",
      "step: 5695 , time : 0.0\n",
      "train: loss: 1204758.375 acc: 0.6466464996337891  val: loss: 2101836.75 acc: 0.7573282718658447\n",
      "step: 5700 , time : 0.0\n",
      "train: loss: 1478131.75 acc: 0.7957043647766113  val: loss: 2601270.0 acc: 0.8270578384399414\n",
      "step: 5705 , time : 0.0\n",
      "train: loss: 1366607.5 acc: 0.8514871001243591  val: loss: 1536619.75 acc: 0.8181180357933044\n",
      "step: 5710 , time : 0.0\n",
      "train: loss: 552715.875 acc: 0.954102098941803  val: loss: 1429054.25 acc: 0.7809692025184631\n",
      "step: 5715 , time : 0.0\n",
      "train: loss: 773310.25 acc: 0.9195534586906433  val: loss: 2557737.5 acc: 0.6514511108398438\n",
      "step: 5720 , time : 0.0\n",
      "train: loss: 624906.3125 acc: 0.8685979843139648  val: loss: 704417.5 acc: 0.5496182441711426\n",
      "step: 5725 , time : 0.0\n",
      "train: loss: 576840.1875 acc: 0.9483537673950195  val: loss: 902591.75 acc: 0.7658413648605347\n",
      "step: 5730 , time : 0.0\n",
      "train: loss: 664635.875 acc: 0.9555238485336304  val: loss: 1600072.75 acc: 0.7611595988273621\n",
      "step: 5735 , time : 0.0\n",
      "train: loss: 434285.78125 acc: 0.9662623405456543  val: loss: 1144129.125 acc: 0.7635018825531006\n",
      "step: 5740 , time : 0.0\n",
      "train: loss: 370737.0625 acc: 0.963352620601654  val: loss: 516179.40625 acc: 0.9339370131492615\n",
      "step: 5745 , time : 0.0010006427764892578\n",
      "train: loss: 298010.84375 acc: 0.9427041411399841  val: loss: 703286.875 acc: 0.7583370208740234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 5750 , time : 0.0\n",
      "train: loss: 118043.2109375 acc: 0.9163975119590759  val: loss: 347953.15625 acc: 0.8672031164169312\n",
      "step: 5755 , time : 0.0\n",
      "train: loss: 56813.140625 acc: 0.9772656559944153  val: loss: 183511.8125 acc: 0.9659958481788635\n",
      "step: 5760 , time : 0.0010008811950683594\n",
      "train: loss: 34656.3359375 acc: 0.9865407347679138  val: loss: 502720.34375 acc: 0.9169526100158691\n",
      "step: 5765 , time : 0.0010006427764892578\n",
      "train: loss: 36732.1171875 acc: 0.9805623292922974  val: loss: 1169567.25 acc: 0.8142685890197754\n",
      "step: 5770 , time : 0.0\n",
      "train: loss: 45256.16796875 acc: 0.9715481400489807  val: loss: 2310408.0 acc: 0.03550219535827637\n",
      "step: 5775 , time : 0.0\n",
      "train: loss: 46928.30859375 acc: 0.9189397692680359  val: loss: 1340085.5 acc: 0.7497455477714539\n",
      "step: 5780 , time : 0.0\n",
      "train: loss: 7130.91259765625 acc: 0.9791549444198608  val: loss: 377970.6875 acc: 0.8657176494598389\n",
      "step: 5785 , time : 0.0\n",
      "train: loss: 14706.234375 acc: 0.9763948321342468  val: loss: 111168.5546875 acc: 0.9772683382034302\n",
      "step: 5790 , time : 0.0\n",
      "train: loss: 4892.99560546875 acc: 0.9939916133880615  val: loss: 76295.84375 acc: 0.9819519519805908\n",
      "step: 5795 , time : 0.0\n",
      "train: loss: 32342.923828125 acc: 0.9708986878395081  val: loss: 1177782.625 acc: 0.5890957713127136\n",
      "step: 5800 , time : 0.0\n",
      "train: loss: 68500.8671875 acc: 0.9665051102638245  val: loss: 1680020.625 acc: 0.7733526825904846\n",
      "step: 5805 , time : 0.0\n",
      "train: loss: 13510.59765625 acc: 0.9878528714179993  val: loss: 160331.375 acc: 0.949933648109436\n",
      "step: 5810 , time : 0.0010004043579101562\n",
      "train: loss: 18883.734375 acc: 0.9917355179786682  val: loss: 968783.6875 acc: 0.8185069561004639\n",
      "step: 5815 , time : 0.0010004043579101562\n",
      "train: loss: 10909.455078125 acc: 0.990698516368866  val: loss: 1687069.875 acc: 0.5965780019760132\n",
      "step: 5820 , time : 0.0010008811950683594\n",
      "train: loss: 7086.171875 acc: 0.9860485792160034  val: loss: 757367.5625 acc: 0.6298168301582336\n",
      "step: 5825 , time : 0.001001119613647461\n",
      "train: loss: 5800.5615234375 acc: 0.9726066589355469  val: loss: 1187937.625 acc: 0.6105519533157349\n",
      "step: 5830 , time : 0.0010004043579101562\n",
      "train: loss: 23340.12109375 acc: 0.9808646440505981  val: loss: 1563971.75 acc: 0.627815306186676\n",
      "step: 5835 , time : 0.0010004043579101562\n",
      "train: loss: 28425.35546875 acc: 0.9906768798828125  val: loss: 334795.75 acc: 0.9227845668792725\n",
      "step: 5840 , time : 0.0010006427764892578\n",
      "train: loss: 23325.576171875 acc: 0.9931817054748535  val: loss: 183563.328125 acc: 0.9764860272407532\n",
      "step: 5845 , time : 0.0010004043579101562\n",
      "train: loss: 27042.033203125 acc: 0.992696225643158  val: loss: 647506.3125 acc: 0.6477312445640564\n",
      "step: 5850 , time : 0.0\n",
      "train: loss: 39117.85546875 acc: 0.986598014831543  val: loss: 226744.75 acc: 0.9851197004318237\n",
      "step: 5855 , time : 0.0010008811950683594\n",
      "train: loss: 231814.359375 acc: 0.9385619163513184  val: loss: 2490578.25 acc: 0.7447439432144165\n",
      "step: 5860 , time : 0.0\n",
      "train: loss: 50667.6796875 acc: 0.9839043021202087  val: loss: 3305189.0 acc: 0.606598973274231\n",
      "step: 5865 , time : 0.0\n",
      "train: loss: 49696.74609375 acc: 0.9772595763206482  val: loss: 379959.25 acc: 0.9537425637245178\n",
      "step: 5870 , time : 0.0\n",
      "train: loss: 160843.59375 acc: 0.9746394157409668  val: loss: 809016.3125 acc: 0.897477924823761\n",
      "step: 5875 , time : 0.0\n",
      "train: loss: 184239.59375 acc: 0.9703401923179626  val: loss: 1666823.375 acc: 0.7924986481666565\n",
      "step: 5880 , time : 0.0\n",
      "train: loss: 61764.8671875 acc: 0.9936235547065735  val: loss: 1405238.0 acc: 0.669630765914917\n",
      "step: 5885 , time : 0.0\n",
      "train: loss: 79788.8515625 acc: 0.9918278455734253  val: loss: 1029190.625 acc: 0.8349019885063171\n",
      "step: 5890 , time : 0.0\n",
      "train: loss: 290031.15625 acc: 0.940321147441864  val: loss: 948525.875 acc: 0.8717618584632874\n",
      "step: 5895 , time : 0.0\n",
      "train: loss: 892807.75 acc: 0.9618062376976013  val: loss: 901979.8125 acc: 0.8710800409317017\n",
      "step: 5900 , time : 0.0\n",
      "train: loss: 507908.125 acc: 0.9682317972183228  val: loss: 758269.3125 acc: 0.8688377737998962\n",
      "step: 5905 , time : 0.0\n",
      "train: loss: 237575.953125 acc: 0.959555447101593  val: loss: 2184221.0 acc: 0.7191381454467773\n",
      "step: 5910 , time : 0.0\n",
      "train: loss: 880369.4375 acc: 0.8701573014259338  val: loss: 1780798.125 acc: 0.639885663986206\n",
      "step: 5915 , time : 0.0\n",
      "train: loss: 675781.375 acc: 0.9837254881858826  val: loss: 2230872.5 acc: 0.6880148649215698\n",
      "step: 5920 , time : 0.0\n",
      "train: loss: 1411103.25 acc: 0.9594963192939758  val: loss: 1068193.0 acc: 0.3434143662452698\n",
      "step: 5925 , time : 0.0\n",
      "train: loss: 1536884.5 acc: 0.9408772587776184  val: loss: 1085345.125 acc: 0.8946028351783752\n",
      "step: 5930 , time : 0.0\n",
      "train: loss: 440925.46875 acc: 0.9743415117263794  val: loss: 1589682.5 acc: -0.9728266000747681\n",
      "step: 5935 , time : 0.0\n",
      "train: loss: 284748.59375 acc: 0.9555837512016296  val: loss: 208933.453125 acc: 0.9104443788528442\n",
      "step: 5940 , time : 0.0\n",
      "train: loss: 274181.875 acc: 0.9615828990936279  val: loss: 1479879.75 acc: 0.7097760438919067\n",
      "step: 5945 , time : 0.0\n",
      "train: loss: 225576.625 acc: 0.9396786689758301  val: loss: 1353716.25 acc: 0.8558192253112793\n",
      "step: 5950 , time : 0.0\n",
      "train: loss: 739970.0625 acc: 0.7014868259429932  val: loss: 620709.4375 acc: 0.8042296171188354\n",
      "step: 5955 , time : 0.0\n",
      "train: loss: 2114192.5 acc: 0.7368891835212708  val: loss: 1382806.125 acc: 0.8293952941894531\n",
      "step: 5960 , time : 0.0\n",
      "train: loss: 939388.4375 acc: 0.7860504984855652  val: loss: 799476.0 acc: 0.8268111348152161\n",
      "step: 5965 , time : 0.0\n",
      "train: loss: 310967.34375 acc: 0.8869220614433289  val: loss: 1609476.875 acc: 0.7991158366203308\n",
      "step: 5970 , time : 0.0\n",
      "train: loss: 1003529.625 acc: 0.6890372037887573  val: loss: 512521.5 acc: 0.77501380443573\n",
      "step: 5975 , time : 0.0\n",
      "train: loss: 1093001.375 acc: 0.3404662609100342  val: loss: 1883180.75 acc: 0.7358424663543701\n",
      "step: 5980 , time : 0.0\n",
      "train: loss: 228540.6875 acc: 0.7835585474967957  val: loss: 4275643.5 acc: 0.6734576225280762\n",
      "step: 5985 , time : 0.0010008811950683594\n",
      "train: loss: 775157.0625 acc: 0.6395715475082397  val: loss: 412508.6875 acc: 0.7058522701263428\n",
      "step: 5990 , time : 0.001001119613647461\n",
      "train: loss: 151182.890625 acc: 0.8721464276313782  val: loss: 1712945.875 acc: 0.7401118278503418\n",
      "step: 5995 , time : 0.0010001659393310547\n",
      "train: loss: 224445.078125 acc: 0.8683612942695618  val: loss: 811423.375 acc: 0.7372702360153198\n",
      "step: 6000 , time : 0.0\n",
      "train: loss: 93621.328125 acc: 0.9372804760932922  val: loss: 1142589.5 acc: 0.8138595819473267\n",
      "step: 6005 , time : 0.0010008811950683594\n",
      "train: loss: 179452.796875 acc: 0.8621745109558105  val: loss: 1783856.5 acc: 0.7236039638519287\n",
      "step: 6010 , time : 0.0010004043579101562\n",
      "train: loss: 338230.03125 acc: 0.7961788177490234  val: loss: 869719.0 acc: 0.7370277643203735\n",
      "step: 6015 , time : 0.0\n",
      "train: loss: 283142.125 acc: 0.8518369793891907  val: loss: 501948.125 acc: 0.8286269903182983\n",
      "step: 6020 , time : 0.0\n",
      "train: loss: 45811.67578125 acc: 0.9446713924407959  val: loss: 1249029.5 acc: 0.7257306575775146\n",
      "step: 6025 , time : 0.0\n",
      "train: loss: 110802.4296875 acc: 0.8771468997001648  val: loss: 1332782.5 acc: 0.7941344976425171\n",
      "step: 6030 , time : 0.0\n",
      "train: loss: 242042.9375 acc: 0.8445265889167786  val: loss: 817194.1875 acc: 0.7695071697235107\n",
      "step: 6035 , time : 0.0\n",
      "train: loss: 507654.875 acc: 0.7514832615852356  val: loss: 1216265.375 acc: 0.73521888256073\n",
      "step: 6040 , time : 0.0010006427764892578\n",
      "train: loss: 1164197.25 acc: 0.7064042091369629  val: loss: 2372517.25 acc: 0.7546334266662598\n",
      "step: 6045 , time : 0.0010004043579101562\n",
      "train: loss: 878472.75 acc: 0.7180732488632202  val: loss: 956095.625 acc: 0.7168353796005249\n",
      "step: 6050 , time : 0.0\n",
      "train: loss: 316347.34375 acc: 0.7610196471214294  val: loss: 1090154.625 acc: 0.7424241304397583\n",
      "step: 6055 , time : 0.0\n",
      "train: loss: 366120.03125 acc: 0.7814194560050964  val: loss: 1720628.5 acc: 0.7220306396484375\n",
      "step: 6060 , time : 0.0\n",
      "train: loss: 1759359.375 acc: 0.7027695178985596  val: loss: 609777.125 acc: 0.7969030141830444\n",
      "step: 6065 , time : 0.0\n",
      "train: loss: 1320404.0 acc: 0.7361913323402405  val: loss: 758359.6875 acc: 0.8176504969596863\n",
      "step: 6070 , time : 0.0009987354278564453\n",
      "train: loss: 1857372.25 acc: 0.8397084474563599  val: loss: 685962.5625 acc: 0.80695641040802\n",
      "step: 6075 , time : 0.0\n",
      "train: loss: 785022.5625 acc: 0.8966283202171326  val: loss: 1453671.75 acc: 0.6126518845558167\n",
      "step: 6080 , time : 0.0\n",
      "train: loss: 567297.75 acc: 0.937826931476593  val: loss: 1357159.5 acc: 0.6451491713523865\n",
      "step: 6085 , time : 0.0\n",
      "train: loss: 230576.296875 acc: 0.9048905372619629  val: loss: 667183.875 acc: 0.7867738604545593\n",
      "step: 6090 , time : 0.0\n",
      "train: loss: 383295.53125 acc: 0.9623851776123047  val: loss: 583776.0625 acc: 0.7976570129394531\n",
      "step: 6095 , time : 0.0010006427764892578\n",
      "train: loss: 163562.015625 acc: 0.987152636051178  val: loss: 533633.875 acc: 0.7960243225097656\n",
      "step: 6100 , time : 0.0\n",
      "train: loss: 298889.34375 acc: 0.9800270199775696  val: loss: 681740.25 acc: 0.9113799333572388\n",
      "step: 6105 , time : 0.0010006427764892578\n",
      "train: loss: 112848.84375 acc: 0.9895557761192322  val: loss: 843232.875 acc: 0.8732352256774902\n",
      "step: 6110 , time : 0.0010006427764892578\n",
      "train: loss: 126250.3828125 acc: 0.970476508140564  val: loss: 197809.59375 acc: 0.9574112892150879\n",
      "step: 6115 , time : 0.0\n",
      "train: loss: 21873.625 acc: 0.988358199596405  val: loss: 81381.921875 acc: 0.9691736102104187\n",
      "step: 6120 , time : 0.0\n",
      "train: loss: 5969.79736328125 acc: 0.9857861399650574  val: loss: 783999.875 acc: 0.7979788780212402\n",
      "step: 6125 , time : 0.0010006427764892578\n",
      "train: loss: 51116.93359375 acc: 0.9788651466369629  val: loss: 75535.421875 acc: 0.9761108160018921\n",
      "step: 6130 , time : 0.0010004043579101562\n",
      "train: loss: 4747.2900390625 acc: 0.9916970729827881  val: loss: 2639286.25 acc: 0.08385443687438965\n",
      "step: 6135 , time : 0.0\n",
      "train: loss: 19181.1640625 acc: 0.9852300882339478  val: loss: 413028.03125 acc: 0.9175216555595398\n",
      "step: 6140 , time : 0.0\n",
      "train: loss: 17769.396484375 acc: 0.9453263282775879  val: loss: 385504.21875 acc: 0.891740083694458\n",
      "step: 6145 , time : 0.0\n",
      "train: loss: 11219.25390625 acc: 0.9597621560096741  val: loss: 424799.21875 acc: 0.8636777997016907\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 6150 , time : 0.0\n",
      "train: loss: 8738.337890625 acc: 0.9941524863243103  val: loss: 973900.6875 acc: 0.475847065448761\n",
      "step: 6155 , time : 0.0\n",
      "train: loss: 20058.966796875 acc: 0.9602508544921875  val: loss: 253230.3125 acc: 0.9400289058685303\n",
      "step: 6160 , time : 0.0010006427764892578\n",
      "train: loss: 28485.9921875 acc: 0.9605767726898193  val: loss: 206754.21875 acc: 0.9659518003463745\n",
      "step: 6165 , time : 0.0010006427764892578\n",
      "train: loss: 5725.18359375 acc: 0.9828113317489624  val: loss: 911343.1875 acc: 0.7799259424209595\n",
      "step: 6170 , time : 0.0010008811950683594\n",
      "train: loss: 80583.109375 acc: 0.9686472415924072  val: loss: 694079.0625 acc: 0.8842011094093323\n",
      "step: 6175 , time : 0.0010008811950683594\n",
      "train: loss: 47873.8828125 acc: 0.976361870765686  val: loss: 1416332.375 acc: 0.7480685710906982\n",
      "step: 6180 , time : 0.0\n",
      "train: loss: 28156.884765625 acc: 0.9834911227226257  val: loss: 1151835.875 acc: 0.8811334371566772\n",
      "step: 6185 , time : 0.0010004043579101562\n",
      "train: loss: 26929.203125 acc: 0.9881709814071655  val: loss: 373931.59375 acc: 0.9600496888160706\n",
      "step: 6190 , time : 0.0010008811950683594\n",
      "train: loss: 11619.7314453125 acc: 0.9913486838340759  val: loss: 660027.8125 acc: 0.9642120599746704\n",
      "step: 6195 , time : 0.0010008811950683594\n",
      "train: loss: 19414.1015625 acc: 0.9924829602241516  val: loss: 818683.9375 acc: 0.8705628514289856\n",
      "step: 6200 , time : 0.0010006427764892578\n",
      "train: loss: 30864.841796875 acc: 0.9849782586097717  val: loss: 1720378.5 acc: 0.3363182544708252\n",
      "step: 6205 , time : 0.0\n",
      "train: loss: 38120.90625 acc: 0.9894258379936218  val: loss: 1018406.5625 acc: 0.8970020413398743\n",
      "step: 6210 , time : 0.001001119613647461\n",
      "train: loss: 71221.8515625 acc: 0.9806575775146484  val: loss: 2364730.5 acc: 0.737191379070282\n",
      "step: 6215 , time : 0.0\n",
      "train: loss: 31094.314453125 acc: 0.9854785799980164  val: loss: 1033310.5625 acc: 0.8789481520652771\n",
      "step: 6220 , time : 0.0\n",
      "train: loss: 70282.8359375 acc: 0.9815025925636292  val: loss: 1705570.25 acc: 0.872282862663269\n",
      "step: 6225 , time : 0.0\n",
      "train: loss: 101955.1484375 acc: 0.9709572792053223  val: loss: 2298907.0 acc: 0.2330152988433838\n",
      "step: 6230 , time : 0.0\n",
      "train: loss: 38089.09765625 acc: 0.9865964651107788  val: loss: 1232993.375 acc: 0.8766559362411499\n",
      "step: 6235 , time : 0.0\n",
      "train: loss: 172880.90625 acc: 0.9738932847976685  val: loss: 545178.9375 acc: 0.9140876531600952\n",
      "step: 6240 , time : 0.0010008811950683594\n",
      "train: loss: 397042.96875 acc: 0.94701087474823  val: loss: 2560869.25 acc: 0.30644357204437256\n",
      "step: 6245 , time : 0.0\n",
      "train: loss: 87851.7734375 acc: 0.9916470050811768  val: loss: 1459937.25 acc: 0.5226092338562012\n",
      "step: 6250 , time : 0.0010004043579101562\n",
      "train: loss: 44192.171875 acc: 0.9927803874015808  val: loss: 436823.875 acc: 0.9515278935432434\n",
      "step: 6255 , time : 0.0010006427764892578\n",
      "train: loss: 305092.625 acc: 0.9780872464179993  val: loss: 1712559.625 acc: 0.38669663667678833\n",
      "step: 6260 , time : 0.0010004043579101562\n",
      "train: loss: 176287.5 acc: 0.9696344137191772  val: loss: 874048.4375 acc: 0.8077616691589355\n",
      "step: 6265 , time : 0.0\n",
      "train: loss: 359172.90625 acc: 0.9619222283363342  val: loss: 1149292.625 acc: 0.8899813890457153\n",
      "step: 6270 , time : 0.0010001659393310547\n",
      "train: loss: 563075.9375 acc: 0.9536020755767822  val: loss: 686953.875 acc: 0.7814057469367981\n",
      "step: 6275 , time : 0.0\n",
      "train: loss: 1469743.875 acc: 0.9006765484809875  val: loss: 3598292.25 acc: 0.28988170623779297\n",
      "step: 6280 , time : 0.0010008811950683594\n",
      "train: loss: 514610.9375 acc: 0.9803348779678345  val: loss: 2281702.0 acc: -0.4461127519607544\n",
      "step: 6285 , time : 0.0010006427764892578\n",
      "train: loss: 2113001.25 acc: 0.9174761176109314  val: loss: 1682849.375 acc: 0.7645375728607178\n",
      "step: 6290 , time : 0.0\n",
      "train: loss: 948696.9375 acc: 0.9451661109924316  val: loss: 621630.875 acc: 0.7864588499069214\n",
      "step: 6295 , time : 0.0010008811950683594\n",
      "train: loss: 706215.5625 acc: 0.9430824518203735  val: loss: 1934303.25 acc: 0.7205435037612915\n",
      "step: 6300 , time : 0.0\n",
      "train: loss: 558384.375 acc: 0.9504785537719727  val: loss: 1701393.0 acc: 0.7498286366462708\n",
      "step: 6305 , time : 0.0010004043579101562\n",
      "train: loss: 745669.0625 acc: 0.9245086312294006  val: loss: 2228574.5 acc: 0.5582743287086487\n",
      "step: 6310 , time : 0.0\n",
      "train: loss: 622950.375 acc: 0.9315452575683594  val: loss: 1094575.75 acc: 0.636305570602417\n",
      "step: 6315 , time : 0.0\n",
      "train: loss: 953565.75 acc: 0.6508444547653198  val: loss: 585728.1875 acc: 0.7106195092201233\n",
      "step: 6320 , time : 0.0010006427764892578\n",
      "train: loss: 482399.40625 acc: 0.7331729531288147  val: loss: 758136.8125 acc: 0.8668398857116699\n",
      "step: 6325 , time : 0.0\n",
      "train: loss: 320526.78125 acc: 0.8344265818595886  val: loss: 937973.5 acc: 0.7924599051475525\n",
      "step: 6330 , time : 0.0010006427764892578\n",
      "train: loss: 587540.75 acc: 0.7933555841445923  val: loss: 1070564.125 acc: 0.8006698489189148\n",
      "step: 6335 , time : 0.0010006427764892578\n",
      "train: loss: 553556.625 acc: 0.8157821893692017  val: loss: 683447.5 acc: 0.8912040591239929\n",
      "step: 6340 , time : 0.0\n",
      "train: loss: 1296830.0 acc: 0.3466394543647766  val: loss: 723425.3125 acc: 0.863574743270874\n",
      "step: 6345 , time : 0.0010006427764892578\n",
      "train: loss: 855437.4375 acc: 0.6532889604568481  val: loss: 808358.1875 acc: 0.7073758840560913\n",
      "step: 6350 , time : 0.0010004043579101562\n",
      "train: loss: 989072.375 acc: 0.5132999420166016  val: loss: 1788712.5 acc: 0.6718167662620544\n",
      "step: 6355 , time : 0.0\n",
      "train: loss: 437197.6875 acc: 0.7691130042076111  val: loss: 1356518.0 acc: 0.690766453742981\n",
      "step: 6360 , time : 0.0\n",
      "train: loss: 161631.640625 acc: 0.8708304166793823  val: loss: 900207.875 acc: 0.758249044418335\n",
      "step: 6365 , time : 0.0010004043579101562\n",
      "train: loss: 109984.5234375 acc: 0.9091798663139343  val: loss: 1155508.625 acc: 0.7363425493240356\n",
      "step: 6370 , time : 0.0\n",
      "train: loss: 136036.09375 acc: 0.888370156288147  val: loss: 1246966.75 acc: 0.7522974014282227\n",
      "step: 6375 , time : 0.0\n",
      "train: loss: 141547.5625 acc: 0.889126718044281  val: loss: 971703.0 acc: 0.6643688678741455\n",
      "step: 6380 , time : 0.0\n",
      "train: loss: 33072.72265625 acc: 0.9716141223907471  val: loss: 1811995.0 acc: 0.7407196164131165\n",
      "step: 6385 , time : 0.0\n",
      "train: loss: 91204.5859375 acc: 0.9109504818916321  val: loss: 800842.25 acc: 0.8215246200561523\n",
      "step: 6390 , time : 0.0010004043579101562\n",
      "train: loss: 179256.296875 acc: 0.8048204183578491  val: loss: 607199.875 acc: 0.76701420545578\n",
      "step: 6395 , time : 0.0\n",
      "train: loss: 561274.0 acc: 0.7642292976379395  val: loss: 498303.28125 acc: 0.7971225380897522\n",
      "step: 6400 , time : 0.0\n",
      "train: loss: 744311.625 acc: 0.6397131681442261  val: loss: 590257.5 acc: 0.6930233836174011\n",
      "step: 6405 , time : 0.0010008811950683594\n",
      "train: loss: 807392.75 acc: 0.6464197039604187  val: loss: 522408.5 acc: 0.7222564220428467\n",
      "step: 6410 , time : 0.0010006427764892578\n",
      "train: loss: 610405.875 acc: 0.442474901676178  val: loss: 960285.625 acc: 0.7640334367752075\n",
      "step: 6415 , time : 0.0\n",
      "train: loss: 335706.84375 acc: 0.7716546058654785  val: loss: 2641199.75 acc: 0.6936206817626953\n",
      "step: 6420 , time : 0.0\n",
      "train: loss: 721610.875 acc: 0.7167342901229858  val: loss: 662805.4375 acc: 0.6887317895889282\n",
      "step: 6425 , time : 0.0\n",
      "train: loss: 1200376.75 acc: 0.7527830600738525  val: loss: 1449167.5 acc: 0.7226624488830566\n",
      "step: 6430 , time : 0.0\n",
      "train: loss: 1476151.875 acc: 0.6650011539459229  val: loss: 590130.125 acc: 0.8195152282714844\n",
      "step: 6435 , time : 0.0010008811950683594\n",
      "train: loss: 1029523.6875 acc: 0.8760306239128113  val: loss: 1075409.875 acc: 0.5647668242454529\n",
      "step: 6440 , time : 0.0\n",
      "train: loss: 907711.375 acc: 0.9039004445075989  val: loss: 877541.25 acc: 0.6004225611686707\n",
      "step: 6445 , time : 0.0\n",
      "train: loss: 809889.75 acc: 0.892282247543335  val: loss: 439986.78125 acc: 0.7815017700195312\n",
      "step: 6450 , time : 0.0\n",
      "train: loss: 500336.46875 acc: 0.9246712327003479  val: loss: 515514.96875 acc: 0.9107807874679565\n",
      "step: 6455 , time : 0.0\n",
      "train: loss: 465632.15625 acc: 0.9569915533065796  val: loss: 331459.5 acc: 0.915843665599823\n",
      "step: 6460 , time : 0.0\n",
      "train: loss: 283808.40625 acc: 0.9731680750846863  val: loss: 1114866.75 acc: 0.6243430972099304\n",
      "step: 6465 , time : 0.0010006427764892578\n",
      "train: loss: 269265.46875 acc: 0.9802491068840027  val: loss: 502498.59375 acc: 0.8544290065765381\n",
      "step: 6470 , time : 0.0\n",
      "train: loss: 219913.75 acc: 0.9797542095184326  val: loss: 421132.59375 acc: 0.833948016166687\n",
      "step: 6475 , time : 0.0\n",
      "train: loss: 162355.046875 acc: 0.9534262418746948  val: loss: 1307889.875 acc: 0.6616061925888062\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 6480 , time : 0.0010006427764892578\n",
      "train: loss: 41166.32421875 acc: 0.9737017154693604  val: loss: 712222.5625 acc: 0.8244818449020386\n",
      "step: 6485 , time : 0.0\n",
      "train: loss: 47795.2265625 acc: 0.97456955909729  val: loss: 337548.0 acc: 0.9507917761802673\n",
      "step: 6490 , time : 0.001001119613647461\n",
      "train: loss: 172266.234375 acc: 0.4275401830673218  val: loss: 834140.5625 acc: 0.9021179676055908\n",
      "step: 6495 , time : 0.0010008811950683594\n",
      "train: loss: 34595.6640625 acc: 0.9620565176010132  val: loss: 777990.0 acc: 0.9330333471298218\n",
      "step: 6500 , time : 0.0010006427764892578\n",
      "train: loss: 48761.2421875 acc: 0.9733180999755859  val: loss: 726215.1875 acc: 0.9197672605514526\n",
      "step: 6505 , time : 0.0\n",
      "train: loss: 5781.18115234375 acc: 0.984281063079834  val: loss: 303687.71875 acc: 0.9411394000053406\n",
      "step: 6510 , time : 0.0\n",
      "train: loss: 12759.984375 acc: 0.97325199842453  val: loss: 1579826.5 acc: 0.7882662415504456\n",
      "step: 6515 , time : 0.0010006427764892578\n",
      "train: loss: 4748.96484375 acc: 0.9714008569717407  val: loss: 1592632.5 acc: 0.9059762954711914\n",
      "step: 6520 , time : 0.0\n",
      "train: loss: 3868.65625 acc: 0.978305459022522  val: loss: 1352274.625 acc: 0.9013476371765137\n",
      "step: 6525 , time : 0.0\n",
      "train: loss: 195812.296875 acc: 0.8466506004333496  val: loss: 1391848.875 acc: 0.35427796840667725\n",
      "step: 6530 , time : 0.0\n",
      "train: loss: 40265.296875 acc: 0.9663912653923035  val: loss: 597855.8125 acc: 0.9126777052879333\n",
      "step: 6535 , time : 0.0010008811950683594\n",
      "train: loss: 55079.11328125 acc: 0.9708815813064575  val: loss: 1914464.625 acc: 0.8256354331970215\n",
      "step: 6540 , time : 0.001001119613647461\n",
      "train: loss: 26126.541015625 acc: 0.9848508834838867  val: loss: 1143957.25 acc: 0.8281276226043701\n",
      "step: 6545 , time : 0.0\n",
      "train: loss: 39931.75 acc: 0.9850171208381653  val: loss: 1492017.0 acc: 0.590962290763855\n",
      "step: 6550 , time : 0.0\n",
      "train: loss: 11022.0458984375 acc: 0.9864344596862793  val: loss: 2494526.0 acc: 0.504033088684082\n",
      "step: 6555 , time : 0.0\n",
      "train: loss: 19490.66015625 acc: 0.9745368361473083  val: loss: 1900390.25 acc: 0.6868528127670288\n",
      "step: 6560 , time : 0.0\n",
      "train: loss: 5921.03662109375 acc: 0.9934143424034119  val: loss: 1779310.5 acc: 0.6864264011383057\n",
      "step: 6565 , time : 0.0\n",
      "train: loss: 27552.755859375 acc: 0.9891118407249451  val: loss: 2057907.5 acc: 0.7438158988952637\n",
      "step: 6570 , time : 0.0\n",
      "train: loss: 44251.06640625 acc: 0.9921201467514038  val: loss: 1809859.0 acc: 0.5347615480422974\n",
      "step: 6575 , time : 0.0\n",
      "train: loss: 55672.7734375 acc: 0.9836825132369995  val: loss: 3907126.25 acc: 0.155805766582489\n",
      "step: 6580 , time : 0.0\n",
      "train: loss: 50772.375 acc: 0.978289008140564  val: loss: 1880149.5 acc: 0.37273287773132324\n",
      "step: 6585 , time : 0.0\n",
      "train: loss: 45583.0546875 acc: 0.987669825553894  val: loss: 2316977.75 acc: 0.4858212471008301\n",
      "step: 6590 , time : 0.0\n",
      "train: loss: 519065.25 acc: 0.8386397361755371  val: loss: 1444262.75 acc: 0.8189846277236938\n",
      "step: 6595 , time : 0.0\n",
      "train: loss: 130162.4609375 acc: 0.9088081121444702  val: loss: 1810345.0 acc: 0.7670009136199951\n",
      "step: 6600 , time : 0.0\n",
      "train: loss: 706115.3125 acc: 0.8935115337371826  val: loss: 1179434.0 acc: 0.4550785422325134\n",
      "step: 6605 , time : 0.0\n",
      "train: loss: 197421.40625 acc: 0.9735490083694458  val: loss: 867451.9375 acc: 0.38614165782928467\n",
      "step: 6610 , time : 0.0\n",
      "train: loss: 532772.375 acc: 0.9483259916305542  val: loss: 762997.5625 acc: 0.8247787952423096\n",
      "step: 6615 , time : 0.0010008811950683594\n",
      "train: loss: 128760.171875 acc: 0.9843283295631409  val: loss: 2100063.25 acc: -0.2757563591003418\n",
      "step: 6620 , time : 0.0\n",
      "train: loss: 93156.6484375 acc: 0.9855070114135742  val: loss: 1323926.25 acc: 0.606099009513855\n",
      "step: 6625 , time : 0.0\n",
      "train: loss: 196778.625 acc: 0.9769916534423828  val: loss: 955464.375 acc: 0.7876316905021667\n",
      "step: 6630 , time : 0.0\n",
      "train: loss: 818663.3125 acc: 0.9263421893119812  val: loss: 838889.4375 acc: 0.7082840204238892\n",
      "step: 6635 , time : 0.0\n",
      "train: loss: 306309.21875 acc: 0.9748335480690002  val: loss: 978089.1875 acc: 0.8881717920303345\n",
      "step: 6640 , time : 0.0\n",
      "train: loss: 355372.9375 acc: 0.970501720905304  val: loss: 284429.46875 acc: 0.9206473231315613\n",
      "step: 6645 , time : 0.0010008811950683594\n",
      "train: loss: 685357.4375 acc: 0.9722163677215576  val: loss: 864113.9375 acc: 0.8941472172737122\n",
      "step: 6650 , time : 0.0\n",
      "train: loss: 1245874.375 acc: 0.9594181776046753  val: loss: 653274.5 acc: 0.9028502702713013\n",
      "step: 6655 , time : 0.0010004043579101562\n",
      "train: loss: 1705912.75 acc: 0.9140475988388062  val: loss: 301891.3125 acc: 0.906647801399231\n",
      "step: 6660 , time : 0.0010006427764892578\n",
      "train: loss: 376712.6875 acc: 0.9386733770370483  val: loss: 832172.6875 acc: 0.7828304767608643\n",
      "step: 6665 , time : 0.001001596450805664\n",
      "train: loss: 159852.921875 acc: 0.9834457039833069  val: loss: 2582453.75 acc: 0.24172335863113403\n",
      "step: 6670 , time : 0.0010013580322265625\n",
      "train: loss: 386489.28125 acc: 0.9260206818580627  val: loss: 454823.875 acc: 0.7892230153083801\n",
      "step: 6675 , time : 0.0010006427764892578\n",
      "train: loss: 420597.5 acc: 0.9406052827835083  val: loss: 326628.5625 acc: 0.8924350738525391\n",
      "step: 6680 , time : 0.0010004043579101562\n",
      "train: loss: 2156442.75 acc: 0.6926866769790649  val: loss: 419887.625 acc: 0.8927378058433533\n",
      "step: 6685 , time : 0.0\n",
      "train: loss: 940101.8125 acc: 0.4546548128128052  val: loss: 1613558.5 acc: 0.737689733505249\n",
      "step: 6690 , time : 0.0\n",
      "train: loss: 488227.78125 acc: 0.7883694171905518  val: loss: 306209.125 acc: 0.8007515072822571\n",
      "step: 6695 , time : 0.0010006427764892578\n",
      "train: loss: 1563072.5 acc: -0.09136474132537842  val: loss: 1079570.25 acc: 0.8814606070518494\n",
      "step: 6700 , time : 0.0010006427764892578\n",
      "train: loss: 727323.75 acc: 0.7679486870765686  val: loss: 594208.875 acc: 0.8511391878128052\n",
      "step: 6705 , time : 0.0\n",
      "train: loss: 1136806.125 acc: 0.4995908737182617  val: loss: 544193.5 acc: 0.8458953499794006\n",
      "step: 6710 , time : 0.0010008811950683594\n",
      "train: loss: 1165071.25 acc: 0.6036903858184814  val: loss: 2921531.25 acc: 0.7586215734481812\n",
      "step: 6715 , time : 0.005003213882446289\n",
      "train: loss: 274774.59375 acc: 0.801232635974884  val: loss: 1254381.75 acc: 0.7956527471542358\n",
      "step: 6720 , time : 0.0\n",
      "train: loss: 357140.90625 acc: 0.7326157093048096  val: loss: 992349.375 acc: 0.7251729965209961\n",
      "step: 6725 , time : 0.0010004043579101562\n",
      "train: loss: 88260.359375 acc: 0.9171227812767029  val: loss: 733272.75 acc: 0.561762273311615\n",
      "step: 6730 , time : 0.0\n",
      "train: loss: 49246.30859375 acc: 0.9584854245185852  val: loss: 2130274.5 acc: 0.6972134113311768\n",
      "step: 6735 , time : 0.001001119613647461\n",
      "train: loss: 34852.16796875 acc: 0.972481369972229  val: loss: 1058480.75 acc: 0.6449390649795532\n",
      "step: 6740 , time : 0.0010006427764892578\n",
      "train: loss: 139991.78125 acc: 0.8907163143157959  val: loss: 204191.65625 acc: 0.813842236995697\n",
      "step: 6745 , time : 0.0010008811950683594\n",
      "train: loss: 322452.15625 acc: 0.8125596642494202  val: loss: 445742.1875 acc: 0.8001478910446167\n",
      "step: 6750 , time : 0.0\n",
      "train: loss: 48905.26171875 acc: 0.9469901919364929  val: loss: 644887.25 acc: 0.7441180348396301\n",
      "step: 6755 , time : 0.0\n",
      "train: loss: 39604.11328125 acc: 0.9568491578102112  val: loss: 836246.125 acc: 0.8673526048660278\n",
      "step: 6760 , time : 0.0\n",
      "train: loss: 135776.71875 acc: 0.8532839417457581  val: loss: 1376596.0 acc: 0.7571201920509338\n",
      "step: 6765 , time : 0.0\n",
      "train: loss: 372611.15625 acc: 0.7930077910423279  val: loss: 1128483.75 acc: 0.6688783168792725\n",
      "step: 6770 , time : 0.0010004043579101562\n",
      "train: loss: 746478.9375 acc: 0.6644382476806641  val: loss: 2488322.5 acc: 0.6986089944839478\n",
      "step: 6775 , time : 0.0\n",
      "train: loss: 1155977.0 acc: 0.5730249881744385  val: loss: 3091747.25 acc: 0.7437266111373901\n",
      "step: 6780 , time : 0.0\n",
      "train: loss: 91803.8203125 acc: 0.910955548286438  val: loss: 1253532.875 acc: 0.7265596389770508\n",
      "step: 6785 , time : 0.0\n",
      "train: loss: 590026.3125 acc: 0.71720290184021  val: loss: 6226603.0 acc: 0.5837140083312988\n",
      "step: 6790 , time : 0.0010008811950683594\n",
      "train: loss: 1846628.25 acc: 0.7185026407241821  val: loss: 3963097.75 acc: 0.7064849138259888\n",
      "step: 6795 , time : 0.001001119613647461\n",
      "train: loss: 1717735.25 acc: 0.8159749507904053  val: loss: 1693480.0 acc: 0.8280171155929565\n",
      "step: 6800 , time : 0.0\n",
      "train: loss: 1211801.5 acc: 0.899453341960907  val: loss: 1007817.5625 acc: 0.7353148460388184\n",
      "step: 6805 , time : 0.0010008811950683594\n",
      "train: loss: 781953.875 acc: 0.9011837244033813  val: loss: 1090002.625 acc: 0.8746850490570068\n",
      "step: 6810 , time : 0.0010008811950683594\n",
      "train: loss: 734119.1875 acc: 0.8252640962600708  val: loss: 1088374.125 acc: 0.8583827018737793\n",
      "step: 6815 , time : 0.0010008811950683594\n",
      "train: loss: 387800.34375 acc: 0.9331092834472656  val: loss: 1254396.25 acc: 0.7635020613670349\n",
      "step: 6820 , time : 0.0\n",
      "train: loss: 442943.25 acc: 0.9379048943519592  val: loss: 654562.375 acc: 0.8148793578147888\n",
      "step: 6825 , time : 0.0010006427764892578\n",
      "train: loss: 478153.40625 acc: 0.9583019614219666  val: loss: 959048.25 acc: 0.5447527170181274\n",
      "step: 6830 , time : 0.0010006427764892578\n",
      "train: loss: 316535.28125 acc: 0.9798884987831116  val: loss: 1243704.25 acc: 0.7996881008148193\n",
      "step: 6835 , time : 0.0010008811950683594\n",
      "train: loss: 602822.1875 acc: 0.9257540106773376  val: loss: 1497528.625 acc: 0.7067115306854248\n",
      "step: 6840 , time : 0.0\n",
      "train: loss: 514864.53125 acc: 0.943601667881012  val: loss: 1098753.75 acc: 0.5010818243026733\n",
      "step: 6845 , time : 0.0010008811950683594\n",
      "train: loss: 420678.3125 acc: 0.8763800859451294  val: loss: 767843.3125 acc: 0.9193946123123169\n",
      "step: 6850 , time : 0.0\n",
      "train: loss: 64603.25 acc: 0.9535312652587891  val: loss: 1339494.25 acc: 0.8696510195732117\n",
      "step: 6855 , time : 0.0\n",
      "train: loss: 32293.046875 acc: 0.9878857135772705  val: loss: 1618351.0 acc: 0.8150820732116699\n",
      "step: 6860 , time : 0.0\n",
      "train: loss: 22826.802734375 acc: 0.9652462601661682  val: loss: 916973.5625 acc: 0.7581104040145874\n",
      "step: 6865 , time : 0.0\n",
      "train: loss: 11334.490234375 acc: 0.9818215370178223  val: loss: 1299333.75 acc: 0.836290717124939\n",
      "step: 6870 , time : 0.0010008811950683594\n",
      "train: loss: 120994.7890625 acc: 0.9065621495246887  val: loss: 1568223.5 acc: 0.7252446413040161\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 6875 , time : 0.0\n",
      "train: loss: 20285.15234375 acc: 0.9659876823425293  val: loss: 1174494.0 acc: 0.7721990346908569\n",
      "step: 6880 , time : 0.001001119613647461\n",
      "train: loss: 30456.421875 acc: 0.9614831805229187  val: loss: 1262381.5 acc: 0.5536128282546997\n",
      "step: 6885 , time : 0.0010008811950683594\n",
      "train: loss: 13420.6396484375 acc: 0.9600078463554382  val: loss: 1197073.125 acc: 0.8972640633583069\n",
      "step: 6890 , time : 0.0\n",
      "train: loss: 11535.8466796875 acc: 0.9615609049797058  val: loss: 568059.8125 acc: 0.9282093644142151\n",
      "step: 6895 , time : 0.0010006427764892578\n",
      "train: loss: 71055.5 acc: 0.8931810855865479  val: loss: 1664873.25 acc: 0.7986838221549988\n",
      "step: 6900 , time : 0.0\n",
      "train: loss: 37238.71484375 acc: 0.9693567752838135  val: loss: 744491.75 acc: 0.7755031585693359\n",
      "step: 6905 , time : 0.0\n",
      "train: loss: 155406.765625 acc: 0.8738565444946289  val: loss: 1062576.25 acc: 0.8937292098999023\n",
      "step: 6910 , time : 0.0010004043579101562\n",
      "train: loss: 114612.0859375 acc: 0.954920768737793  val: loss: 2733128.0 acc: -0.6917433738708496\n",
      "step: 6915 , time : 0.0\n",
      "train: loss: 161521.375 acc: 0.9264703989028931  val: loss: 4731221.5 acc: 0.020615875720977783\n",
      "step: 6920 , time : 0.0\n",
      "train: loss: 21143.0625 acc: 0.8829831480979919  val: loss: 1881879.75 acc: 0.49404120445251465\n",
      "step: 6925 , time : 0.0010004043579101562\n",
      "train: loss: 67895.3828125 acc: 0.9621559977531433  val: loss: 2541880.25 acc: 0.0028023719787597656\n",
      "step: 6930 , time : 0.0\n",
      "train: loss: 75814.046875 acc: 0.9689935445785522  val: loss: 407010.90625 acc: 0.9340534806251526\n",
      "step: 6935 , time : 0.0\n",
      "train: loss: 142406.578125 acc: 0.9697666764259338  val: loss: 1243972.875 acc: 0.788102388381958\n",
      "step: 6940 , time : 0.0010008811950683594\n",
      "train: loss: 66308.1015625 acc: 0.9864367246627808  val: loss: 1794149.25 acc: 0.2938089966773987\n",
      "step: 6945 , time : 0.0010004043579101562\n",
      "train: loss: 161542.078125 acc: 0.957646906375885  val: loss: 2782916.25 acc: 0.40839987993240356\n",
      "step: 6950 , time : 0.0\n",
      "train: loss: 36275.65625 acc: 0.9813657402992249  val: loss: 677462.3125 acc: 0.758205771446228\n",
      "step: 6955 , time : 0.0\n",
      "train: loss: 183577.96875 acc: 0.9680083394050598  val: loss: 860992.8125 acc: -0.21784496307373047\n",
      "step: 6960 , time : 0.0\n",
      "train: loss: 228414.703125 acc: 0.9037386178970337  val: loss: 232872.640625 acc: 0.9552978873252869\n",
      "step: 6965 , time : 0.0\n",
      "train: loss: 89087.109375 acc: 0.9782957434654236  val: loss: 1738033.0 acc: 0.5051521062850952\n",
      "step: 6970 , time : 0.0\n",
      "train: loss: 351206.96875 acc: 0.9505101442337036  val: loss: 1546831.75 acc: 0.8235850930213928\n",
      "step: 6975 , time : 0.0\n",
      "train: loss: 257755.765625 acc: 0.9766732454299927  val: loss: 1137678.25 acc: 0.6818087100982666\n",
      "step: 6980 , time : 0.0\n",
      "train: loss: 58163.234375 acc: 0.9928383827209473  val: loss: 359705.625 acc: 0.9103061556816101\n",
      "step: 6985 , time : 0.0\n",
      "train: loss: 191936.890625 acc: 0.9687084555625916  val: loss: 330309.59375 acc: 0.8581019043922424\n",
      "step: 6990 , time : 0.0\n",
      "train: loss: 142372.203125 acc: 0.9844407439231873  val: loss: 419283.46875 acc: 0.71427321434021\n",
      "step: 6995 , time : 0.0\n",
      "train: loss: 251040.578125 acc: 0.9865322113037109  val: loss: 592672.375 acc: 0.8148802518844604\n",
      "step: 7000 , time : 0.0010008811950683594\n",
      "train: loss: 264177.71875 acc: 0.9679962396621704  val: loss: 519734.21875 acc: 0.6864893436431885\n",
      "step: 7005 , time : 0.0\n",
      "train: loss: 276141.71875 acc: 0.9671929478645325  val: loss: 649297.4375 acc: 0.8592562079429626\n",
      "step: 7010 , time : 0.0\n",
      "train: loss: 1154741.875 acc: 0.9609422087669373  val: loss: 440385.34375 acc: 0.9359015226364136\n",
      "step: 7015 , time : 0.0010006427764892578\n",
      "train: loss: 570520.5625 acc: 0.9778172969818115  val: loss: 141750.34375 acc: 0.9791125655174255\n",
      "step: 7020 , time : 0.0\n",
      "train: loss: 585475.375 acc: 0.9685609936714172  val: loss: 194353.328125 acc: 0.9311990141868591\n",
      "step: 7025 , time : 0.0010004043579101562\n",
      "train: loss: 968418.5625 acc: 0.94560307264328  val: loss: 364578.9375 acc: 0.8854953050613403\n",
      "step: 7030 , time : 0.0\n",
      "train: loss: 692288.3125 acc: 0.9622676968574524  val: loss: 797937.9375 acc: 0.873304545879364\n",
      "step: 7035 , time : 0.0\n",
      "train: loss: 239228.984375 acc: 0.9645758867263794  val: loss: 913120.625 acc: 0.2761757969856262\n",
      "step: 7040 , time : 0.0\n",
      "train: loss: 166423.265625 acc: 0.9153732657432556  val: loss: 2049820.125 acc: 0.49262601137161255\n",
      "step: 7045 , time : 0.0\n",
      "train: loss: 372261.0625 acc: 0.9595963954925537  val: loss: 603320.3125 acc: 0.6284375190734863\n",
      "step: 7050 , time : 0.0\n",
      "train: loss: 1542593.5 acc: 0.7255421280860901  val: loss: 1631204.0 acc: 0.35113203525543213\n",
      "step: 7055 , time : 0.0\n",
      "train: loss: 636853.5625 acc: 0.7638865113258362  val: loss: 1528228.875 acc: 0.7944523692131042\n",
      "step: 7060 , time : 0.0\n",
      "train: loss: 798851.5625 acc: 0.6591305732727051  val: loss: 714672.0 acc: 0.7837987542152405\n",
      "step: 7065 , time : 0.0\n",
      "train: loss: 837992.75 acc: 0.7427058815956116  val: loss: 338664.75 acc: 0.929530918598175\n",
      "step: 7070 , time : 0.0\n",
      "train: loss: 1719185.125 acc: -0.1989818811416626  val: loss: 1761245.5 acc: 0.612797737121582\n",
      "step: 7075 , time : 0.0\n",
      "train: loss: 791000.125 acc: 0.5930502414703369  val: loss: 1983520.375 acc: 0.5169961452484131\n",
      "step: 7080 , time : 0.0\n",
      "train: loss: 479035.59375 acc: 0.7535314559936523  val: loss: 1220999.0 acc: 0.7001345753669739\n",
      "step: 7085 , time : 0.0\n",
      "train: loss: 479725.6875 acc: 0.46908050775527954  val: loss: 936374.5625 acc: 0.6690298318862915\n",
      "step: 7090 , time : 0.0\n",
      "train: loss: 163563.453125 acc: 0.863029956817627  val: loss: 1353983.875 acc: 0.8235752582550049\n",
      "step: 7095 , time : 0.0\n",
      "train: loss: 215019.734375 acc: 0.8482351899147034  val: loss: 1196584.375 acc: 0.8899813890457153\n",
      "step: 7100 , time : 0.0\n",
      "train: loss: 116217.609375 acc: 0.8985799551010132  val: loss: 784660.5 acc: 0.8476911783218384\n",
      "step: 7105 , time : 0.0\n",
      "train: loss: 365213.0 acc: 0.767184317111969  val: loss: 2117535.75 acc: 0.7832568287849426\n",
      "step: 7110 , time : 0.0010006427764892578\n",
      "train: loss: 180350.75 acc: 0.8427664637565613  val: loss: 952677.125 acc: 0.8242622017860413\n",
      "step: 7115 , time : 0.0010008811950683594\n",
      "train: loss: 370624.03125 acc: 0.7057123184204102  val: loss: 2566598.75 acc: 0.8098011016845703\n",
      "step: 7120 , time : 0.0010006427764892578\n",
      "train: loss: 81416.09375 acc: 0.9254713654518127  val: loss: 764423.5625 acc: 0.8711373209953308\n",
      "step: 7125 , time : 0.0\n",
      "train: loss: 220833.5 acc: 0.8246636390686035  val: loss: 912406.3125 acc: 0.8928300142288208\n",
      "step: 7130 , time : 0.0\n",
      "train: loss: 422445.28125 acc: 0.69863361120224  val: loss: 655885.5625 acc: 0.8437379598617554\n",
      "step: 7135 , time : 0.0010004043579101562\n",
      "train: loss: 362957.03125 acc: 0.6289701461791992  val: loss: 888075.6875 acc: 0.7280844449996948\n",
      "step: 7140 , time : 0.0\n",
      "train: loss: 644232.8125 acc: 0.750674843788147  val: loss: 1232375.25 acc: 0.7657532691955566\n",
      "step: 7145 , time : 0.0\n",
      "train: loss: 309627.125 acc: 0.6846811771392822  val: loss: 810910.5625 acc: 0.7730510830879211\n",
      "step: 7150 , time : 0.0\n",
      "train: loss: 558482.5 acc: 0.7511546611785889  val: loss: 2043344.0 acc: 0.7648845314979553\n",
      "step: 7155 , time : 0.0\n",
      "train: loss: 1104646.25 acc: 0.7508851885795593  val: loss: 3065819.75 acc: 0.7638765573501587\n",
      "step: 7160 , time : 0.0\n",
      "train: loss: 1001459.875 acc: 0.7984784841537476  val: loss: 1183517.375 acc: 0.8580792546272278\n",
      "step: 7165 , time : 0.0\n",
      "train: loss: 913446.375 acc: 0.9225080013275146  val: loss: 1229286.5 acc: 0.8278192281723022\n",
      "step: 7170 , time : 0.0\n",
      "train: loss: 739708.1875 acc: 0.9382342100143433  val: loss: 1210934.0 acc: 0.8122352361679077\n",
      "step: 7175 , time : 0.0\n",
      "train: loss: 463826.09375 acc: 0.950490415096283  val: loss: 1264971.125 acc: 0.8462666273117065\n",
      "step: 7180 , time : 0.0\n",
      "train: loss: 411688.21875 acc: 0.9175252914428711  val: loss: 1764043.75 acc: 0.7282640933990479\n",
      "step: 7185 , time : 0.015625715255737305\n",
      "train: loss: 188020.90625 acc: 0.9690665006637573  val: loss: 1376418.875 acc: 0.843389093875885\n",
      "step: 7190 , time : 0.0\n",
      "train: loss: 170189.234375 acc: 0.9833087921142578  val: loss: 2257102.25 acc: 0.4790428876876831\n",
      "step: 7195 , time : 0.0\n",
      "train: loss: 121882.875 acc: 0.9922884702682495  val: loss: 1496712.0 acc: 0.41851478815078735\n",
      "step: 7200 , time : 0.0\n",
      "train: loss: 394947.375 acc: 0.9594281315803528  val: loss: 3363284.0 acc: 0.5068202018737793\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 7205 , time : 0.0\n",
      "train: loss: 251529.171875 acc: 0.9569247364997864  val: loss: 1412209.25 acc: 0.7480899691581726\n",
      "step: 7210 , time : 0.0\n",
      "train: loss: 105282.3125 acc: 0.9763702154159546  val: loss: 585227.25 acc: 0.918971598148346\n",
      "step: 7215 , time : 0.0\n",
      "train: loss: 50130.58203125 acc: 0.9673119187355042  val: loss: 367141.90625 acc: 0.9207684397697449\n",
      "step: 7220 , time : 0.0010006427764892578\n",
      "train: loss: 42726.90234375 acc: 0.9703654646873474  val: loss: 930460.5625 acc: 0.8377981781959534\n",
      "step: 7225 , time : 0.0\n",
      "train: loss: 8477.8291015625 acc: 0.9948244094848633  val: loss: 732979.3125 acc: 0.7833294868469238\n",
      "step: 7230 , time : 0.0010004043579101562\n",
      "train: loss: 60454.77734375 acc: 0.9518808722496033  val: loss: 2014182.125 acc: 0.6249285936355591\n",
      "step: 7235 , time : 0.0\n",
      "train: loss: 17735.240234375 acc: 0.9466961622238159  val: loss: 1424302.375 acc: 0.5717829465866089\n",
      "step: 7240 , time : 0.0\n",
      "train: loss: 98399.390625 acc: 0.917190670967102  val: loss: 605707.5625 acc: 0.9123351573944092\n",
      "step: 7245 , time : 0.0\n",
      "train: loss: 10998.0244140625 acc: 0.9697396159172058  val: loss: 1343884.5 acc: 0.8174009323120117\n",
      "step: 7250 , time : 0.0010004043579101562\n",
      "train: loss: 7638.00634765625 acc: 0.9670661091804504  val: loss: 463458.9375 acc: 0.8577805161476135\n",
      "step: 7255 , time : 0.0\n",
      "train: loss: 22444.548828125 acc: 0.9573761820793152  val: loss: 2682263.75 acc: 0.4665491580963135\n",
      "step: 7260 , time : 0.0\n",
      "train: loss: 71714.265625 acc: 0.9600247144699097  val: loss: 1210534.625 acc: 0.691754937171936\n",
      "step: 7265 , time : 0.0\n",
      "train: loss: 70085.90625 acc: 0.9539096355438232  val: loss: 155993.046875 acc: 0.975287139415741\n",
      "step: 7270 , time : 0.0\n",
      "train: loss: 122974.3125 acc: 0.9454221725463867  val: loss: 784525.0625 acc: 0.9104456305503845\n",
      "step: 7275 , time : 0.0\n",
      "train: loss: 39190.7578125 acc: 0.9725386500358582  val: loss: 1445239.875 acc: 0.5616961717605591\n",
      "step: 7280 , time : 0.0\n",
      "train: loss: 42270.74609375 acc: 0.9736120104789734  val: loss: 1310056.875 acc: 0.7531443238258362\n",
      "step: 7285 , time : 0.0\n",
      "train: loss: 8651.5546875 acc: 0.9862223863601685  val: loss: 1474303.125 acc: 0.8033477067947388\n",
      "step: 7290 , time : 0.0\n",
      "train: loss: 19364.591796875 acc: 0.9820939898490906  val: loss: 2388420.75 acc: 0.32913637161254883\n",
      "step: 7295 , time : 0.015625715255737305\n",
      "train: loss: 45421.6015625 acc: 0.9773045182228088  val: loss: 782580.3125 acc: 0.8665072917938232\n",
      "step: 7300 , time : 0.0\n",
      "train: loss: 119168.3125 acc: 0.9766231775283813  val: loss: 2064109.0 acc: 0.7742691040039062\n",
      "step: 7305 , time : 0.0\n",
      "train: loss: 74287.8125 acc: 0.9498421549797058  val: loss: 1748426.625 acc: 0.37647008895874023\n",
      "step: 7310 , time : 0.0\n",
      "train: loss: 48922.69140625 acc: 0.9628766775131226  val: loss: 993757.0625 acc: 0.5294532775878906\n",
      "step: 7315 , time : 0.0\n",
      "train: loss: 93766.828125 acc: 0.9706681966781616  val: loss: 602940.5 acc: 0.9577301144599915\n",
      "step: 7320 , time : 0.0\n",
      "train: loss: 97419.2265625 acc: 0.9739735126495361  val: loss: 329853.03125 acc: 0.9408232569694519\n",
      "step: 7325 , time : 0.0\n",
      "train: loss: 139053.765625 acc: 0.9388976097106934  val: loss: 504726.84375 acc: 0.7879769802093506\n",
      "step: 7330 , time : 0.0010008811950683594\n",
      "train: loss: 245955.59375 acc: 0.91141676902771  val: loss: 144568.359375 acc: 0.9584314823150635\n",
      "step: 7335 , time : 0.0\n",
      "train: loss: 745437.5625 acc: 0.9258341789245605  val: loss: 113541.7734375 acc: 0.8072845935821533\n",
      "step: 7340 , time : 0.0010008811950683594\n",
      "train: loss: 166814.46875 acc: 0.9839863777160645  val: loss: 1259998.25 acc: 0.8528488874435425\n",
      "step: 7345 , time : 0.0010006427764892578\n",
      "train: loss: 107222.4609375 acc: 0.9860702157020569  val: loss: 2823856.75 acc: 0.653501033782959\n",
      "step: 7350 , time : 0.0\n",
      "train: loss: 195954.15625 acc: 0.9694659113883972  val: loss: 533877.1875 acc: 0.8733217120170593\n",
      "step: 7355 , time : 0.0\n",
      "train: loss: 262880.59375 acc: 0.9705817103385925  val: loss: 474790.0 acc: 0.8146086931228638\n",
      "step: 7360 , time : 0.0\n",
      "train: loss: 1837558.5 acc: 0.8415654897689819  val: loss: 1555809.25 acc: 0.1767805814743042\n",
      "step: 7365 , time : 0.0\n",
      "train: loss: 628506.375 acc: 0.9251455664634705  val: loss: 743904.75 acc: 0.6108731031417847\n",
      "step: 7370 , time : 0.0\n",
      "train: loss: 317988.90625 acc: 0.9605424404144287  val: loss: 690675.9375 acc: 0.8333046436309814\n",
      "step: 7375 , time : 0.0\n",
      "train: loss: 910068.5625 acc: 0.9586056470870972  val: loss: 1395784.875 acc: 0.5579571723937988\n",
      "step: 7380 , time : 0.0\n",
      "train: loss: 777661.3125 acc: 0.9675543904304504  val: loss: 328713.375 acc: 0.8893757462501526\n",
      "step: 7385 , time : 0.0\n",
      "train: loss: 972772.0625 acc: 0.9662936925888062  val: loss: 643032.125 acc: 0.6977715492248535\n",
      "step: 7390 , time : 0.0\n",
      "train: loss: 881120.5625 acc: 0.952211320400238  val: loss: 587840.4375 acc: 0.8568713068962097\n",
      "step: 7395 , time : 0.0\n",
      "train: loss: 906473.9375 acc: 0.9478577971458435  val: loss: 257291.15625 acc: 0.9454107284545898\n",
      "step: 7400 , time : 0.0\n",
      "train: loss: 378085.34375 acc: 0.9639774560928345  val: loss: 650558.625 acc: 0.7690580487251282\n",
      "step: 7405 , time : 0.0\n",
      "train: loss: 240226.75 acc: 0.9074714183807373  val: loss: 204417.140625 acc: 0.8246083855628967\n",
      "step: 7410 , time : 0.0\n",
      "train: loss: 1019206.5625 acc: 0.6396826505661011  val: loss: 697395.375 acc: 0.8370542526245117\n",
      "step: 7415 , time : 0.0\n",
      "train: loss: 946760.25 acc: 0.7012126445770264  val: loss: 3032448.25 acc: 0.7453112602233887\n",
      "step: 7420 , time : 0.0\n",
      "train: loss: 1177965.75 acc: 0.773898720741272  val: loss: 694245.0625 acc: 0.7014888525009155\n",
      "step: 7425 , time : 0.0\n",
      "train: loss: 1138141.0 acc: 0.5236475467681885  val: loss: 608231.8125 acc: 0.9307639598846436\n",
      "step: 7430 , time : 0.015625476837158203\n",
      "train: loss: 692241.0 acc: 0.8585972785949707  val: loss: 782890.75 acc: 0.8861590027809143\n",
      "step: 7435 , time : 0.0\n",
      "train: loss: 2692634.0 acc: -0.824803352355957  val: loss: 709270.1875 acc: 0.8695200085639954\n",
      "step: 7440 , time : 0.0\n",
      "train: loss: 967280.25 acc: 0.43417030572891235  val: loss: 2639915.25 acc: 0.4745335578918457\n",
      "step: 7445 , time : 0.0010008811950683594\n",
      "train: loss: 478770.8125 acc: 0.60074782371521  val: loss: 3208378.5 acc: 0.7592270374298096\n",
      "step: 7450 , time : 0.0\n",
      "train: loss: 1037800.9375 acc: 0.2148534655570984  val: loss: 1397405.875 acc: 0.8024237155914307\n",
      "step: 7455 , time : 0.0010006427764892578\n",
      "train: loss: 122343.84375 acc: 0.8745394945144653  val: loss: 571789.9375 acc: 0.9254282116889954\n",
      "step: 7460 , time : 0.0\n",
      "train: loss: 251249.703125 acc: 0.8049387335777283  val: loss: 662299.125 acc: 0.840447187423706\n",
      "step: 7465 , time : 0.001001119613647461\n",
      "train: loss: 93632.7890625 acc: 0.9185497164726257  val: loss: 893263.1875 acc: 0.7982949614524841\n",
      "step: 7470 , time : 0.0011856555938720703\n",
      "train: loss: 220020.3125 acc: 0.8057941198348999  val: loss: 1227530.75 acc: 0.8483615517616272\n",
      "step: 7475 , time : 0.0\n",
      "train: loss: 274182.40625 acc: 0.7888597249984741  val: loss: 779066.8125 acc: 0.9053949117660522\n",
      "step: 7480 , time : 0.0\n",
      "train: loss: 369626.5 acc: 0.653113603591919  val: loss: 1045873.4375 acc: 0.6603841781616211\n",
      "step: 7485 , time : 0.0\n",
      "train: loss: 82398.5078125 acc: 0.9226956367492676  val: loss: 817605.0625 acc: 0.8507766723632812\n",
      "step: 7490 , time : 0.0\n",
      "train: loss: 122559.9375 acc: 0.875931441783905  val: loss: 785179.8125 acc: 0.8431217670440674\n",
      "step: 7495 , time : 0.0\n",
      "train: loss: 697204.125 acc: 0.1994866132736206  val: loss: 1238456.875 acc: 0.7340326309204102\n",
      "step: 7500 , time : 0.0\n",
      "train: loss: 489168.84375 acc: 0.5472667217254639  val: loss: 898441.5 acc: 0.6340558528900146\n",
      "step: 7505 , time : 0.0\n",
      "train: loss: 1410769.125 acc: 0.39733976125717163  val: loss: 3293005.25 acc: 0.7464553117752075\n",
      "step: 7510 , time : 0.0\n",
      "train: loss: 302791.40625 acc: 0.7825337052345276  val: loss: 2172118.75 acc: 0.7079278230667114\n",
      "step: 7515 , time : 0.0\n",
      "train: loss: 153175.90625 acc: 0.7349862456321716  val: loss: 1440153.125 acc: 0.7604565024375916\n",
      "step: 7520 , time : 0.0\n",
      "train: loss: 931313.0 acc: 0.6561452150344849  val: loss: 526456.875 acc: 0.6947668790817261\n",
      "step: 7525 , time : 0.0\n",
      "train: loss: 874324.875 acc: 0.8305994272232056  val: loss: 1594731.25 acc: 0.7402901649475098\n",
      "step: 7530 , time : 0.0010006427764892578\n",
      "train: loss: 678768.3125 acc: 0.8851267695426941  val: loss: 1235578.125 acc: 0.7968403100967407\n",
      "step: 7535 , time : 0.0010006427764892578\n",
      "train: loss: 391414.15625 acc: 0.9690662622451782  val: loss: 788675.875 acc: 0.8830572962760925\n",
      "step: 7540 , time : 0.0010006427764892578\n",
      "train: loss: 482292.875 acc: 0.9540006518363953  val: loss: 1293206.375 acc: 0.46083593368530273\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 7545 , time : 0.0\n",
      "train: loss: 414312.15625 acc: 0.9413750171661377  val: loss: 870954.0 acc: 0.5947100520133972\n",
      "step: 7550 , time : 0.0\n",
      "train: loss: 543060.5625 acc: 0.9356497526168823  val: loss: 880335.0 acc: 0.8249396681785583\n",
      "step: 7555 , time : 0.0010006427764892578\n",
      "train: loss: 269820.90625 acc: 0.9644914269447327  val: loss: 2021298.0 acc: 0.7605665326118469\n",
      "step: 7560 , time : 0.0010013580322265625\n",
      "train: loss: 364551.1875 acc: 0.9748565554618835  val: loss: 837150.875 acc: 0.8327615857124329\n",
      "step: 7565 , time : 0.0010008811950683594\n",
      "train: loss: 344408.5625 acc: 0.9695791602134705  val: loss: 2460196.5 acc: -0.005867958068847656\n",
      "step: 7570 , time : 0.0010008811950683594\n",
      "train: loss: 292875.125 acc: 0.9658435583114624  val: loss: 528178.5 acc: 0.936788022518158\n",
      "step: 7575 , time : 0.0\n",
      "train: loss: 124912.0078125 acc: 0.9739024639129639  val: loss: 716668.75 acc: 0.8463879823684692\n",
      "step: 7580 , time : 0.0\n",
      "train: loss: 25687.47265625 acc: 0.989538311958313  val: loss: 3275584.0 acc: 0.18776530027389526\n",
      "step: 7585 , time : 0.0\n",
      "train: loss: 55339.65234375 acc: 0.9448270797729492  val: loss: 252083.796875 acc: 0.9393629431724548\n",
      "step: 7590 , time : 0.0\n",
      "train: loss: 13967.2197265625 acc: 0.991957426071167  val: loss: 2501364.5 acc: 0.5775828957557678\n",
      "step: 7595 , time : 0.0\n",
      "train: loss: 25871.580078125 acc: 0.9698441624641418  val: loss: 817807.9375 acc: 0.9008032083511353\n",
      "step: 7600 , time : 0.0010008811950683594\n",
      "train: loss: 14653.47265625 acc: 0.9854081273078918  val: loss: 2161022.0 acc: 0.24408453702926636\n",
      "step: 7605 , time : 0.0\n",
      "train: loss: 71263.3671875 acc: 0.9350125789642334  val: loss: 786540.6875 acc: 0.6678869724273682\n",
      "step: 7610 , time : 0.001001119613647461\n",
      "train: loss: 25887.666015625 acc: 0.9597789645195007  val: loss: 759260.3125 acc: 0.9045374989509583\n",
      "step: 7615 , time : 0.0\n",
      "train: loss: 110229.515625 acc: 0.9235944747924805  val: loss: 1292511.375 acc: 0.7813625335693359\n",
      "step: 7620 , time : 0.0\n",
      "train: loss: 15466.7919921875 acc: 0.9594338536262512  val: loss: 1016130.75 acc: 0.8752618432044983\n",
      "step: 7625 , time : 0.0\n",
      "train: loss: 70548.5859375 acc: 0.9591749906539917  val: loss: 358492.59375 acc: 0.8824722766876221\n",
      "step: 7630 , time : 0.0\n",
      "train: loss: 40993.4375 acc: 0.9741000533103943  val: loss: 807402.8125 acc: 0.9175551533699036\n",
      "step: 7635 , time : 0.0\n",
      "train: loss: 60429.35546875 acc: 0.9657875299453735  val: loss: 812168.4375 acc: 0.7975870370864868\n",
      "step: 7640 , time : 0.0\n",
      "train: loss: 160196.5625 acc: 0.9346550703048706  val: loss: 88910.3203125 acc: 0.979426383972168\n",
      "step: 7645 , time : 0.0\n",
      "train: loss: 63935.3359375 acc: 0.9484958052635193  val: loss: 300959.90625 acc: 0.9336324334144592\n",
      "step: 7650 , time : 0.0\n",
      "train: loss: 19861.3984375 acc: 0.9675996899604797  val: loss: 1002415.6875 acc: 0.8332862854003906\n",
      "step: 7655 , time : 0.0\n",
      "train: loss: 27315.36328125 acc: 0.9673644304275513  val: loss: 632015.3125 acc: 0.7835565805435181\n",
      "step: 7660 , time : 0.0010008811950683594\n",
      "train: loss: 53690.984375 acc: 0.9778035879135132  val: loss: 1006216.8125 acc: 0.43313878774642944\n",
      "step: 7665 , time : 0.0\n",
      "train: loss: 107142.0234375 acc: 0.9773815870285034  val: loss: 381698.625 acc: 0.7482807040214539\n",
      "step: 7670 , time : 0.0\n",
      "train: loss: 83802.1328125 acc: 0.9832221865653992  val: loss: 1390975.125 acc: 0.8310143947601318\n",
      "step: 7675 , time : 0.0010004043579101562\n",
      "train: loss: 133349.5 acc: 0.9505751729011536  val: loss: 1394795.25 acc: 0.2933368682861328\n",
      "step: 7680 , time : 0.0010006427764892578\n",
      "train: loss: 255385.25 acc: 0.8810741305351257  val: loss: 2263862.75 acc: 0.5809845328330994\n",
      "step: 7685 , time : 0.0010008811950683594\n",
      "train: loss: 505338.03125 acc: 0.7988173365592957  val: loss: 336344.03125 acc: 0.9564496278762817\n",
      "step: 7690 , time : 0.0\n",
      "train: loss: 131289.953125 acc: 0.9255645871162415  val: loss: 1014148.5 acc: 0.7787535190582275\n",
      "step: 7695 , time : 0.0\n",
      "train: loss: 151669.5 acc: 0.9674497842788696  val: loss: 462062.0625 acc: 0.898371696472168\n",
      "step: 7700 , time : 0.0\n",
      "train: loss: 596712.3125 acc: 0.8951071500778198  val: loss: 528052.5625 acc: 0.7426701784133911\n",
      "step: 7705 , time : 0.0\n",
      "train: loss: 101222.7265625 acc: 0.9861903190612793  val: loss: 692237.9375 acc: 0.8188356161117554\n",
      "step: 7710 , time : 0.0\n",
      "train: loss: 503685.46875 acc: 0.8840360641479492  val: loss: 188692.875 acc: 0.9565766453742981\n",
      "step: 7715 , time : 0.0\n",
      "train: loss: 117753.1875 acc: 0.9837275147438049  val: loss: 248732.375 acc: 0.9128174185752869\n",
      "step: 7720 , time : 0.0\n",
      "train: loss: 368418.71875 acc: 0.9775106310844421  val: loss: 640101.0625 acc: 0.9282252192497253\n",
      "step: 7725 , time : 0.0\n",
      "train: loss: 491380.25 acc: 0.9743710160255432  val: loss: 234088.171875 acc: 0.979150652885437\n",
      "step: 7730 , time : 0.0\n",
      "train: loss: 511651.375 acc: 0.9580048322677612  val: loss: 945144.375 acc: 0.48611873388290405\n",
      "step: 7735 , time : 0.0\n",
      "train: loss: 283958.6875 acc: 0.9545151591300964  val: loss: 1066492.375 acc: 0.9118241667747498\n",
      "step: 7740 , time : 0.0\n",
      "train: loss: 1183444.125 acc: 0.9498795866966248  val: loss: 479351.03125 acc: 0.8687204122543335\n",
      "step: 7745 , time : 0.0\n",
      "train: loss: 1350515.25 acc: 0.9582697153091431  val: loss: 450711.84375 acc: 0.9301298260688782\n",
      "step: 7750 , time : 0.0\n",
      "train: loss: 664767.8125 acc: 0.969569206237793  val: loss: 703543.25 acc: 0.8891002535820007\n",
      "step: 7755 , time : 0.0\n",
      "train: loss: 1293093.125 acc: 0.9436993598937988  val: loss: 3721295.25 acc: 0.7146187424659729\n",
      "step: 7760 , time : 0.0\n",
      "train: loss: 241879.234375 acc: 0.966178297996521  val: loss: 1146442.375 acc: 0.814429759979248\n",
      "step: 7765 , time : 0.0\n",
      "train: loss: 295554.84375 acc: 0.9711763858795166  val: loss: 1006857.1875 acc: 0.7698574066162109\n",
      "step: 7770 , time : 0.0\n",
      "train: loss: 373015.84375 acc: 0.9620575904846191  val: loss: 1629686.625 acc: 0.19213742017745972\n",
      "step: 7775 , time : 0.0\n",
      "train: loss: 845506.6875 acc: 0.7470189332962036  val: loss: 371192.4375 acc: 0.9377439618110657\n",
      "step: 7780 , time : 0.0010004043579101562\n",
      "train: loss: 469029.3125 acc: 0.8142442107200623  val: loss: 1955892.625 acc: 0.7728132009506226\n",
      "step: 7785 , time : 0.0010004043579101562\n",
      "train: loss: 860774.125 acc: 0.7699891328811646  val: loss: 2323953.0 acc: 0.6950605511665344\n",
      "step: 7790 , time : 0.0010004043579101562\n",
      "train: loss: 580595.5625 acc: 0.6792600154876709  val: loss: 950463.375 acc: 0.899013340473175\n",
      "step: 7795 , time : 0.0\n",
      "train: loss: 355582.4375 acc: 0.87668377161026  val: loss: 907548.3125 acc: 0.8577954769134521\n",
      "step: 7800 , time : 0.0\n",
      "train: loss: 2494056.25 acc: -0.6601347923278809  val: loss: 1674051.75 acc: 0.7920265793800354\n",
      "step: 7805 , time : 0.0\n",
      "train: loss: 1447520.0 acc: 0.40404224395751953  val: loss: 1806141.0 acc: 0.27056747674942017\n",
      "step: 7810 , time : 0.0010004043579101562\n",
      "train: loss: 1377844.5 acc: 0.6135333776473999  val: loss: 740042.75 acc: 0.5639222860336304\n",
      "step: 7815 , time : 0.0\n",
      "train: loss: 834590.875 acc: 0.2905392646789551  val: loss: 932006.0625 acc: 0.6082346439361572\n",
      "step: 7820 , time : 0.0010004043579101562\n",
      "train: loss: 295716.75 acc: 0.7759409546852112  val: loss: 2572924.5 acc: 0.6887243390083313\n",
      "step: 7825 , time : 0.0010006427764892578\n",
      "train: loss: 438440.15625 acc: 0.6686038374900818  val: loss: 1117549.125 acc: 0.7821569442749023\n",
      "step: 7830 , time : 0.0\n",
      "train: loss: 111064.2421875 acc: 0.9154138565063477  val: loss: 760600.9375 acc: 0.8153356909751892\n",
      "step: 7835 , time : 0.0\n",
      "train: loss: 391277.5625 acc: 0.6842101812362671  val: loss: 1122012.5 acc: 0.7542324066162109\n",
      "step: 7840 , time : 0.0010008811950683594\n",
      "train: loss: 149259.78125 acc: 0.8749123215675354  val: loss: 1175092.375 acc: 0.7367300391197205\n",
      "step: 7845 , time : 0.0\n",
      "train: loss: 158935.703125 acc: 0.8535052537918091  val: loss: 349788.96875 acc: 0.9426755905151367\n",
      "step: 7850 , time : 0.0\n",
      "train: loss: 485689.65625 acc: 0.39330899715423584  val: loss: 1309782.5 acc: 0.8200166821479797\n",
      "step: 7855 , time : 0.0010008811950683594\n",
      "train: loss: 116392.0390625 acc: 0.8814598321914673  val: loss: 1390494.375 acc: 0.8293954133987427\n",
      "step: 7860 , time : 0.0010006427764892578\n",
      "train: loss: 313576.6875 acc: 0.780614972114563  val: loss: 761049.6875 acc: 0.8869964480400085\n",
      "step: 7865 , time : 0.0\n",
      "train: loss: 939643.9375 acc: 0.6519452929496765  val: loss: 2400072.75 acc: 0.7263443470001221\n",
      "step: 7870 , time : 0.0\n",
      "train: loss: 634155.9375 acc: 0.6371930837631226  val: loss: 878727.6875 acc: 0.7635869979858398\n",
      "step: 7875 , time : 0.0\n",
      "train: loss: 442960.9375 acc: 0.7162525653839111  val: loss: 790045.25 acc: 0.7107636332511902\n",
      "step: 7880 , time : 0.0\n",
      "train: loss: 775285.5 acc: 0.6946738362312317  val: loss: 2660876.25 acc: 0.7805810570716858\n",
      "step: 7885 , time : 0.0010004043579101562\n",
      "train: loss: 559579.25 acc: 0.7407541275024414  val: loss: 543246.625 acc: 0.7953132390975952\n",
      "step: 7890 , time : 0.0010006427764892578\n",
      "train: loss: 1829999.625 acc: 0.6318774223327637  val: loss: 1031556.25 acc: 0.810938835144043\n",
      "step: 7895 , time : 0.0\n",
      "train: loss: 867254.5 acc: 0.9248368144035339  val: loss: 801988.5 acc: 0.8716752529144287\n",
      "step: 7900 , time : 0.0010006427764892578\n",
      "train: loss: 1418705.625 acc: 0.8590120077133179  val: loss: 780845.0625 acc: 0.8291756510734558\n",
      "step: 7905 , time : 0.0\n",
      "train: loss: 454923.8125 acc: 0.9444615840911865  val: loss: 868002.8125 acc: 0.6895849108695984\n",
      "step: 7910 , time : 0.0010004043579101562\n",
      "train: loss: 566482.625 acc: 0.9339616298675537  val: loss: 767570.4375 acc: 0.7249214053153992\n",
      "step: 7915 , time : 0.001001119613647461\n",
      "train: loss: 346364.21875 acc: 0.9446201324462891  val: loss: 1740771.25 acc: 0.7109938859939575\n",
      "step: 7920 , time : 0.0\n",
      "train: loss: 288771.5625 acc: 0.9737197160720825  val: loss: 1057987.125 acc: 0.6612977981567383\n",
      "step: 7925 , time : 0.0010006427764892578\n",
      "train: loss: 324327.9375 acc: 0.9674853086471558  val: loss: 731992.3125 acc: 0.9235122799873352\n",
      "step: 7930 , time : 0.0\n",
      "train: loss: 290337.8125 acc: 0.9746306538581848  val: loss: 977415.6875 acc: 0.12901294231414795\n",
      "step: 7935 , time : 0.0010006427764892578\n",
      "train: loss: 349504.46875 acc: 0.9507392048835754  val: loss: 891184.8125 acc: 0.7549411058425903\n",
      "step: 7940 , time : 0.001001119613647461\n",
      "train: loss: 212114.796875 acc: 0.9649015665054321  val: loss: 641108.5625 acc: 0.8336468935012817\n",
      "step: 7945 , time : 0.0010008811950683594\n",
      "train: loss: 108712.2890625 acc: 0.9527397155761719  val: loss: 900733.8125 acc: 0.6792984008789062\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 7950 , time : 0.001001119613647461\n",
      "train: loss: 78415.2890625 acc: 0.9392843246459961  val: loss: 1415266.5 acc: 0.6297231912612915\n",
      "step: 7955 , time : 0.0\n",
      "train: loss: 11277.958984375 acc: 0.993355393409729  val: loss: 348144.3125 acc: 0.8128499388694763\n",
      "step: 7960 , time : 0.0\n",
      "train: loss: 10974.126953125 acc: 0.9644563794136047  val: loss: 2097609.25 acc: 0.6008878946304321\n",
      "step: 7965 , time : 0.0\n",
      "train: loss: 21072.974609375 acc: 0.9623964428901672  val: loss: 510206.71875 acc: 0.8452800512313843\n",
      "step: 7970 , time : 0.0010006427764892578\n",
      "train: loss: 21493.447265625 acc: 0.9620856642723083  val: loss: 154966.203125 acc: 0.9467170238494873\n",
      "step: 7975 , time : 0.0010004043579101562\n",
      "train: loss: 22510.884765625 acc: 0.9450883269309998  val: loss: 670362.5 acc: 0.8604134321212769\n",
      "step: 7980 , time : 0.0\n",
      "train: loss: 18647.61328125 acc: 0.9563444256782532  val: loss: 1669934.5 acc: 0.8094736337661743\n",
      "step: 7985 , time : 0.0010001659393310547\n",
      "train: loss: 20648.1328125 acc: 0.9660022258758545  val: loss: 452232.9375 acc: 0.7872267365455627\n",
      "step: 7990 , time : 0.0\n",
      "train: loss: 64341.91796875 acc: 0.9425972104072571  val: loss: 350026.0 acc: 0.8070257306098938\n",
      "step: 7995 , time : 0.0010006427764892578\n",
      "train: loss: 112651.984375 acc: 0.9523804783821106  val: loss: 97565.0390625 acc: 0.9650481939315796\n",
      "step: 8000 , time : 0.0010008811950683594\n",
      "train: loss: 103382.2421875 acc: 0.9472483396530151  val: loss: 305967.6875 acc: 0.8297589421272278\n",
      "step: 8005 , time : 0.0\n",
      "train: loss: 41214.1328125 acc: 0.981891393661499  val: loss: 740572.9375 acc: 0.6868654489517212\n",
      "step: 8010 , time : 0.0\n",
      "train: loss: 31927.912109375 acc: 0.98341965675354  val: loss: 611768.875 acc: 0.833810031414032\n",
      "step: 8015 , time : 0.0010008811950683594\n",
      "train: loss: 49893.84765625 acc: 0.9466051459312439  val: loss: 168517.84375 acc: 0.9616265892982483\n",
      "step: 8020 , time : 0.0010004043579101562\n",
      "train: loss: 12098.0146484375 acc: 0.9811895489692688  val: loss: 1891288.875 acc: 0.7717961072921753\n",
      "step: 8025 , time : 0.0\n",
      "train: loss: 32829.01953125 acc: 0.9844827055931091  val: loss: 987111.5625 acc: 0.8372802734375\n",
      "step: 8030 , time : 0.0010004043579101562\n",
      "train: loss: 79342.8984375 acc: 0.9702724814414978  val: loss: 1302371.375 acc: 0.8714120984077454\n",
      "step: 8035 , time : 0.0\n",
      "train: loss: 115909.1640625 acc: 0.9704005718231201  val: loss: 521110.0625 acc: 0.9583420753479004\n",
      "step: 8040 , time : 0.0\n",
      "train: loss: 87876.6171875 acc: 0.9676502346992493  val: loss: 423930.4375 acc: 0.9519532322883606\n",
      "step: 8045 , time : 0.0\n",
      "train: loss: 82580.1171875 acc: 0.9751012325286865  val: loss: 377703.625 acc: 0.9098629951477051\n",
      "step: 8050 , time : 0.0\n",
      "train: loss: 320043.78125 acc: 0.9158326387405396  val: loss: 1443840.625 acc: 0.8423686027526855\n",
      "step: 8055 , time : 0.0\n",
      "train: loss: 122589.015625 acc: 0.9664526581764221  val: loss: 1018449.0625 acc: 0.8787930011749268\n",
      "step: 8060 , time : 0.0\n",
      "train: loss: 202575.5625 acc: 0.894519031047821  val: loss: 438025.3125 acc: 0.9779267311096191\n",
      "step: 8065 , time : 0.0010004043579101562\n",
      "train: loss: 100558.25 acc: 0.9586825966835022  val: loss: 3685005.5 acc: 0.7139359712600708\n",
      "step: 8070 , time : 0.001001119613647461\n",
      "train: loss: 130428.484375 acc: 0.9894399046897888  val: loss: 183698.8125 acc: 0.9706991910934448\n",
      "step: 8075 , time : 0.0\n",
      "train: loss: 187952.84375 acc: 0.9811891913414001  val: loss: 734039.5 acc: 0.8834239840507507\n",
      "step: 8080 , time : 0.0010004043579101562\n",
      "train: loss: 310703.9375 acc: 0.9493300318717957  val: loss: 1321223.5 acc: 0.7746952772140503\n",
      "step: 8085 , time : 0.0\n",
      "train: loss: 212475.484375 acc: 0.9805338978767395  val: loss: 483146.78125 acc: 0.9494835138320923\n",
      "step: 8090 , time : 0.001001119613647461\n",
      "train: loss: 435288.40625 acc: 0.967818558216095  val: loss: 349813.90625 acc: 0.9793246984481812\n",
      "step: 8095 , time : 0.0\n",
      "train: loss: 625433.875 acc: 0.950371503829956  val: loss: 3288590.75 acc: 0.7369095683097839\n",
      "step: 8100 , time : 0.0010006427764892578\n",
      "train: loss: 309834.625 acc: 0.8802769184112549  val: loss: 2535704.5 acc: 0.8072575330734253\n",
      "step: 8105 , time : 0.0010008811950683594\n",
      "train: loss: 258358.375 acc: 0.9839825630187988  val: loss: 1501230.0 acc: 0.6249223351478577\n",
      "step: 8110 , time : 0.0\n",
      "train: loss: 1128770.875 acc: 0.9680721759796143  val: loss: 832812.75 acc: 0.5899142622947693\n",
      "step: 8115 , time : 0.0010006427764892578\n",
      "train: loss: 1171501.75 acc: 0.9628909230232239  val: loss: 1787315.0 acc: -0.038834571838378906\n",
      "step: 8120 , time : 0.0\n",
      "train: loss: 727333.1875 acc: 0.9677308201789856  val: loss: 645283.1875 acc: 0.8721387386322021\n",
      "step: 8125 , time : 0.001001119613647461\n",
      "train: loss: 426746.28125 acc: 0.9692249894142151  val: loss: 1092017.0 acc: 0.8587808609008789\n",
      "step: 8130 , time : 0.0010008811950683594\n",
      "train: loss: 782152.6875 acc: 0.9211488366127014  val: loss: 972026.375 acc: 0.7973619103431702\n",
      "step: 8135 , time : 0.0\n",
      "train: loss: 328100.09375 acc: 0.8966481685638428  val: loss: 969582.8125 acc: 0.7461825609207153\n",
      "step: 8140 , time : 0.0\n",
      "train: loss: 451920.90625 acc: 0.9463513493537903  val: loss: 964679.8125 acc: 0.8614859580993652\n",
      "step: 8145 , time : 0.0010008811950683594\n",
      "train: loss: 1542055.75 acc: 0.553612232208252  val: loss: 1519565.375 acc: 0.49220412969589233\n",
      "step: 8150 , time : 0.0010008811950683594\n",
      "train: loss: 399201.09375 acc: 0.9368103742599487  val: loss: 1009698.3125 acc: 0.8984764814376831\n",
      "step: 8155 , time : 0.0010006427764892578\n",
      "train: loss: 405554.8125 acc: 0.8541264533996582  val: loss: 870575.1875 acc: 0.7909442782402039\n",
      "step: 8160 , time : 0.0\n",
      "train: loss: 995299.8125 acc: 0.7490857839584351  val: loss: 511390.8125 acc: 0.9072825908660889\n",
      "step: 8165 , time : 0.0\n",
      "train: loss: 1606702.75 acc: 0.3515051007270813  val: loss: 670434.4375 acc: 0.44390857219696045\n",
      "step: 8170 , time : 0.0\n",
      "train: loss: 1443439.5 acc: 0.5730812549591064  val: loss: 1096953.625 acc: 0.34111273288726807\n",
      "step: 8175 , time : 0.0\n",
      "train: loss: 883766.1875 acc: 0.7048656940460205  val: loss: 1441388.75 acc: 0.3292510509490967\n",
      "step: 8180 , time : 0.0\n",
      "train: loss: 400973.78125 acc: 0.7054135203361511  val: loss: 1530771.625 acc: 0.6937086582183838\n",
      "step: 8185 , time : 0.0010006427764892578\n",
      "train: loss: 216005.234375 acc: 0.8085007667541504  val: loss: 913236.6875 acc: 0.7152612805366516\n",
      "step: 8190 , time : 0.0\n",
      "train: loss: 142468.25 acc: 0.8756039142608643  val: loss: 1567926.0 acc: 0.7894337177276611\n",
      "step: 8195 , time : 0.0\n",
      "train: loss: 181878.09375 acc: 0.868532121181488  val: loss: 993154.0 acc: 0.6962937116622925\n",
      "step: 8200 , time : 0.0010004043579101562\n",
      "train: loss: 65293.37109375 acc: 0.9470119476318359  val: loss: 643080.5 acc: 0.8009889125823975\n",
      "step: 8205 , time : 0.0\n",
      "train: loss: 421943.65625 acc: 0.790804386138916  val: loss: 349730.03125 acc: 0.7927848696708679\n",
      "step: 8210 , time : 0.0010013580322265625\n",
      "train: loss: 178125.671875 acc: 0.8772137761116028  val: loss: 1400830.375 acc: 0.7791752815246582\n",
      "step: 8215 , time : 0.0\n",
      "train: loss: 200328.328125 acc: 0.8851187229156494  val: loss: 1032262.5625 acc: 0.7954192757606506\n",
      "step: 8220 , time : 0.0\n",
      "train: loss: 87485.3828125 acc: 0.8864160776138306  val: loss: 1066479.375 acc: 0.8116162419319153\n",
      "step: 8225 , time : 0.0010006427764892578\n",
      "train: loss: 166172.359375 acc: 0.8250477910041809  val: loss: 1548311.625 acc: 0.7776778936386108\n",
      "step: 8230 , time : 0.0\n",
      "train: loss: 725428.5 acc: 0.6798528432846069  val: loss: 1193178.625 acc: 0.850801408290863\n",
      "step: 8235 , time : 0.0\n",
      "train: loss: 456536.96875 acc: 0.722710132598877  val: loss: 1267179.875 acc: 0.7856369018554688\n",
      "step: 8240 , time : 0.0010006427764892578\n",
      "train: loss: 669347.1875 acc: 0.22838258743286133  val: loss: 1167117.0 acc: 0.6674061417579651\n",
      "step: 8245 , time : 0.0\n",
      "train: loss: 922092.875 acc: 0.571545422077179  val: loss: 257300.265625 acc: 0.7466657757759094\n",
      "step: 8250 , time : 0.0010006427764892578\n",
      "train: loss: 882716.875 acc: 0.6461018323898315  val: loss: 1277065.25 acc: 0.75639408826828\n",
      "step: 8255 , time : 0.0\n",
      "train: loss: 1393240.0 acc: 0.7463399171829224  val: loss: 456245.59375 acc: 0.9082996249198914\n",
      "step: 8260 , time : 0.001001119613647461\n",
      "train: loss: 1414778.125 acc: 0.7502685189247131  val: loss: 886357.1875 acc: 0.7003885507583618\n",
      "step: 8265 , time : 0.0\n",
      "train: loss: 976141.4375 acc: 0.9289438724517822  val: loss: 593357.1875 acc: 0.8783910870552063\n",
      "step: 8270 , time : 0.0\n",
      "train: loss: 417575.46875 acc: 0.9647296071052551  val: loss: 1249846.875 acc: 0.3394467830657959\n",
      "step: 8275 , time : 0.0010006427764892578\n",
      "train: loss: 465771.84375 acc: 0.9167675375938416  val: loss: 482839.28125 acc: 0.899858832359314\n",
      "step: 8280 , time : 0.0010006427764892578\n",
      "train: loss: 460609.28125 acc: 0.9382915496826172  val: loss: 616496.875 acc: 0.7171217203140259\n",
      "step: 8285 , time : 0.0\n",
      "train: loss: 392176.96875 acc: 0.9651899337768555  val: loss: 560632.125 acc: 0.8097038269042969\n",
      "step: 8290 , time : 0.0\n",
      "train: loss: 306377.03125 acc: 0.978122889995575  val: loss: 822087.1875 acc: 0.7912291884422302\n",
      "step: 8295 , time : 0.0010004043579101562\n",
      "train: loss: 155932.28125 acc: 0.990254819393158  val: loss: 2958827.75 acc: 0.4226309061050415\n",
      "step: 8300 , time : 0.0010006427764892578\n",
      "train: loss: 320532.21875 acc: 0.9528974890708923  val: loss: 629060.5 acc: 0.8604044914245605\n",
      "step: 8305 , time : 0.0\n",
      "train: loss: 119601.375 acc: 0.8282480239868164  val: loss: 571593.3125 acc: 0.8594462871551514\n",
      "step: 8310 , time : 0.0\n",
      "train: loss: 37186.92578125 acc: 0.894891083240509  val: loss: 854482.1875 acc: 0.6649243831634521\n",
      "step: 8315 , time : 0.0010004043579101562\n",
      "train: loss: 141800.625 acc: 0.9083276987075806  val: loss: 969146.9375 acc: 0.8186296224594116\n",
      "step: 8320 , time : 0.0010006427764892578\n",
      "train: loss: 105986.7109375 acc: 0.9572698473930359  val: loss: 1030691.5625 acc: 0.5185547471046448\n",
      "step: 8325 , time : 0.0\n",
      "train: loss: 57401.0078125 acc: 0.9497631788253784  val: loss: 233788.84375 acc: 0.9394527673721313\n",
      "step: 8330 , time : 0.0\n",
      "train: loss: 16624.095703125 acc: 0.9724741578102112  val: loss: 253381.875 acc: 0.9218661189079285\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 8335 , time : 0.0010004043579101562\n",
      "train: loss: 20584.662109375 acc: 0.9519553184509277  val: loss: 230922.15625 acc: 0.9441312551498413\n",
      "step: 8340 , time : 0.0010008811950683594\n",
      "train: loss: 58688.546875 acc: 0.9379293322563171  val: loss: 297574.84375 acc: 0.8815826773643494\n",
      "step: 8345 , time : 0.0\n",
      "train: loss: 20902.76953125 acc: 0.958609938621521  val: loss: 302749.59375 acc: 0.8846011161804199\n",
      "step: 8350 , time : 0.001001119613647461\n",
      "train: loss: 13735.7724609375 acc: 0.9708674550056458  val: loss: 310172.65625 acc: 0.9634066224098206\n",
      "step: 8355 , time : 0.0\n",
      "train: loss: 65669.8671875 acc: 0.9500574469566345  val: loss: 335554.1875 acc: 0.9549282193183899\n",
      "step: 8360 , time : 0.0\n",
      "train: loss: 22540.1796875 acc: 0.9795436263084412  val: loss: 524811.8125 acc: 0.9518600702285767\n",
      "step: 8365 , time : 0.0010008811950683594\n",
      "train: loss: 131176.8125 acc: 0.9206777811050415  val: loss: 578515.5 acc: 0.9148038029670715\n",
      "step: 8370 , time : 0.0010006427764892578\n",
      "train: loss: 84410.1875 acc: 0.963337242603302  val: loss: 1117062.25 acc: 0.8626121878623962\n",
      "step: 8375 , time : 0.0\n",
      "train: loss: 50185.4140625 acc: 0.9620847105979919  val: loss: 747156.8125 acc: 0.9244012832641602\n",
      "step: 8380 , time : 0.0\n",
      "train: loss: 160587.671875 acc: 0.9272401928901672  val: loss: 534119.125 acc: 0.9509949684143066\n",
      "step: 8385 , time : 0.0\n",
      "train: loss: 17609.748046875 acc: 0.9788326621055603  val: loss: 1244070.0 acc: 0.8814648985862732\n",
      "step: 8390 , time : 0.0\n",
      "train: loss: 65245.8515625 acc: 0.95028156042099  val: loss: 667333.625 acc: 0.9173181653022766\n",
      "step: 8395 , time : 0.0\n",
      "train: loss: 96767.078125 acc: 0.9772740006446838  val: loss: 247745.765625 acc: 0.8512191772460938\n",
      "step: 8400 , time : 0.0\n",
      "train: loss: 113332.6953125 acc: 0.9782330989837646  val: loss: 1200008.375 acc: 0.9066303968429565\n",
      "step: 8405 , time : 0.0\n",
      "train: loss: 75968.6953125 acc: 0.9517800807952881  val: loss: 3817094.75 acc: 0.7106097936630249\n",
      "step: 8410 , time : 0.0\n",
      "train: loss: 86351.234375 acc: 0.9667208194732666  val: loss: 1942048.0 acc: 0.599768340587616\n",
      "step: 8415 , time : 0.0\n",
      "train: loss: 242847.828125 acc: 0.9080065488815308  val: loss: 1691327.875 acc: 0.31258946657180786\n",
      "step: 8420 , time : 0.0\n",
      "train: loss: 88294.328125 acc: 0.9607382416725159  val: loss: 1119893.75 acc: 0.9042121171951294\n",
      "step: 8425 , time : 0.0\n",
      "train: loss: 230866.734375 acc: 0.9377614259719849  val: loss: 352163.5625 acc: 0.9382269382476807\n",
      "step: 8430 , time : 0.0\n",
      "train: loss: 322177.0625 acc: 0.8896690607070923  val: loss: 1560334.625 acc: 0.4799877405166626\n",
      "step: 8435 , time : 0.0\n",
      "train: loss: 197574.03125 acc: 0.9647089242935181  val: loss: 1278871.25 acc: 0.6081298589706421\n",
      "step: 8440 , time : 0.0\n",
      "train: loss: 102365.4453125 acc: 0.9842795729637146  val: loss: 2363623.25 acc: 0.40658432245254517\n",
      "step: 8445 , time : 0.0\n",
      "train: loss: 304497.34375 acc: 0.9560608863830566  val: loss: 324264.84375 acc: 0.8522710800170898\n",
      "step: 8450 , time : 0.0\n",
      "train: loss: 238132.265625 acc: 0.9704488515853882  val: loss: 2424115.25 acc: 0.4004971981048584\n",
      "step: 8455 , time : 0.0\n",
      "train: loss: 217835.6875 acc: 0.9873969554901123  val: loss: 1260250.875 acc: 0.733208954334259\n",
      "step: 8460 , time : 0.0010006427764892578\n",
      "train: loss: 518555.21875 acc: 0.9501138925552368  val: loss: 2314195.5 acc: -0.23518037796020508\n",
      "step: 8465 , time : 0.0\n",
      "train: loss: 251434.359375 acc: 0.9161544442176819  val: loss: 3986986.25 acc: 0.20934921503067017\n",
      "step: 8470 , time : 0.0\n",
      "train: loss: 1797193.625 acc: 0.9103302359580994  val: loss: 717545.5625 acc: 0.9411417245864868\n",
      "step: 8475 , time : 0.0\n",
      "train: loss: 1074575.0 acc: 0.9748733043670654  val: loss: 1687238.0 acc: 0.5428153276443481\n",
      "step: 8480 , time : 0.0\n",
      "train: loss: 1911797.0 acc: 0.955565333366394  val: loss: 1106862.75 acc: 0.6853221654891968\n",
      "step: 8485 , time : 0.0\n",
      "train: loss: 633172.4375 acc: 0.9703102111816406  val: loss: 2310827.75 acc: 0.44701164960861206\n",
      "step: 8490 , time : 0.0\n",
      "train: loss: 143396.5625 acc: 0.9777846336364746  val: loss: 1340283.5 acc: 0.80279141664505\n",
      "step: 8495 , time : 0.0\n",
      "train: loss: 194043.4375 acc: 0.985316812992096  val: loss: 1505079.5 acc: 0.7241519689559937\n",
      "step: 8500 , time : 0.0\n",
      "train: loss: 528263.9375 acc: 0.9412614703178406  val: loss: 2688547.0 acc: 0.41456079483032227\n",
      "step: 8505 , time : 0.0\n",
      "train: loss: 402011.8125 acc: 0.9444200396537781  val: loss: 610660.25 acc: 0.8615255951881409\n",
      "step: 8510 , time : 0.0\n",
      "train: loss: 1457601.25 acc: 0.3960341215133667  val: loss: 1433995.0 acc: 0.8282167911529541\n",
      "step: 8515 , time : 0.0\n",
      "train: loss: 1831622.625 acc: 0.5376167297363281  val: loss: 2073951.25 acc: 0.8275992274284363\n",
      "step: 8520 , time : 0.0\n",
      "train: loss: 1758272.625 acc: 0.22794002294540405  val: loss: 570214.4375 acc: 0.8703383803367615\n",
      "step: 8525 , time : 0.0\n",
      "train: loss: 475498.71875 acc: 0.8584752082824707  val: loss: 959523.8125 acc: 0.8034368753433228\n",
      "step: 8530 , time : 0.0\n",
      "train: loss: 1762548.375 acc: 0.38572758436203003  val: loss: 535671.0625 acc: 0.8879019618034363\n",
      "step: 8535 , time : 0.0\n",
      "train: loss: 1669117.75 acc: 0.39672183990478516  val: loss: 2235840.75 acc: 0.4623097777366638\n",
      "step: 8540 , time : 0.0\n",
      "train: loss: 686098.3125 acc: 0.5167218446731567  val: loss: 3780675.25 acc: 0.44701194763183594\n",
      "step: 8545 , time : 0.0\n",
      "train: loss: 447375.84375 acc: 0.6380476951599121  val: loss: 1565636.75 acc: 0.6626408696174622\n",
      "step: 8550 , time : 0.0\n",
      "train: loss: 662686.375 acc: 0.5434075593948364  val: loss: 1007377.5625 acc: 0.7134333848953247\n",
      "step: 8555 , time : 0.0\n",
      "train: loss: 240302.421875 acc: 0.7966234683990479  val: loss: 150868.125 acc: 0.9558311700820923\n",
      "step: 8560 , time : 0.015625715255737305\n",
      "train: loss: 164296.90625 acc: 0.8775663375854492  val: loss: 869617.4375 acc: 0.7130053043365479\n",
      "step: 8565 , time : 0.0010008811950683594\n",
      "train: loss: 421400.4375 acc: 0.674096941947937  val: loss: 1924489.25 acc: 0.7497351169586182\n",
      "step: 8570 , time : 0.0010008811950683594\n",
      "train: loss: 121736.9765625 acc: 0.8943237066268921  val: loss: 536788.375 acc: 0.8351674675941467\n",
      "step: 8575 , time : 0.0\n",
      "train: loss: 153614.328125 acc: 0.8578227162361145  val: loss: 816791.9375 acc: 0.6099897623062134\n",
      "step: 8580 , time : 0.0\n",
      "train: loss: 285795.15625 acc: 0.7473956942558289  val: loss: 361984.1875 acc: 0.9020548462867737\n",
      "step: 8585 , time : 0.0\n",
      "train: loss: 91730.421875 acc: 0.907829999923706  val: loss: 252856.40625 acc: 0.8224501609802246\n",
      "step: 8590 , time : 0.0\n",
      "train: loss: 146921.296875 acc: 0.8558062314987183  val: loss: 358842.09375 acc: 0.8086600303649902\n",
      "step: 8595 , time : 0.0010008811950683594\n",
      "train: loss: 600507.375 acc: 0.6986445784568787  val: loss: 1438320.375 acc: 0.7746274471282959\n",
      "step: 8600 , time : 0.0\n",
      "train: loss: 642316.5 acc: 0.728053092956543  val: loss: 1181444.375 acc: 0.8593453168869019\n",
      "step: 8605 , time : 0.0\n",
      "train: loss: 1358871.25 acc: 0.3108854293823242  val: loss: 877442.375 acc: 0.6678425669670105\n",
      "step: 8610 , time : 0.0\n",
      "train: loss: 304579.78125 acc: 0.7764126062393188  val: loss: 981841.875 acc: 0.740127444267273\n",
      "step: 8615 , time : 0.0\n",
      "train: loss: 943566.5 acc: 0.682270884513855  val: loss: 512296.59375 acc: 0.7253537774085999\n",
      "step: 8620 , time : 0.0\n",
      "train: loss: 2038292.0 acc: 0.7554458379745483  val: loss: 513423.0 acc: 0.7944310307502747\n",
      "step: 8625 , time : 0.0\n",
      "train: loss: 1103376.25 acc: 0.8463537693023682  val: loss: 1226396.875 acc: 0.7716907262802124\n",
      "step: 8630 , time : 0.0\n",
      "train: loss: 969265.375 acc: 0.906919538974762  val: loss: 1357011.375 acc: 0.562853217124939\n",
      "step: 8635 , time : 0.0\n",
      "train: loss: 623722.125 acc: 0.9180529117584229  val: loss: 1615851.625 acc: 0.5029067397117615\n",
      "step: 8640 , time : 0.0\n",
      "train: loss: 626685.5 acc: 0.9051588773727417  val: loss: 958354.875 acc: 0.8408592939376831\n",
      "step: 8645 , time : 0.0\n",
      "train: loss: 216204.6875 acc: 0.9630893468856812  val: loss: 610675.4375 acc: 0.8992430567741394\n",
      "step: 8650 , time : 0.0\n",
      "train: loss: 693131.6875 acc: 0.9196760058403015  val: loss: 435860.03125 acc: 0.7845090627670288\n",
      "step: 8655 , time : 0.0\n",
      "train: loss: 248226.8125 acc: 0.979790210723877  val: loss: 802711.875 acc: 0.9122971892356873\n",
      "step: 8660 , time : 0.0\n",
      "train: loss: 247397.96875 acc: 0.9836326241493225  val: loss: 1051233.5 acc: 0.8343961834907532\n",
      "step: 8665 , time : 0.0\n",
      "train: loss: 158129.609375 acc: 0.9877959489822388  val: loss: 1069740.875 acc: 0.8811940550804138\n",
      "step: 8670 , time : 0.0\n",
      "train: loss: 272014.46875 acc: 0.9216182827949524  val: loss: 653885.5625 acc: 0.9469634294509888\n",
      "step: 8675 , time : 0.0\n",
      "train: loss: 143394.9375 acc: 0.9621830582618713  val: loss: 155110.125 acc: 0.9664455652236938\n",
      "step: 8680 , time : 0.0\n",
      "train: loss: 43738.60546875 acc: 0.976382851600647  val: loss: 700917.4375 acc: 0.9341985583305359\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 8685 , time : 0.0\n",
      "train: loss: 102504.8359375 acc: 0.9714462757110596  val: loss: 683935.9375 acc: 0.9308543801307678\n",
      "step: 8690 , time : 0.0010006427764892578\n",
      "train: loss: 17797.99609375 acc: 0.9896447062492371  val: loss: 492602.4375 acc: 0.9075183868408203\n",
      "step: 8695 , time : 0.0\n",
      "train: loss: 25059.521484375 acc: 0.9477232694625854  val: loss: 1309950.25 acc: 0.8790563344955444\n",
      "step: 8700 , time : 0.0010008811950683594\n",
      "train: loss: 101276.9453125 acc: 0.9293975830078125  val: loss: 392335.59375 acc: 0.4360266327857971\n",
      "step: 8705 , time : 0.0\n",
      "train: loss: 27668.138671875 acc: 0.9200971126556396  val: loss: 1143377.625 acc: 0.8687160611152649\n",
      "step: 8710 , time : 0.0\n",
      "train: loss: 12839.2109375 acc: 0.9538611769676208  val: loss: 840725.4375 acc: 0.8155308365821838\n",
      "step: 8715 , time : 0.0010008811950683594\n",
      "train: loss: 12685.8369140625 acc: 0.9799699783325195  val: loss: 2519068.5 acc: 0.7799068689346313\n",
      "step: 8720 , time : 0.0010006427764892578\n",
      "train: loss: 11649.1484375 acc: 0.9700408577919006  val: loss: 740403.625 acc: 0.8149566650390625\n",
      "step: 8725 , time : 0.0\n",
      "train: loss: 87060.7421875 acc: 0.9621326327323914  val: loss: 2247145.5 acc: 0.7856628894805908\n",
      "step: 8730 , time : 0.0\n",
      "train: loss: 97164.078125 acc: 0.9364144802093506  val: loss: 579129.1875 acc: 0.9229359030723572\n",
      "step: 8735 , time : 0.0\n",
      "train: loss: 34195.80859375 acc: 0.9732100367546082  val: loss: 834023.375 acc: 0.6962997317314148\n",
      "step: 8740 , time : 0.0\n",
      "train: loss: 37228.76953125 acc: 0.9683417677879333  val: loss: 1993710.125 acc: 0.17025136947631836\n",
      "step: 8745 , time : 0.0\n",
      "train: loss: 33293.91796875 acc: 0.974533200263977  val: loss: 231267.078125 acc: 0.9501646161079407\n",
      "step: 8750 , time : 0.0\n",
      "train: loss: 11200.4931640625 acc: 0.9605797529220581  val: loss: 1130499.375 acc: 0.5304921865463257\n",
      "step: 8755 , time : 0.0\n",
      "train: loss: 56734.7734375 acc: 0.9798359274864197  val: loss: 1573960.0 acc: 0.5216925740242004\n",
      "step: 8760 , time : 0.0\n",
      "train: loss: 132228.09375 acc: 0.964849591255188  val: loss: 656041.4375 acc: 0.8363018035888672\n",
      "step: 8765 , time : 0.0\n",
      "train: loss: 107544.546875 acc: 0.9770374298095703  val: loss: 895956.9375 acc: 0.8052073121070862\n",
      "step: 8770 , time : 0.0\n",
      "train: loss: 59819.84765625 acc: 0.9819408059120178  val: loss: 1504334.125 acc: 0.05286300182342529\n",
      "step: 8775 , time : 0.0\n",
      "train: loss: 70132.5390625 acc: 0.9687435030937195  val: loss: 2064547.875 acc: -0.28145742416381836\n",
      "step: 8780 , time : 0.0\n",
      "train: loss: 239527.96875 acc: 0.9341468214988708  val: loss: 805492.3125 acc: 0.892301082611084\n",
      "step: 8785 , time : 0.0\n",
      "train: loss: 319787.75 acc: 0.9203159809112549  val: loss: 2710412.25 acc: 0.59676593542099\n",
      "step: 8790 , time : 0.0\n",
      "train: loss: 74068.1328125 acc: 0.934945821762085  val: loss: 1469261.5 acc: 0.859771192073822\n",
      "step: 8795 , time : 0.0\n",
      "train: loss: 137806.796875 acc: 0.9709917306900024  val: loss: 1059643.875 acc: 0.7937132120132446\n",
      "step: 8800 , time : 0.0\n",
      "train: loss: 561956.3125 acc: 0.9405050873756409  val: loss: 1471743.625 acc: 0.8051817417144775\n",
      "step: 8805 , time : 0.0\n",
      "train: loss: 110590.9375 acc: 0.9861341118812561  val: loss: 1793747.75 acc: 0.11294388771057129\n",
      "step: 8810 , time : 0.0\n",
      "train: loss: 103598.3359375 acc: 0.9871416091918945  val: loss: 4177257.75 acc: 0.5108113884925842\n",
      "step: 8815 , time : 0.0\n",
      "train: loss: 188050.78125 acc: 0.9778216481208801  val: loss: 1850487.875 acc: 0.7375735640525818\n",
      "step: 8820 , time : 0.0010008811950683594\n",
      "train: loss: 743977.8125 acc: 0.9659436941146851  val: loss: 3476299.5 acc: 0.21758878231048584\n",
      "step: 8825 , time : 0.001001119613647461\n",
      "train: loss: 755873.5625 acc: 0.9264416694641113  val: loss: 1220282.125 acc: 0.8491146564483643\n",
      "step: 8830 , time : 0.0\n",
      "train: loss: 213285.59375 acc: 0.9621472954750061  val: loss: 1786476.875 acc: 0.619786262512207\n",
      "step: 8835 , time : 0.0\n",
      "train: loss: 617113.875 acc: 0.9531627297401428  val: loss: 1011973.625 acc: 0.8240395188331604\n",
      "step: 8840 , time : 0.0\n",
      "train: loss: 909810.9375 acc: 0.9471654295921326  val: loss: 555302.0625 acc: 0.8920940160751343\n",
      "step: 8845 , time : 0.0\n",
      "train: loss: 968118.0 acc: 0.9656887054443359  val: loss: 768644.125 acc: 0.8074804544448853\n",
      "step: 8850 , time : 0.0\n",
      "train: loss: 407614.21875 acc: 0.9802781343460083  val: loss: 435444.28125 acc: 0.9344431757926941\n",
      "step: 8855 , time : 0.0\n",
      "train: loss: 417877.6875 acc: 0.9733762145042419  val: loss: 1068150.375 acc: 0.5696065425872803\n",
      "step: 8860 , time : 0.0\n",
      "train: loss: 420366.09375 acc: 0.933551013469696  val: loss: 274721.875 acc: 0.9533711075782776\n",
      "step: 8865 , time : 0.0\n",
      "train: loss: 340038.375 acc: 0.9626638889312744  val: loss: 1052143.25 acc: 0.8845793008804321\n",
      "step: 8870 , time : 0.0\n",
      "train: loss: 425517.8125 acc: 0.9126086831092834  val: loss: 847081.4375 acc: 0.759415864944458\n",
      "step: 8875 , time : 0.0\n",
      "train: loss: 1245268.875 acc: 0.7571839690208435  val: loss: 954107.9375 acc: 0.8401899337768555\n",
      "step: 8880 , time : 0.0\n",
      "train: loss: 702304.1875 acc: 0.6907899975776672  val: loss: 1225863.5 acc: 0.8320255279541016\n",
      "step: 8885 , time : 0.0\n",
      "train: loss: 623483.625 acc: 0.7881567478179932  val: loss: 814610.125 acc: 0.7966476082801819\n",
      "step: 8890 , time : 0.0\n",
      "train: loss: 1259654.75 acc: 0.6897666454315186  val: loss: 500609.125 acc: 0.8902361392974854\n",
      "step: 8895 , time : 0.0\n",
      "train: loss: 911133.0 acc: 0.4098578691482544  val: loss: 1015058.8125 acc: 0.7135396599769592\n",
      "step: 8900 , time : 0.0\n",
      "train: loss: 1227930.75 acc: 0.6837551593780518  val: loss: 1675216.375 acc: 0.3168255686759949\n",
      "step: 8905 , time : 0.0\n",
      "train: loss: 1695898.0 acc: 0.471618115901947  val: loss: 1969148.375 acc: 0.3339366912841797\n",
      "step: 8910 , time : 0.0\n",
      "train: loss: 648553.875 acc: 0.47409313917160034  val: loss: 923135.625 acc: 0.6832064390182495\n",
      "step: 8915 , time : 0.0\n",
      "train: loss: 443708.75 acc: 0.6762986183166504  val: loss: 843805.3125 acc: 0.6640737056732178\n",
      "step: 8920 , time : 0.0\n",
      "train: loss: 223226.375 acc: 0.8162366151809692  val: loss: 1079547.0 acc: 0.7898913621902466\n",
      "step: 8925 , time : 0.0\n",
      "train: loss: 295159.8125 acc: 0.8560159206390381  val: loss: 337698.28125 acc: 0.8183460831642151\n",
      "step: 8930 , time : 0.0010008811950683594\n",
      "train: loss: 125999.1171875 acc: 0.8933473825454712  val: loss: 1412006.5 acc: 0.7608486413955688\n",
      "step: 8935 , time : 0.0\n",
      "train: loss: 128091.78125 acc: 0.9053499698638916  val: loss: 520777.09375 acc: 0.7946580052375793\n",
      "step: 8940 , time : 0.0\n",
      "train: loss: 264603.09375 acc: 0.8581185340881348  val: loss: 822088.25 acc: 0.7731379270553589\n",
      "step: 8945 , time : 0.0\n",
      "train: loss: 289789.78125 acc: 0.8202279806137085  val: loss: 1247243.75 acc: 0.6600700616836548\n",
      "step: 8950 , time : 0.0\n",
      "train: loss: 81084.734375 acc: 0.8831533193588257  val: loss: 1127460.125 acc: 0.7955889701843262\n",
      "step: 8955 , time : 0.0\n",
      "train: loss: 229844.09375 acc: 0.8550727963447571  val: loss: 645364.1875 acc: 0.8683410286903381\n",
      "step: 8960 , time : 0.0\n",
      "train: loss: 1421997.5 acc: 0.2884862422943115  val: loss: 986462.8125 acc: 0.7686285376548767\n",
      "step: 8965 , time : 0.015625953674316406\n",
      "train: loss: 365297.40625 acc: 0.6824548244476318  val: loss: 1138336.25 acc: 0.8643981218338013\n",
      "step: 8970 , time : 0.0\n",
      "train: loss: 738786.375 acc: 0.6065446138381958  val: loss: 1037393.0625 acc: 0.757832407951355\n",
      "step: 8975 , time : 0.015625476837158203\n",
      "train: loss: 244641.53125 acc: 0.8254369497299194  val: loss: 1114008.25 acc: 0.8254656195640564\n",
      "step: 8980 , time : 0.0010004043579101562\n",
      "train: loss: 816103.75 acc: 0.6880708932876587  val: loss: 856871.0 acc: 0.711180567741394\n",
      "step: 8985 , time : 0.0\n",
      "train: loss: 1222410.5 acc: 0.8363214731216431  val: loss: 1271083.875 acc: 0.8463295698165894\n",
      "step: 8990 , time : 0.0010006427764892578\n",
      "train: loss: 1522049.375 acc: 0.7256232500076294  val: loss: 1182517.0 acc: 0.44182807207107544\n",
      "step: 8995 , time : 0.0010004043579101562\n",
      "train: loss: 561612.125 acc: 0.9500519633293152  val: loss: 880774.5 acc: 0.88160640001297\n",
      "step: 9000 , time : 0.0\n",
      "train: loss: 575694.0625 acc: 0.9594683647155762  val: loss: 580831.8125 acc: 0.9113256335258484\n",
      "step: 9005 , time : 0.0\n",
      "train: loss: 840348.1875 acc: 0.8190592527389526  val: loss: 724986.5625 acc: 0.8225077986717224\n",
      "step: 9010 , time : 0.0010013580322265625\n",
      "train: loss: 499137.78125 acc: 0.930529773235321  val: loss: 845459.875 acc: 0.861106812953949\n",
      "step: 9015 , time : 0.0010004043579101562\n",
      "train: loss: 475195.25 acc: 0.9525091052055359  val: loss: 957055.4375 acc: 0.9254409670829773\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 9020 , time : 0.0010008811950683594\n",
      "train: loss: 155598.5625 acc: 0.989680290222168  val: loss: 1599069.0 acc: 0.8173258304595947\n",
      "step: 9025 , time : 0.0\n",
      "train: loss: 290423.03125 acc: 0.9782903790473938  val: loss: 1283415.75 acc: 0.6766240000724792\n",
      "step: 9030 , time : 0.0\n",
      "train: loss: 283041.09375 acc: 0.9705459475517273  val: loss: 1082273.25 acc: 0.7782579064369202\n",
      "step: 9035 , time : 0.0\n",
      "train: loss: 313483.65625 acc: 0.9586634635925293  val: loss: 597134.5625 acc: 0.9194602966308594\n",
      "step: 9040 , time : 0.0\n",
      "train: loss: 128523.3203125 acc: 0.9183604121208191  val: loss: 1160291.0 acc: 0.661431610584259\n",
      "step: 9045 , time : 0.0010008811950683594\n",
      "train: loss: 25814.689453125 acc: 0.9810757040977478  val: loss: 1003125.125 acc: 0.8320831060409546\n",
      "step: 9050 , time : 0.0\n",
      "train: loss: 108626.8125 acc: 0.9228909015655518  val: loss: 938389.125 acc: 0.6856565475463867\n",
      "step: 9055 , time : 0.0\n",
      "train: loss: 78410.3359375 acc: 0.9426980018615723  val: loss: 478681.28125 acc: 0.9539512395858765\n",
      "step: 9060 , time : 0.0\n",
      "train: loss: 11614.3154296875 acc: 0.9848969578742981  val: loss: 1259718.75 acc: 0.6735273599624634\n",
      "step: 9065 , time : 0.0\n",
      "train: loss: 22170.935546875 acc: 0.9601157903671265  val: loss: 1463782.25 acc: 0.4547332525253296\n",
      "step: 9070 , time : 0.0\n",
      "train: loss: 78502.953125 acc: 0.909764289855957  val: loss: 1769875.375 acc: 0.7507507801055908\n",
      "step: 9075 , time : 0.0\n",
      "train: loss: 11134.1474609375 acc: 0.9920032620429993  val: loss: 1928940.25 acc: 0.7357327938079834\n",
      "step: 9080 , time : 0.0\n",
      "train: loss: 24587.220703125 acc: 0.959943413734436  val: loss: 1524878.75 acc: 0.5218315720558167\n",
      "step: 9085 , time : 0.0\n",
      "train: loss: 24474.8125 acc: 0.9632917046546936  val: loss: 1882891.625 acc: 0.7506176233291626\n",
      "step: 9090 , time : 0.0\n",
      "train: loss: 82090.921875 acc: 0.9389196634292603  val: loss: 1746068.375 acc: 0.8380203247070312\n",
      "step: 9095 , time : 0.0\n",
      "train: loss: 150773.125 acc: 0.9448809623718262  val: loss: 2030734.25 acc: 0.4463437795639038\n",
      "step: 9100 , time : 0.0\n",
      "train: loss: 78919.0 acc: 0.9281617403030396  val: loss: 880770.1875 acc: 0.5348849296569824\n",
      "step: 9105 , time : 0.0\n",
      "train: loss: 24375.416015625 acc: 0.9829131364822388  val: loss: 1423597.5 acc: 0.6830518245697021\n",
      "step: 9110 , time : 0.0\n",
      "train: loss: 56749.93359375 acc: 0.9341607689857483  val: loss: 1234571.0 acc: 0.6088016033172607\n",
      "step: 9115 , time : 0.0\n",
      "train: loss: 16361.09765625 acc: 0.9814286231994629  val: loss: 1029039.4375 acc: 0.44399088621139526\n",
      "step: 9120 , time : 0.0\n",
      "train: loss: 61858.34375 acc: 0.9682539105415344  val: loss: 844671.125 acc: 0.8647210597991943\n",
      "step: 9125 , time : 0.0\n",
      "train: loss: 63586.38671875 acc: 0.9788705706596375  val: loss: 646302.3125 acc: 0.9232618808746338\n",
      "step: 9130 , time : 0.0\n",
      "train: loss: 54864.32421875 acc: 0.984132707118988  val: loss: 701695.6875 acc: 0.8522520065307617\n",
      "step: 9135 , time : 0.0\n",
      "train: loss: 92628.0625 acc: 0.9813980460166931  val: loss: 1269384.25 acc: 0.85694420337677\n",
      "step: 9140 , time : 0.0\n",
      "train: loss: 92494.8828125 acc: 0.9661585092544556  val: loss: 991791.1875 acc: 0.9070444703102112\n",
      "step: 9145 , time : 0.0\n",
      "train: loss: 211198.578125 acc: 0.8766111731529236  val: loss: 1908600.125 acc: 0.2550773620605469\n",
      "step: 9150 , time : 0.0010004043579101562\n",
      "train: loss: 91873.3828125 acc: 0.965584933757782  val: loss: 860057.0625 acc: 0.8139176964759827\n",
      "step: 9155 , time : 0.0010008811950683594\n",
      "train: loss: 257285.109375 acc: 0.9114686250686646  val: loss: 2410878.25 acc: 0.38534975051879883\n",
      "step: 9160 , time : 0.0\n",
      "train: loss: 138629.734375 acc: 0.9495337009429932  val: loss: 1065002.25 acc: 0.6968216896057129\n",
      "step: 9165 , time : 0.0\n",
      "train: loss: 114810.6953125 acc: 0.9855346083641052  val: loss: 582698.75 acc: 0.9360548853874207\n",
      "step: 9170 , time : 0.0\n",
      "train: loss: 116972.0390625 acc: 0.9871159195899963  val: loss: 113945.0859375 acc: 0.9668189287185669\n",
      "step: 9175 , time : 0.0\n",
      "train: loss: 99081.0234375 acc: 0.9825873970985413  val: loss: 620201.5625 acc: 0.7891692519187927\n",
      "step: 9180 , time : 0.0\n",
      "train: loss: 50912.83984375 acc: 0.9848295450210571  val: loss: 3916738.75 acc: 0.3649495244026184\n",
      "step: 9185 , time : 0.0\n",
      "train: loss: 405405.8125 acc: 0.97898930311203  val: loss: 460945.6875 acc: 0.8359781503677368\n",
      "step: 9190 , time : 0.0\n",
      "train: loss: 323380.75 acc: 0.9823449850082397  val: loss: 674495.9375 acc: 0.8546199798583984\n",
      "step: 9195 , time : 0.0\n",
      "train: loss: 131644.015625 acc: 0.9633921384811401  val: loss: 1025528.8125 acc: 0.6483907699584961\n",
      "step: 9200 , time : 0.0\n",
      "train: loss: 616853.0 acc: 0.9202006459236145  val: loss: 923594.8125 acc: 0.85854172706604\n",
      "step: 9205 , time : 0.0\n",
      "train: loss: 589486.5625 acc: 0.9656583070755005  val: loss: 2848223.25 acc: 0.5494703054428101\n",
      "step: 9210 , time : 0.0\n",
      "train: loss: 1092703.25 acc: 0.9528943300247192  val: loss: 683846.5 acc: 0.6636635065078735\n",
      "step: 9215 , time : 0.0\n",
      "train: loss: 2757233.25 acc: 0.8941426277160645  val: loss: 1116036.625 acc: 0.8418745398521423\n",
      "step: 9220 , time : 0.0\n",
      "train: loss: 470786.125 acc: 0.955089271068573  val: loss: 1808773.25 acc: 0.33795952796936035\n",
      "step: 9225 , time : 0.0010006427764892578\n",
      "train: loss: 603363.25 acc: 0.9573294520378113  val: loss: 395377.4375 acc: 0.9007941484451294\n",
      "step: 9230 , time : 0.0\n",
      "train: loss: 556043.8125 acc: 0.9537501335144043  val: loss: 220240.640625 acc: 0.9304165840148926\n",
      "step: 9235 , time : 0.0\n",
      "train: loss: 180183.75 acc: 0.9691942930221558  val: loss: 330554.875 acc: 0.9370643496513367\n",
      "step: 9240 , time : 0.0\n",
      "train: loss: 2295417.75 acc: 0.24419689178466797  val: loss: 957036.4375 acc: 0.7327636480331421\n",
      "step: 9245 , time : 0.0\n",
      "train: loss: 591698.625 acc: 0.8110653162002563  val: loss: 1088176.25 acc: 0.7047548890113831\n",
      "step: 9250 , time : 0.0\n",
      "train: loss: 957917.375 acc: 0.29856711626052856  val: loss: 1230973.375 acc: 0.7806086540222168\n",
      "step: 9255 , time : 0.0\n",
      "train: loss: 768704.3125 acc: 0.7222018837928772  val: loss: 521946.46875 acc: 0.9119983911514282\n",
      "step: 9260 , time : 0.001001119613647461\n",
      "train: loss: 740904.875 acc: 0.6958178281784058  val: loss: 810942.0625 acc: 0.8236390352249146\n",
      "step: 9265 , time : 0.001001119613647461\n",
      "train: loss: 2222202.25 acc: -0.06119048595428467  val: loss: 3268186.75 acc: 0.6720296144485474\n",
      "step: 9270 , time : 0.0\n",
      "train: loss: 1336284.25 acc: 0.38004863262176514  val: loss: 3363397.25 acc: 0.40781456232070923\n",
      "step: 9275 , time : 0.0\n",
      "train: loss: 926661.0 acc: 0.5440645217895508  val: loss: 1984154.875 acc: 0.6338455677032471\n",
      "step: 9280 , time : 0.0\n",
      "train: loss: 1326319.625 acc: 0.46141064167022705  val: loss: 1239607.625 acc: 0.7808033227920532\n",
      "step: 9285 , time : 0.0\n",
      "train: loss: 1024605.375 acc: 0.7231425046920776  val: loss: 438159.90625 acc: 0.9008211493492126\n",
      "step: 9290 , time : 0.0010008811950683594\n",
      "train: loss: 305013.625 acc: 0.7489791512489319  val: loss: 1259278.125 acc: 0.7260637283325195\n",
      "step: 9295 , time : 0.0010013580322265625\n",
      "train: loss: 533484.5 acc: 0.6854868531227112  val: loss: 1069380.5 acc: 0.7107674479484558\n",
      "step: 9300 , time : 0.0\n",
      "train: loss: 237081.96875 acc: 0.8561242818832397  val: loss: 2354387.5 acc: 0.8444914817810059\n",
      "step: 9305 , time : 0.0\n",
      "train: loss: 198582.515625 acc: 0.8206252455711365  val: loss: 1819976.125 acc: 0.7233322858810425\n",
      "step: 9310 , time : 0.0\n",
      "train: loss: 84169.671875 acc: 0.9072355031967163  val: loss: 587004.0 acc: 0.8107637166976929\n",
      "step: 9315 , time : 0.0\n",
      "train: loss: 334407.625 acc: 0.6276964545249939  val: loss: 509516.5 acc: 0.9526516795158386\n",
      "step: 9320 , time : 0.0\n",
      "train: loss: 152208.0625 acc: 0.9014264345169067  val: loss: 1033200.75 acc: 0.8606811761856079\n",
      "step: 9325 , time : 0.0\n",
      "train: loss: 766785.5 acc: 0.7924792766571045  val: loss: 614884.125 acc: 0.8514783382415771\n",
      "step: 9330 , time : 0.0\n",
      "train: loss: 1022480.5 acc: 0.5760418772697449  val: loss: 2059985.25 acc: 0.7728633880615234\n",
      "step: 9335 , time : 0.0\n",
      "train: loss: 1162096.75 acc: 0.6533977389335632  val: loss: 1787509.875 acc: 0.8404508829116821\n",
      "step: 9340 , time : 0.0\n",
      "train: loss: 484002.0625 acc: 0.6804996728897095  val: loss: 616400.875 acc: 0.7124305963516235\n",
      "step: 9345 , time : 0.0\n",
      "train: loss: 516230.90625 acc: 0.7129743099212646  val: loss: 1753184.0 acc: 0.7684957981109619\n",
      "step: 9350 , time : 0.00099945068359375\n",
      "train: loss: 1667221.25 acc: 0.6883400678634644  val: loss: 1071019.0 acc: 0.836540699005127\n",
      "step: 9355 , time : 0.0\n",
      "train: loss: 1462363.25 acc: 0.7729620933532715  val: loss: 1302453.0 acc: 0.8333278894424438\n",
      "step: 9360 , time : 0.0\n",
      "train: loss: 1090307.0 acc: 0.897508978843689  val: loss: 798147.0 acc: 0.8222616314888\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 9365 , time : 0.0\n",
      "train: loss: 693102.625 acc: 0.9436124563217163  val: loss: 2053562.125 acc: 0.6989623308181763\n",
      "step: 9370 , time : 0.0010004043579101562\n",
      "train: loss: 352921.09375 acc: 0.9508473873138428  val: loss: 1068874.125 acc: 0.6954725980758667\n",
      "step: 9375 , time : 0.0\n",
      "train: loss: 455645.125 acc: 0.90506911277771  val: loss: 1558093.75 acc: 0.3305548429489136\n",
      "step: 9380 , time : 0.0\n",
      "train: loss: 410852.25 acc: 0.9587640166282654  val: loss: 1609110.25 acc: 0.6600844860076904\n",
      "step: 9385 , time : 0.0\n",
      "train: loss: 326955.1875 acc: 0.9645796418190002  val: loss: 1214727.625 acc: 0.8592419624328613\n",
      "step: 9390 , time : 0.0\n",
      "train: loss: 261657.453125 acc: 0.9818901419639587  val: loss: 2007577.625 acc: 0.2064024806022644\n",
      "step: 9395 , time : 0.0\n",
      "train: loss: 337326.40625 acc: 0.9660274982452393  val: loss: 2601815.25 acc: -0.7010446786880493\n",
      "step: 9400 , time : 0.0\n",
      "train: loss: 197886.78125 acc: 0.953866183757782  val: loss: 470596.71875 acc: 0.8213955760002136\n",
      "step: 9405 , time : 0.0\n",
      "train: loss: 86155.40625 acc: 0.9802839159965515  val: loss: 408899.90625 acc: 0.9454616904258728\n",
      "step: 9410 , time : 0.0\n",
      "train: loss: 89211.59375 acc: 0.9754776954650879  val: loss: 1538294.75 acc: 0.5975935459136963\n",
      "step: 9415 , time : 0.0\n",
      "train: loss: 17265.263671875 acc: 0.9938974976539612  val: loss: 1012587.25 acc: 0.8193920850753784\n",
      "step: 9420 , time : 0.0\n",
      "train: loss: 47279.421875 acc: 0.9807481169700623  val: loss: 1946948.625 acc: 0.1816083788871765\n",
      "step: 9425 , time : 0.0\n",
      "train: loss: 25572.3828125 acc: 0.9680442810058594  val: loss: 974643.0 acc: 0.7476422786712646\n",
      "step: 9430 , time : 0.0\n",
      "train: loss: 26456.6171875 acc: 0.9613643288612366  val: loss: 172426.75 acc: 0.9616833925247192\n",
      "step: 9435 , time : 0.0\n",
      "train: loss: 22911.103515625 acc: 0.963963508605957  val: loss: 434857.46875 acc: 0.9462523460388184\n",
      "step: 9440 , time : 0.0\n",
      "train: loss: 11278.302734375 acc: 0.9757194519042969  val: loss: 1875278.875 acc: 0.2995836138725281\n",
      "step: 9445 , time : 0.0\n",
      "train: loss: 15871.4521484375 acc: 0.8983001708984375  val: loss: 525901.375 acc: 0.9208777546882629\n",
      "step: 9450 , time : 0.0\n",
      "train: loss: 11311.38671875 acc: 0.9802725315093994  val: loss: 1148687.0 acc: 0.8504073023796082\n",
      "step: 9455 , time : 0.0\n",
      "train: loss: 91819.265625 acc: 0.9419470429420471  val: loss: 761938.25 acc: 0.9460386037826538\n",
      "step: 9460 , time : 0.0\n",
      "train: loss: 121167.9921875 acc: 0.9540262818336487  val: loss: 2061966.375 acc: 0.38889437913894653\n",
      "step: 9465 , time : 0.0\n",
      "train: loss: 104389.140625 acc: 0.9386278986930847  val: loss: 408542.5 acc: 0.9399831891059875\n",
      "step: 9470 , time : 0.0\n",
      "train: loss: 38212.89453125 acc: 0.9669280648231506  val: loss: 1389323.75 acc: 0.6325021982192993\n",
      "step: 9475 , time : 0.0\n",
      "train: loss: 77718.8828125 acc: 0.9508522152900696  val: loss: 373234.78125 acc: 0.8754772543907166\n",
      "step: 9480 , time : 0.0\n",
      "train: loss: 19821.85546875 acc: 0.9726459980010986  val: loss: 1172239.0 acc: 0.6596795320510864\n",
      "step: 9485 , time : 0.0010006427764892578\n",
      "train: loss: 32405.30859375 acc: 0.9735666513442993  val: loss: 1352695.75 acc: 0.9064265489578247\n",
      "step: 9490 , time : 0.0\n",
      "train: loss: 135601.375 acc: 0.9699955582618713  val: loss: 731511.9375 acc: 0.9449009299278259\n",
      "step: 9495 , time : 0.0\n",
      "train: loss: 37920.2421875 acc: 0.986819863319397  val: loss: 1309119.0 acc: 0.38806402683258057\n",
      "step: 9500 , time : 0.0\n",
      "train: loss: 45663.75 acc: 0.9885545372962952  val: loss: 447361.53125 acc: 0.8823769092559814\n",
      "step: 9505 , time : 0.0\n",
      "train: loss: 129225.6484375 acc: 0.9592788815498352  val: loss: 603239.8125 acc: 0.6008347272872925\n",
      "step: 9510 , time : 0.0\n",
      "train: loss: 128009.5859375 acc: 0.9654191732406616  val: loss: 486519.78125 acc: 0.9240273237228394\n",
      "step: 9515 , time : 0.0010006427764892578\n",
      "train: loss: 307535.5625 acc: 0.8728163242340088  val: loss: 1194502.0 acc: 0.8053834438323975\n",
      "step: 9520 , time : 0.0010013580322265625\n",
      "train: loss: 155504.953125 acc: 0.9218690991401672  val: loss: 126136.0546875 acc: 0.9611250758171082\n",
      "step: 9525 , time : 0.0010006427764892578\n",
      "train: loss: 610939.0 acc: 0.7179650664329529  val: loss: 687707.5 acc: 0.8589105606079102\n",
      "step: 9530 , time : 0.0010004043579101562\n",
      "train: loss: 174758.921875 acc: 0.9719082117080688  val: loss: 290339.96875 acc: 0.9116514921188354\n",
      "step: 9535 , time : 7.2479248046875e-05\n",
      "train: loss: 572458.75 acc: 0.9570397734642029  val: loss: 185553.125 acc: 0.9345917105674744\n",
      "step: 9540 , time : 0.0\n",
      "train: loss: 52460.11328125 acc: 0.9935293793678284  val: loss: 4032888.25 acc: -0.022905826568603516\n",
      "step: 9545 , time : 0.0\n",
      "train: loss: 96711.6171875 acc: 0.9825576543807983  val: loss: 1008294.1875 acc: 0.7615575194358826\n",
      "step: 9550 , time : 0.0\n",
      "train: loss: 268383.21875 acc: 0.9740303158760071  val: loss: 847596.8125 acc: 0.697060227394104\n",
      "step: 9555 , time : 0.0\n",
      "train: loss: 413118.875 acc: 0.9679158926010132  val: loss: 1138536.25 acc: 0.4861263632774353\n",
      "step: 9560 , time : 0.0\n",
      "train: loss: 1833598.75 acc: 0.8336836695671082  val: loss: 826279.0625 acc: 0.7749632596969604\n",
      "step: 9565 , time : 0.0\n",
      "train: loss: 701087.1875 acc: 0.9494137763977051  val: loss: 818521.75 acc: 0.836789071559906\n",
      "step: 9570 , time : 0.0\n",
      "train: loss: 546063.0625 acc: 0.9736087918281555  val: loss: 578698.75 acc: 0.9179094433784485\n",
      "step: 9575 , time : 0.0\n",
      "train: loss: 2777331.25 acc: 0.9072875380516052  val: loss: 560975.3125 acc: 0.8484371900558472\n",
      "step: 9580 , time : 0.0\n",
      "train: loss: 2375424.0 acc: 0.8254877328872681  val: loss: 99140.1953125 acc: 0.9461984634399414\n",
      "step: 9585 , time : 0.0\n",
      "train: loss: 579521.75 acc: 0.9629372954368591  val: loss: 1535243.125 acc: -0.07991361618041992\n",
      "step: 9590 , time : 0.0\n",
      "train: loss: 818507.1875 acc: 0.932043731212616  val: loss: 503720.46875 acc: 0.9403012990951538\n",
      "step: 9595 , time : 0.0\n",
      "train: loss: 493986.4375 acc: 0.9466735124588013  val: loss: 1045097.0625 acc: 0.5422624349594116\n",
      "step: 9600 , time : 0.0\n",
      "train: loss: 767712.125 acc: 0.9465773701667786  val: loss: 469840.28125 acc: 0.9557207822799683\n",
      "step: 9605 , time : 0.0010008811950683594\n",
      "train: loss: 1982712.125 acc: 0.7422604560852051  val: loss: 1484872.625 acc: 0.8714727163314819\n",
      "step: 9610 , time : 0.0\n",
      "train: loss: 512082.90625 acc: 0.6886334419250488  val: loss: 784836.5625 acc: 0.8246479630470276\n",
      "step: 9615 , time : 0.001001119613647461\n",
      "train: loss: 692854.5625 acc: 0.6729835867881775  val: loss: 934015.1875 acc: 0.5707011818885803\n",
      "step: 9620 , time : 0.00016450881958007812\n",
      "train: loss: 606475.5 acc: 0.8430647253990173  val: loss: 937577.0 acc: 0.8978872299194336\n",
      "step: 9625 , time : 0.0\n",
      "train: loss: 1159107.0 acc: 0.6618795394897461  val: loss: 555145.4375 acc: 0.841092050075531\n",
      "step: 9630 , time : 0.0\n",
      "train: loss: 1914360.25 acc: -0.12949299812316895  val: loss: 2760913.75 acc: 0.635522723197937\n",
      "step: 9635 , time : 0.0\n",
      "train: loss: 1725694.25 acc: 0.4221559166908264  val: loss: 1900213.25 acc: 0.3451935052871704\n",
      "step: 9640 , time : 0.0\n",
      "train: loss: 707498.125 acc: 0.5070369243621826  val: loss: 7032384.0 acc: 0.3824222683906555\n",
      "step: 9645 , time : 0.0\n",
      "train: loss: 721341.25 acc: 0.6900820732116699  val: loss: 1538862.0 acc: 0.7973902225494385\n",
      "step: 9650 , time : 0.0\n",
      "train: loss: 418149.25 acc: 0.8339426517486572  val: loss: 816383.125 acc: 0.8047411441802979\n",
      "step: 9655 , time : 0.0\n",
      "train: loss: 83397.7578125 acc: 0.9308598041534424  val: loss: 2035089.25 acc: 0.8146385550498962\n",
      "step: 9660 , time : 0.0\n",
      "train: loss: 249610.390625 acc: 0.867124080657959  val: loss: 1147165.75 acc: 0.8640415668487549\n",
      "step: 9665 , time : 0.0\n",
      "train: loss: 298761.8125 acc: 0.7535202503204346  val: loss: 991289.875 acc: 0.8506511449813843\n",
      "step: 9670 , time : 0.0\n",
      "train: loss: 107394.6953125 acc: 0.8996133208274841  val: loss: 1072034.25 acc: 0.6701956987380981\n",
      "step: 9675 , time : 0.0\n",
      "train: loss: 204373.3125 acc: 0.8380193710327148  val: loss: 2238549.75 acc: 0.7234820127487183\n",
      "step: 9680 , time : 0.0\n",
      "train: loss: 196734.21875 acc: 0.7977120876312256  val: loss: 360668.125 acc: 0.8680605888366699\n",
      "step: 9685 , time : 0.0\n",
      "train: loss: 196646.671875 acc: 0.7308549880981445  val: loss: 515679.40625 acc: 0.8888483047485352\n",
      "step: 9690 , time : 0.0\n",
      "train: loss: 533693.125 acc: 0.7170207500457764  val: loss: 1965769.125 acc: 0.7082719802856445\n",
      "step: 9695 , time : 0.0\n",
      "train: loss: 679059.5625 acc: 0.5808446407318115  val: loss: 826797.875 acc: 0.8617019653320312\n",
      "step: 9700 , time : 0.0\n",
      "train: loss: 1159559.25 acc: 0.6179108619689941  val: loss: 2046821.5 acc: 0.7174115777015686\n",
      "step: 9705 , time : 0.0\n",
      "train: loss: 963760.125 acc: 0.3309069871902466  val: loss: 907609.5 acc: 0.4814862012863159\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 9710 , time : 0.0\n",
      "train: loss: 294550.34375 acc: 0.8296537399291992  val: loss: 483201.84375 acc: 0.8747104406356812\n",
      "step: 9715 , time : 0.0\n",
      "train: loss: 683807.0 acc: 0.8028019666671753  val: loss: 1303895.875 acc: 0.845546543598175\n",
      "step: 9720 , time : 0.0\n",
      "train: loss: 798860.25 acc: 0.8781996369361877  val: loss: 591310.4375 acc: 0.7512742280960083\n",
      "step: 9725 , time : 0.0\n",
      "train: loss: 1091114.75 acc: 0.8629100322723389  val: loss: 1663190.375 acc: 0.8767673373222351\n",
      "step: 9730 , time : 0.0\n",
      "train: loss: 508166.375 acc: 0.9490296840667725  val: loss: 2032503.625 acc: 0.5957945585250854\n",
      "step: 9735 , time : 0.0\n",
      "train: loss: 406736.9375 acc: 0.9588075876235962  val: loss: 1685798.0 acc: 0.8232478499412537\n",
      "step: 9740 , time : 0.001001119613647461\n",
      "train: loss: 125522.2578125 acc: 0.9861836433410645  val: loss: 513929.0625 acc: 0.8494613170623779\n",
      "step: 9745 , time : 0.0\n",
      "train: loss: 222590.125 acc: 0.9703665971755981  val: loss: 978921.625 acc: 0.6043695211410522\n",
      "step: 9750 , time : 0.0\n",
      "train: loss: 265290.1875 acc: 0.9739282727241516  val: loss: 1266707.5 acc: 0.5441173315048218\n",
      "step: 9755 , time : 0.0\n",
      "train: loss: 269982.625 acc: 0.98211669921875  val: loss: 1497969.5 acc: 0.5803400874137878\n",
      "step: 9760 , time : 0.0\n",
      "train: loss: 410800.0625 acc: 0.9489024877548218  val: loss: 1377869.125 acc: 0.8017896413803101\n",
      "step: 9765 , time : 0.0\n",
      "train: loss: 227286.734375 acc: 0.9622293710708618  val: loss: 996629.875 acc: 0.8350574970245361\n",
      "step: 9770 , time : 0.0\n",
      "train: loss: 67933.125 acc: 0.9822208285331726  val: loss: 392137.03125 acc: 0.8473536968231201\n",
      "step: 9775 , time : 0.0\n",
      "train: loss: 82581.0 acc: 0.9660948514938354  val: loss: 630781.75 acc: 0.7095779180526733\n",
      "step: 9780 , time : 0.0\n",
      "train: loss: 9886.826171875 acc: 0.9924925565719604  val: loss: 603520.375 acc: 0.8820746541023254\n",
      "step: 9785 , time : 0.0\n",
      "train: loss: 102909.65625 acc: 0.9740927815437317  val: loss: 1146699.375 acc: 0.8047730922698975\n",
      "step: 9790 , time : 0.0\n",
      "train: loss: 13809.431640625 acc: 0.9817958474159241  val: loss: 1340690.375 acc: 0.7068279981613159\n",
      "step: 9795 , time : 0.0\n",
      "train: loss: 12245.431640625 acc: 0.980733335018158  val: loss: 1714266.0 acc: 0.7159563302993774\n",
      "step: 9800 , time : 0.0\n",
      "train: loss: 14197.6953125 acc: 0.95638108253479  val: loss: 721636.3125 acc: 0.7786110639572144\n",
      "step: 9805 , time : 0.0\n",
      "train: loss: 19761.59375 acc: 0.9569708704948425  val: loss: 309259.84375 acc: 0.9347995519638062\n",
      "step: 9810 , time : 0.0\n",
      "train: loss: 9292.23046875 acc: 0.9727973937988281  val: loss: 499602.78125 acc: 0.9305136799812317\n",
      "step: 9815 , time : 0.0\n",
      "train: loss: 32872.56640625 acc: 0.9544862508773804  val: loss: 285997.40625 acc: 0.9704484939575195\n",
      "step: 9820 , time : 0.0\n",
      "train: loss: 113565.9609375 acc: 0.9421424269676208  val: loss: 219475.453125 acc: 0.9581945538520813\n",
      "step: 9825 , time : 0.0\n",
      "train: loss: 15309.5263671875 acc: 0.9848600029945374  val: loss: 492256.34375 acc: 0.9476453065872192\n",
      "step: 9830 , time : 0.0\n",
      "train: loss: 121675.640625 acc: 0.9231901168823242  val: loss: 151946.109375 acc: 0.9530218243598938\n",
      "step: 9835 , time : 0.0\n",
      "train: loss: 77729.1171875 acc: 0.961328387260437  val: loss: 1872366.75 acc: 0.09230470657348633\n",
      "step: 9840 , time : 0.0\n",
      "train: loss: 168078.84375 acc: 0.7432755827903748  val: loss: 735804.0 acc: 0.8674959540367126\n",
      "step: 9845 , time : 0.0\n",
      "train: loss: 15936.5751953125 acc: 0.9691346287727356  val: loss: 1546432.0 acc: 0.42005664110183716\n",
      "step: 9850 , time : 0.0010004043579101562\n",
      "train: loss: 21276.275390625 acc: 0.9728925824165344  val: loss: 1825881.875 acc: 0.20172148942947388\n",
      "step: 9855 , time : 0.0\n",
      "train: loss: 98121.34375 acc: 0.9728372693061829  val: loss: 774874.375 acc: 0.8555634021759033\n",
      "step: 9860 , time : 0.0\n",
      "train: loss: 52236.6328125 acc: 0.9768907427787781  val: loss: 160321.140625 acc: 0.9684873223304749\n",
      "step: 9865 , time : 0.0\n",
      "train: loss: 110655.328125 acc: 0.9826889634132385  val: loss: 2090132.125 acc: 0.7397280931472778\n",
      "step: 9870 , time : 0.0\n",
      "train: loss: 98002.7265625 acc: 0.9603919386863708  val: loss: 1380940.625 acc: 0.6049504280090332\n",
      "step: 9875 , time : 0.0\n",
      "train: loss: 92790.3671875 acc: 0.9769389033317566  val: loss: 984334.8125 acc: 0.5880759358406067\n",
      "step: 9880 , time : 0.0010006427764892578\n",
      "train: loss: 501841.46875 acc: 0.7137500047683716  val: loss: 522625.34375 acc: 0.8955848217010498\n",
      "step: 9885 , time : 0.001001119613647461\n",
      "train: loss: 184550.1875 acc: 0.8940830230712891  val: loss: 363294.65625 acc: 0.8204898238182068\n",
      "step: 9890 , time : 0.0010004043579101562\n",
      "train: loss: 183414.484375 acc: 0.9397781491279602  val: loss: 218833.984375 acc: 0.9452583193778992\n",
      "step: 9895 , time : 0.0\n",
      "train: loss: 350014.53125 acc: 0.9194284677505493  val: loss: 244715.328125 acc: 0.9410387873649597\n",
      "step: 9900 , time : 0.0\n",
      "train: loss: 179777.09375 acc: 0.9867410659790039  val: loss: 235353.375 acc: 0.847884476184845\n",
      "step: 9905 , time : 0.0\n",
      "train: loss: 163947.421875 acc: 0.9822052121162415  val: loss: 342820.8125 acc: 0.932349443435669\n",
      "step: 9910 , time : 0.0\n",
      "train: loss: 328254.625 acc: 0.9652605056762695  val: loss: 1132273.625 acc: 0.8686732649803162\n",
      "step: 9915 , time : 0.0010004043579101562\n",
      "train: loss: 130920.0859375 acc: 0.9907193183898926  val: loss: 366684.53125 acc: 0.921458899974823\n",
      "step: 9920 , time : 2.8133392333984375e-05\n",
      "train: loss: 283130.375 acc: 0.9854341745376587  val: loss: 5647714.0 acc: 0.3997519016265869\n",
      "step: 9925 , time : 0.0\n",
      "train: loss: 279251.71875 acc: 0.9445206522941589  val: loss: 1458886.5 acc: 0.7593048214912415\n",
      "step: 9930 , time : 0.0\n",
      "train: loss: 342736.3125 acc: 0.9521344900131226  val: loss: 1213060.25 acc: 0.8325914740562439\n",
      "step: 9935 , time : 0.0\n",
      "train: loss: 763994.8125 acc: 0.9757846593856812  val: loss: 511094.53125 acc: 0.9508604407310486\n",
      "step: 9940 , time : 0.0\n",
      "train: loss: 2520880.25 acc: 0.9490833282470703  val: loss: 365656.5 acc: 0.9642049074172974\n",
      "step: 9945 , time : 0.0\n",
      "train: loss: 1501538.25 acc: 0.9221727848052979  val: loss: 868858.4375 acc: 0.9322988986968994\n",
      "step: 9950 , time : 0.0010013580322265625\n",
      "train: loss: 343546.46875 acc: 0.9801432490348816  val: loss: 352159.21875 acc: 0.9493023753166199\n",
      "step: 9955 , time : 0.0\n",
      "train: loss: 508195.84375 acc: 0.9683949947357178  val: loss: 372322.0 acc: 0.9213064908981323\n",
      "step: 9960 , time : 0.0\n",
      "train: loss: 344439.8125 acc: 0.9745104312896729  val: loss: 361494.90625 acc: 0.9517200589179993\n",
      "step: 9965 , time : 0.0\n",
      "train: loss: 182881.71875 acc: 0.9643246531486511  val: loss: 1689030.75 acc: 0.7955414056777954\n",
      "step: 9970 , time : 0.0\n",
      "train: loss: 488307.90625 acc: 0.9182025194168091  val: loss: 1153870.375 acc: 0.8649598360061646\n",
      "step: 9975 , time : 0.0\n",
      "train: loss: 1429181.375 acc: 0.44474923610687256  val: loss: 1489961.25 acc: 0.451404869556427\n",
      "step: 9980 , time : 0.0\n",
      "train: loss: 574667.4375 acc: 0.5402683615684509  val: loss: 672583.1875 acc: 0.7597097158432007\n",
      "step: 9985 , time : 0.0\n",
      "train: loss: 363274.53125 acc: 0.7724023461341858  val: loss: 860199.375 acc: 0.8858230113983154\n",
      "step: 9990 , time : 0.0\n",
      "train: loss: 485337.8125 acc: 0.9056799411773682  val: loss: 574137.375 acc: 0.8336991667747498\n",
      "step: 9995 , time : 0.0\n",
      "train: loss: 3177591.0 acc: -0.6712337732315063  val: loss: 1073107.0 acc: 0.8343383073806763\n",
      "step: 10000 , time : 0.0\n",
      "train: loss: 1087987.25 acc: 0.39140862226486206  val: loss: 1551032.75 acc: 0.38487982749938965\n",
      "step: 10005 , time : 0.0\n",
      "train: loss: 810620.375 acc: 0.398997962474823  val: loss: 1859615.875 acc: 0.6049543619155884\n",
      "step: 10010 , time : 0.0\n",
      "train: loss: 887083.375 acc: 0.34258079528808594  val: loss: 1420313.75 acc: 0.7025842666625977\n",
      "step: 10015 , time : 0.0\n",
      "train: loss: 178268.078125 acc: 0.8380964994430542  val: loss: 1426238.75 acc: 0.7665306329727173\n",
      "step: 10020 , time : 0.0\n",
      "train: loss: 306031.96875 acc: 0.7804087400436401  val: loss: 1414666.125 acc: 0.8564027547836304\n",
      "step: 10025 , time : 0.0\n",
      "train: loss: 368673.1875 acc: 0.8256746530532837  val: loss: 463171.625 acc: 0.9099301695823669\n",
      "step: 10030 , time : 0.0\n",
      "train: loss: 212427.953125 acc: 0.875359296798706  val: loss: 1509047.375 acc: 0.6674470901489258\n",
      "step: 10035 , time : 0.0\n",
      "train: loss: 180059.03125 acc: 0.8763587474822998  val: loss: 778768.1875 acc: 0.6782700419425964\n",
      "step: 10040 , time : 0.0\n",
      "train: loss: 160806.8125 acc: 0.8575452566146851  val: loss: 795898.5 acc: 0.8358988165855408\n",
      "step: 10045 , time : 0.0\n",
      "train: loss: 135733.40625 acc: 0.892716646194458  val: loss: 976605.875 acc: 0.6805337071418762\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 10050 , time : 0.0\n",
      "train: loss: 184155.625 acc: 0.7478137016296387  val: loss: 528821.125 acc: 0.8906200528144836\n",
      "step: 10055 , time : 0.0\n",
      "train: loss: 749538.0 acc: 0.40783679485321045  val: loss: 799058.625 acc: 0.7451289892196655\n",
      "step: 10060 , time : 0.0\n",
      "train: loss: 911860.0 acc: 0.5176398754119873  val: loss: 720721.375 acc: 0.5340081453323364\n",
      "step: 10065 , time : 0.0010006427764892578\n",
      "train: loss: 1053709.25 acc: 0.4876607060432434  val: loss: 716089.875 acc: 0.6568961143493652\n",
      "step: 10070 , time : 0.0010008811950683594\n",
      "train: loss: 557031.3125 acc: 0.6716627478599548  val: loss: 1286082.75 acc: 0.7288051247596741\n",
      "step: 10075 , time : 0.0\n",
      "train: loss: 1251736.5 acc: 0.5417340993881226  val: loss: 2289014.25 acc: 0.7460842132568359\n",
      "step: 10080 , time : 0.0\n",
      "train: loss: 306980.3125 acc: 0.8239949941635132  val: loss: 1268583.75 acc: 0.7165507078170776\n",
      "step: 10085 , time : 0.0\n",
      "train: loss: 1332002.5 acc: 0.8371316194534302  val: loss: 1319705.375 acc: 0.8430746793746948\n",
      "step: 10090 , time : 0.0\n",
      "train: loss: 1143739.75 acc: 0.87346351146698  val: loss: 708360.625 acc: 0.8109362721443176\n",
      "step: 10095 , time : 0.0\n",
      "train: loss: 706981.6875 acc: 0.9334102272987366  val: loss: 890153.125 acc: 0.8499863147735596\n",
      "step: 10100 , time : 0.0\n",
      "train: loss: 588357.9375 acc: 0.9022596478462219  val: loss: 1276687.375 acc: -0.16071879863739014\n",
      "step: 10105 , time : 0.0\n",
      "train: loss: 291591.15625 acc: 0.9271489381790161  val: loss: 789010.75 acc: 0.5925894975662231\n",
      "step: 10110 , time : 0.0\n",
      "train: loss: 377698.125 acc: 0.9385250210762024  val: loss: 1848907.0 acc: 0.5286073088645935\n",
      "step: 10115 , time : 0.0\n",
      "train: loss: 369879.5 acc: 0.9734975099563599  val: loss: 1149744.25 acc: 0.82743239402771\n",
      "step: 10120 , time : 0.0\n",
      "train: loss: 225938.421875 acc: 0.9828627109527588  val: loss: 1108779.0 acc: 0.5681934356689453\n",
      "step: 10125 , time : 0.0\n",
      "train: loss: 211086.15625 acc: 0.9839732050895691  val: loss: 1187170.5 acc: 0.8405776619911194\n",
      "step: 10130 , time : 0.0\n",
      "train: loss: 190189.484375 acc: 0.9796187877655029  val: loss: 561678.0 acc: 0.7955531477928162\n",
      "step: 10135 , time : 0.0\n",
      "train: loss: 86372.4921875 acc: 0.9781343340873718  val: loss: 946038.9375 acc: 0.8571077585220337\n",
      "step: 10140 , time : 0.0\n",
      "train: loss: 122717.234375 acc: 0.9679985046386719  val: loss: 733848.9375 acc: 0.31152093410491943\n",
      "step: 10145 , time : 0.0\n",
      "train: loss: 102470.5 acc: 0.9780621528625488  val: loss: 679512.5 acc: 0.7895992398262024\n",
      "step: 10150 , time : 0.0\n",
      "train: loss: 19776.5859375 acc: 0.9531658291816711  val: loss: 209903.328125 acc: 0.9166576266288757\n",
      "step: 10155 , time : 0.0\n",
      "train: loss: 16663.201171875 acc: 0.9944180250167847  val: loss: 233146.078125 acc: 0.9285758137702942\n",
      "step: 10160 , time : 0.0\n",
      "train: loss: 40255.44140625 acc: 0.8967543840408325  val: loss: 651848.0 acc: 0.9148616194725037\n",
      "step: 10165 , time : 0.0\n",
      "train: loss: 12393.5576171875 acc: 0.9706761240959167  val: loss: 209087.140625 acc: 0.963836133480072\n",
      "step: 10170 , time : 0.0\n",
      "train: loss: 5454.8955078125 acc: 0.9886198043823242  val: loss: 336666.03125 acc: 0.9563286304473877\n",
      "step: 10175 , time : 0.0010004043579101562\n",
      "train: loss: 13816.3095703125 acc: 0.9540911316871643  val: loss: 687335.125 acc: 0.8900935053825378\n",
      "step: 10180 , time : 0.0\n",
      "train: loss: 17013.455078125 acc: 0.8899217247962952  val: loss: 1235974.75 acc: 0.5792689919471741\n",
      "step: 10185 , time : 0.0010006427764892578\n",
      "train: loss: 24783.43359375 acc: 0.9668030142784119  val: loss: 176290.015625 acc: 0.9571325778961182\n",
      "step: 10190 , time : 0.0\n",
      "train: loss: 171561.453125 acc: 0.8986537456512451  val: loss: 219281.625 acc: 0.9559661149978638\n",
      "step: 10195 , time : 0.0\n",
      "train: loss: 42065.9921875 acc: 0.9671028256416321  val: loss: 95755.3515625 acc: 0.9690272808074951\n",
      "step: 10200 , time : 0.0\n",
      "train: loss: 99994.515625 acc: 0.936547577381134  val: loss: 429226.6875 acc: 0.8719696402549744\n",
      "step: 10205 , time : 0.0\n",
      "train: loss: 83126.40625 acc: 0.9617346525192261  val: loss: 297807.71875 acc: 0.9019902348518372\n",
      "step: 10210 , time : 0.0\n",
      "train: loss: 23792.453125 acc: 0.9658951163291931  val: loss: 305797.65625 acc: 0.9629234671592712\n",
      "step: 10215 , time : 0.0\n",
      "train: loss: 25603.81640625 acc: 0.9371814727783203  val: loss: 197358.59375 acc: 0.9436520338058472\n",
      "step: 10220 , time : 0.0\n",
      "train: loss: 68322.65625 acc: 0.9789780378341675  val: loss: 575248.25 acc: 0.876160204410553\n",
      "step: 10225 , time : 0.0\n",
      "train: loss: 108771.734375 acc: 0.9710232019424438  val: loss: 746852.5625 acc: 0.6990858912467957\n",
      "step: 10230 , time : 0.0\n",
      "train: loss: 50373.1796875 acc: 0.9865081906318665  val: loss: 703964.125 acc: 0.9237789511680603\n",
      "step: 10235 , time : 0.0\n",
      "train: loss: 108845.015625 acc: 0.9766871333122253  val: loss: 226687.6875 acc: 0.9393437504768372\n",
      "step: 10240 , time : 0.0\n",
      "train: loss: 144738.34375 acc: 0.9699863195419312  val: loss: 1275562.125 acc: 0.8566118478775024\n",
      "step: 10245 , time : 0.0\n",
      "train: loss: 129680.1640625 acc: 0.9377714395523071  val: loss: 445960.625 acc: 0.8472901582717896\n",
      "step: 10250 , time : 0.0\n",
      "train: loss: 148544.578125 acc: 0.9144155979156494  val: loss: 711396.5 acc: 0.8075641989707947\n",
      "step: 10255 , time : 0.0\n",
      "train: loss: 85525.734375 acc: 0.967293918132782  val: loss: 1486385.625 acc: 0.6696814298629761\n",
      "step: 10260 , time : 0.015625476837158203\n",
      "train: loss: 188785.1875 acc: 0.9601888060569763  val: loss: 1264926.25 acc: 0.6278554201126099\n",
      "step: 10265 , time : 0.0\n",
      "train: loss: 126512.421875 acc: 0.9899290204048157  val: loss: 1799429.125 acc: 0.7539003491401672\n",
      "step: 10270 , time : 0.0\n",
      "train: loss: 103629.9375 acc: 0.9851006865501404  val: loss: 932485.8125 acc: 0.8909030556678772\n",
      "step: 10275 , time : 0.0\n",
      "train: loss: 142680.125 acc: 0.9761128425598145  val: loss: 1764500.375 acc: 0.6622538566589355\n",
      "step: 10280 , time : 0.0010001659393310547\n",
      "train: loss: 185529.328125 acc: 0.9775071144104004  val: loss: 1500490.0 acc: 0.6042459011077881\n",
      "step: 10285 , time : 0.0\n",
      "train: loss: 536559.8125 acc: 0.9695290923118591  val: loss: 1150922.125 acc: 0.8831234574317932\n",
      "step: 10290 , time : 0.0010008811950683594\n",
      "train: loss: 517392.5 acc: 0.964203417301178  val: loss: 880956.1875 acc: 0.6052106022834778\n",
      "step: 10295 , time : 0.0010008811950683594\n",
      "train: loss: 119202.3203125 acc: 0.979609489440918  val: loss: 1324060.125 acc: 0.8122808933258057\n",
      "step: 10300 , time : 0.0\n",
      "train: loss: 1159738.0 acc: 0.9516028165817261  val: loss: 1173065.0 acc: 0.5281819105148315\n",
      "step: 10305 , time : 0.0\n",
      "train: loss: 671204.75 acc: 0.982245922088623  val: loss: 1256906.75 acc: 0.6641254425048828\n",
      "step: 10310 , time : 0.0\n",
      "train: loss: 2560519.75 acc: 0.9311747550964355  val: loss: 503017.59375 acc: 0.9109319448471069\n",
      "step: 10315 , time : 0.0\n",
      "train: loss: 675263.5625 acc: 0.9682285785675049  val: loss: 1783719.625 acc: 0.6560251712799072\n",
      "step: 10320 , time : 0.0\n",
      "train: loss: 1440480.625 acc: 0.8771356344223022  val: loss: 364989.46875 acc: 0.9573248028755188\n",
      "step: 10325 , time : 0.0\n",
      "train: loss: 1255078.5 acc: 0.8391579389572144  val: loss: 1741969.75 acc: 0.6744899153709412\n",
      "step: 10330 , time : 0.0\n",
      "train: loss: 223429.40625 acc: 0.9619372487068176  val: loss: 730527.4375 acc: 0.808449923992157\n",
      "step: 10335 , time : 0.0\n",
      "train: loss: 595624.9375 acc: 0.8329892158508301  val: loss: 509044.03125 acc: 0.9395000338554382\n",
      "step: 10340 , time : 0.0\n",
      "train: loss: 849014.0 acc: 0.7112475633621216  val: loss: 1637865.875 acc: 0.26277047395706177\n",
      "step: 10345 , time : 0.0\n",
      "train: loss: 843739.5 acc: 0.8824451565742493  val: loss: 1333199.25 acc: 0.5872327089309692\n",
      "step: 10350 , time : 0.0\n",
      "train: loss: 1184278.0 acc: 0.14907652139663696  val: loss: 839095.875 acc: 0.8897606134414673\n",
      "step: 10355 , time : 0.0\n",
      "train: loss: 858078.375 acc: 0.744550347328186  val: loss: 851755.4375 acc: 0.34273409843444824\n",
      "step: 10360 , time : 0.0\n",
      "train: loss: 2107561.0 acc: 0.6304113864898682  val: loss: 2379175.25 acc: 0.7537186741828918\n",
      "step: 10365 , time : 0.0\n",
      "train: loss: 1052813.75 acc: 0.38886165618896484  val: loss: 3594639.0 acc: 0.3688623905181885\n",
      "step: 10370 , time : 0.0010008811950683594\n",
      "train: loss: 1057845.0 acc: 0.4761752486228943  val: loss: 2525717.25 acc: 0.6255772113800049\n",
      "step: 10375 , time : 0.0\n",
      "train: loss: 582539.4375 acc: 0.5102378129959106  val: loss: 1875724.75 acc: 0.6117522120475769\n",
      "step: 10380 , time : 0.0\n",
      "train: loss: 212273.875 acc: 0.8210172653198242  val: loss: 1417822.0 acc: 0.731890857219696\n",
      "step: 10385 , time : 0.0\n",
      "train: loss: 640117.0 acc: 0.5336915254592896  val: loss: 773701.3125 acc: 0.8845452070236206\n",
      "step: 10390 , time : 0.0\n",
      "train: loss: 320891.6875 acc: 0.8257931470870972  val: loss: 2367037.25 acc: 0.7090499401092529\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 10395 , time : 0.0\n",
      "train: loss: 219476.953125 acc: 0.856486976146698  val: loss: 457120.34375 acc: 0.7975469827651978\n",
      "step: 10400 , time : 0.0010006427764892578\n",
      "train: loss: 374618.96875 acc: 0.7941750884056091  val: loss: 1423503.125 acc: 0.8807312250137329\n",
      "step: 10405 , time : 0.0010008811950683594\n",
      "train: loss: 266086.9375 acc: 0.8403403759002686  val: loss: 959389.9375 acc: 0.8208900690078735\n",
      "step: 10410 , time : 0.0\n",
      "train: loss: 201188.171875 acc: 0.8005975484848022  val: loss: 255160.59375 acc: 0.8068990707397461\n",
      "step: 10415 , time : 0.0\n",
      "train: loss: 222425.40625 acc: 0.7087162733078003  val: loss: 813726.9375 acc: 0.8276416659355164\n",
      "step: 10420 , time : 0.0\n",
      "train: loss: 653132.625 acc: 0.439578652381897  val: loss: 727891.75 acc: 0.7951499223709106\n",
      "step: 10425 , time : 0.0010004043579101562\n",
      "train: loss: 819986.6875 acc: 0.7215583324432373  val: loss: 1276000.375 acc: 0.7116843461990356\n",
      "step: 10430 , time : 0.0\n",
      "train: loss: 889939.3125 acc: 0.4411444664001465  val: loss: 744041.4375 acc: 0.8261880874633789\n",
      "step: 10435 , time : 0.0\n",
      "train: loss: 1424192.625 acc: -0.030490994453430176  val: loss: 947340.125 acc: 0.7555484175682068\n",
      "step: 10440 , time : 0.0010008811950683594\n",
      "train: loss: 601936.625 acc: 0.7177413702011108  val: loss: 861210.625 acc: 0.7396542429924011\n",
      "step: 10445 , time : 0.0\n",
      "train: loss: 753845.5 acc: 0.7143992781639099  val: loss: 960398.0 acc: 0.6786217093467712\n",
      "step: 10450 , time : 0.0\n",
      "train: loss: 1089112.75 acc: 0.8489981889724731  val: loss: 1602839.375 acc: 0.7771618962287903\n",
      "step: 10455 , time : 0.0\n",
      "train: loss: 1055912.0 acc: 0.8006898164749146  val: loss: 573243.5625 acc: 0.9005244374275208\n",
      "step: 10460 , time : 0.0\n",
      "train: loss: 775494.625 acc: 0.9358286261558533  val: loss: 857413.6875 acc: 0.7778770923614502\n",
      "step: 10465 , time : 0.0\n",
      "train: loss: 1037481.5625 acc: 0.8517856001853943  val: loss: 1172648.625 acc: 0.4848019480705261\n",
      "step: 10470 , time : 0.0\n",
      "train: loss: 361059.65625 acc: 0.9453251361846924  val: loss: 314809.75 acc: 0.8591867685317993\n",
      "step: 10475 , time : 0.0\n",
      "train: loss: 360865.9375 acc: 0.9358841776847839  val: loss: 329852.21875 acc: 0.9373186826705933\n",
      "step: 10480 , time : 0.0\n",
      "train: loss: 267150.625 acc: 0.9709402918815613  val: loss: 402988.0 acc: 0.8245010375976562\n",
      "step: 10485 , time : 0.0\n",
      "train: loss: 198767.96875 acc: 0.9878280758857727  val: loss: 722350.3125 acc: 0.8949029445648193\n",
      "step: 10490 , time : 0.015625476837158203\n",
      "train: loss: 265835.09375 acc: 0.9815011620521545  val: loss: 1363124.75 acc: 0.8116913437843323\n",
      "step: 10495 , time : 0.0\n",
      "train: loss: 481570.75 acc: 0.9429465532302856  val: loss: 808073.125 acc: 0.74391770362854\n",
      "step: 10500 , time : 0.0\n",
      "train: loss: 210634.171875 acc: 0.9547116160392761  val: loss: 398873.125 acc: 0.822904109954834\n",
      "step: 10505 , time : 0.0\n",
      "train: loss: 42667.265625 acc: 0.983816385269165  val: loss: 535228.0 acc: 0.9299217462539673\n",
      "step: 10510 , time : 0.001001119613647461\n",
      "train: loss: 204119.640625 acc: 0.9537850022315979  val: loss: 617174.375 acc: 0.8509501814842224\n",
      "step: 10515 , time : 0.0010006427764892578\n",
      "train: loss: 15184.060546875 acc: 0.9404602646827698  val: loss: 180860.90625 acc: 0.9793530702590942\n",
      "step: 10520 , time : 0.0010006427764892578\n",
      "train: loss: 19095.314453125 acc: 0.9698083996772766  val: loss: 377425.9375 acc: 0.9112563729286194\n",
      "step: 10525 , time : 0.0\n",
      "train: loss: 21478.578125 acc: 0.9575870037078857  val: loss: 939562.5625 acc: 0.6748027801513672\n",
      "step: 10530 , time : 0.0\n",
      "train: loss: 8395.8134765625 acc: 0.9815658330917358  val: loss: 1045691.75 acc: 0.8278378248214722\n",
      "step: 10535 , time : 0.0\n",
      "train: loss: 36399.8125 acc: 0.9237216114997864  val: loss: 1561180.375 acc: 0.6445851922035217\n",
      "step: 10540 , time : 0.0\n",
      "train: loss: 26679.994140625 acc: 0.950689971446991  val: loss: 2935593.75 acc: 0.6231976747512817\n",
      "step: 10545 , time : 0.0\n",
      "train: loss: 12417.423828125 acc: 0.9921503067016602  val: loss: 1900249.0 acc: 0.8507732152938843\n",
      "step: 10550 , time : 0.0\n",
      "train: loss: 88012.4140625 acc: 0.9206336140632629  val: loss: 664445.125 acc: 0.8186367154121399\n",
      "step: 10555 , time : 0.0\n",
      "train: loss: 39838.58203125 acc: 0.9610047340393066  val: loss: 1585942.375 acc: 0.7192808389663696\n",
      "step: 10560 , time : 0.0\n",
      "train: loss: 68457.3203125 acc: 0.95623779296875  val: loss: 294653.09375 acc: 0.9265459179878235\n",
      "step: 10565 , time : 0.0\n",
      "train: loss: 22918.046875 acc: 0.9823505282402039  val: loss: 558106.4375 acc: 0.9396028518676758\n",
      "step: 10570 , time : 0.0\n",
      "train: loss: 90120.984375 acc: 0.9645588397979736  val: loss: 2171323.0 acc: 0.7697104811668396\n",
      "step: 10575 , time : 0.0\n",
      "train: loss: 54788.734375 acc: 0.9591002464294434  val: loss: 685708.3125 acc: 0.930602490901947\n",
      "step: 10580 , time : 0.0\n",
      "train: loss: 46072.59765625 acc: 0.9340581893920898  val: loss: 1554779.625 acc: 0.7561451196670532\n",
      "step: 10585 , time : 0.0\n",
      "train: loss: 59657.484375 acc: 0.9645003080368042  val: loss: 188079.546875 acc: 0.9693690538406372\n",
      "step: 10590 , time : 0.0\n",
      "train: loss: 88915.171875 acc: 0.9854929447174072  val: loss: 660348.3125 acc: 0.9394403100013733\n",
      "step: 10595 , time : 0.0\n",
      "train: loss: 50984.09765625 acc: 0.9793993830680847  val: loss: 525620.125 acc: 0.9451583027839661\n",
      "step: 10600 , time : 0.0\n",
      "train: loss: 136196.15625 acc: 0.9584821462631226  val: loss: 367879.84375 acc: 0.9639757871627808\n",
      "step: 10605 , time : 0.0\n",
      "train: loss: 102660.7890625 acc: 0.964503288269043  val: loss: 697465.625 acc: 0.9302370548248291\n",
      "step: 10610 , time : 0.0\n",
      "train: loss: 405472.03125 acc: 0.9057505130767822  val: loss: 1434884.0 acc: 0.6098909378051758\n",
      "step: 10615 , time : 0.0\n",
      "train: loss: 194161.796875 acc: 0.840111494064331  val: loss: 1221406.625 acc: 0.9010238647460938\n",
      "step: 10620 , time : 0.0\n",
      "train: loss: 319208.34375 acc: 0.8609851002693176  val: loss: 628891.125 acc: 0.9396594166755676\n",
      "step: 10625 , time : 0.0010001659393310547\n",
      "train: loss: 181946.640625 acc: 0.9611916542053223  val: loss: 1954293.25 acc: 0.23054087162017822\n",
      "step: 10630 , time : 0.0\n",
      "train: loss: 90699.96875 acc: 0.9901999831199646  val: loss: 1657816.25 acc: 0.34881871938705444\n",
      "step: 10635 , time : 0.0\n",
      "train: loss: 84789.8359375 acc: 0.9903411865234375  val: loss: 1146919.375 acc: 0.6275765895843506\n",
      "step: 10640 , time : 0.0010008811950683594\n",
      "train: loss: 266139.71875 acc: 0.9608972072601318  val: loss: 1760826.25 acc: 0.814538836479187\n",
      "step: 10645 , time : 0.0\n",
      "train: loss: 290544.0 acc: 0.9230462312698364  val: loss: 835856.1875 acc: 0.9005531072616577\n",
      "step: 10650 , time : 0.0\n",
      "train: loss: 274739.625 acc: 0.9810085892677307  val: loss: 564851.375 acc: 0.8506096601486206\n",
      "step: 10655 , time : 0.0\n",
      "train: loss: 524847.375 acc: 0.9578314423561096  val: loss: 1190924.75 acc: 0.8556234240531921\n",
      "step: 10660 , time : 0.0\n",
      "train: loss: 273529.5625 acc: 0.96477872133255  val: loss: 791982.8125 acc: 0.9102018475532532\n",
      "step: 10665 , time : 0.0\n",
      "train: loss: 666444.8125 acc: 0.9422451257705688  val: loss: 1796931.0 acc: 0.7790433168411255\n",
      "step: 10670 , time : 0.0\n",
      "train: loss: 832021.0625 acc: 0.9716121554374695  val: loss: 3299985.25 acc: 0.3926005959510803\n",
      "step: 10675 , time : 0.0\n",
      "train: loss: 1172314.0 acc: 0.9542360901832581  val: loss: 2278012.75 acc: 0.8140550851821899\n",
      "step: 10680 , time : 0.0\n",
      "train: loss: 320778.78125 acc: 0.9836246967315674  val: loss: 1520510.75 acc: 0.5203214883804321\n",
      "step: 10685 , time : 0.0010004043579101562\n",
      "train: loss: 216592.875 acc: 0.9836897850036621  val: loss: 789589.5625 acc: 0.7797431945800781\n",
      "step: 10690 , time : 0.0\n",
      "train: loss: 299932.125 acc: 0.9565317034721375  val: loss: 586076.3125 acc: 0.8751025199890137\n",
      "step: 10695 , time : 0.0\n",
      "train: loss: 834314.4375 acc: 0.9429651498794556  val: loss: 917902.5 acc: 0.8577457070350647\n",
      "step: 10700 , time : 0.0\n",
      "train: loss: 725035.3125 acc: 0.6528487801551819  val: loss: 1766382.75 acc: 0.7038984298706055\n",
      "step: 10705 , time : 0.0\n",
      "train: loss: 1898668.875 acc: 0.7458637356758118  val: loss: 1150181.25 acc: 0.7254748344421387\n",
      "step: 10710 , time : 0.0\n",
      "train: loss: 1003254.0 acc: 0.811371386051178  val: loss: 1382047.75 acc: 0.6723005771636963\n",
      "step: 10715 , time : 0.0\n",
      "train: loss: 619918.4375 acc: 0.7802643775939941  val: loss: 620453.625 acc: 0.9152280688285828\n",
      "step: 10720 , time : 0.0\n",
      "train: loss: 443078.90625 acc: 0.9075561761856079  val: loss: 1277505.75 acc: 0.6122561097145081\n",
      "step: 10725 , time : 0.0\n",
      "train: loss: 1657395.375 acc: 0.1134498119354248  val: loss: 847068.0 acc: 0.7252039909362793\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 10730 , time : 0.0\n",
      "train: loss: 693929.3125 acc: 0.43200790882110596  val: loss: 6823660.5 acc: -0.14909160137176514\n",
      "step: 10735 , time : 0.0010008811950683594\n",
      "train: loss: 900682.5 acc: 0.2795092463493347  val: loss: 2451318.25 acc: 0.47575831413269043\n",
      "step: 10740 , time : 0.0010008811950683594\n",
      "train: loss: 652969.625 acc: 0.6485355496406555  val: loss: 1040394.75 acc: 0.756284236907959\n",
      "step: 10745 , time : 0.0\n",
      "train: loss: 904926.0625 acc: 0.2811279892921448  val: loss: 1445016.5 acc: 0.7467153668403625\n",
      "step: 10750 , time : 0.0\n",
      "train: loss: 221333.09375 acc: 0.8437440991401672  val: loss: 946380.0 acc: 0.5982121229171753\n",
      "step: 10755 , time : 0.0\n",
      "train: loss: 387596.875 acc: 0.7405662536621094  val: loss: 1833015.0 acc: 0.8218485116958618\n",
      "step: 10760 , time : 0.0\n",
      "train: loss: 97883.5703125 acc: 0.9132700562477112  val: loss: 837842.875 acc: 0.6624369621276855\n",
      "step: 10765 , time : 0.0\n",
      "train: loss: 89843.078125 acc: 0.9325060844421387  val: loss: 1039352.5 acc: 0.7253316044807434\n",
      "step: 10770 , time : 0.0\n",
      "train: loss: 269185.78125 acc: 0.8462072610855103  val: loss: 710798.125 acc: 0.7946277856826782\n",
      "step: 10775 , time : 0.0\n",
      "train: loss: 70076.4375 acc: 0.9168068766593933  val: loss: 968627.125 acc: 0.8048853874206543\n",
      "step: 10780 , time : 0.0\n",
      "train: loss: 176766.5625 acc: 0.8271922469139099  val: loss: 854648.125 acc: 0.7252530455589294\n",
      "step: 10785 , time : 0.0\n",
      "train: loss: 338668.625 acc: 0.7806288003921509  val: loss: 321332.4375 acc: 0.812325119972229\n",
      "step: 10790 , time : 0.0\n",
      "train: loss: 446698.84375 acc: 0.5804240703582764  val: loss: 1062566.5 acc: 0.7711832523345947\n",
      "step: 10795 , time : 0.015625715255737305\n",
      "train: loss: 869334.875 acc: 0.4667290449142456  val: loss: 988486.5 acc: 0.783610463142395\n",
      "step: 10800 , time : 0.0\n",
      "train: loss: 1733891.25 acc: 0.012164413928985596  val: loss: 754585.0 acc: 0.777079701423645\n",
      "step: 10805 , time : 9.846687316894531e-05\n",
      "train: loss: 888850.1875 acc: 0.5894558429718018  val: loss: 1097331.0 acc: 0.5843610763549805\n",
      "step: 10810 , time : 0.0010006427764892578\n",
      "train: loss: 1205911.25 acc: 0.5829368829727173  val: loss: 755568.375 acc: 0.7995226979255676\n",
      "step: 10815 , time : 0.000125885009765625\n",
      "train: loss: 1166532.75 acc: 0.8131832480430603  val: loss: 608762.1875 acc: 0.8417106866836548\n",
      "step: 10820 , time : 0.0\n",
      "train: loss: 1259928.625 acc: 0.8639877438545227  val: loss: 419054.0 acc: 0.8885326385498047\n",
      "step: 10825 , time : 0.0\n",
      "train: loss: 575097.1875 acc: 0.9567394852638245  val: loss: 619347.0 acc: 0.7596297860145569\n",
      "step: 10830 , time : 0.0\n",
      "train: loss: 1184082.375 acc: 0.8717824816703796  val: loss: 533505.25 acc: 0.7512295246124268\n",
      "step: 10835 , time : 0.0\n",
      "train: loss: 394578.625 acc: 0.9522405862808228  val: loss: 440039.75 acc: 0.9342266917228699\n",
      "step: 10840 , time : 0.0\n",
      "train: loss: 203739.671875 acc: 0.9611190557479858  val: loss: 852703.1875 acc: 0.8024054765701294\n",
      "step: 10845 , time : 0.0\n",
      "train: loss: 737220.5625 acc: 0.9205427169799805  val: loss: 407483.1875 acc: 0.9207841157913208\n",
      "step: 10850 , time : 0.0010006427764892578\n",
      "train: loss: 195254.5625 acc: 0.9832602739334106  val: loss: 775371.75 acc: 0.8970187902450562\n",
      "step: 10855 , time : 0.0\n",
      "train: loss: 265361.6875 acc: 0.9774647951126099  val: loss: 923578.25 acc: 0.8175522089004517\n",
      "step: 10860 , time : 0.0\n",
      "train: loss: 244173.734375 acc: 0.9676223993301392  val: loss: 1729559.0 acc: 0.7369557619094849\n",
      "step: 10865 , time : 0.0010008811950683594\n",
      "train: loss: 155902.046875 acc: 0.9578216075897217  val: loss: 764590.5 acc: 0.9005486965179443\n",
      "step: 10870 , time : 0.0\n",
      "train: loss: 54753.328125 acc: 0.9829257130622864  val: loss: 1423489.625 acc: 0.7840863466262817\n",
      "step: 10875 , time : 0.0\n",
      "train: loss: 48390.78125 acc: 0.9656977653503418  val: loss: 678368.375 acc: 0.9416728019714355\n",
      "step: 10880 , time : 0.0\n",
      "train: loss: 27325.5859375 acc: 0.978927731513977  val: loss: 296829.625 acc: 0.9613083004951477\n",
      "step: 10885 , time : 0.0\n",
      "train: loss: 20530.6953125 acc: 0.9590758681297302  val: loss: 2095743.5 acc: 0.7507308125495911\n",
      "step: 10890 , time : 0.0\n",
      "train: loss: 13233.9423828125 acc: 0.9908432960510254  val: loss: 821926.5625 acc: 0.9218588471412659\n",
      "step: 10895 , time : 0.0\n",
      "train: loss: 7013.859375 acc: 0.9708043932914734  val: loss: 604236.6875 acc: 0.956885814666748\n",
      "step: 10900 , time : 0.0\n",
      "train: loss: 36398.5625 acc: 0.9442060589790344  val: loss: 200661.421875 acc: 0.9333938360214233\n",
      "step: 10905 , time : 0.0\n",
      "train: loss: 9166.515625 acc: 0.9805474281311035  val: loss: 2027184.75 acc: 0.7349103093147278\n",
      "step: 10910 , time : 0.0\n",
      "train: loss: 23584.6171875 acc: 0.9743388891220093  val: loss: 2224588.75 acc: 0.6667059659957886\n",
      "step: 10915 , time : 0.0\n",
      "train: loss: 100806.4765625 acc: 0.8579661846160889  val: loss: 499743.1875 acc: 0.7472871541976929\n",
      "step: 10920 , time : 0.0\n",
      "train: loss: 157546.390625 acc: 0.9246221780776978  val: loss: 2032318.5 acc: 0.7420995235443115\n",
      "step: 10925 , time : 0.0\n",
      "train: loss: 49369.90234375 acc: 0.9700387120246887  val: loss: 644147.875 acc: 0.713982105255127\n",
      "step: 10930 , time : 0.0\n",
      "train: loss: 44807.1953125 acc: 0.980323851108551  val: loss: 743510.5 acc: 0.9369170069694519\n",
      "step: 10935 , time : 0.0\n",
      "train: loss: 81225.515625 acc: 0.9640916585922241  val: loss: 1778123.5 acc: 0.6868377923965454\n",
      "step: 10940 , time : 0.0\n",
      "train: loss: 66974.953125 acc: 0.9571540355682373  val: loss: 1076116.5 acc: 0.8532102704048157\n",
      "step: 10945 , time : 0.0\n",
      "train: loss: 44953.19921875 acc: 0.9566469788551331  val: loss: 1754878.75 acc: -0.1942530870437622\n",
      "step: 10950 , time : 0.0\n",
      "train: loss: 28926.88671875 acc: 0.9851845502853394  val: loss: 1052867.75 acc: 0.7739887237548828\n",
      "step: 10955 , time : 0.015625953674316406\n",
      "train: loss: 46192.3515625 acc: 0.9848963618278503  val: loss: 924249.5625 acc: 0.8024771213531494\n",
      "step: 10960 , time : 0.0\n",
      "train: loss: 75456.0390625 acc: 0.9851185083389282  val: loss: 1875310.5 acc: 0.37259870767593384\n",
      "step: 10965 , time : 0.0\n",
      "train: loss: 53829.5546875 acc: 0.9716126322746277  val: loss: 2273158.5 acc: 0.7443909049034119\n",
      "step: 10970 , time : 0.0\n",
      "train: loss: 54602.453125 acc: 0.9672405123710632  val: loss: 1778671.125 acc: 0.5938389301300049\n",
      "step: 10975 , time : 0.0010004043579101562\n",
      "train: loss: 126314.1640625 acc: 0.960089385509491  val: loss: 1116288.125 acc: 0.8211379647254944\n",
      "step: 10980 , time : 0.0\n",
      "train: loss: 111998.6640625 acc: 0.9287393093109131  val: loss: 1520876.125 acc: 0.6475862264633179\n",
      "step: 10985 , time : 0.0\n",
      "train: loss: 139041.71875 acc: 0.9465572834014893  val: loss: 3707515.0 acc: 0.16982442140579224\n",
      "step: 10990 , time : 0.0\n",
      "train: loss: 447305.21875 acc: 0.9419575929641724  val: loss: 421419.84375 acc: 0.9292081594467163\n",
      "step: 10995 , time : 0.0\n",
      "train: loss: 217524.640625 acc: 0.9821049571037292  val: loss: 1466732.875 acc: 0.6491806507110596\n",
      "step: 11000 , time : 0.0\n",
      "train: loss: 137237.171875 acc: 0.9677448868751526  val: loss: 1426151.125 acc: 0.706027090549469\n",
      "step: 11005 , time : 0.0\n",
      "train: loss: 132349.5625 acc: 0.9851503372192383  val: loss: 311182.65625 acc: 0.9665846824645996\n",
      "step: 11010 , time : 0.0\n",
      "train: loss: 181177.140625 acc: 0.9677678942680359  val: loss: 3519706.75 acc: 0.5046576261520386\n",
      "step: 11015 , time : 0.0\n",
      "train: loss: 643877.0625 acc: 0.9653712511062622  val: loss: 1036078.3125 acc: 0.7810367345809937\n",
      "step: 11020 , time : 0.0\n",
      "train: loss: 544422.6875 acc: 0.9465545415878296  val: loss: 657935.75 acc: 0.9192934632301331\n",
      "step: 11025 , time : 0.0\n",
      "train: loss: 680992.8125 acc: 0.9269943237304688  val: loss: 700260.8125 acc: 0.8581738471984863\n",
      "step: 11030 , time : 0.0\n",
      "train: loss: 495984.15625 acc: 0.9795145988464355  val: loss: 414763.8125 acc: 0.8993920683860779\n",
      "step: 11035 , time : 0.0\n",
      "train: loss: 1363254.625 acc: 0.9324668049812317  val: loss: 463249.0 acc: 0.7446770668029785\n",
      "step: 11040 , time : 0.0\n",
      "train: loss: 723792.5 acc: 0.9638155102729797  val: loss: 370465.96875 acc: 0.8295019865036011\n",
      "step: 11045 , time : 0.0\n",
      "train: loss: 1309053.25 acc: 0.9446069002151489  val: loss: 242801.875 acc: 0.9118762016296387\n",
      "step: 11050 , time : 0.0\n",
      "train: loss: 937172.8125 acc: 0.9595850110054016  val: loss: 1278993.5 acc: 0.7524431943893433\n",
      "step: 11055 , time : 0.0\n",
      "train: loss: 348659.46875 acc: 0.9696574211120605  val: loss: 196886.921875 acc: 0.9395437240600586\n",
      "step: 11060 , time : 0.0\n",
      "train: loss: 483947.5 acc: 0.9312489032745361  val: loss: 567071.375 acc: 0.7169731855392456\n",
      "step: 11065 , time : 0.0\n",
      "train: loss: 409564.15625 acc: 0.8872804045677185  val: loss: 548001.25 acc: 0.7630113959312439\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 11070 , time : 0.0\n",
      "train: loss: 1659561.0 acc: 0.2802666425704956  val: loss: 1474848.375 acc: 0.28088879585266113\n",
      "step: 11075 , time : 0.0\n",
      "train: loss: 866260.875 acc: 0.6105555295944214  val: loss: 2302423.0 acc: 0.6898300647735596\n",
      "step: 11080 , time : 0.0\n",
      "train: loss: 550470.25 acc: 0.7353962659835815  val: loss: 976503.4375 acc: 0.8082321882247925\n",
      "step: 11085 , time : 0.0\n",
      "train: loss: 494761.125 acc: 0.8469105362892151  val: loss: 827779.875 acc: 0.6059466600418091\n",
      "step: 11090 , time : 0.0010004043579101562\n",
      "train: loss: 1341014.375 acc: 0.6819993257522583  val: loss: 631329.5 acc: 0.7939751744270325\n",
      "step: 11095 , time : 0.0\n",
      "train: loss: 1048165.125 acc: 0.5752436518669128  val: loss: 2266150.5 acc: 0.3983815908432007\n",
      "step: 11100 , time : 0.0\n",
      "train: loss: 896360.75 acc: 0.27660101652145386  val: loss: 2432375.25 acc: 0.19508087635040283\n",
      "step: 11105 , time : 0.0\n",
      "train: loss: 1012783.125 acc: 0.6702802181243896  val: loss: 2581036.25 acc: 0.48346275091171265\n",
      "step: 11110 , time : 0.015624523162841797\n",
      "train: loss: 630599.5 acc: 0.6506725549697876  val: loss: 1291759.125 acc: 0.779394268989563\n",
      "step: 11115 , time : 0.0\n",
      "train: loss: 171516.078125 acc: 0.8544859886169434  val: loss: 989513.75 acc: 0.6668399572372437\n",
      "step: 11120 , time : 0.0004942417144775391\n",
      "train: loss: 305884.65625 acc: 0.8118219971656799  val: loss: 899387.75 acc: 0.6760291457176208\n",
      "step: 11125 , time : 0.0\n",
      "train: loss: 152752.6875 acc: 0.8604335784912109  val: loss: 811844.75 acc: 0.7733453512191772\n",
      "step: 11130 , time : 0.0\n",
      "train: loss: 234283.59375 acc: 0.8255090713500977  val: loss: 1724210.75 acc: 0.7424206733703613\n",
      "step: 11135 , time : 0.0\n",
      "train: loss: 260362.5625 acc: 0.8663254976272583  val: loss: 473801.4375 acc: 0.7897354960441589\n",
      "step: 11140 , time : 0.0\n",
      "train: loss: 82907.46875 acc: 0.9113211035728455  val: loss: 132761.359375 acc: 0.8956369161605835\n",
      "step: 11145 , time : 0.0\n",
      "train: loss: 108946.2578125 acc: 0.8880132436752319  val: loss: 695331.375 acc: 0.8076035380363464\n",
      "step: 11150 , time : 0.0\n",
      "train: loss: 143738.09375 acc: 0.8754214644432068  val: loss: 443527.75 acc: 0.8810825347900391\n",
      "step: 11155 , time : 0.0\n",
      "train: loss: 466407.8125 acc: 0.7309613227844238  val: loss: 1061696.25 acc: 0.7581786513328552\n",
      "step: 11160 , time : 0.0\n",
      "train: loss: 1113170.5 acc: 0.5571008920669556  val: loss: 1117277.125 acc: 0.7567635774612427\n",
      "step: 11165 , time : 0.0\n",
      "train: loss: 992316.6875 acc: 0.30503493547439575  val: loss: 1066504.625 acc: 0.7782608270645142\n",
      "step: 11170 , time : 0.0\n",
      "train: loss: 947976.25 acc: 0.2301880121231079  val: loss: 3471336.5 acc: 0.7616369128227234\n",
      "step: 11175 , time : 0.0\n",
      "train: loss: 817476.4375 acc: 0.6835585832595825  val: loss: 3943652.75 acc: 0.7994374632835388\n",
      "step: 11180 , time : 0.0\n",
      "train: loss: 556536.0625 acc: 0.798268735408783  val: loss: 2779424.0 acc: 0.7992610931396484\n",
      "step: 11185 , time : 0.0\n",
      "train: loss: 948354.0 acc: 0.7919538021087646  val: loss: 1017770.0625 acc: 0.8702889680862427\n",
      "step: 11190 , time : 0.0\n",
      "train: loss: 903746.8125 acc: 0.8646374940872192  val: loss: 2063601.5 acc: 0.8296303749084473\n",
      "step: 11195 , time : 0.0\n",
      "train: loss: 323942.46875 acc: 0.9699243307113647  val: loss: 1463739.75 acc: 0.8639105558395386\n",
      "step: 11200 , time : 0.0010008811950683594\n",
      "train: loss: 219676.171875 acc: 0.9665864706039429  val: loss: 2018623.25 acc: 0.7098298668861389\n",
      "step: 11205 , time : 0.0\n",
      "train: loss: 420099.875 acc: 0.9431717991828918  val: loss: 342930.6875 acc: 0.907846212387085\n",
      "step: 11210 , time : 0.0010006427764892578\n",
      "train: loss: 245856.9375 acc: 0.9744890332221985  val: loss: 1163823.75 acc: 0.7645326852798462\n",
      "step: 11215 , time : 0.0\n",
      "train: loss: 313533.125 acc: 0.974774181842804  val: loss: 1835232.25 acc: 0.5492525100708008\n",
      "step: 11220 , time : 0.0\n",
      "train: loss: 167031.125 acc: 0.9895691275596619  val: loss: 2944739.75 acc: 0.44425857067108154\n",
      "step: 11225 , time : 0.0\n",
      "train: loss: 226161.546875 acc: 0.9715477824211121  val: loss: 2671504.75 acc: 0.7004692554473877\n",
      "step: 11230 , time : 0.0\n",
      "train: loss: 306502.15625 acc: 0.9625369906425476  val: loss: 745513.4375 acc: 0.881240963935852\n",
      "step: 11235 , time : 0.0\n",
      "train: loss: 44968.28515625 acc: 0.9835644960403442  val: loss: 365264.9375 acc: 0.8737217783927917\n",
      "step: 11240 , time : 0.0\n",
      "train: loss: 68059.359375 acc: 0.920653223991394  val: loss: 621895.375 acc: 0.9374067783355713\n",
      "step: 11245 , time : 0.0\n",
      "train: loss: 60412.35546875 acc: 0.9785860776901245  val: loss: 2345222.75 acc: 0.18154561519622803\n",
      "step: 11250 , time : 0.0\n",
      "train: loss: 200069.109375 acc: 0.9047371745109558  val: loss: 662595.0 acc: 0.8055150508880615\n",
      "step: 11255 , time : 0.0\n",
      "train: loss: 24176.083984375 acc: 0.9877816438674927  val: loss: 1278202.5 acc: -1.1271893978118896\n",
      "step: 11260 , time : 0.0\n",
      "train: loss: 26279.05078125 acc: 0.9360212087631226  val: loss: 1928372.25 acc: 0.6499197483062744\n",
      "step: 11265 , time : 0.0\n",
      "train: loss: 22007.794921875 acc: 0.8948195576667786  val: loss: 1207140.125 acc: 0.74613356590271\n",
      "step: 11270 , time : 0.0\n",
      "train: loss: 20450.134765625 acc: 0.9494222402572632  val: loss: 3830842.0 acc: 0.6665918827056885\n",
      "step: 11275 , time : 0.0\n",
      "train: loss: 44313.81640625 acc: 0.9520348906517029  val: loss: 1203364.375 acc: 0.8391216993331909\n",
      "step: 11280 , time : 0.0\n",
      "train: loss: 19871.546875 acc: 0.9475929141044617  val: loss: 2236265.75 acc: 0.046580374240875244\n",
      "step: 11285 , time : 0.0\n",
      "train: loss: 69883.484375 acc: 0.9585928320884705  val: loss: 3188362.75 acc: -0.13088572025299072\n",
      "step: 11290 , time : 0.0\n",
      "train: loss: 177374.484375 acc: 0.9374622106552124  val: loss: 430576.15625 acc: 0.8791372179985046\n",
      "step: 11295 , time : 0.0\n",
      "train: loss: 19662.66796875 acc: 0.9840700030326843  val: loss: 650400.0 acc: 0.8185473680496216\n",
      "step: 11300 , time : 0.0\n",
      "train: loss: 80088.875 acc: 0.9617613554000854  val: loss: 1101765.0 acc: 0.8082107305526733\n",
      "step: 11305 , time : 0.0010008811950683594\n",
      "train: loss: 16220.9921875 acc: 0.9783461689949036  val: loss: 1231944.5 acc: 0.7031453847885132\n",
      "step: 11310 , time : 0.0\n",
      "train: loss: 47078.18359375 acc: 0.9622645974159241  val: loss: 316032.53125 acc: 0.9413905739784241\n",
      "step: 11315 , time : 0.0\n",
      "train: loss: 38575.45703125 acc: 0.9729340076446533  val: loss: 1758763.5 acc: 0.8363475799560547\n",
      "step: 11320 , time : 0.0\n",
      "train: loss: 46747.62890625 acc: 0.9851741790771484  val: loss: 1343701.0 acc: 0.8104100227355957\n",
      "step: 11325 , time : 0.0\n",
      "train: loss: 168752.875 acc: 0.9544300436973572  val: loss: 1428467.75 acc: 0.7699894905090332\n",
      "step: 11330 , time : 0.0\n",
      "train: loss: 38382.94140625 acc: 0.9749178886413574  val: loss: 3095871.25 acc: 0.3427790403366089\n",
      "step: 11335 , time : 0.0\n",
      "train: loss: 91261.3359375 acc: 0.9726582765579224  val: loss: 2860118.75 acc: 0.1988421082496643\n",
      "step: 11340 , time : 0.0\n",
      "train: loss: 165395.40625 acc: 0.9503210186958313  val: loss: 742707.8125 acc: 0.9220103621482849\n",
      "step: 11345 , time : 0.0\n",
      "train: loss: 543287.4375 acc: 0.7698270082473755  val: loss: 674126.25 acc: 0.368023157119751\n",
      "step: 11350 , time : 0.0\n",
      "train: loss: 168493.65625 acc: 0.9343371987342834  val: loss: 898481.4375 acc: 0.8932836651802063\n",
      "step: 11355 , time : 0.0\n",
      "train: loss: 415158.21875 acc: 0.8990355730056763  val: loss: 707099.6875 acc: 0.8106138706207275\n",
      "step: 11360 , time : 0.0\n",
      "train: loss: 1033478.8125 acc: 0.9102550148963928  val: loss: 350300.78125 acc: 0.9011401534080505\n",
      "step: 11365 , time : 0.0\n",
      "train: loss: 257215.640625 acc: 0.9660618901252747  val: loss: 1353360.5 acc: 0.5832710266113281\n",
      "step: 11370 , time : 0.0\n",
      "train: loss: 140689.609375 acc: 0.9809756278991699  val: loss: 417952.9375 acc: 0.8751649260520935\n",
      "step: 11375 , time : 0.0\n",
      "train: loss: 338057.0625 acc: 0.9619073271751404  val: loss: 854989.3125 acc: 0.8884371519088745\n",
      "step: 11380 , time : 0.0\n",
      "train: loss: 325482.375 acc: 0.9692469239234924  val: loss: 1569514.25 acc: 0.662825345993042\n",
      "step: 11385 , time : 0.0\n",
      "train: loss: 649007.25 acc: 0.9684760570526123  val: loss: 902944.375 acc: 0.8023386001586914\n",
      "step: 11390 , time : 0.0\n",
      "train: loss: 294360.625 acc: 0.9706413149833679  val: loss: 1486168.625 acc: 0.522121012210846\n",
      "step: 11395 , time : 0.0\n",
      "train: loss: 667139.5625 acc: 0.9252514243125916  val: loss: 1177881.75 acc: 0.4350353479385376\n",
      "step: 11400 , time : 0.0\n",
      "train: loss: 1028192.25 acc: 0.9634099006652832  val: loss: 1418130.625 acc: 0.31703048944473267\n",
      "step: 11405 , time : 0.0\n",
      "train: loss: 1520475.5 acc: 0.9482945203781128  val: loss: 2304882.25 acc: 0.2716951370239258\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 11410 , time : 0.0\n",
      "train: loss: 768495.375 acc: 0.9637647271156311  val: loss: 931317.5625 acc: 0.831519365310669\n",
      "step: 11415 , time : 0.0010006427764892578\n",
      "train: loss: 686807.25 acc: 0.9512328505516052  val: loss: 409378.90625 acc: 0.8645297288894653\n",
      "step: 11420 , time : 0.0\n",
      "train: loss: 380510.5 acc: 0.9473446607589722  val: loss: 400283.65625 acc: 0.8275327682495117\n",
      "step: 11425 , time : 0.0\n",
      "train: loss: 561874.5625 acc: 0.9556494951248169  val: loss: 422385.40625 acc: 0.9547244906425476\n",
      "step: 11430 , time : 0.0\n",
      "train: loss: 576734.1875 acc: 0.8604865074157715  val: loss: 1182474.75 acc: 0.7612452507019043\n",
      "step: 11435 , time : 0.0\n",
      "train: loss: 1628444.75 acc: -0.1492757797241211  val: loss: 1855931.125 acc: 0.5665935277938843\n",
      "step: 11440 , time : 0.0\n",
      "train: loss: 1508117.5 acc: 0.19523310661315918  val: loss: 1991221.25 acc: 0.6123701333999634\n",
      "step: 11445 , time : 0.0\n",
      "train: loss: 1216033.25 acc: 0.4421340823173523  val: loss: 1175217.375 acc: 0.7995972037315369\n",
      "step: 11450 , time : 0.0\n",
      "train: loss: 614320.0 acc: 0.7665449380874634  val: loss: 595992.6875 acc: 0.8807644844055176\n",
      "step: 11455 , time : 0.0\n",
      "train: loss: 1139162.0 acc: 0.7064065933227539  val: loss: 459870.53125 acc: 0.8589235544204712\n",
      "step: 11460 , time : 0.0\n",
      "train: loss: 2410294.0 acc: -0.1788238286972046  val: loss: 2245144.0 acc: 0.5370847582817078\n",
      "step: 11465 , time : 0.0\n",
      "train: loss: 1386211.625 acc: 0.6708246469497681  val: loss: 1525059.875 acc: 0.6190499067306519\n",
      "step: 11470 , time : 0.0\n",
      "train: loss: 1049435.125 acc: 0.4900131821632385  val: loss: 1579031.875 acc: 0.7004085779190063\n",
      "step: 11475 , time : 0.015625476837158203\n",
      "train: loss: 649065.875 acc: 0.5078730583190918  val: loss: 1898242.75 acc: 0.8109802603721619\n",
      "step: 11480 , time : 0.0\n",
      "train: loss: 201421.59375 acc: 0.8238608837127686  val: loss: 749444.5 acc: 0.9221683740615845\n",
      "step: 11485 , time : 0.0\n",
      "train: loss: 381851.75 acc: 0.715613842010498  val: loss: 1851082.0 acc: 0.7644780874252319\n",
      "step: 11490 , time : 0.0\n",
      "train: loss: 260553.75 acc: 0.8428252339363098  val: loss: 782356.125 acc: 0.7016718983650208\n",
      "step: 11495 , time : 0.0\n",
      "train: loss: 305141.53125 acc: 0.8162465691566467  val: loss: 1102649.375 acc: 0.8314104080200195\n",
      "step: 11500 , time : 0.0\n",
      "train: loss: 242846.21875 acc: 0.7721041440963745  val: loss: 528375.0 acc: 0.7138731479644775\n",
      "step: 11505 , time : 0.0\n",
      "train: loss: 193647.765625 acc: 0.8730844259262085  val: loss: 1534230.375 acc: 0.8764520883560181\n",
      "step: 11510 , time : 0.0\n",
      "train: loss: 385834.875 acc: 0.6851069927215576  val: loss: 1659014.375 acc: 0.8498274683952332\n",
      "step: 11515 , time : 0.0\n",
      "train: loss: 277101.84375 acc: 0.8306432366371155  val: loss: 312119.875 acc: 0.9076790809631348\n",
      "step: 11520 , time : 0.0\n",
      "train: loss: 377412.5625 acc: 0.5511649250984192  val: loss: 859953.25 acc: 0.851848840713501\n",
      "step: 11525 , time : 0.0\n",
      "train: loss: 204497.359375 acc: 0.8499523401260376  val: loss: 2631116.25 acc: 0.8502174019813538\n",
      "step: 11530 , time : 0.0\n",
      "train: loss: 1179108.0 acc: 0.4557437300682068  val: loss: 1781302.0 acc: 0.6681647300720215\n",
      "step: 11535 , time : 0.0\n",
      "train: loss: 696499.25 acc: 0.6646599173545837  val: loss: 291163.59375 acc: 0.9224178791046143\n",
      "step: 11540 , time : 0.0\n",
      "train: loss: 527686.25 acc: 0.7438185811042786  val: loss: 946007.1875 acc: 0.6663639545440674\n",
      "step: 11545 , time : 0.0010008811950683594\n",
      "train: loss: 650599.875 acc: 0.8139060735702515  val: loss: 1543266.25 acc: 0.7868059277534485\n",
      "step: 11550 , time : 0.0\n",
      "train: loss: 645395.375 acc: 0.8781470060348511  val: loss: 738375.5 acc: 0.913101315498352\n",
      "step: 11555 , time : 0.0\n",
      "train: loss: 1850538.25 acc: 0.7728956341743469  val: loss: 1437109.75 acc: 0.8509417772293091\n",
      "step: 11560 , time : 0.0\n",
      "train: loss: 513716.6875 acc: 0.9546190500259399  val: loss: 705412.0 acc: 0.7696442604064941\n",
      "step: 11565 , time : 0.0\n",
      "train: loss: 583054.125 acc: 0.8605960607528687  val: loss: 453191.4375 acc: 0.8417322635650635\n",
      "step: 11570 , time : 0.0\n",
      "train: loss: 341332.6875 acc: 0.9536507725715637  val: loss: 735690.6875 acc: 0.6727606058120728\n",
      "step: 11575 , time : 0.0\n",
      "train: loss: 228035.78125 acc: 0.9729537963867188  val: loss: 966216.0 acc: 0.5083451867103577\n",
      "step: 11580 , time : 0.0\n",
      "train: loss: 567383.6875 acc: 0.9529011249542236  val: loss: 908649.0625 acc: 0.6788643002510071\n",
      "step: 11585 , time : 0.0\n",
      "train: loss: 804023.75 acc: 0.9230548739433289  val: loss: 2314935.5 acc: 0.006799876689910889\n",
      "step: 11590 , time : 0.0\n",
      "train: loss: 269118.75 acc: 0.9573509693145752  val: loss: 1856761.25 acc: 0.1556851863861084\n",
      "step: 11595 , time : 0.0\n",
      "train: loss: 290636.40625 acc: 0.9400646090507507  val: loss: 1004848.75 acc: 0.91438889503479\n",
      "step: 11600 , time : 0.0\n",
      "train: loss: 50590.11328125 acc: 0.9786189198493958  val: loss: 941763.9375 acc: 0.8688697218894958\n",
      "step: 11605 , time : 0.0\n",
      "train: loss: 51499.625 acc: 0.9743101000785828  val: loss: 532697.25 acc: 0.9283657670021057\n",
      "step: 11610 , time : 0.0\n",
      "train: loss: 136634.625 acc: 0.9478687644004822  val: loss: 1125414.5 acc: 0.8501460552215576\n",
      "step: 11615 , time : 0.0\n",
      "train: loss: 11642.1708984375 acc: 0.9755066633224487  val: loss: 1015597.1875 acc: 0.7764775156974792\n",
      "step: 11620 , time : 0.0010004043579101562\n",
      "train: loss: 35265.25 acc: 0.9704816937446594  val: loss: 1564312.25 acc: 0.8087329864501953\n",
      "step: 11625 , time : 0.0010004043579101562\n",
      "train: loss: 46410.078125 acc: 0.9539545178413391  val: loss: 659055.125 acc: 0.9021883010864258\n",
      "step: 11630 , time : 0.0\n",
      "train: loss: 22866.32421875 acc: 0.9664598107337952  val: loss: 284315.59375 acc: 0.8856891393661499\n",
      "step: 11635 , time : 0.0010013580322265625\n",
      "train: loss: 17604.751953125 acc: 0.9634062647819519  val: loss: 1080437.0 acc: 0.8762004971504211\n",
      "step: 11640 , time : 0.0\n",
      "train: loss: 14167.30078125 acc: 0.9549617767333984  val: loss: 961619.375 acc: 0.7975877523422241\n",
      "step: 11645 , time : 0.0\n",
      "train: loss: 20768.81640625 acc: 0.9785516858100891  val: loss: 218598.015625 acc: 0.9538170695304871\n",
      "step: 11650 , time : 0.0\n",
      "train: loss: 195121.765625 acc: 0.9238473176956177  val: loss: 757053.875 acc: 0.7176629304885864\n",
      "step: 11655 , time : 0.0\n",
      "train: loss: 40011.60546875 acc: 0.9663833379745483  val: loss: 1071339.0 acc: 0.45001840591430664\n",
      "step: 11660 , time : 0.0\n",
      "train: loss: 85517.8359375 acc: 0.9652743935585022  val: loss: 1649757.0 acc: 0.7570971846580505\n",
      "step: 11665 , time : 0.0\n",
      "train: loss: 37268.4453125 acc: 0.9772096276283264  val: loss: 1287227.875 acc: 0.8076604604721069\n",
      "step: 11670 , time : 0.0\n",
      "train: loss: 25024.720703125 acc: 0.9594525694847107  val: loss: 1023255.5 acc: 0.8371504545211792\n",
      "step: 11675 , time : 0.0\n",
      "train: loss: 29059.619140625 acc: 0.972831130027771  val: loss: 475907.15625 acc: 0.8324738144874573\n",
      "step: 11680 , time : 0.0\n",
      "train: loss: 48781.71875 acc: 0.9767466187477112  val: loss: 1159576.75 acc: 0.7567775249481201\n",
      "step: 11685 , time : 0.0\n",
      "train: loss: 107834.375 acc: 0.9746139049530029  val: loss: 689039.0 acc: 0.8793926239013672\n",
      "step: 11690 , time : 0.0\n",
      "train: loss: 51094.3515625 acc: 0.9891765117645264  val: loss: 519895.09375 acc: 0.8328911066055298\n",
      "step: 11695 , time : 0.0\n",
      "train: loss: 61632.421875 acc: 0.9799160361289978  val: loss: 1941928.375 acc: 0.6172842979431152\n",
      "step: 11700 , time : 0.0\n",
      "train: loss: 27999.599609375 acc: 0.9829157590866089  val: loss: 1098635.625 acc: 0.6743862628936768\n",
      "step: 11705 , time : 0.0010006427764892578\n",
      "train: loss: 120111.5859375 acc: 0.9557662606239319  val: loss: 7039469.5 acc: -0.5033407211303711\n",
      "step: 11710 , time : 0.0010004043579101562\n",
      "train: loss: 178979.96875 acc: 0.9541478753089905  val: loss: 1345333.625 acc: 0.477236270904541\n",
      "step: 11715 , time : 0.0010006427764892578\n",
      "train: loss: 171747.984375 acc: 0.9512779116630554  val: loss: 1057008.125 acc: 0.5419183373451233\n",
      "step: 11720 , time : 0.0\n",
      "train: loss: 308780.28125 acc: 0.9583942294120789  val: loss: 766697.375 acc: 0.6988608837127686\n",
      "step: 11725 , time : 0.0\n",
      "train: loss: 152721.265625 acc: 0.9807406663894653  val: loss: 127052.7734375 acc: 0.9352015852928162\n",
      "step: 11730 , time : 0.0\n",
      "train: loss: 92342.9609375 acc: 0.9903777837753296  val: loss: 1218508.375 acc: 0.5653538703918457\n",
      "step: 11735 , time : 0.0\n",
      "train: loss: 41655.61328125 acc: 0.9939202666282654  val: loss: 277189.96875 acc: 0.9142577648162842\n",
      "step: 11740 , time : 0.0\n",
      "train: loss: 203217.921875 acc: 0.9859693050384521  val: loss: 299070.9375 acc: 0.9259200692176819\n",
      "step: 11745 , time : 0.0\n",
      "train: loss: 283186.875 acc: 0.9723256230354309  val: loss: 427079.8125 acc: 0.884675920009613\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 11750 , time : 0.0\n",
      "train: loss: 613521.9375 acc: 0.9712741374969482  val: loss: 1532000.5 acc: 0.3647559881210327\n",
      "step: 11755 , time : 0.0010008811950683594\n",
      "train: loss: 437022.8125 acc: 0.9700891971588135  val: loss: 671125.0 acc: 0.7843530178070068\n",
      "step: 11760 , time : 0.0010006427764892578\n",
      "train: loss: 245750.453125 acc: 0.9521963596343994  val: loss: 374172.75 acc: 0.8984086513519287\n",
      "step: 11765 , time : 0.0010004043579101562\n",
      "train: loss: 478158.90625 acc: 0.9758759140968323  val: loss: 1078014.75 acc: 0.8199212551116943\n",
      "step: 11770 , time : 0.0\n",
      "train: loss: 2082108.125 acc: 0.9397526383399963  val: loss: 334551.59375 acc: 0.9481074213981628\n",
      "step: 11775 , time : 0.0\n",
      "train: loss: 906982.9375 acc: 0.9622558355331421  val: loss: 237140.171875 acc: 0.9270671606063843\n",
      "step: 11780 , time : 0.0\n",
      "train: loss: 716146.6875 acc: 0.9553723931312561  val: loss: 195593.0625 acc: 0.9345617890357971\n",
      "step: 11785 , time : 0.015624761581420898\n",
      "train: loss: 477202.71875 acc: 0.9584134221076965  val: loss: 1966136.0 acc: 0.6911002397537231\n",
      "step: 11790 , time : 0.0\n",
      "train: loss: 274203.125 acc: 0.9674704670906067  val: loss: 519355.65625 acc: 0.8785080313682556\n",
      "step: 11795 , time : 0.0\n",
      "train: loss: 416334.59375 acc: 0.9583560824394226  val: loss: 332997.03125 acc: 0.9639778137207031\n",
      "step: 11800 , time : 0.0\n",
      "train: loss: 3523464.75 acc: 0.34072422981262207  val: loss: 486092.71875 acc: 0.9090839624404907\n",
      "step: 11805 , time : 0.0\n",
      "train: loss: 1048135.0 acc: 0.578151524066925  val: loss: 2069936.875 acc: 0.8430118560791016\n",
      "step: 11810 , time : 0.0\n",
      "train: loss: 866397.5625 acc: 0.7753700613975525  val: loss: 587397.875 acc: 0.9043965339660645\n",
      "step: 11815 , time : 0.0\n",
      "train: loss: 662341.25 acc: 0.8180214166641235  val: loss: 1110528.5 acc: 0.7409844398498535\n",
      "step: 11820 , time : 0.0\n",
      "train: loss: 568328.75 acc: 0.8775829672813416  val: loss: 781823.875 acc: 0.8887326717376709\n",
      "step: 11825 , time : 0.0\n",
      "train: loss: 1744446.625 acc: 0.07548129558563232  val: loss: 1009670.75 acc: 0.634619951248169\n",
      "step: 11830 , time : 0.0\n",
      "train: loss: 782214.5625 acc: 0.4619308114051819  val: loss: 6507622.5 acc: 0.10054630041122437\n",
      "step: 11835 , time : 0.0\n",
      "train: loss: 536909.875 acc: 0.6509878635406494  val: loss: 1640080.375 acc: 0.6085253953933716\n",
      "step: 11840 , time : 0.0\n",
      "train: loss: 581240.875 acc: 0.6099451780319214  val: loss: 1177900.125 acc: 0.6815319061279297\n",
      "step: 11845 , time : 0.0\n",
      "train: loss: 81977.4453125 acc: 0.9394006729125977  val: loss: 1917298.375 acc: 0.720869779586792\n",
      "step: 11850 , time : 0.0\n",
      "train: loss: 241026.390625 acc: 0.8656145334243774  val: loss: 1339061.625 acc: 0.2759314775466919\n",
      "step: 11855 , time : 0.0\n",
      "train: loss: 544826.125 acc: 0.645648181438446  val: loss: 2163229.5 acc: 0.6363533735275269\n",
      "step: 11860 , time : 0.0\n",
      "train: loss: 476217.625 acc: 0.8905043601989746  val: loss: 1997566.25 acc: 0.7439084053039551\n",
      "step: 11865 , time : 0.0\n",
      "train: loss: 247812.5625 acc: 0.8404306769371033  val: loss: 1934224.375 acc: 0.7916045188903809\n",
      "step: 11870 , time : 0.0010006427764892578\n",
      "train: loss: 189245.640625 acc: 0.8407065868377686  val: loss: 1027874.0625 acc: 0.8369783163070679\n",
      "step: 11875 , time : 0.0010006427764892578\n",
      "train: loss: 189951.96875 acc: 0.7674146890640259  val: loss: 950387.375 acc: 0.8079764246940613\n",
      "step: 11880 , time : 0.0\n",
      "train: loss: 157309.5 acc: 0.8874143362045288  val: loss: 1440675.75 acc: 0.8568934202194214\n",
      "step: 11885 , time : 0.0010004043579101562\n",
      "train: loss: 730463.5625 acc: 0.5630122423171997  val: loss: 651419.75 acc: 0.7626620531082153\n",
      "step: 11890 , time : 0.0\n",
      "train: loss: 1014852.8125 acc: 0.6419578790664673  val: loss: 1761191.0 acc: 0.7488366961479187\n",
      "step: 11895 , time : 0.0\n",
      "train: loss: 998763.125 acc: 0.5831882953643799  val: loss: 864997.875 acc: 0.7526413202285767\n",
      "step: 11900 , time : 0.0\n",
      "train: loss: 94520.203125 acc: 0.903858482837677  val: loss: 1444866.375 acc: 0.748461127281189\n",
      "step: 11905 , time : 0.0\n",
      "train: loss: 504987.71875 acc: 0.7494650483131409  val: loss: 301571.03125 acc: 0.8085356950759888\n",
      "step: 11910 , time : 0.0\n",
      "train: loss: 373765.65625 acc: 0.8499935269355774  val: loss: 1528217.5 acc: 0.8031405210494995\n",
      "step: 11915 , time : 0.0\n",
      "train: loss: 986333.25 acc: 0.8565566539764404  val: loss: 980148.625 acc: 0.7667096257209778\n",
      "step: 11920 , time : 0.0\n",
      "train: loss: 1754464.0 acc: 0.7961787581443787  val: loss: 940606.0625 acc: 0.8605192303657532\n",
      "step: 11925 , time : 0.0\n",
      "train: loss: 461313.59375 acc: 0.9603381156921387  val: loss: 1011397.125 acc: 0.7759345769882202\n",
      "step: 11930 , time : 0.0\n",
      "train: loss: 768978.9375 acc: 0.8600883483886719  val: loss: 1374118.25 acc: 0.7623522281646729\n",
      "step: 11935 , time : 0.0\n",
      "train: loss: 499725.875 acc: 0.9226878881454468  val: loss: 897886.5 acc: 0.884547770023346\n",
      "step: 11940 , time : 0.0\n",
      "train: loss: 280149.0 acc: 0.9569599628448486  val: loss: 610210.125 acc: 0.832823634147644\n",
      "step: 11945 , time : 0.0\n",
      "train: loss: 200348.453125 acc: 0.9816441535949707  val: loss: 608933.375 acc: 0.8562053442001343\n",
      "step: 11950 , time : 0.0\n",
      "train: loss: 277504.0 acc: 0.9818295836448669  val: loss: 2003203.25 acc: 0.3699532151222229\n",
      "step: 11955 , time : 0.0\n",
      "train: loss: 315279.8125 acc: 0.9577597975730896  val: loss: 2697390.0 acc: 0.6889349222183228\n",
      "step: 11960 , time : 0.0\n",
      "train: loss: 253533.890625 acc: 0.9714820981025696  val: loss: 1181876.625 acc: 0.7510375380516052\n",
      "step: 11965 , time : 0.0\n",
      "train: loss: 122980.25 acc: 0.9451313614845276  val: loss: 1216566.125 acc: 0.7689589262008667\n",
      "step: 11970 , time : 0.0\n",
      "train: loss: 146998.828125 acc: 0.9494479298591614  val: loss: 1337155.375 acc: 0.5967048406600952\n",
      "step: 11975 , time : 0.0\n",
      "train: loss: 20202.830078125 acc: 0.943040668964386  val: loss: 800755.625 acc: 0.7412252426147461\n",
      "step: 11980 , time : 0.0\n",
      "train: loss: 12698.318359375 acc: 0.9677761197090149  val: loss: 2023635.375 acc: 0.6422497034072876\n",
      "step: 11985 , time : 0.0010004043579101562\n",
      "train: loss: 54754.46484375 acc: 0.9730115532875061  val: loss: 938834.4375 acc: 0.8705056309700012\n",
      "step: 11990 , time : 0.0\n",
      "train: loss: 120860.046875 acc: 0.8451856970787048  val: loss: 679349.0 acc: 0.7776216268539429\n",
      "step: 11995 , time : 0.0010006427764892578\n",
      "train: loss: 14817.076171875 acc: 0.9722422361373901  val: loss: 526317.25 acc: 0.9022092819213867\n",
      "step: 12000 , time : 0.0010006427764892578\n",
      "train: loss: 11259.865234375 acc: 0.9732224345207214  val: loss: 145284.46875 acc: 0.961800217628479\n",
      "step: 12005 , time : 0.001001119613647461\n",
      "train: loss: 14560.466796875 acc: 0.9603559374809265  val: loss: 812225.75 acc: 0.8255947232246399\n",
      "step: 12010 , time : 0.001001119613647461\n",
      "train: loss: 22129.28515625 acc: 0.9773871898651123  val: loss: 161495.140625 acc: 0.8734948635101318\n",
      "step: 12015 , time : 0.0010006427764892578\n",
      "train: loss: 109821.5 acc: 0.9535511136054993  val: loss: 962458.5625 acc: 0.8052836656570435\n",
      "step: 12020 , time : 0.0\n",
      "train: loss: 47991.8359375 acc: 0.9676934480667114  val: loss: 956835.5 acc: 0.8556219935417175\n",
      "step: 12025 , time : 0.0\n",
      "train: loss: 62639.2734375 acc: 0.968869686126709  val: loss: 272502.96875 acc: 0.9189555644989014\n",
      "step: 12030 , time : 0.0\n",
      "train: loss: 71077.1640625 acc: 0.9701762199401855  val: loss: 1133775.125 acc: 0.8509202003479004\n",
      "step: 12035 , time : 0.0\n",
      "train: loss: 82112.4296875 acc: 0.9465540647506714  val: loss: 1577929.75 acc: 0.26669877767562866\n",
      "step: 12040 , time : 0.0\n",
      "train: loss: 38752.8984375 acc: 0.7722213268280029  val: loss: 593198.375 acc: 0.9517331719398499\n",
      "step: 12045 , time : 0.0\n",
      "train: loss: 12287.4345703125 acc: 0.9925668239593506  val: loss: 370655.65625 acc: 0.8923383951187134\n",
      "step: 12050 , time : 0.0\n",
      "train: loss: 19586.0078125 acc: 0.9882940053939819  val: loss: 636366.0 acc: 0.8756584525108337\n",
      "step: 12055 , time : 0.0\n",
      "train: loss: 97903.8359375 acc: 0.9808387756347656  val: loss: 412595.28125 acc: 0.9290422201156616\n",
      "step: 12060 , time : 0.0\n",
      "train: loss: 46273.7421875 acc: 0.9882397055625916  val: loss: 261770.3125 acc: 0.9281324148178101\n",
      "step: 12065 , time : 0.0\n",
      "train: loss: 46637.0 acc: 0.9833720326423645  val: loss: 1086987.75 acc: 0.4727845788002014\n",
      "step: 12070 , time : 0.0\n",
      "train: loss: 60949.6015625 acc: 0.9769781231880188  val: loss: 87540.4296875 acc: 0.9803007245063782\n",
      "step: 12075 , time : 0.0\n",
      "train: loss: 189900.03125 acc: 0.9481073617935181  val: loss: 227455.875 acc: 0.9704266786575317\n",
      "step: 12080 , time : 0.0\n",
      "train: loss: 143139.515625 acc: 0.934349775314331  val: loss: 188595.328125 acc: 0.9451878070831299\n",
      "step: 12085 , time : 0.0\n",
      "train: loss: 285246.90625 acc: 0.9531779289245605  val: loss: 744438.4375 acc: 0.9021241068840027\n",
      "step: 12090 , time : 0.0\n",
      "train: loss: 136591.4375 acc: 0.9831065535545349  val: loss: 1089588.5 acc: 0.6596540212631226\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 12095 , time : 0.0\n",
      "train: loss: 84220.4140625 acc: 0.9924113750457764  val: loss: 511288.40625 acc: 0.9343456029891968\n",
      "step: 12100 , time : 0.0\n",
      "train: loss: 83111.65625 acc: 0.9883869290351868  val: loss: 528796.0 acc: 0.9454694390296936\n",
      "step: 12105 , time : 0.0\n",
      "train: loss: 117307.265625 acc: 0.9864646792411804  val: loss: 1257222.5 acc: 0.7100231647491455\n",
      "step: 12110 , time : 0.0010008811950683594\n",
      "train: loss: 843278.6875 acc: 0.8056530952453613  val: loss: 604927.0625 acc: 0.9526647925376892\n",
      "step: 12115 , time : 0.0010006427764892578\n",
      "train: loss: 446557.59375 acc: 0.966184675693512  val: loss: 235377.140625 acc: 0.9146347641944885\n",
      "step: 12120 , time : 0.0010006427764892578\n",
      "train: loss: 876931.375 acc: 0.9523983001708984  val: loss: 2096674.5 acc: 0.6328883171081543\n",
      "step: 12125 , time : 0.0\n",
      "train: loss: 493550.375 acc: 0.897373378276825  val: loss: 1487059.25 acc: 0.8650192022323608\n",
      "step: 12130 , time : 0.0\n",
      "train: loss: 403087.0 acc: 0.971752941608429  val: loss: 981438.4375 acc: 0.9018750786781311\n",
      "step: 12135 , time : 0.0\n",
      "train: loss: 3590260.75 acc: 0.8638790249824524  val: loss: 891023.6875 acc: 0.830098032951355\n",
      "step: 12140 , time : 0.0\n",
      "train: loss: 1946607.25 acc: 0.9412301778793335  val: loss: 499399.96875 acc: 0.8949378132820129\n",
      "step: 12145 , time : 0.0\n",
      "train: loss: 1450556.25 acc: 0.9272477030754089  val: loss: 1211603.125 acc: 0.9103132486343384\n",
      "step: 12150 , time : 0.0\n",
      "train: loss: 706158.1875 acc: 0.9031458497047424  val: loss: 329806.71875 acc: 0.9195096492767334\n",
      "step: 12155 , time : 0.0\n",
      "train: loss: 96272.6484375 acc: 0.9673336148262024  val: loss: 451682.0 acc: 0.8073629140853882\n",
      "step: 12160 , time : 0.0\n",
      "train: loss: 144047.953125 acc: 0.965009868144989  val: loss: 1570713.875 acc: 0.6094111800193787\n",
      "step: 12165 , time : 0.0\n",
      "train: loss: 687573.0 acc: 0.900133490562439  val: loss: 789401.25 acc: 0.8754680156707764\n",
      "step: 12170 , time : 0.0\n",
      "train: loss: 794545.0 acc: 0.8286714553833008  val: loss: 875049.75 acc: 0.8066579103469849\n",
      "step: 12175 , time : 0.0\n",
      "train: loss: 602260.25 acc: 0.8525861501693726  val: loss: 2444028.5 acc: 0.6942877769470215\n",
      "step: 12180 , time : 0.015625476837158203\n",
      "train: loss: 261201.703125 acc: 0.8622826337814331  val: loss: 786240.9375 acc: 0.8219254016876221\n",
      "step: 12185 , time : 0.0\n",
      "train: loss: 1180678.625 acc: 0.4252309203147888  val: loss: 607681.8125 acc: 0.9359139800071716\n",
      "step: 12190 , time : 0.0005526542663574219\n",
      "train: loss: 1602457.5 acc: 0.3153267502784729  val: loss: 800070.625 acc: 0.7292187213897705\n",
      "step: 12195 , time : 0.0\n",
      "train: loss: 1515972.5 acc: 0.2765662670135498  val: loss: 2111980.75 acc: 0.6069626212120056\n",
      "step: 12200 , time : 0.0\n",
      "train: loss: 835171.75 acc: 0.54590904712677  val: loss: 1798455.625 acc: 0.6640617847442627\n",
      "step: 12205 , time : 0.0\n",
      "train: loss: 356452.375 acc: 0.6781705617904663  val: loss: 965994.3125 acc: 0.8045320510864258\n",
      "step: 12210 , time : 0.001001119613647461\n",
      "train: loss: 141886.140625 acc: 0.8466649055480957  val: loss: 425076.375 acc: 0.7791181206703186\n",
      "step: 12215 , time : 0.0010001659393310547\n",
      "train: loss: 281810.59375 acc: 0.8166178464889526  val: loss: 1029358.5625 acc: 0.8670561909675598\n",
      "step: 12220 , time : 0.0\n",
      "train: loss: 203410.90625 acc: 0.8181337118148804  val: loss: 401219.71875 acc: 0.811312198638916\n",
      "step: 12225 , time : 0.0\n",
      "train: loss: 84828.90625 acc: 0.9411419630050659  val: loss: 1438492.75 acc: 0.7726532816886902\n",
      "step: 12230 , time : 0.0\n",
      "train: loss: 254521.625 acc: 0.8448338508605957  val: loss: 1048830.5 acc: 0.5365340709686279\n",
      "step: 12235 , time : 0.0010006427764892578\n",
      "train: loss: 205888.5625 acc: 0.8168075084686279  val: loss: 1912763.25 acc: 0.7915023565292358\n",
      "step: 12240 , time : 0.0010004043579101562\n",
      "train: loss: 111849.71875 acc: 0.9081569910049438  val: loss: 1108719.0 acc: 0.7338108420372009\n",
      "step: 12245 , time : 0.0\n",
      "train: loss: 134750.140625 acc: 0.8342163562774658  val: loss: 426916.96875 acc: 0.8207173347473145\n",
      "step: 12250 , time : 0.0\n",
      "train: loss: 250069.90625 acc: 0.8212995529174805  val: loss: 1008176.875 acc: 0.7849817276000977\n",
      "step: 12255 , time : 0.0\n",
      "train: loss: 710983.3125 acc: 0.5900533199310303  val: loss: 1518699.0 acc: 0.7631354331970215\n",
      "step: 12260 , time : 0.0\n",
      "train: loss: 568743.875 acc: 0.5680550932884216  val: loss: 1382207.375 acc: 0.7489521503448486\n",
      "step: 12265 , time : 0.0\n",
      "train: loss: 1311747.625 acc: 0.35272377729415894  val: loss: 2456380.75 acc: 0.715175986289978\n",
      "step: 12270 , time : 0.0\n",
      "train: loss: 683450.6875 acc: 0.7425796985626221  val: loss: 2856869.25 acc: 0.7337349057197571\n",
      "step: 12275 , time : 0.0\n",
      "train: loss: 739258.5 acc: 0.6911951303482056  val: loss: 677413.0 acc: 0.8000988960266113\n",
      "step: 12280 , time : 0.0\n",
      "train: loss: 1268598.125 acc: 0.7124375104904175  val: loss: 832479.3125 acc: 0.6320974826812744\n",
      "step: 12285 , time : 0.0\n",
      "train: loss: 833279.875 acc: 0.8647028207778931  val: loss: 521852.65625 acc: 0.8657698631286621\n",
      "step: 12290 , time : 0.0\n",
      "train: loss: 671726.0 acc: 0.940820574760437  val: loss: 1568125.0 acc: 0.09367179870605469\n",
      "step: 12295 , time : 0.0\n",
      "train: loss: 408632.75 acc: 0.9591265916824341  val: loss: 1264238.5 acc: 0.8051461577415466\n",
      "step: 12300 , time : 0.0\n",
      "train: loss: 470466.3125 acc: 0.9418001174926758  val: loss: 409507.53125 acc: 0.9303980469703674\n",
      "step: 12305 , time : 0.0\n",
      "train: loss: 211571.0625 acc: 0.9616022706031799  val: loss: 1003090.8125 acc: 0.8699162602424622\n",
      "step: 12310 , time : 0.0\n",
      "train: loss: 241474.0 acc: 0.9804328083992004  val: loss: 1659511.5 acc: 0.7476087808609009\n",
      "step: 12315 , time : 0.0\n",
      "train: loss: 218671.921875 acc: 0.9837366342544556  val: loss: 2050489.5 acc: 0.7360700368881226\n",
      "step: 12320 , time : 0.0\n",
      "train: loss: 309000.25 acc: 0.9737171530723572  val: loss: 1033160.0625 acc: 0.8197473883628845\n",
      "step: 12325 , time : 0.0\n",
      "train: loss: 183498.265625 acc: 0.9741920232772827  val: loss: 860104.875 acc: 0.9300594329833984\n",
      "step: 12330 , time : 0.001001119613647461\n",
      "train: loss: 255597.5 acc: 0.9164175987243652  val: loss: 1762831.25 acc: -0.15251600742340088\n",
      "step: 12335 , time : 0.0\n",
      "train: loss: 191674.953125 acc: 0.9132360816001892  val: loss: 343367.0 acc: 0.8992889523506165\n",
      "step: 12340 , time : 0.0\n",
      "train: loss: 339533.6875 acc: 0.7180689573287964  val: loss: 896233.875 acc: 0.7798317670822144\n",
      "step: 12345 , time : 0.0\n",
      "train: loss: 24825.447265625 acc: 0.9644384384155273  val: loss: 995307.4375 acc: 0.616883397102356\n",
      "step: 12350 , time : 0.0\n",
      "train: loss: 90642.3984375 acc: 0.9368452429771423  val: loss: 221554.75 acc: 0.9752840995788574\n",
      "step: 12355 , time : 0.0\n",
      "train: loss: 20147.369140625 acc: 0.9613444805145264  val: loss: 1095507.375 acc: 0.689313530921936\n",
      "step: 12360 , time : 0.0\n",
      "train: loss: 35052.09765625 acc: 0.8338796496391296  val: loss: 1256016.5 acc: -0.3894219398498535\n",
      "step: 12365 , time : 0.0\n",
      "train: loss: 31530.064453125 acc: 0.9618672132492065  val: loss: 251821.21875 acc: 0.9477896094322205\n",
      "step: 12370 , time : 0.0\n",
      "train: loss: 15562.73046875 acc: 0.9815777540206909  val: loss: 197491.78125 acc: 0.971262514591217\n",
      "step: 12375 , time : 0.0\n",
      "train: loss: 15500.853515625 acc: 0.9734295606613159  val: loss: 794411.0 acc: 0.8070802688598633\n",
      "step: 12380 , time : 0.00016355514526367188\n",
      "train: loss: 108858.5546875 acc: 0.9464247226715088  val: loss: 343140.4375 acc: 0.8995345234870911\n",
      "step: 12385 , time : 0.0010006427764892578\n",
      "train: loss: 85221.7578125 acc: 0.9391924142837524  val: loss: 258452.265625 acc: 0.9462040662765503\n",
      "step: 12390 , time : 0.0\n",
      "train: loss: 45305.59375 acc: 0.9537904262542725  val: loss: 435633.375 acc: 0.9068601727485657\n",
      "step: 12395 , time : 0.0\n",
      "train: loss: 26421.10546875 acc: 0.9862617254257202  val: loss: 930724.75 acc: 0.7992256283760071\n",
      "step: 12400 , time : 0.0\n",
      "train: loss: 68847.015625 acc: 0.9454243779182434  val: loss: 300741.09375 acc: 0.8731389045715332\n",
      "step: 12405 , time : 0.0\n",
      "train: loss: 43921.51171875 acc: 0.9626162648200989  val: loss: 998851.125 acc: 0.711514949798584\n",
      "step: 12410 , time : 0.0\n",
      "train: loss: 30736.5859375 acc: 0.9695421457290649  val: loss: 370016.03125 acc: 0.8722139000892639\n",
      "step: 12415 , time : 0.0\n",
      "train: loss: 83165.0 acc: 0.9714764356613159  val: loss: 413219.28125 acc: 0.9676427841186523\n",
      "step: 12420 , time : 0.0\n",
      "train: loss: 119708.8515625 acc: 0.9743688106536865  val: loss: 498954.09375 acc: 0.9674721360206604\n",
      "step: 12425 , time : 0.0\n",
      "train: loss: 135812.09375 acc: 0.9728817939758301  val: loss: 3369821.5 acc: 0.7043956518173218\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 12430 , time : 0.0\n",
      "train: loss: 70810.078125 acc: 0.9646536111831665  val: loss: 594474.5625 acc: 0.9103336334228516\n",
      "step: 12435 , time : 0.0\n",
      "train: loss: 64805.2265625 acc: 0.9753823280334473  val: loss: 704250.9375 acc: 0.849019467830658\n",
      "step: 12440 , time : 0.0\n",
      "train: loss: 151158.203125 acc: 0.9571593403816223  val: loss: 842131.8125 acc: 0.7570218443870544\n",
      "step: 12445 , time : 0.0010006427764892578\n",
      "train: loss: 138773.4375 acc: 0.9600063562393188  val: loss: 1080463.5 acc: 0.8980405926704407\n",
      "step: 12450 , time : 0.0\n",
      "train: loss: 719410.5 acc: 0.7531241774559021  val: loss: 1110520.625 acc: 0.7958887815475464\n",
      "step: 12455 , time : 0.0\n",
      "train: loss: 492517.40625 acc: 0.9193021655082703  val: loss: 558388.25 acc: 0.932780921459198\n",
      "step: 12460 , time : 0.0010004043579101562\n",
      "train: loss: 221638.953125 acc: 0.9733386635780334  val: loss: 1618361.75 acc: 0.7448165416717529\n",
      "step: 12465 , time : 0.0\n",
      "train: loss: 228184.1875 acc: 0.9661383032798767  val: loss: 1740707.5 acc: 0.8035327196121216\n",
      "step: 12470 , time : 0.0\n",
      "train: loss: 132895.578125 acc: 0.9828396439552307  val: loss: 1885428.0 acc: 0.5399050712585449\n",
      "step: 12475 , time : 0.0\n",
      "train: loss: 201094.765625 acc: 0.9775171875953674  val: loss: 843954.625 acc: 0.880161464214325\n",
      "step: 12480 , time : 0.0\n",
      "train: loss: 2847939.25 acc: 0.8805979490280151  val: loss: 1859237.75 acc: -0.25952911376953125\n",
      "step: 12485 , time : 0.0\n",
      "train: loss: 629319.4375 acc: 0.9663913249969482  val: loss: 2223213.5 acc: 0.5716754794120789\n",
      "step: 12490 , time : 0.0\n",
      "train: loss: 211034.359375 acc: 0.9042478203773499  val: loss: 816448.125 acc: 0.8510437607765198\n",
      "step: 12495 , time : 0.0\n",
      "train: loss: 1822198.625 acc: 0.9372310042381287  val: loss: 1411154.5 acc: 0.8490725755691528\n",
      "step: 12500 , time : 0.0\n",
      "train: loss: 1998876.625 acc: 0.9556036591529846  val: loss: 1325062.875 acc: 0.6297780275344849\n",
      "step: 12505 , time : 0.0\n",
      "train: loss: 709732.0 acc: 0.9761940240859985  val: loss: 1545340.625 acc: 0.753505527973175\n",
      "step: 12510 , time : 0.0\n",
      "train: loss: 727112.375 acc: 0.9742717146873474  val: loss: 1286518.0 acc: 0.29319316148757935\n",
      "step: 12515 , time : 0.0\n",
      "train: loss: 1980144.75 acc: 0.89827561378479  val: loss: 1699588.5 acc: 0.20566260814666748\n",
      "step: 12520 , time : 0.0\n",
      "train: loss: 252767.609375 acc: 0.9632335305213928  val: loss: 1824414.0 acc: 0.760511040687561\n",
      "step: 12525 , time : 0.0\n",
      "train: loss: 358350.0625 acc: 0.9194431900978088  val: loss: 3408342.75 acc: 0.36524826288223267\n",
      "step: 12530 , time : 0.015625715255737305\n",
      "train: loss: 1161122.625 acc: 0.6751898527145386  val: loss: 2407706.5 acc: 0.4727107286453247\n",
      "step: 12535 , time : 0.0\n",
      "train: loss: 1410110.5 acc: 0.49323004484176636  val: loss: 586617.5 acc: 0.7351922988891602\n",
      "step: 12540 , time : 0.0\n",
      "train: loss: 1237415.5 acc: 0.3438034653663635  val: loss: 675952.3125 acc: 0.8432421684265137\n",
      "step: 12545 , time : 0.0\n",
      "train: loss: 316283.15625 acc: 0.8080943822860718  val: loss: 1605233.0 acc: 0.7893177270889282\n",
      "step: 12550 , time : 0.0010008811950683594\n",
      "train: loss: 849048.9375 acc: 0.7200009822845459  val: loss: 1198188.75 acc: 0.7437819838523865\n",
      "step: 12555 , time : 0.0\n",
      "train: loss: 1429021.375 acc: 0.5254918336868286  val: loss: 1511474.5 acc: 0.7038828134536743\n",
      "step: 12560 , time : 0.0\n",
      "train: loss: 1354272.75 acc: 0.5517421960830688  val: loss: 1856565.875 acc: 0.696306049823761\n",
      "step: 12565 , time : 0.0010006427764892578\n",
      "train: loss: 1375739.75 acc: 0.29121702909469604  val: loss: 3441192.5 acc: 0.586453914642334\n",
      "step: 12570 , time : 0.0010008811950683594\n",
      "train: loss: 670810.5 acc: 0.4000006318092346  val: loss: 2495663.25 acc: 0.6492890119552612\n",
      "step: 12575 , time : 0.0\n",
      "train: loss: 505835.9375 acc: 0.6285219192504883  val: loss: 958821.75 acc: 0.8686457276344299\n",
      "step: 12580 , time : 0.0\n",
      "train: loss: 72649.3828125 acc: 0.9408175349235535  val: loss: 1170806.0 acc: 0.7126244902610779\n",
      "step: 12585 , time : 0.0\n",
      "train: loss: 488130.875 acc: 0.633861780166626  val: loss: 1197005.75 acc: 0.7265269160270691\n",
      "step: 12590 , time : 0.0\n",
      "train: loss: 297690.03125 acc: 0.8224223256111145  val: loss: 710754.4375 acc: 0.7015285491943359\n",
      "step: 12595 , time : 0.0\n",
      "train: loss: 147900.59375 acc: 0.9072334170341492  val: loss: 657992.125 acc: 0.6522460579872131\n",
      "step: 12600 , time : 0.0\n",
      "train: loss: 155527.953125 acc: 0.8780871629714966  val: loss: 1543154.875 acc: 0.770386815071106\n",
      "step: 12605 , time : 0.0\n",
      "train: loss: 204628.140625 acc: 0.7787846326828003  val: loss: 914298.375 acc: 0.8478445410728455\n",
      "step: 12610 , time : 0.0\n",
      "train: loss: 106874.3515625 acc: 0.8352699875831604  val: loss: 1027290.0625 acc: 0.7947223782539368\n",
      "step: 12615 , time : 0.0\n",
      "train: loss: 342524.75 acc: 0.7342002391815186  val: loss: 590104.1875 acc: 0.6483920812606812\n",
      "step: 12620 , time : 0.0\n",
      "train: loss: 565756.375 acc: 0.4122922420501709  val: loss: 1068623.75 acc: 0.8529713153839111\n",
      "step: 12625 , time : 0.0\n",
      "train: loss: 966949.625 acc: 0.6315579414367676  val: loss: 1642501.75 acc: 0.7813045382499695\n",
      "step: 12630 , time : 0.0\n",
      "train: loss: 939739.5 acc: 0.6503764390945435  val: loss: 1242400.75 acc: 0.7080285549163818\n",
      "step: 12635 , time : 0.0\n",
      "train: loss: 297861.25 acc: 0.693791925907135  val: loss: 1256242.0 acc: 0.609311580657959\n",
      "step: 12640 , time : 0.0\n",
      "train: loss: 579687.6875 acc: 0.6993848085403442  val: loss: 473365.34375 acc: 0.7656847834587097\n",
      "step: 12645 , time : 0.0\n",
      "train: loss: 1049364.375 acc: 0.8313121795654297  val: loss: 879611.875 acc: 0.8473163843154907\n",
      "step: 12650 , time : 0.0\n",
      "train: loss: 978079.5 acc: 0.8104239702224731  val: loss: 955151.4375 acc: 0.8232855200767517\n",
      "step: 12655 , time : 0.0\n",
      "train: loss: 917960.0 acc: 0.9291211366653442  val: loss: 519689.4375 acc: 0.7988421320915222\n",
      "step: 12660 , time : 0.0\n",
      "train: loss: 654861.125 acc: 0.8858889937400818  val: loss: 744809.9375 acc: 0.8957608938217163\n",
      "step: 12665 , time : 0.0\n",
      "train: loss: 328503.9375 acc: 0.8957065939903259  val: loss: 325863.625 acc: 0.9294262528419495\n",
      "step: 12670 , time : 0.0\n",
      "train: loss: 279390.3125 acc: 0.9504243731498718  val: loss: 404587.0 acc: 0.9164491891860962\n",
      "step: 12675 , time : 0.0010004043579101562\n",
      "train: loss: 213401.6875 acc: 0.9831792116165161  val: loss: 419639.125 acc: 0.8745018243789673\n",
      "step: 12680 , time : 0.0010004043579101562\n",
      "train: loss: 199504.796875 acc: 0.9849459528923035  val: loss: 324030.09375 acc: 0.9081193208694458\n",
      "step: 12685 , time : 0.0\n",
      "train: loss: 320005.5 acc: 0.9688486456871033  val: loss: 540150.0 acc: 0.8026137948036194\n",
      "step: 12690 , time : 0.0010006427764892578\n",
      "train: loss: 227065.015625 acc: 0.9781391620635986  val: loss: 300907.1875 acc: 0.959683895111084\n",
      "step: 12695 , time : 0.0010008811950683594\n",
      "train: loss: 126929.9609375 acc: 0.9624524116516113  val: loss: 557276.625 acc: 0.7884620428085327\n",
      "step: 12700 , time : 0.0010008811950683594\n",
      "train: loss: 30086.560546875 acc: 0.9850327968597412  val: loss: 362398.28125 acc: 0.930985689163208\n",
      "step: 12705 , time : 0.0010004043579101562\n",
      "train: loss: 121728.3515625 acc: 0.9497472047805786  val: loss: 462221.03125 acc: 0.9006966948509216\n",
      "step: 12710 , time : 0.0010004043579101562\n",
      "train: loss: 118291.109375 acc: 0.9555854201316833  val: loss: 286486.59375 acc: 0.8392418622970581\n",
      "step: 12715 , time : 0.0\n",
      "train: loss: 16835.470703125 acc: 0.9782619476318359  val: loss: 285983.03125 acc: 0.9347866773605347\n",
      "step: 12720 , time : 0.0\n",
      "train: loss: 10022.552734375 acc: 0.9829570055007935  val: loss: 324387.40625 acc: 0.8607950806617737\n",
      "step: 12725 , time : 0.0\n",
      "train: loss: 23509.548828125 acc: 0.9682565927505493  val: loss: 954764.1875 acc: 0.9421315789222717\n",
      "step: 12730 , time : 0.0\n",
      "train: loss: 27913.806640625 acc: 0.9440255761146545  val: loss: 247064.796875 acc: 0.9495738744735718\n",
      "step: 12735 , time : 0.0\n",
      "train: loss: 74742.828125 acc: 0.9239761829376221  val: loss: 610493.0 acc: 0.9518032073974609\n",
      "step: 12740 , time : 0.0\n",
      "train: loss: 28856.419921875 acc: 0.984317421913147  val: loss: 240389.546875 acc: 0.9482196569442749\n",
      "step: 12745 , time : 0.0\n",
      "train: loss: 69260.109375 acc: 0.9514833092689514  val: loss: 387072.53125 acc: 0.9430395364761353\n",
      "step: 12750 , time : 0.0\n",
      "train: loss: 80137.9609375 acc: 0.919264554977417  val: loss: 331166.15625 acc: 0.9595847725868225\n",
      "step: 12755 , time : 0.0\n",
      "train: loss: 115994.8984375 acc: 0.9328337907791138  val: loss: 202841.671875 acc: 0.9706114530563354\n",
      "step: 12760 , time : 0.0\n",
      "train: loss: 68367.4609375 acc: 0.9710895419120789  val: loss: 3968737.75 acc: 0.8215579390525818\n",
      "step: 12765 , time : 0.0\n",
      "train: loss: 118567.2109375 acc: 0.9526280760765076  val: loss: 951162.9375 acc: 0.9109049439430237\n",
      "step: 12770 , time : 0.015625715255737305\n",
      "train: loss: 34301.4609375 acc: 0.9538894891738892  val: loss: 791806.625 acc: 0.9330088496208191\n",
      "step: 12775 , time : 0.0\n",
      "train: loss: 16106.236328125 acc: 0.9672977328300476  val: loss: 1920851.625 acc: 0.8730300664901733\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 12780 , time : 0.0\n",
      "train: loss: 54606.58984375 acc: 0.9776288270950317  val: loss: 2076853.75 acc: 0.29437172412872314\n",
      "step: 12785 , time : 0.0\n",
      "train: loss: 38963.2890625 acc: 0.9884691834449768  val: loss: 1723037.75 acc: 0.7578909397125244\n",
      "step: 12790 , time : 0.0010004043579101562\n",
      "train: loss: 45595.29296875 acc: 0.9877029061317444  val: loss: 2646770.75 acc: 0.2539817690849304\n",
      "step: 12795 , time : 0.0\n",
      "train: loss: 76810.2265625 acc: 0.9373744130134583  val: loss: 1581681.875 acc: 0.7844035029411316\n",
      "step: 12800 , time : 0.0010004043579101562\n",
      "train: loss: 77302.8828125 acc: 0.9782438278198242  val: loss: 1321068.375 acc: 0.6838034391403198\n",
      "step: 12805 , time : 0.0010006427764892578\n",
      "train: loss: 234291.140625 acc: 0.935366153717041  val: loss: 611150.0625 acc: 0.9566061496734619\n",
      "step: 12810 , time : 0.0\n",
      "train: loss: 224274.109375 acc: 0.9370215535163879  val: loss: 826063.3125 acc: 0.7955085039138794\n",
      "step: 12815 , time : 0.0\n",
      "train: loss: 650891.25 acc: 0.7698314189910889  val: loss: 1142987.75 acc: 0.8166003227233887\n",
      "step: 12820 , time : 0.0\n",
      "train: loss: 237253.578125 acc: 0.9673965573310852  val: loss: 808065.1875 acc: 0.829203724861145\n",
      "step: 12825 , time : 0.0\n",
      "train: loss: 154821.71875 acc: 0.982448935508728  val: loss: 2154216.25 acc: 0.6039530038833618\n",
      "step: 12830 , time : 0.0010006427764892578\n",
      "train: loss: 354557.21875 acc: 0.9507271647453308  val: loss: 1779851.875 acc: 0.5678895711898804\n",
      "step: 12835 , time : 0.0009996891021728516\n",
      "train: loss: 341338.28125 acc: 0.9570848345756531  val: loss: 722292.375 acc: 0.8689435720443726\n",
      "step: 12840 , time : 0.0010004043579101562\n",
      "train: loss: 288942.5625 acc: 0.9813934564590454  val: loss: 984028.875 acc: 0.6737362742424011\n",
      "step: 12845 , time : 0.0\n",
      "train: loss: 1917505.0 acc: 0.8197462558746338  val: loss: 256395.96875 acc: 0.8887292742729187\n",
      "step: 12850 , time : 0.0\n",
      "train: loss: 293918.375 acc: 0.9678230881690979  val: loss: 492278.8125 acc: 0.802809476852417\n",
      "step: 12855 , time : 0.0\n",
      "train: loss: 307869.15625 acc: 0.9605861902236938  val: loss: 433950.65625 acc: 0.8596402406692505\n",
      "step: 12860 , time : 0.0\n",
      "train: loss: 1255006.125 acc: 0.9438665509223938  val: loss: 2120446.0 acc: 0.4242902398109436\n",
      "step: 12865 , time : 0.0\n",
      "train: loss: 2342721.25 acc: 0.9179397225379944  val: loss: 1909612.25 acc: 0.3773297667503357\n",
      "step: 12870 , time : 0.0\n",
      "train: loss: 908916.375 acc: 0.9596166610717773  val: loss: 2043599.375 acc: 0.17033272981643677\n",
      "step: 12875 , time : 0.0\n",
      "train: loss: 384242.84375 acc: 0.979256272315979  val: loss: 1560951.625 acc: 0.6393511295318604\n",
      "step: 12880 , time : 0.0\n",
      "train: loss: 697446.375 acc: 0.9415410161018372  val: loss: 1636181.0 acc: 0.7531867027282715\n",
      "step: 12885 , time : 0.0\n",
      "train: loss: 608725.8125 acc: 0.8832870721817017  val: loss: 1388839.75 acc: 0.8150298595428467\n",
      "step: 12890 , time : 0.0\n",
      "train: loss: 652441.75 acc: 0.9439913034439087  val: loss: 457821.84375 acc: 0.8651818633079529\n",
      "step: 12895 , time : 0.0\n",
      "train: loss: 856028.1875 acc: 0.8165358304977417  val: loss: 698132.0625 acc: 0.7849621772766113\n",
      "step: 12900 , time : 0.0\n",
      "train: loss: 1495888.5 acc: 0.3986932635307312  val: loss: 1559766.5 acc: 0.7120139598846436\n",
      "step: 12905 , time : 0.0\n",
      "train: loss: 1827366.625 acc: 0.4758049249649048  val: loss: 1030673.5625 acc: 0.7361164093017578\n",
      "step: 12910 , time : 0.0\n",
      "train: loss: 843439.625 acc: 0.6997109055519104  val: loss: 857411.875 acc: 0.8269726037979126\n",
      "step: 12915 , time : 0.0\n",
      "train: loss: 350004.0625 acc: 0.8641631603240967  val: loss: 1183155.75 acc: 0.6939264535903931\n",
      "step: 12920 , time : 0.0010006427764892578\n",
      "train: loss: 1586361.25 acc: 0.3843076229095459  val: loss: 844297.6875 acc: 0.7832324504852295\n",
      "step: 12925 , time : 0.0\n",
      "train: loss: 2022072.0 acc: -0.04869067668914795  val: loss: 1741355.125 acc: 0.5363211631774902\n",
      "step: 12930 , time : 0.0\n",
      "train: loss: 812931.5 acc: 0.4273902177810669  val: loss: 1486065.75 acc: 0.44886118173599243\n",
      "step: 12935 , time : 0.0010004043579101562\n",
      "train: loss: 479023.5 acc: 0.5989865064620972  val: loss: 1195151.0 acc: 0.5431060791015625\n",
      "step: 12940 , time : 0.0010004043579101562\n",
      "train: loss: 611938.5625 acc: 0.5194927453994751  val: loss: 906204.8125 acc: 0.5287787914276123\n",
      "step: 12945 , time : 0.0\n",
      "train: loss: 345327.09375 acc: 0.7670606374740601  val: loss: 2040421.25 acc: 0.7832868099212646\n",
      "step: 12950 , time : 0.0\n",
      "train: loss: 60418.5703125 acc: 0.9453722238540649  val: loss: 838429.5625 acc: 0.7423985004425049\n",
      "step: 12955 , time : 0.0\n",
      "train: loss: 207286.640625 acc: 0.8450153470039368  val: loss: 693421.4375 acc: 0.7782660126686096\n",
      "step: 12960 , time : 0.0\n",
      "train: loss: 246477.640625 acc: 0.8584335446357727  val: loss: 1263880.125 acc: 0.6582081317901611\n",
      "step: 12965 , time : 0.0\n",
      "train: loss: 217447.09375 acc: 0.8501180410385132  val: loss: 933166.6875 acc: 0.8503735661506653\n",
      "step: 12970 , time : 0.0\n",
      "train: loss: 88294.3828125 acc: 0.9126327037811279  val: loss: 541331.9375 acc: 0.72117018699646\n",
      "step: 12975 , time : 0.0\n",
      "train: loss: 46926.4921875 acc: 0.9245850443840027  val: loss: 172908.640625 acc: 0.865607500076294\n",
      "step: 12980 , time : 0.0\n",
      "train: loss: 260653.125 acc: 0.7752432823181152  val: loss: 246651.4375 acc: 0.8443735837936401\n",
      "step: 12985 , time : 0.0\n",
      "train: loss: 417591.96875 acc: 0.7434268593788147  val: loss: 2087602.5 acc: 0.5487195253372192\n",
      "step: 12990 , time : 0.0\n",
      "train: loss: 1073778.25 acc: 0.3408246636390686  val: loss: 950128.875 acc: 0.7677550315856934\n",
      "step: 12995 , time : 0.0\n",
      "train: loss: 937531.1875 acc: 0.6727232336997986  val: loss: 1524786.25 acc: 0.6929630041122437\n",
      "step: 13000 , time : 0.0\n",
      "train: loss: 518961.0 acc: 0.7657012939453125  val: loss: 1252593.375 acc: 0.8004635572433472\n",
      "step: 13005 , time : 0.0\n",
      "train: loss: 611071.75 acc: 0.6813362836837769  val: loss: 667043.5 acc: 0.7876095175743103\n",
      "step: 13010 , time : 0.0\n",
      "train: loss: 1401493.75 acc: 0.7971638441085815  val: loss: 925859.1875 acc: 0.8772448897361755\n",
      "step: 13015 , time : 0.0010008811950683594\n",
      "train: loss: 1236875.0 acc: 0.8170738816261292  val: loss: 225289.140625 acc: 0.8772901296615601\n",
      "step: 13020 , time : 0.0\n",
      "train: loss: 1096133.25 acc: 0.9056795239448547  val: loss: 759516.25 acc: 0.8851292729377747\n",
      "step: 13025 , time : 0.0010004043579101562\n",
      "train: loss: 834390.625 acc: 0.8997200131416321  val: loss: 1227421.125 acc: 0.5104614496231079\n",
      "step: 13030 , time : 0.0\n",
      "train: loss: 382993.46875 acc: 0.9133544564247131  val: loss: 679998.625 acc: 0.9086954593658447\n",
      "step: 13035 , time : 0.0\n",
      "train: loss: 147591.59375 acc: 0.9733567833900452  val: loss: 1029278.25 acc: 0.9084957242012024\n",
      "step: 13040 , time : 0.0\n",
      "train: loss: 261288.09375 acc: 0.9718426465988159  val: loss: 613779.5 acc: 0.9203143119812012\n",
      "step: 13045 , time : 0.0\n",
      "train: loss: 214982.984375 acc: 0.9839940667152405  val: loss: 503671.28125 acc: 0.9604282975196838\n",
      "step: 13050 , time : 0.0\n",
      "train: loss: 281216.9375 acc: 0.9755610823631287  val: loss: 1108668.25 acc: 0.5708976984024048\n",
      "step: 13055 , time : 0.0\n",
      "train: loss: 202785.34375 acc: 0.9688095450401306  val: loss: 825386.9375 acc: 0.8322608470916748\n",
      "step: 13060 , time : 0.0\n",
      "train: loss: 127794.265625 acc: 0.978814959526062  val: loss: 275480.6875 acc: 0.9472066164016724\n",
      "step: 13065 , time : 0.0\n",
      "train: loss: 101205.859375 acc: 0.9593899250030518  val: loss: 1511203.25 acc: 0.7051193118095398\n",
      "step: 13070 , time : 0.0\n",
      "train: loss: 20239.25390625 acc: 0.985971987247467  val: loss: 1101542.5 acc: 0.8235197067260742\n",
      "step: 13075 , time : 0.0\n",
      "train: loss: 93847.8125 acc: 0.9292975068092346  val: loss: 2066677.5 acc: 0.5081866383552551\n",
      "step: 13080 , time : 0.0\n",
      "train: loss: 10925.1533203125 acc: 0.9936383366584778  val: loss: 1273338.25 acc: 0.7366180419921875\n",
      "step: 13085 , time : 0.0\n",
      "train: loss: 30803.146484375 acc: 0.8832224011421204  val: loss: 717765.5 acc: 0.9267018437385559\n",
      "step: 13090 , time : 0.0\n",
      "train: loss: 11305.5146484375 acc: 0.9665709137916565  val: loss: 1433198.5 acc: 0.5240074396133423\n",
      "step: 13095 , time : 0.0\n",
      "train: loss: 95282.4296875 acc: 0.8999106884002686  val: loss: 1179012.125 acc: 0.7516309022903442\n",
      "step: 13100 , time : 0.0\n",
      "train: loss: 21252.9296875 acc: 0.95704185962677  val: loss: 1021254.0625 acc: 0.8251705169677734\n",
      "step: 13105 , time : 0.015625953674316406\n",
      "train: loss: 32169.720703125 acc: 0.9283800721168518  val: loss: 2523262.0 acc: 0.13023066520690918\n",
      "step: 13110 , time : 0.0\n",
      "train: loss: 30283.74609375 acc: 0.9709175825119019  val: loss: 1709384.25 acc: 0.7543686628341675\n",
      "step: 13115 , time : 0.015625476837158203\n",
      "train: loss: 16586.203125 acc: 0.9814940094947815  val: loss: 2144733.0 acc: 0.45703113079071045\n",
      "step: 13120 , time : 0.0\n",
      "train: loss: 19726.09765625 acc: 0.981584370136261  val: loss: 1595129.75 acc: 0.6077516078948975\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 13125 , time : 0.0\n",
      "train: loss: 20851.21484375 acc: 0.9791489839553833  val: loss: 541839.125 acc: 0.9286089539527893\n",
      "step: 13130 , time : 0.0\n",
      "train: loss: 45183.9609375 acc: 0.9652931094169617  val: loss: 1034975.9375 acc: 0.5014225244522095\n",
      "step: 13135 , time : 0.0\n",
      "train: loss: 40575.34375 acc: 0.965654194355011  val: loss: 1957970.125 acc: 0.06875509023666382\n",
      "step: 13140 , time : 0.0\n",
      "train: loss: 45304.8046875 acc: 0.9493613243103027  val: loss: 1065137.875 acc: 0.4133791923522949\n",
      "step: 13145 , time : 0.0010008811950683594\n",
      "train: loss: 43199.20703125 acc: 0.976161003112793  val: loss: 1009257.125 acc: 0.9028927087783813\n",
      "step: 13150 , time : 0.0\n",
      "train: loss: 73342.109375 acc: 0.9841318726539612  val: loss: 465533.125 acc: 0.9174368381500244\n",
      "step: 13155 , time : 0.0\n",
      "train: loss: 66324.125 acc: 0.9722845554351807  val: loss: 2399634.25 acc: 0.4091690182685852\n",
      "step: 13160 , time : 0.0\n",
      "train: loss: 46037.66015625 acc: 0.9769112467765808  val: loss: 1508642.25 acc: 0.6659349799156189\n",
      "step: 13165 , time : 0.0\n",
      "train: loss: 119267.1875 acc: 0.9685525298118591  val: loss: 1521613.625 acc: 0.8190925717353821\n",
      "step: 13170 , time : 0.0\n",
      "train: loss: 255311.71875 acc: 0.9265883564949036  val: loss: 878342.1875 acc: 0.8490365743637085\n",
      "step: 13175 , time : 0.0\n",
      "train: loss: 176781.734375 acc: 0.9328346848487854  val: loss: 457479.5 acc: 0.9310129880905151\n",
      "step: 13180 , time : 0.0\n",
      "train: loss: 105955.421875 acc: 0.9525656700134277  val: loss: 1305807.375 acc: 0.5991302728652954\n",
      "step: 13185 , time : 0.0\n",
      "train: loss: 396598.9375 acc: 0.8426805138587952  val: loss: 338424.90625 acc: 0.9405766725540161\n",
      "step: 13190 , time : 0.0\n",
      "train: loss: 72514.3046875 acc: 0.9904366135597229  val: loss: 1138194.75 acc: 0.5919729471206665\n",
      "step: 13195 , time : 0.0\n",
      "train: loss: 145923.109375 acc: 0.9844772219657898  val: loss: 1107698.625 acc: 0.8036274313926697\n",
      "step: 13200 , time : 0.0\n",
      "train: loss: 328804.34375 acc: 0.9389351010322571  val: loss: 457918.5625 acc: 0.9595320820808411\n",
      "step: 13205 , time : 0.0\n",
      "train: loss: 818693.25 acc: 0.9067959785461426  val: loss: 1243507.0 acc: 0.5131956338882446\n",
      "step: 13210 , time : 0.0\n",
      "train: loss: 716051.5625 acc: 0.9571104645729065  val: loss: 952164.1875 acc: 0.8128721117973328\n",
      "step: 13215 , time : 0.0\n",
      "train: loss: 435114.5 acc: 0.95774245262146  val: loss: 1268844.375 acc: 0.5260802507400513\n",
      "step: 13220 , time : 0.0\n",
      "train: loss: 512161.21875 acc: 0.9497261643409729  val: loss: 1231100.0 acc: 0.9022111892700195\n",
      "step: 13225 , time : 0.0\n",
      "train: loss: 615492.8125 acc: 0.9725435972213745  val: loss: 999903.1875 acc: 0.8165737390518188\n",
      "step: 13230 , time : 0.0\n",
      "train: loss: 315173.5625 acc: 0.9772931337356567  val: loss: 1874431.75 acc: 0.3437499403953552\n",
      "step: 13235 , time : 0.0\n",
      "train: loss: 1252775.5 acc: 0.959600031375885  val: loss: 1766305.0 acc: 0.6102156639099121\n",
      "step: 13240 , time : 0.0\n",
      "train: loss: 1915411.125 acc: 0.9133158326148987  val: loss: 1072131.125 acc: 0.7735385894775391\n",
      "step: 13245 , time : 0.0\n",
      "train: loss: 739665.0625 acc: 0.9641770720481873  val: loss: 4473443.5 acc: 0.12587934732437134\n",
      "step: 13250 , time : 0.0\n",
      "train: loss: 430901.15625 acc: 0.9611005783081055  val: loss: 332801.4375 acc: 0.7625956535339355\n",
      "step: 13255 , time : 0.0\n",
      "train: loss: 654286.375 acc: 0.9143876433372498  val: loss: 1121424.0 acc: 0.8536515831947327\n",
      "step: 13260 , time : 0.0010006427764892578\n",
      "train: loss: 1083262.375 acc: 0.8907554149627686  val: loss: 731324.4375 acc: 0.811188280582428\n",
      "step: 13265 , time : 0.0\n",
      "train: loss: 1499977.75 acc: 0.7211325168609619  val: loss: 1460518.75 acc: 0.6287174820899963\n",
      "step: 13270 , time : 0.0\n",
      "train: loss: 1255563.0 acc: 0.504038393497467  val: loss: 1342996.25 acc: 0.642128586769104\n",
      "step: 13275 , time : 0.001001119613647461\n",
      "train: loss: 447977.90625 acc: 0.7085176706314087  val: loss: 589493.125 acc: 0.712040364742279\n",
      "step: 13280 , time : 5.054473876953125e-05\n",
      "train: loss: 490477.1875 acc: 0.7391234636306763  val: loss: 2284487.75 acc: 0.5327370166778564\n",
      "step: 13285 , time : 0.0\n",
      "train: loss: 1195394.5 acc: 0.5855664014816284  val: loss: 867050.625 acc: 0.8416603803634644\n",
      "step: 13290 , time : 0.0\n",
      "train: loss: 1299109.0 acc: 0.44867223501205444  val: loss: 2886724.75 acc: 0.621548056602478\n",
      "step: 13295 , time : 0.0\n",
      "train: loss: 1010951.5 acc: 0.5009942054748535  val: loss: 2414077.75 acc: 0.4934214949607849\n",
      "step: 13300 , time : 0.0\n",
      "train: loss: 293221.78125 acc: 0.8125643134117126  val: loss: 930906.125 acc: 0.36311161518096924\n",
      "step: 13305 , time : 0.0\n",
      "train: loss: 456099.65625 acc: 0.7215220928192139  val: loss: 1992215.5 acc: 0.5908828973770142\n",
      "step: 13310 , time : 0.0\n",
      "train: loss: 157559.84375 acc: 0.8792334794998169  val: loss: 734738.0 acc: 0.793136715888977\n",
      "step: 13315 , time : 0.0\n",
      "train: loss: 195590.203125 acc: 0.8776843547821045  val: loss: 1172130.0 acc: 0.7822477221488953\n",
      "step: 13320 , time : 0.0\n",
      "train: loss: 113639.1953125 acc: 0.9057391881942749  val: loss: 350812.1875 acc: 0.9223085641860962\n",
      "step: 13325 , time : 0.0\n",
      "train: loss: 276916.90625 acc: 0.8542135953903198  val: loss: 1472186.0 acc: 0.7635014057159424\n",
      "step: 13330 , time : 0.0\n",
      "train: loss: 238942.65625 acc: 0.7812628746032715  val: loss: 1272483.0 acc: 0.8478491306304932\n",
      "step: 13335 , time : 0.0\n",
      "train: loss: 106772.125 acc: 0.9080982208251953  val: loss: 270580.59375 acc: 0.8053537011146545\n",
      "step: 13340 , time : 0.0\n",
      "train: loss: 65149.6328125 acc: 0.8922041058540344  val: loss: 696492.25 acc: 0.8913934826850891\n",
      "step: 13345 , time : 0.0\n",
      "train: loss: 254034.625 acc: 0.8264561891555786  val: loss: 233504.375 acc: 0.8743365406990051\n",
      "step: 13350 , time : 0.0\n",
      "train: loss: 533583.75 acc: 0.41764962673187256  val: loss: 1655756.875 acc: 0.7710431814193726\n",
      "step: 13355 , time : 0.0\n",
      "train: loss: 804854.75 acc: 0.6774823665618896  val: loss: 3944890.25 acc: 0.7282610535621643\n",
      "step: 13360 , time : 0.0\n",
      "train: loss: 901201.625 acc: 0.6066120862960815  val: loss: 2345264.25 acc: 0.6916179656982422\n",
      "step: 13365 , time : 0.0\n",
      "train: loss: 252488.734375 acc: 0.8203193545341492  val: loss: 1180312.875 acc: 0.74607253074646\n",
      "step: 13370 , time : 0.0\n",
      "train: loss: 427017.84375 acc: 0.7895161509513855  val: loss: 2009686.125 acc: 0.7189332842826843\n",
      "step: 13375 , time : 0.0\n",
      "train: loss: 1513673.25 acc: 0.7666015625  val: loss: 782836.375 acc: 0.857589602470398\n",
      "step: 13380 , time : 0.0\n",
      "train: loss: 1033322.4375 acc: 0.7059128880500793  val: loss: 543132.4375 acc: 0.9107547998428345\n",
      "step: 13385 , time : 0.0010004043579101562\n",
      "train: loss: 612129.5 acc: 0.9441046714782715  val: loss: 1133917.375 acc: 0.9062544703483582\n",
      "step: 13390 , time : 0.0010008811950683594\n",
      "train: loss: 832068.4375 acc: 0.9284117221832275  val: loss: 743590.4375 acc: 0.8398325443267822\n",
      "step: 13395 , time : 0.0010006427764892578\n",
      "train: loss: 380760.96875 acc: 0.9557578563690186  val: loss: 487000.53125 acc: 0.941658616065979\n",
      "step: 13400 , time : 0.0\n",
      "train: loss: 423577.28125 acc: 0.8992021083831787  val: loss: 4566026.5 acc: 0.6141694784164429\n",
      "step: 13405 , time : 0.0\n",
      "train: loss: 172706.25 acc: 0.97980135679245  val: loss: 1440061.75 acc: 0.7838847637176514\n",
      "step: 13410 , time : 0.0\n",
      "train: loss: 209943.8125 acc: 0.9853531122207642  val: loss: 3381861.25 acc: 0.16699081659317017\n",
      "step: 13415 , time : 0.0\n",
      "train: loss: 160460.34375 acc: 0.9862233996391296  val: loss: 2168782.5 acc: 0.5408464670181274\n",
      "step: 13420 , time : 0.0\n",
      "train: loss: 350330.96875 acc: 0.9647613167762756  val: loss: 1206701.75 acc: 0.8225833177566528\n",
      "step: 13425 , time : 0.0\n",
      "train: loss: 131470.328125 acc: 0.9769515991210938  val: loss: 1131293.5 acc: 0.8143904805183411\n",
      "step: 13430 , time : 0.0\n",
      "train: loss: 86990.515625 acc: 0.9780673384666443  val: loss: 1325061.0 acc: 0.8584415316581726\n",
      "step: 13435 , time : 0.0\n",
      "train: loss: 18199.6171875 acc: 0.9890578985214233  val: loss: 2067515.75 acc: 0.7451339364051819\n",
      "step: 13440 , time : 0.0\n",
      "train: loss: 63089.12109375 acc: 0.9825643301010132  val: loss: 674517.625 acc: 0.8282123804092407\n",
      "step: 13445 , time : 0.0\n",
      "train: loss: 17184.54296875 acc: 0.9730606079101562  val: loss: 1042393.8125 acc: 0.8472729921340942\n",
      "step: 13450 , time : 0.0\n",
      "train: loss: 31584.421875 acc: 0.9513155817985535  val: loss: 1159247.25 acc: 0.8562577962875366\n",
      "step: 13455 , time : 0.0010008811950683594\n",
      "train: loss: 47571.25390625 acc: 0.953419029712677  val: loss: 1147083.875 acc: 0.8672486543655396\n",
      "step: 13460 , time : 0.0010004043579101562\n",
      "train: loss: 17871.71484375 acc: 0.9549981355667114  val: loss: 2506997.75 acc: 0.4326237440109253\n",
      "step: 13465 , time : 0.0\n",
      "train: loss: 39219.40625 acc: 0.9336589574813843  val: loss: 1335792.25 acc: 0.7904194593429565\n",
      "step: 13470 , time : 0.0\n",
      "train: loss: 16344.0693359375 acc: 0.971735954284668  val: loss: 698654.5 acc: 0.868490993976593\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 13475 , time : 0.0\n",
      "train: loss: 43030.49609375 acc: 0.84493088722229  val: loss: 1224119.125 acc: 0.8427517414093018\n",
      "step: 13480 , time : 0.0\n",
      "train: loss: 73966.7421875 acc: 0.9641466736793518  val: loss: 671720.3125 acc: 0.9225119948387146\n",
      "step: 13485 , time : 0.0010004043579101562\n",
      "train: loss: 55710.1015625 acc: 0.9759390354156494  val: loss: 461146.40625 acc: 0.9234384894371033\n",
      "step: 13490 , time : 0.0010004043579101562\n",
      "train: loss: 12284.564453125 acc: 0.9904327988624573  val: loss: 1112864.875 acc: 0.7558292150497437\n",
      "step: 13495 , time : 0.0010008811950683594\n",
      "train: loss: 60444.98828125 acc: 0.9737932085990906  val: loss: 2323159.75 acc: 0.404904305934906\n",
      "step: 13500 , time : 0.0010006427764892578\n",
      "train: loss: 104180.53125 acc: 0.9227765202522278  val: loss: 2264859.75 acc: 0.559933066368103\n",
      "step: 13505 , time : 0.0\n",
      "train: loss: 24444.568359375 acc: 0.9608826637268066  val: loss: 1016151.75 acc: 0.8565254211425781\n",
      "step: 13510 , time : 0.0\n",
      "train: loss: 26293.314453125 acc: 0.9822078347206116  val: loss: 1546291.5 acc: 0.4348335266113281\n",
      "step: 13515 , time : 0.0\n",
      "train: loss: 72017.546875 acc: 0.98687744140625  val: loss: 2850400.25 acc: 0.5804219245910645\n",
      "step: 13520 , time : 0.0\n",
      "train: loss: 37077.3828125 acc: 0.9834492802619934  val: loss: 1527560.5 acc: 0.7359534502029419\n",
      "step: 13525 , time : 0.0010004043579101562\n",
      "train: loss: 62528.69140625 acc: 0.9798641800880432  val: loss: 645971.3125 acc: 0.9338737726211548\n",
      "step: 13530 , time : 0.0\n",
      "train: loss: 28410.18359375 acc: 0.9866160750389099  val: loss: 1287377.5 acc: 0.49265873432159424\n",
      "step: 13535 , time : 0.0\n",
      "train: loss: 93281.875 acc: 0.9758151173591614  val: loss: 717041.125 acc: 0.8735219240188599\n",
      "step: 13540 , time : 0.0010006427764892578\n",
      "train: loss: 119206.015625 acc: 0.9638976454734802  val: loss: 945938.8125 acc: 0.7686370611190796\n",
      "step: 13545 , time : 0.0\n",
      "train: loss: 227232.84375 acc: 0.9381746649742126  val: loss: 555467.3125 acc: 0.6009445786476135\n",
      "step: 13550 , time : 0.0\n",
      "train: loss: 358184.4375 acc: 0.943212628364563  val: loss: 446638.6875 acc: 0.9225919246673584\n",
      "step: 13555 , time : 0.0\n",
      "train: loss: 173697.421875 acc: 0.9857379794120789  val: loss: 561919.3125 acc: 0.8600300550460815\n",
      "step: 13560 , time : 0.0\n",
      "train: loss: 129147.4609375 acc: 0.9857574701309204  val: loss: 260622.34375 acc: 0.9177849888801575\n",
      "step: 13565 , time : 0.0\n",
      "train: loss: 65262.03125 acc: 0.9925851821899414  val: loss: 2439900.25 acc: -0.41103148460388184\n",
      "step: 13570 , time : 0.0\n",
      "train: loss: 94809.9296875 acc: 0.982541024684906  val: loss: 513354.8125 acc: 0.8289356231689453\n",
      "step: 13575 , time : 0.0\n",
      "train: loss: 1837192.5 acc: 0.8815934062004089  val: loss: 1038028.9375 acc: 0.7798515558242798\n",
      "step: 13580 , time : 0.0\n",
      "train: loss: 612722.6875 acc: 0.9214560985565186  val: loss: 1817560.375 acc: -0.32098424434661865\n",
      "step: 13585 , time : 0.0\n",
      "train: loss: 353446.59375 acc: 0.9562236666679382  val: loss: 1528677.625 acc: 0.6656882762908936\n",
      "step: 13590 , time : 0.0\n",
      "train: loss: 1374420.875 acc: 0.950566828250885  val: loss: 455727.6875 acc: 0.8368188738822937\n",
      "step: 13595 , time : 0.0\n",
      "train: loss: 719681.1875 acc: 0.9774346351623535  val: loss: 610333.875 acc: 0.8546650409698486\n",
      "step: 13600 , time : 0.0\n",
      "train: loss: 1745050.25 acc: 0.9466255307197571  val: loss: 439804.5 acc: 0.9188004732131958\n",
      "step: 13605 , time : 0.001001119613647461\n",
      "train: loss: 707571.5625 acc: 0.9634076952934265  val: loss: 407475.3125 acc: 0.8253750801086426\n",
      "step: 13610 , time : 0.0\n",
      "train: loss: 478892.90625 acc: 0.9390941858291626  val: loss: 2070047.75 acc: 0.012685418128967285\n",
      "step: 13615 , time : 0.0\n",
      "train: loss: 402788.1875 acc: 0.954863429069519  val: loss: 500454.21875 acc: 0.8172263503074646\n",
      "step: 13620 , time : 0.0\n",
      "train: loss: 679740.375 acc: 0.8759341239929199  val: loss: 951485.25 acc: 0.5709405541419983\n",
      "step: 13625 , time : 0.0\n",
      "train: loss: 1147473.0 acc: 0.7723305225372314  val: loss: 532066.375 acc: 0.9358885288238525\n",
      "step: 13630 , time : 0.0\n",
      "train: loss: 1089418.375 acc: 0.5442042350769043  val: loss: 1586922.5 acc: 0.8104966878890991\n",
      "step: 13635 , time : 0.0\n",
      "train: loss: 861611.1875 acc: 0.7383188009262085  val: loss: 1438656.125 acc: 0.33855241537094116\n",
      "step: 13640 , time : 0.0\n",
      "train: loss: 1284895.0 acc: 0.6393450498580933  val: loss: 1099726.75 acc: 0.8372915983200073\n",
      "step: 13645 , time : 0.0\n",
      "train: loss: 775893.8125 acc: 0.6185479164123535  val: loss: 813127.4375 acc: 0.8236092329025269\n",
      "step: 13650 , time : 0.0\n",
      "train: loss: 865130.1875 acc: 0.8685223460197449  val: loss: 520678.875 acc: 0.8580613136291504\n",
      "step: 13655 , time : 0.0\n",
      "train: loss: 1222003.0 acc: 0.4224100112915039  val: loss: 1859173.375 acc: 0.7988169193267822\n",
      "step: 13660 , time : 0.0\n",
      "train: loss: 654159.375 acc: 0.6274294853210449  val: loss: 2265264.5 acc: 0.5473700761795044\n",
      "step: 13665 , time : 0.0\n",
      "train: loss: 516710.84375 acc: 0.6378560066223145  val: loss: 1961501.0 acc: 0.6799834966659546\n",
      "step: 13670 , time : 0.0\n",
      "train: loss: 309646.8125 acc: 0.7268679141998291  val: loss: 1218976.5 acc: 0.733244776725769\n",
      "step: 13675 , time : 0.0\n",
      "train: loss: 265540.21875 acc: 0.774660587310791  val: loss: 2012417.25 acc: 0.7652625441551208\n",
      "step: 13680 , time : 0.0\n",
      "train: loss: 65383.6484375 acc: 0.9396733045578003  val: loss: 798342.375 acc: 0.6716091632843018\n",
      "step: 13685 , time : 0.0\n",
      "train: loss: 155449.3125 acc: 0.8939599990844727  val: loss: 2402870.25 acc: 0.7209511995315552\n",
      "step: 13690 , time : 0.0\n",
      "train: loss: 53199.24609375 acc: 0.9567430019378662  val: loss: 2205243.5 acc: 0.7156847715377808\n",
      "step: 13695 , time : 0.0\n",
      "train: loss: 350642.875 acc: 0.7488692402839661  val: loss: 1517635.5 acc: 0.8306955695152283\n",
      "step: 13700 , time : 0.0\n",
      "train: loss: 198259.625 acc: 0.8669339418411255  val: loss: 1190411.25 acc: 0.7831091284751892\n",
      "step: 13705 , time : 0.0\n",
      "train: loss: 26109.193359375 acc: 0.9666370749473572  val: loss: 1126977.75 acc: 0.8729032278060913\n",
      "step: 13710 , time : 0.0\n",
      "train: loss: 145012.59375 acc: 0.9106557965278625  val: loss: 957179.6875 acc: 0.7938977479934692\n",
      "step: 13715 , time : 0.0\n",
      "train: loss: 519346.0625 acc: 0.7506309747695923  val: loss: 1433093.0 acc: 0.6881669759750366\n",
      "step: 13720 , time : 0.0\n",
      "train: loss: 515331.34375 acc: 0.7951579689979553  val: loss: 1156849.875 acc: 0.7812486886978149\n",
      "step: 13725 , time : 0.001001119613647461\n",
      "train: loss: 478742.65625 acc: 0.429063618183136  val: loss: 1161908.375 acc: 0.7060608267784119\n",
      "step: 13730 , time : 0.0010006427764892578\n",
      "train: loss: 568669.1875 acc: 0.7414802312850952  val: loss: 888192.125 acc: 0.7108530402183533\n",
      "step: 13735 , time : 0.0010006427764892578\n",
      "train: loss: 391852.0 acc: 0.7769700288772583  val: loss: 2335874.25 acc: 0.6927669644355774\n",
      "step: 13740 , time : 0.0\n",
      "train: loss: 2241791.75 acc: 0.7181024551391602  val: loss: 1448267.875 acc: 0.7832759618759155\n",
      "step: 13745 , time : 0.0\n",
      "train: loss: 1008806.875 acc: 0.800062894821167  val: loss: 1129585.75 acc: 0.723604679107666\n",
      "step: 13750 , time : 0.0\n",
      "train: loss: 910906.375 acc: 0.9040294289588928  val: loss: 1307897.0 acc: 0.8209465742111206\n",
      "step: 13755 , time : 0.0\n",
      "train: loss: 666412.3125 acc: 0.9405045509338379  val: loss: 1432069.5 acc: 0.717409610748291\n",
      "step: 13760 , time : 0.0\n",
      "train: loss: 1164371.0 acc: 0.761702835559845  val: loss: 1176741.125 acc: 0.8264110088348389\n",
      "step: 13765 , time : 0.0\n",
      "train: loss: 421132.21875 acc: 0.9209826588630676  val: loss: 1148380.25 acc: 0.8330351114273071\n",
      "step: 13770 , time : 0.0\n",
      "train: loss: 298941.09375 acc: 0.9590392112731934  val: loss: 1571017.125 acc: 0.7626422643661499\n",
      "step: 13775 , time : 0.0\n",
      "train: loss: 179217.796875 acc: 0.9838076233863831  val: loss: 1657077.5 acc: 0.6239233016967773\n",
      "step: 13780 , time : 0.0\n",
      "train: loss: 150043.078125 acc: 0.9899976253509521  val: loss: 2803763.0 acc: 0.21397793292999268\n",
      "step: 13785 , time : 0.0\n",
      "train: loss: 206434.953125 acc: 0.9714062213897705  val: loss: 727748.8125 acc: 0.8946755528450012\n",
      "step: 13790 , time : 0.0\n",
      "train: loss: 38631.1640625 acc: 0.9926350712776184  val: loss: 365551.28125 acc: 0.8927579522132874\n",
      "step: 13795 , time : 0.0\n",
      "train: loss: 108068.09375 acc: 0.9783616065979004  val: loss: 1625059.5 acc: 0.7219400405883789\n",
      "step: 13800 , time : 0.0\n",
      "train: loss: 31413.654296875 acc: 0.9835184812545776  val: loss: 3257830.25 acc: 0.7022908926010132\n",
      "step: 13805 , time : 0.0\n",
      "train: loss: 35686.93359375 acc: 0.9400138854980469  val: loss: 871890.875 acc: 0.8194579482078552\n",
      "step: 13810 , time : 0.0\n",
      "train: loss: 66527.9296875 acc: 0.9755318760871887  val: loss: 525885.5 acc: 0.8523558378219604\n",
      "step: 13815 , time : 0.0\n",
      "train: loss: 87103.25 acc: 0.9521252512931824  val: loss: 1323434.75 acc: 0.7756726145744324\n",
      "step: 13820 , time : 0.0\n",
      "train: loss: 28994.953125 acc: 0.879243016242981  val: loss: 128986.6796875 acc: 0.9713872075080872\n",
      "step: 13825 , time : 0.0\n",
      "train: loss: 21896.203125 acc: 0.9625973701477051  val: loss: 1101651.5 acc: 0.8596597909927368\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 13830 , time : 0.0\n",
      "train: loss: 25125.58203125 acc: 0.944576621055603  val: loss: 622390.5625 acc: 0.8012033104896545\n",
      "step: 13835 , time : 0.0\n",
      "train: loss: 27390.744140625 acc: 0.9574626088142395  val: loss: 1832892.75 acc: 0.14748752117156982\n",
      "step: 13840 , time : 0.0\n",
      "train: loss: 32911.8203125 acc: 0.96251380443573  val: loss: 4986342.5 acc: 0.476873517036438\n",
      "step: 13845 , time : 0.0\n",
      "train: loss: 61603.1328125 acc: 0.9420568943023682  val: loss: 515378.71875 acc: 0.9127594232559204\n",
      "step: 13850 , time : 0.0\n",
      "train: loss: 59696.70703125 acc: 0.9737480878829956  val: loss: 2578162.25 acc: -0.18053674697875977\n",
      "step: 13855 , time : 0.0\n",
      "train: loss: 117346.0859375 acc: 0.9381043910980225  val: loss: 999033.625 acc: 0.6595098972320557\n",
      "step: 13860 , time : 0.0010004043579101562\n",
      "train: loss: 30061.126953125 acc: 0.9746279716491699  val: loss: 513431.25 acc: 0.9018208980560303\n",
      "step: 13865 , time : 0.0\n",
      "train: loss: 47763.07421875 acc: 0.9774472117424011  val: loss: 1316587.25 acc: 0.7905799746513367\n",
      "step: 13870 , time : 0.0\n",
      "train: loss: 39558.515625 acc: 0.9646556973457336  val: loss: 516116.5625 acc: 0.7825629115104675\n",
      "step: 13875 , time : 0.0\n",
      "train: loss: 50371.5078125 acc: 0.9784876108169556  val: loss: 1375594.125 acc: 0.439766526222229\n",
      "step: 13880 , time : 0.0\n",
      "train: loss: 71818.4765625 acc: 0.9662923812866211  val: loss: 417455.46875 acc: 0.9565942883491516\n",
      "step: 13885 , time : 0.0\n",
      "train: loss: 94349.4453125 acc: 0.9751037955284119  val: loss: 205592.125 acc: 0.9503054618835449\n",
      "step: 13890 , time : 0.0\n",
      "train: loss: 73613.6875 acc: 0.9625186920166016  val: loss: 743029.625 acc: 0.895668625831604\n",
      "step: 13895 , time : 0.0\n",
      "train: loss: 60729.83984375 acc: 0.9770671129226685  val: loss: 178906.734375 acc: 0.9499898552894592\n",
      "step: 13900 , time : 0.0\n",
      "train: loss: 374860.96875 acc: 0.8386309742927551  val: loss: 1534793.0 acc: 0.48936569690704346\n",
      "step: 13905 , time : 0.0\n",
      "train: loss: 288312.25 acc: 0.9154991507530212  val: loss: 951756.9375 acc: 0.8011475205421448\n",
      "step: 13910 , time : 0.0\n",
      "train: loss: 50648.45703125 acc: 0.9781672954559326  val: loss: 672497.75 acc: 0.7186940312385559\n",
      "step: 13915 , time : 0.0\n",
      "train: loss: 258328.5 acc: 0.9143407940864563  val: loss: 727444.8125 acc: 0.8135206699371338\n",
      "step: 13920 , time : 0.0\n",
      "train: loss: 171251.96875 acc: 0.9770171046257019  val: loss: 1444643.125 acc: 0.6162744760513306\n",
      "step: 13925 , time : 0.0\n",
      "train: loss: 119073.8125 acc: 0.9882155656814575  val: loss: 1131033.625 acc: 0.8209216594696045\n",
      "step: 13930 , time : 0.0\n",
      "train: loss: 45689.98046875 acc: 0.9938238263130188  val: loss: 274840.125 acc: 0.9277452826499939\n",
      "step: 13935 , time : 0.0\n",
      "train: loss: 362156.0 acc: 0.9596827626228333  val: loss: 499597.09375 acc: 0.6962562799453735\n",
      "step: 13940 , time : 0.015625715255737305\n",
      "train: loss: 297251.28125 acc: 0.9750987887382507  val: loss: 829242.5625 acc: 0.8636096715927124\n",
      "step: 13945 , time : 0.0\n",
      "train: loss: 452589.3125 acc: 0.9670981764793396  val: loss: 465541.78125 acc: 0.9242295622825623\n",
      "step: 13950 , time : 0.0010008811950683594\n",
      "train: loss: 339771.25 acc: 0.9440944194793701  val: loss: 698507.9375 acc: 0.8490747213363647\n",
      "step: 13955 , time : 0.0010006427764892578\n",
      "train: loss: 1047655.25 acc: 0.9579707384109497  val: loss: 566955.6875 acc: 0.9123163223266602\n",
      "step: 13960 , time : 0.0\n",
      "train: loss: 635840.3125 acc: 0.9715335369110107  val: loss: 609676.0 acc: 0.8828607201576233\n",
      "step: 13965 , time : 0.0\n",
      "train: loss: 1916724.375 acc: 0.9323386549949646  val: loss: 378520.1875 acc: 0.8125524520874023\n",
      "step: 13970 , time : 0.0\n",
      "train: loss: 1091715.0 acc: 0.9427107572555542  val: loss: 894603.8125 acc: 0.8317688703536987\n",
      "step: 13975 , time : 0.0\n",
      "train: loss: 397334.625 acc: 0.9697308540344238  val: loss: 716341.1875 acc: 0.9158914685249329\n",
      "step: 13980 , time : 0.0\n",
      "train: loss: 1275443.0 acc: 0.9035258293151855  val: loss: 765707.0625 acc: 0.9314754605293274\n",
      "step: 13985 , time : 0.0\n",
      "train: loss: 319576.4375 acc: 0.9574819207191467  val: loss: 4033478.25 acc: 0.4464988708496094\n",
      "step: 13990 , time : 0.0\n",
      "train: loss: 493443.0625 acc: 0.8967218399047852  val: loss: 555616.4375 acc: 0.9231342673301697\n",
      "step: 13995 , time : 0.0\n",
      "train: loss: 2560571.75 acc: 0.43171101808547974  val: loss: 574024.1875 acc: 0.8195509910583496\n",
      "step: 14000 , time : 0.0\n",
      "train: loss: 840108.6875 acc: 0.6520920991897583  val: loss: 572466.9375 acc: 0.7414278984069824\n",
      "step: 14005 , time : 0.0\n",
      "train: loss: 1377561.0 acc: 0.5205296277999878  val: loss: 1619836.25 acc: 0.8586853742599487\n",
      "step: 14010 , time : 0.0\n",
      "train: loss: 311128.59375 acc: 0.8336732983589172  val: loss: 550746.8125 acc: 0.7552856802940369\n",
      "step: 14015 , time : 0.0\n",
      "train: loss: 831552.8125 acc: 0.7109301090240479  val: loss: 1122911.25 acc: 0.8326423168182373\n",
      "step: 14020 , time : 0.0\n",
      "train: loss: 1273713.375 acc: 0.4823044538497925  val: loss: 2413131.0 acc: 0.6331986784934998\n",
      "step: 14025 , time : 0.0\n",
      "train: loss: 626557.5 acc: 0.6757979393005371  val: loss: 2526954.25 acc: 0.5627908706665039\n",
      "step: 14030 , time : 0.0\n",
      "train: loss: 585963.4375 acc: 0.4941229224205017  val: loss: 2213106.75 acc: 0.7475316524505615\n",
      "step: 14035 , time : 0.0\n",
      "train: loss: 222461.421875 acc: 0.7851852774620056  val: loss: 2074787.0 acc: 0.7794775366783142\n",
      "step: 14040 , time : 0.0\n",
      "train: loss: 186845.921875 acc: 0.8695216178894043  val: loss: 1337304.25 acc: 0.696061372756958\n",
      "step: 14045 , time : 0.0\n",
      "train: loss: 214329.375 acc: 0.8640117645263672  val: loss: 505635.90625 acc: 0.7083264589309692\n",
      "step: 14050 , time : 0.0\n",
      "train: loss: 43491.80859375 acc: 0.964954674243927  val: loss: 1781005.25 acc: 0.715796172618866\n",
      "step: 14055 , time : 0.0\n",
      "train: loss: 112598.8515625 acc: 0.9169850945472717  val: loss: 827650.125 acc: 0.812166690826416\n",
      "step: 14060 , time : 0.0\n",
      "train: loss: 227831.21875 acc: 0.8451615571975708  val: loss: 780956.9375 acc: 0.7853665351867676\n",
      "step: 14065 , time : 0.0\n",
      "train: loss: 139611.625 acc: 0.8770469427108765  val: loss: 220071.515625 acc: 0.8699125647544861\n",
      "step: 14070 , time : 0.001001119613647461\n",
      "train: loss: 53375.87109375 acc: 0.945614218711853  val: loss: 1715448.75 acc: 0.6620121598243713\n",
      "step: 14075 , time : 0.0\n",
      "train: loss: 136241.5625 acc: 0.8813732862472534  val: loss: 408853.0 acc: 0.7791054844856262\n",
      "step: 14080 , time : 0.0\n",
      "train: loss: 389350.53125 acc: 0.7931385040283203  val: loss: 2231569.5 acc: 0.7160946726799011\n",
      "step: 14085 , time : 0.0010006427764892578\n",
      "train: loss: 322118.59375 acc: 0.7631610631942749  val: loss: 1588935.75 acc: 0.6835814714431763\n",
      "step: 14090 , time : 0.0\n",
      "train: loss: 780054.0 acc: 0.7367970943450928  val: loss: 308304.75 acc: 0.7580141425132751\n",
      "step: 14095 , time : 0.0\n",
      "train: loss: 238626.828125 acc: 0.8447822332382202  val: loss: 255072.53125 acc: 0.8540593385696411\n",
      "step: 14100 , time : 0.0\n",
      "train: loss: 217547.140625 acc: 0.8639499545097351  val: loss: 2852758.5 acc: 0.6251416206359863\n",
      "step: 14105 , time : 0.0\n",
      "train: loss: 1472620.0 acc: 0.6896432042121887  val: loss: 1934898.25 acc: 0.6984944343566895\n",
      "step: 14110 , time : 0.0\n",
      "train: loss: 1573873.25 acc: 0.7561794519424438  val: loss: 847945.375 acc: 0.8026303052902222\n",
      "step: 14115 , time : 0.0\n",
      "train: loss: 1646403.5 acc: 0.7839420437812805  val: loss: 1114924.0 acc: 0.8735793828964233\n",
      "step: 14120 , time : 0.0\n",
      "train: loss: 938354.5625 acc: 0.9121509194374084  val: loss: 2807756.0 acc: 0.6296320557594299\n",
      "step: 14125 , time : 0.0\n",
      "train: loss: 504465.28125 acc: 0.9397608041763306  val: loss: 1051672.5 acc: 0.8348597288131714\n",
      "step: 14130 , time : 0.0\n",
      "train: loss: 496077.3125 acc: 0.9113274812698364  val: loss: 1582014.125 acc: 0.8383437395095825\n",
      "step: 14135 , time : 0.0\n",
      "train: loss: 200668.890625 acc: 0.9771195650100708  val: loss: 1740042.375 acc: 0.6358758211135864\n",
      "step: 14140 , time : 0.0\n",
      "train: loss: 165676.671875 acc: 0.9867569804191589  val: loss: 1370808.125 acc: 0.6394948363304138\n",
      "step: 14145 , time : 0.0010008811950683594\n",
      "train: loss: 215529.09375 acc: 0.9849160313606262  val: loss: 1116289.0 acc: 0.8083333969116211\n",
      "step: 14150 , time : 0.0010008811950683594\n",
      "train: loss: 198525.40625 acc: 0.9788124561309814  val: loss: 521528.28125 acc: 0.7360073328018188\n",
      "step: 14155 , time : 0.0010004043579101562\n",
      "train: loss: 82422.9296875 acc: 0.9835358262062073  val: loss: 289445.25 acc: 0.9284696578979492\n",
      "step: 14160 , time : 0.0\n",
      "train: loss: 55245.97265625 acc: 0.9874285459518433  val: loss: 264504.65625 acc: 0.9451271295547485\n",
      "step: 14165 , time : 0.0\n",
      "train: loss: 45405.12109375 acc: 0.9883095026016235  val: loss: 502198.78125 acc: 0.9250276684761047\n",
      "step: 14170 , time : 0.0\n",
      "train: loss: 18525.0703125 acc: 0.992452085018158  val: loss: 802209.5625 acc: 0.9238629341125488\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 14175 , time : 0.0\n",
      "train: loss: 11171.076171875 acc: 0.9827287793159485  val: loss: 1133326.375 acc: 0.5636770725250244\n",
      "step: 14180 , time : 0.0\n",
      "train: loss: 23310.2578125 acc: 0.9756046533584595  val: loss: 1430319.0 acc: 0.7718294858932495\n",
      "step: 14185 , time : 0.0\n",
      "train: loss: 24288.0078125 acc: 0.8691931962966919  val: loss: 1046946.8125 acc: 0.7865550518035889\n",
      "step: 14190 , time : 0.0\n",
      "train: loss: 131234.375 acc: 0.7985056042671204  val: loss: 1075257.75 acc: 0.7718894481658936\n",
      "step: 14195 , time : 0.0010006427764892578\n",
      "train: loss: 21524.806640625 acc: 0.9579604268074036  val: loss: 1539247.5 acc: 0.7071898579597473\n",
      "step: 14200 , time : 0.0010006427764892578\n",
      "train: loss: 27367.7578125 acc: 0.9118696451187134  val: loss: 2088381.0 acc: 0.2738726735115051\n",
      "step: 14205 , time : 0.0\n",
      "train: loss: 13582.3896484375 acc: 0.9719659686088562  val: loss: 546245.8125 acc: 0.799521267414093\n",
      "step: 14210 , time : 0.0\n",
      "train: loss: 109313.4609375 acc: 0.9480141997337341  val: loss: 453601.1875 acc: 0.8382511138916016\n",
      "step: 14215 , time : 0.0\n",
      "train: loss: 66365.2890625 acc: 0.9674253463745117  val: loss: 1073992.625 acc: 0.6695467829704285\n",
      "step: 14220 , time : 0.0\n",
      "train: loss: 63413.296875 acc: 0.9651792049407959  val: loss: 1718469.5 acc: 0.38217437267303467\n",
      "step: 14225 , time : 0.0\n",
      "train: loss: 43022.21484375 acc: 0.9784128665924072  val: loss: 1463044.25 acc: 0.7180901765823364\n",
      "step: 14230 , time : 0.0\n",
      "train: loss: 31273.73046875 acc: 0.9636419415473938  val: loss: 1146349.0 acc: 0.8397828340530396\n",
      "step: 14235 , time : 0.0\n",
      "train: loss: 44256.48828125 acc: 0.9636475443840027  val: loss: 1485907.375 acc: 0.7965821027755737\n",
      "step: 14240 , time : 0.0\n",
      "train: loss: 19367.787109375 acc: 0.9836301803588867  val: loss: 626328.4375 acc: 0.8314921855926514\n",
      "step: 14245 , time : 0.0\n",
      "train: loss: 45968.11328125 acc: 0.9851534366607666  val: loss: 175247.890625 acc: 0.9443523287773132\n",
      "step: 14250 , time : 0.0\n",
      "train: loss: 80828.7734375 acc: 0.9815433621406555  val: loss: 1954111.875 acc: -0.9871916770935059\n",
      "step: 14255 , time : 0.0\n",
      "train: loss: 47461.66015625 acc: 0.9869105219841003  val: loss: 196704.34375 acc: 0.9430668354034424\n",
      "step: 14260 , time : 0.0\n",
      "train: loss: 56370.8828125 acc: 0.984994649887085  val: loss: 769135.0625 acc: 0.6511542201042175\n",
      "step: 14265 , time : 0.0\n",
      "train: loss: 151026.46875 acc: 0.9359071254730225  val: loss: 215186.46875 acc: 0.9548831582069397\n",
      "step: 14270 , time : 0.0\n",
      "train: loss: 194558.125 acc: 0.9597679376602173  val: loss: 199983.859375 acc: 0.9154736995697021\n",
      "step: 14275 , time : 0.0\n",
      "train: loss: 80118.296875 acc: 0.9610409736633301  val: loss: 302401.71875 acc: 0.9672757387161255\n",
      "step: 14280 , time : 0.015625476837158203\n",
      "train: loss: 299925.75 acc: 0.87904953956604  val: loss: 240515.640625 acc: 0.9644055962562561\n",
      "step: 14285 , time : 0.001001119613647461\n",
      "train: loss: 676375.4375 acc: 0.9137852787971497  val: loss: 223564.28125 acc: 0.9083000421524048\n",
      "step: 14290 , time : 0.0\n",
      "train: loss: 165472.8125 acc: 0.9838885068893433  val: loss: 297018.96875 acc: 0.9403740167617798\n",
      "step: 14295 , time : 0.0010004043579101562\n",
      "train: loss: 47467.5703125 acc: 0.9944489598274231  val: loss: 514154.125 acc: 0.9378088116645813\n",
      "step: 14300 , time : 0.0\n",
      "train: loss: 532651.625 acc: 0.9538446068763733  val: loss: 423346.4375 acc: 0.9564835429191589\n",
      "step: 14305 , time : 0.001001119613647461\n",
      "train: loss: 264772.875 acc: 0.95524662733078  val: loss: 594747.9375 acc: 0.9573090076446533\n",
      "step: 14310 , time : 0.0010008811950683594\n",
      "train: loss: 1291691.75 acc: 0.9440235495567322  val: loss: 463058.21875 acc: 0.8742777705192566\n",
      "step: 14315 , time : 0.0\n",
      "train: loss: 468311.65625 acc: 0.9612921476364136  val: loss: 1083523.375 acc: 0.8464411497116089\n",
      "step: 14320 , time : 0.0\n",
      "train: loss: 337268.21875 acc: 0.9139739274978638  val: loss: 1868082.5 acc: 0.8821814656257629\n",
      "step: 14325 , time : 0.0\n",
      "train: loss: 2939242.25 acc: 0.886630117893219  val: loss: 1257350.875 acc: 0.7223211526870728\n",
      "step: 14330 , time : 0.0\n",
      "train: loss: 2044959.25 acc: 0.9159249663352966  val: loss: 958535.25 acc: 0.9065402746200562\n",
      "step: 14335 , time : 0.0\n",
      "train: loss: 1635677.375 acc: 0.9455835819244385  val: loss: 1355748.125 acc: 0.759256899356842\n",
      "step: 14340 , time : 0.0\n",
      "train: loss: 759884.0 acc: 0.8818960189819336  val: loss: 842940.0 acc: 0.7766414284706116\n",
      "step: 14345 , time : 0.0\n",
      "train: loss: 399354.9375 acc: 0.9619128704071045  val: loss: 786327.0625 acc: 0.8923789262771606\n",
      "step: 14350 , time : 0.0\n",
      "train: loss: 147343.90625 acc: 0.9838374257087708  val: loss: 1661242.75 acc: 0.7231220006942749\n",
      "step: 14355 , time : 0.0\n",
      "train: loss: 396603.03125 acc: 0.8756279945373535  val: loss: 1969090.75 acc: 0.8440539836883545\n",
      "step: 14360 , time : 0.0\n",
      "train: loss: 939183.0625 acc: 0.6192258596420288  val: loss: 2119261.5 acc: 0.20983654260635376\n",
      "step: 14365 , time : 0.0\n",
      "train: loss: 1366266.5 acc: 0.6916627287864685  val: loss: 1235006.125 acc: 0.8629401326179504\n",
      "step: 14370 , time : 0.0\n",
      "train: loss: 829962.0 acc: 0.698961615562439  val: loss: 1110720.375 acc: 0.6825556755065918\n",
      "step: 14375 , time : 0.0\n",
      "train: loss: 280819.96875 acc: 0.9411063194274902  val: loss: 582757.3125 acc: 0.9268717765808105\n",
      "step: 14380 , time : 0.0\n",
      "train: loss: 477770.59375 acc: 0.8592219948768616  val: loss: 470607.65625 acc: 0.8111783266067505\n",
      "step: 14385 , time : 0.0\n",
      "train: loss: 3073047.25 acc: -0.8495906591415405  val: loss: 685307.0625 acc: 0.9315764307975769\n",
      "step: 14390 , time : 0.0\n",
      "train: loss: 1189921.0 acc: 0.4122138023376465  val: loss: 3439447.5 acc: 0.6150586009025574\n",
      "step: 14395 , time : 0.0\n",
      "train: loss: 438674.625 acc: 0.6681584119796753  val: loss: 1459215.0 acc: 0.6173288822174072\n",
      "step: 14400 , time : 0.0\n",
      "train: loss: 575878.875 acc: 0.4557883143424988  val: loss: 778940.125 acc: 0.5943482518196106\n",
      "step: 14405 , time : 0.0\n",
      "train: loss: 322976.34375 acc: 0.7726752161979675  val: loss: 669196.5625 acc: 0.682654857635498\n",
      "step: 14410 , time : 0.0\n",
      "train: loss: 219642.25 acc: 0.866479754447937  val: loss: 140124.453125 acc: 0.9036698937416077\n",
      "step: 14415 , time : 0.0010004043579101562\n",
      "train: loss: 120370.8203125 acc: 0.9025188684463501  val: loss: 412003.53125 acc: 0.6989700198173523\n",
      "step: 14420 , time : 0.0\n",
      "train: loss: 224047.71875 acc: 0.8364772200584412  val: loss: 1052261.75 acc: 0.7331154346466064\n",
      "step: 14425 , time : 0.0\n",
      "train: loss: 265618.09375 acc: 0.8664512634277344  val: loss: 2564303.25 acc: 0.7751116156578064\n",
      "step: 14430 , time : 0.0\n",
      "train: loss: 287172.625 acc: 0.6711001396179199  val: loss: 2826420.0 acc: 0.7450420260429382\n",
      "step: 14435 , time : 0.015624523162841797\n",
      "train: loss: 145880.90625 acc: 0.8653430938720703  val: loss: 1649199.5 acc: 0.7789562344551086\n",
      "step: 14440 , time : 0.0\n",
      "train: loss: 52270.7890625 acc: 0.8985944390296936  val: loss: 907669.8125 acc: 0.8116950988769531\n",
      "step: 14445 , time : 0.0\n",
      "train: loss: 559420.5625 acc: 0.7359578013420105  val: loss: 701465.0 acc: 0.7247934341430664\n",
      "step: 14450 , time : 0.0\n",
      "train: loss: 669110.5625 acc: 0.7316679954528809  val: loss: 672926.3125 acc: 0.7356940507888794\n",
      "step: 14455 , time : 0.0\n",
      "train: loss: 152685.953125 acc: 0.8088170886039734  val: loss: 1051551.25 acc: 0.6901822090148926\n",
      "step: 14460 , time : 0.0\n",
      "train: loss: 223400.921875 acc: 0.8407076001167297  val: loss: 3652385.75 acc: 0.6563925743103027\n",
      "step: 14465 , time : 0.0\n",
      "train: loss: 598488.75 acc: 0.6631239652633667  val: loss: 3070337.0 acc: 0.5528771877288818\n",
      "step: 14470 , time : 0.0\n",
      "train: loss: 1547702.75 acc: 0.6342920660972595  val: loss: 548999.125 acc: 0.6535638570785522\n",
      "step: 14475 , time : 0.0\n",
      "train: loss: 1606943.125 acc: 0.7593053579330444  val: loss: 795763.5625 acc: 0.7231208682060242\n",
      "step: 14480 , time : 0.0\n",
      "train: loss: 852938.3125 acc: 0.8696291446685791  val: loss: 2326762.0 acc: -0.051798462867736816\n",
      "step: 14485 , time : 0.0\n",
      "train: loss: 1104524.75 acc: 0.9122957587242126  val: loss: 1125633.5 acc: 0.6260810494422913\n",
      "step: 14490 , time : 0.0\n",
      "train: loss: 798703.25 acc: 0.8676528930664062  val: loss: 969822.625 acc: 0.7073129415512085\n",
      "step: 14495 , time : 0.0\n",
      "train: loss: 245440.015625 acc: 0.9590772390365601  val: loss: 288868.09375 acc: 0.9023008942604065\n",
      "step: 14500 , time : 0.0\n",
      "train: loss: 187627.609375 acc: 0.9651831388473511  val: loss: 1291670.25 acc: 0.6943395137786865\n",
      "step: 14505 , time : 0.0\n",
      "train: loss: 188191.03125 acc: 0.9875575304031372  val: loss: 929354.6875 acc: 0.7865275144577026\n",
      "step: 14510 , time : 0.0\n",
      "train: loss: 120592.5234375 acc: 0.9921451210975647  val: loss: 1197512.625 acc: 0.7254243493080139\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 14515 , time : 0.0\n",
      "train: loss: 451919.0625 acc: 0.9568116068840027  val: loss: 317910.4375 acc: 0.934084951877594\n",
      "step: 14520 , time : 0.0\n",
      "train: loss: 113383.28125 acc: 0.9846745729446411  val: loss: 657947.5625 acc: 0.8138017058372498\n",
      "step: 14525 , time : 0.0\n",
      "train: loss: 80342.359375 acc: 0.9707006216049194  val: loss: 635444.4375 acc: 0.8733581304550171\n",
      "step: 14530 , time : 0.0010004043579101562\n",
      "train: loss: 31402.80859375 acc: 0.9669647812843323  val: loss: 122699.375 acc: 0.9671750068664551\n",
      "step: 14535 , time : 0.0\n",
      "train: loss: 20478.03515625 acc: 0.9623156785964966  val: loss: 435442.71875 acc: 0.9313191771507263\n",
      "step: 14540 , time : 0.0\n",
      "train: loss: 51922.375 acc: 0.9627788662910461  val: loss: 149858.375 acc: 0.9659178256988525\n",
      "step: 14545 , time : 0.0010006427764892578\n",
      "train: loss: 25822.697265625 acc: 0.9771836996078491  val: loss: 665806.5625 acc: 0.8010509014129639\n",
      "step: 14550 , time : 0.0010008811950683594\n",
      "train: loss: 30276.939453125 acc: 0.9636611938476562  val: loss: 1764826.0 acc: 0.7947609424591064\n",
      "step: 14555 , time : 0.0\n",
      "train: loss: 22905.3046875 acc: 0.9488083124160767  val: loss: 323615.0 acc: 0.9272654056549072\n",
      "step: 14560 , time : 0.0\n",
      "train: loss: 11843.8701171875 acc: 0.9790194034576416  val: loss: 268912.125 acc: 0.8808359503746033\n",
      "step: 14565 , time : 0.0\n",
      "train: loss: 21976.90234375 acc: 0.9506585597991943  val: loss: 117833.125 acc: 0.9399205446243286\n",
      "step: 14570 , time : 0.0\n",
      "train: loss: 22339.8125 acc: 0.88885897397995  val: loss: 478965.34375 acc: 0.8385275602340698\n",
      "step: 14575 , time : 0.0\n",
      "train: loss: 80925.7890625 acc: 0.9591286778450012  val: loss: 395497.59375 acc: 0.8888043761253357\n",
      "step: 14580 , time : 0.0\n",
      "train: loss: 46482.53125 acc: 0.9672408699989319  val: loss: 591153.8125 acc: 0.9134142398834229\n",
      "step: 14585 , time : 0.0\n",
      "train: loss: 37324.23828125 acc: 0.972345769405365  val: loss: 367050.3125 acc: 0.8440380692481995\n",
      "step: 14590 , time : 0.0\n",
      "train: loss: 55836.8046875 acc: 0.9732221364974976  val: loss: 657983.1875 acc: 0.8366449475288391\n",
      "step: 14595 , time : 0.0\n",
      "train: loss: 36866.16015625 acc: 0.9769818186759949  val: loss: 666457.125 acc: 0.788635790348053\n",
      "step: 14600 , time : 0.0\n",
      "train: loss: 97872.5546875 acc: 0.8785890936851501  val: loss: 776564.1875 acc: 0.9496014714241028\n",
      "step: 14605 , time : 0.0\n",
      "train: loss: 38993.99609375 acc: 0.966137707233429  val: loss: 385673.59375 acc: 0.9559988975524902\n",
      "step: 14610 , time : 0.0\n",
      "train: loss: 107196.8203125 acc: 0.9681533575057983  val: loss: 947799.1875 acc: 0.8410594463348389\n",
      "step: 14615 , time : 0.0\n",
      "train: loss: 54663.62109375 acc: 0.9892279505729675  val: loss: 1099788.375 acc: 0.908976137638092\n",
      "step: 14620 , time : 0.0\n",
      "train: loss: 103861.3515625 acc: 0.9847341179847717  val: loss: 655728.75 acc: 0.9023916721343994\n",
      "step: 14625 , time : 0.0\n",
      "train: loss: 85279.8203125 acc: 0.928178071975708  val: loss: 777477.125 acc: 0.7578902244567871\n",
      "step: 14630 , time : 0.0\n",
      "train: loss: 33668.9140625 acc: 0.9817714691162109  val: loss: 1923029.875 acc: 0.6887058019638062\n",
      "step: 14635 , time : 0.0\n",
      "train: loss: 163376.59375 acc: 0.9550630450248718  val: loss: 1081170.5 acc: 0.873823881149292\n",
      "step: 14640 , time : 0.0\n",
      "train: loss: 180368.953125 acc: 0.9619202613830566  val: loss: 1744707.5 acc: 0.5703350305557251\n",
      "step: 14645 , time : 0.0010004043579101562\n",
      "train: loss: 190892.453125 acc: 0.9505174160003662  val: loss: 648027.3125 acc: 0.9327600598335266\n",
      "step: 14650 , time : 0.0\n",
      "train: loss: 542601.5625 acc: 0.9360765814781189  val: loss: 4146632.75 acc: 0.4691566824913025\n",
      "step: 14655 , time : 0.0010006427764892578\n",
      "train: loss: 469619.03125 acc: 0.9617345333099365  val: loss: 411631.6875 acc: 0.9274638891220093\n",
      "step: 14660 , time : 0.0010004043579101562\n",
      "train: loss: 97597.390625 acc: 0.987348198890686  val: loss: 532116.6875 acc: 0.9462974071502686\n",
      "step: 14665 , time : 0.0010006427764892578\n",
      "train: loss: 47214.63671875 acc: 0.9923138618469238  val: loss: 1554276.875 acc: 0.6489921808242798\n",
      "step: 14670 , time : 0.0010004043579101562\n",
      "train: loss: 222585.078125 acc: 0.9761337637901306  val: loss: 968627.1875 acc: 0.6085669994354248\n",
      "step: 14675 , time : 0.0\n",
      "train: loss: 564064.375 acc: 0.9694927930831909  val: loss: 3637454.75 acc: 0.7204748392105103\n",
      "step: 14680 , time : 0.0\n",
      "train: loss: 509638.28125 acc: 0.9663015007972717  val: loss: 1647880.875 acc: 0.7027047872543335\n",
      "step: 14685 , time : 0.0\n",
      "train: loss: 232581.421875 acc: 0.8817775249481201  val: loss: 572975.4375 acc: 0.949765682220459\n",
      "step: 14690 , time : 0.0\n",
      "train: loss: 806532.5625 acc: 0.9705302119255066  val: loss: 2361677.75 acc: 0.7628445029258728\n",
      "step: 14695 , time : 0.0\n",
      "train: loss: 677474.875 acc: 0.9773469567298889  val: loss: 941216.5 acc: 0.8846226930618286\n",
      "step: 14700 , time : 0.0\n",
      "train: loss: 1260973.75 acc: 0.9490521550178528  val: loss: 4876622.0 acc: 0.2556198239326477\n",
      "step: 14705 , time : 0.0\n",
      "train: loss: 612730.0625 acc: 0.9728409051895142  val: loss: 883083.1875 acc: 0.6605775952339172\n",
      "step: 14710 , time : 0.0\n",
      "train: loss: 660741.25 acc: 0.9664773941040039  val: loss: 684419.3125 acc: 0.8796095252037048\n",
      "step: 14715 , time : 0.0\n",
      "train: loss: 644378.3125 acc: 0.9483736157417297  val: loss: 1260047.875 acc: 0.7605647444725037\n",
      "step: 14720 , time : 0.0\n",
      "train: loss: 167150.28125 acc: 0.9670736789703369  val: loss: 1874798.125 acc: 0.16852331161499023\n",
      "step: 14725 , time : 0.0\n",
      "train: loss: 2157349.75 acc: 0.13160783052444458  val: loss: 597204.5 acc: 0.7179685235023499\n",
      "step: 14730 , time : 0.0\n",
      "train: loss: 425549.34375 acc: 0.6287170052528381  val: loss: 1874290.5 acc: 0.5187697410583496\n",
      "step: 14735 , time : 0.0\n",
      "train: loss: 1171552.25 acc: 0.7280768156051636  val: loss: 1743616.5 acc: 0.6646716594696045\n",
      "step: 14740 , time : 0.0\n",
      "train: loss: 915005.6875 acc: 0.49186521768569946  val: loss: 1326946.75 acc: 0.8351938724517822\n",
      "step: 14745 , time : 0.0010001659393310547\n",
      "train: loss: 535541.6875 acc: 0.8394169807434082  val: loss: 598728.9375 acc: 0.8224863409996033\n",
      "step: 14750 , time : 0.0\n",
      "train: loss: 2106370.25 acc: -0.3110555410385132  val: loss: 764427.5625 acc: 0.7903514504432678\n",
      "step: 14755 , time : 0.0\n",
      "train: loss: 1224045.625 acc: 0.4359126091003418  val: loss: 2355322.75 acc: 0.6803890466690063\n",
      "step: 14760 , time : 0.0\n",
      "train: loss: 1398419.375 acc: 0.318556547164917  val: loss: 1061293.75 acc: 0.73567795753479\n",
      "step: 14765 , time : 0.0\n",
      "train: loss: 203659.0 acc: 0.833844780921936  val: loss: 1584414.125 acc: 0.7732881307601929\n",
      "step: 14770 , time : 0.0\n",
      "train: loss: 229834.1875 acc: 0.8069355487823486  val: loss: 914278.0 acc: 0.7711216807365417\n",
      "step: 14775 , time : 0.0\n",
      "train: loss: 294262.46875 acc: 0.8420333862304688  val: loss: 1563936.75 acc: 0.7457536458969116\n",
      "step: 14780 , time : 0.0\n",
      "train: loss: 245545.65625 acc: 0.8433507084846497  val: loss: 960527.125 acc: 0.8968639373779297\n",
      "step: 14785 , time : 0.0\n",
      "train: loss: 279573.28125 acc: 0.8582555055618286  val: loss: 1259763.75 acc: 0.714799165725708\n",
      "step: 14790 , time : 0.0\n",
      "train: loss: 467330.90625 acc: 0.717500627040863  val: loss: 1340011.625 acc: 0.8496102094650269\n",
      "step: 14795 , time : 0.0\n",
      "train: loss: 220116.609375 acc: 0.8043481111526489  val: loss: 1773489.75 acc: 0.8163420557975769\n",
      "step: 14800 , time : 0.0\n",
      "train: loss: 401715.03125 acc: 0.738064169883728  val: loss: 558927.375 acc: 0.860558032989502\n",
      "step: 14805 , time : 0.0\n",
      "train: loss: 96786.7734375 acc: 0.9204820394515991  val: loss: 725877.3125 acc: 0.8697732090950012\n",
      "step: 14810 , time : 0.0\n",
      "train: loss: 258914.25 acc: 0.7908750772476196  val: loss: 1117532.375 acc: 0.8533068895339966\n",
      "step: 14815 , time : 0.0\n",
      "train: loss: 701076.0 acc: 0.6966649889945984  val: loss: 657199.25 acc: 0.7434922456741333\n",
      "step: 14820 , time : 0.0\n",
      "train: loss: 602952.25 acc: 0.7805312871932983  val: loss: 471990.375 acc: 0.8266544938087463\n",
      "step: 14825 , time : 0.0\n",
      "train: loss: 715444.5 acc: 0.450217068195343  val: loss: 505903.5625 acc: 0.7517173290252686\n",
      "step: 14830 , time : 0.0\n",
      "train: loss: 229902.015625 acc: 0.692621648311615  val: loss: 740167.8125 acc: 0.7506433725357056\n",
      "step: 14835 , time : 0.0\n",
      "train: loss: 1064164.5 acc: 0.7574715614318848  val: loss: 401147.28125 acc: 0.7759681344032288\n",
      "step: 14840 , time : 0.0\n",
      "train: loss: 1023516.8125 acc: 0.8406358957290649  val: loss: 1253846.25 acc: 0.7967045903205872\n",
      "step: 14845 , time : 0.0\n",
      "train: loss: 899889.125 acc: 0.8739532232284546  val: loss: 721614.5 acc: 0.8153244256973267\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 14850 , time : 0.0\n",
      "train: loss: 1125586.25 acc: 0.9142509698867798  val: loss: 998118.5 acc: 0.7953076958656311\n",
      "step: 14855 , time : 0.0010008811950683594\n",
      "train: loss: 605973.6875 acc: 0.9205884337425232  val: loss: 772376.8125 acc: 0.8830893039703369\n",
      "step: 14860 , time : 0.0\n",
      "train: loss: 537577.5625 acc: 0.933498203754425  val: loss: 403652.09375 acc: 0.8557024598121643\n",
      "step: 14865 , time : 0.0\n",
      "train: loss: 237687.53125 acc: 0.9688624143600464  val: loss: 698093.8125 acc: 0.7770477533340454\n",
      "step: 14870 , time : 0.0\n",
      "train: loss: 343935.03125 acc: 0.9728417992591858  val: loss: 474972.40625 acc: 0.6740490198135376\n",
      "step: 14875 , time : 0.0\n",
      "train: loss: 231718.375 acc: 0.9835498332977295  val: loss: 608503.6875 acc: 0.8648416996002197\n",
      "step: 14880 , time : 0.0\n",
      "train: loss: 291576.6875 acc: 0.9693053364753723  val: loss: 1351708.5 acc: 0.5301485061645508\n",
      "step: 14885 , time : 0.0\n",
      "train: loss: 232374.484375 acc: 0.9607734084129333  val: loss: 1030647.8125 acc: 0.8018907904624939\n",
      "step: 14890 , time : 0.0\n",
      "train: loss: 294664.75 acc: 0.9671815037727356  val: loss: 431078.375 acc: 0.9059301018714905\n",
      "step: 14895 , time : 0.0\n",
      "train: loss: 214178.546875 acc: 0.8784729242324829  val: loss: 362548.1875 acc: 0.8851908445358276\n",
      "step: 14900 , time : 0.0\n",
      "train: loss: 111295.4375 acc: 0.95679771900177  val: loss: 769575.1875 acc: 0.8770086765289307\n",
      "step: 14905 , time : 0.0\n",
      "train: loss: 59596.25390625 acc: 0.9601597189903259  val: loss: 474176.75 acc: 0.9117296934127808\n",
      "step: 14910 , time : 0.0\n",
      "train: loss: 70124.203125 acc: 0.9558277130126953  val: loss: 424937.46875 acc: 0.8769707083702087\n",
      "step: 14915 , time : 0.0\n",
      "train: loss: 17102.58203125 acc: 0.9642813801765442  val: loss: 1014597.1875 acc: 0.8571957349777222\n",
      "step: 14920 , time : 0.0\n",
      "train: loss: 19279.650390625 acc: 0.9688456058502197  val: loss: 589247.1875 acc: 0.9107493162155151\n",
      "step: 14925 , time : 0.0\n",
      "train: loss: 16158.0146484375 acc: 0.953837513923645  val: loss: 716384.125 acc: 0.8977512121200562\n",
      "step: 14930 , time : 0.0\n",
      "train: loss: 23723.419921875 acc: 0.95278000831604  val: loss: 701796.625 acc: 0.8725942373275757\n",
      "step: 14935 , time : 0.0\n",
      "train: loss: 10692.3271484375 acc: 0.974396288394928  val: loss: 855088.5625 acc: 0.8575146198272705\n",
      "step: 14940 , time : 0.0\n",
      "train: loss: 105679.6875 acc: 0.9364628791809082  val: loss: 299318.1875 acc: 0.9471558332443237\n",
      "step: 14945 , time : 0.0\n",
      "train: loss: 31789.55078125 acc: 0.9688692688941956  val: loss: 808165.5 acc: 0.8372630476951599\n",
      "step: 14950 , time : 0.0\n",
      "train: loss: 141956.015625 acc: 0.925358772277832  val: loss: 309866.875 acc: 0.9438365697860718\n",
      "step: 14955 , time : 0.0\n",
      "train: loss: 47455.0625 acc: 0.9755986332893372  val: loss: 697048.0 acc: 0.8760098814964294\n",
      "step: 14960 , time : 0.0\n",
      "train: loss: 32073.591796875 acc: 0.9764630794525146  val: loss: 1069958.625 acc: 0.837709367275238\n",
      "step: 14965 , time : 0.0\n",
      "train: loss: 22451.833984375 acc: 0.9735719561576843  val: loss: 2246504.25 acc: 0.6167107224464417\n",
      "step: 14970 , time : 0.0\n",
      "train: loss: 30560.28515625 acc: 0.9645636081695557  val: loss: 672001.75 acc: 0.9143550992012024\n",
      "step: 14975 , time : 0.0\n",
      "train: loss: 48221.07421875 acc: 0.9735947251319885  val: loss: 1100828.875 acc: 0.9201653003692627\n",
      "step: 14980 , time : 0.0010006427764892578\n",
      "train: loss: 78985.5546875 acc: 0.9819203019142151  val: loss: 1379962.375 acc: 0.5577398538589478\n",
      "step: 14985 , time : 0.0\n",
      "train: loss: 84810.7109375 acc: 0.9763669967651367  val: loss: 2703831.75 acc: 0.5835951566696167\n",
      "step: 14990 , time : 0.0\n",
      "train: loss: 146868.953125 acc: 0.9680130481719971  val: loss: 2104532.5 acc: 0.8551521897315979\n",
      "step: 14995 , time : 0.0\n",
      "train: loss: 92878.25 acc: 0.9716290235519409  val: loss: 439816.5625 acc: 0.9015100002288818\n",
      "step: 15000 , time : 0.0\n",
      "train: loss: 125113.984375 acc: 0.9799415469169617  val: loss: 1131948.5 acc: 0.7282896637916565\n",
      "step: 15005 , time : 0.0\n",
      "train: loss: 276688.59375 acc: 0.8845651745796204  val: loss: 1366296.75 acc: 0.6196187734603882\n",
      "step: 15010 , time : 0.0\n",
      "train: loss: 169746.359375 acc: 0.968186616897583  val: loss: 1664215.125 acc: 0.3363924026489258\n",
      "step: 15015 , time : 0.0\n",
      "train: loss: 137226.15625 acc: 0.9790078997612  val: loss: 1408573.375 acc: 0.6180764436721802\n",
      "step: 15020 , time : 0.0\n",
      "train: loss: 455607.46875 acc: 0.9526263475418091  val: loss: 2135426.75 acc: -0.761664628982544\n",
      "step: 15025 , time : 0.0\n",
      "train: loss: 85314.484375 acc: 0.9896920919418335  val: loss: 1424881.375 acc: 0.5892294049263\n",
      "step: 15030 , time : 0.015625476837158203\n",
      "train: loss: 264409.25 acc: 0.9559556841850281  val: loss: 448054.28125 acc: 0.7201600074768066\n",
      "step: 15035 , time : 0.0\n",
      "train: loss: 258381.59375 acc: 0.9744961261749268  val: loss: 1947969.125 acc: 0.8120515942573547\n",
      "step: 15040 , time : 0.0\n",
      "train: loss: 1976295.5 acc: 0.8911339044570923  val: loss: 2196938.75 acc: 0.4912063479423523\n",
      "step: 15045 , time : 0.0\n",
      "train: loss: 307358.78125 acc: 0.9682257175445557  val: loss: 1285448.5 acc: 0.6416501998901367\n",
      "step: 15050 , time : 0.0\n",
      "train: loss: 342652.71875 acc: 0.9695355892181396  val: loss: 1003482.3125 acc: 0.8901742696762085\n",
      "step: 15055 , time : 0.0\n",
      "train: loss: 2501291.5 acc: 0.8784326910972595  val: loss: 1686096.25 acc: 0.4586947560310364\n",
      "step: 15060 , time : 0.0\n",
      "train: loss: 1149098.875 acc: 0.9604164361953735  val: loss: 366421.96875 acc: 0.9603366255760193\n",
      "step: 15065 , time : 0.0\n",
      "train: loss: 911178.375 acc: 0.9597412943840027  val: loss: 2407708.75 acc: -0.13833320140838623\n",
      "step: 15070 , time : 0.0\n",
      "train: loss: 1216419.875 acc: 0.9339566826820374  val: loss: 507620.75 acc: 0.8813014626502991\n",
      "step: 15075 , time : 0.0\n",
      "train: loss: 1102440.375 acc: 0.8763846158981323  val: loss: 378001.09375 acc: 0.9575484991073608\n",
      "step: 15080 , time : 0.0\n",
      "train: loss: 273750.78125 acc: 0.9539207816123962  val: loss: 1034127.8125 acc: 0.8393867611885071\n",
      "step: 15085 , time : 0.0010006427764892578\n",
      "train: loss: 573201.4375 acc: 0.878670871257782  val: loss: 1818542.375 acc: 0.7297849655151367\n",
      "step: 15090 , time : 0.001001119613647461\n",
      "train: loss: 1490063.125 acc: 0.6328201293945312  val: loss: 1502882.75 acc: 0.441378116607666\n",
      "step: 15095 , time : 0.0\n",
      "train: loss: 904711.75 acc: 0.6063617467880249  val: loss: 4770734.0 acc: 0.10852241516113281\n",
      "step: 15100 , time : 0.0010008811950683594\n",
      "train: loss: 778759.875 acc: 0.5857958197593689  val: loss: 1869967.5 acc: 0.6922284960746765\n",
      "step: 15105 , time : 0.0010008811950683594\n",
      "train: loss: 569408.25 acc: 0.8115389943122864  val: loss: 559741.0 acc: 0.84991055727005\n",
      "step: 15110 , time : 0.0\n",
      "train: loss: 679434.6875 acc: 0.8080350756645203  val: loss: 1074802.125 acc: 0.7871476411819458\n",
      "step: 15115 , time : 0.0\n",
      "train: loss: 1564723.125 acc: 0.16380095481872559  val: loss: 1131463.5 acc: 0.8477046489715576\n",
      "step: 15120 , time : 0.0\n",
      "train: loss: 1421918.75 acc: 0.08468383550643921  val: loss: 1999741.5 acc: 0.3614092469215393\n",
      "step: 15125 , time : 0.0\n",
      "train: loss: 1094092.875 acc: 0.502741813659668  val: loss: 2649232.25 acc: 0.7596460580825806\n",
      "step: 15130 , time : 0.0\n",
      "train: loss: 1094816.75 acc: 0.3048516511917114  val: loss: 1848725.125 acc: 0.6234921216964722\n",
      "step: 15135 , time : 0.0\n",
      "train: loss: 394627.34375 acc: 0.6564862728118896  val: loss: 1761600.625 acc: 0.7946213483810425\n",
      "step: 15140 , time : 0.0\n",
      "train: loss: 214731.328125 acc: 0.808429479598999  val: loss: 932170.0625 acc: 0.8243396878242493\n",
      "step: 15145 , time : 0.0\n",
      "train: loss: 58258.94921875 acc: 0.9501802921295166  val: loss: 1152297.75 acc: 0.6952924728393555\n",
      "step: 15150 , time : 0.0\n",
      "train: loss: 186115.53125 acc: 0.8649510741233826  val: loss: 1619323.75 acc: 0.5080515742301941\n",
      "step: 15155 , time : 0.0\n",
      "train: loss: 364632.15625 acc: 0.8116896152496338  val: loss: 783366.1875 acc: 0.42673563957214355\n",
      "step: 15160 , time : 0.0\n",
      "train: loss: 200704.53125 acc: 0.8125599026679993  val: loss: 1132676.875 acc: 0.5945779085159302\n",
      "step: 15165 , time : 0.0\n",
      "train: loss: 116557.2109375 acc: 0.8942208886146545  val: loss: 332100.84375 acc: 0.8792357444763184\n",
      "step: 15170 , time : 0.0\n",
      "train: loss: 169848.390625 acc: 0.8391849994659424  val: loss: 520627.65625 acc: 0.8418404459953308\n",
      "step: 15175 , time : 0.0\n",
      "train: loss: 455208.34375 acc: 0.7559711337089539  val: loss: 829213.875 acc: 0.7402429580688477\n",
      "step: 15180 , time : 0.0\n",
      "train: loss: 320493.3125 acc: 0.8311179876327515  val: loss: 576523.0 acc: 0.5611725449562073\n",
      "step: 15185 , time : 0.0\n",
      "train: loss: 807566.25 acc: 0.5044116973876953  val: loss: 379092.375 acc: 0.8398196697235107\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 15190 , time : 0.0\n",
      "train: loss: 261242.53125 acc: 0.7921814918518066  val: loss: 1260033.25 acc: 0.8128343820571899\n",
      "step: 15195 , time : 0.0\n",
      "train: loss: 625747.625 acc: 0.7096657156944275  val: loss: 948950.5625 acc: 0.7259241938591003\n",
      "step: 15200 , time : 0.0010006427764892578\n",
      "train: loss: 428205.5625 acc: 0.7132927179336548  val: loss: 945573.0625 acc: 0.8087218999862671\n",
      "step: 15205 , time : 0.0010008811950683594\n",
      "train: loss: 1257709.0 acc: 0.7959080338478088  val: loss: 1491040.125 acc: 0.7961792349815369\n",
      "step: 15210 , time : 0.0010006427764892578\n",
      "train: loss: 1273977.25 acc: 0.8656355142593384  val: loss: 433378.28125 acc: 0.8741241693496704\n",
      "step: 15215 , time : 0.0\n",
      "train: loss: 649186.375 acc: 0.9502437710762024  val: loss: 677563.3125 acc: 0.9046750068664551\n",
      "step: 15220 , time : 0.0\n",
      "train: loss: 717379.1875 acc: 0.9235219955444336  val: loss: 705860.0625 acc: 0.9116540551185608\n",
      "step: 15225 , time : 0.0\n",
      "train: loss: 176719.5 acc: 0.9640185832977295  val: loss: 776652.1875 acc: 0.8921431303024292\n",
      "step: 15230 , time : 0.0\n",
      "train: loss: 251481.3125 acc: 0.9225897192955017  val: loss: 796024.5 acc: 0.8656408786773682\n",
      "step: 15235 , time : 0.0\n",
      "train: loss: 226568.171875 acc: 0.9752956032752991  val: loss: 1734159.75 acc: 0.8703441023826599\n",
      "step: 15240 , time : 0.0\n",
      "train: loss: 226149.234375 acc: 0.9850847125053406  val: loss: 292460.4375 acc: 0.9730636477470398\n",
      "step: 15245 , time : 0.0\n",
      "train: loss: 237508.5 acc: 0.9831740260124207  val: loss: 3239326.25 acc: 0.3223177194595337\n",
      "step: 15250 , time : 0.0\n",
      "train: loss: 251594.640625 acc: 0.9708354473114014  val: loss: 717636.75 acc: 0.8458532691001892\n",
      "step: 15255 , time : 0.0\n",
      "train: loss: 115668.265625 acc: 0.9405331611633301  val: loss: 559233.875 acc: 0.584857702255249\n",
      "step: 15260 , time : 0.0\n",
      "train: loss: 93836.296875 acc: 0.9515461921691895  val: loss: 407732.78125 acc: 0.8473390936851501\n",
      "step: 15265 , time : 0.0\n",
      "train: loss: 152944.328125 acc: 0.9389107823371887  val: loss: 384473.25 acc: 0.9772091507911682\n",
      "step: 15270 , time : 0.0\n",
      "train: loss: 93200.125 acc: 0.9549301862716675  val: loss: 2378635.5 acc: 0.6933449506759644\n",
      "step: 15275 , time : 0.0\n",
      "train: loss: 12618.76171875 acc: 0.9760177135467529  val: loss: 499155.75 acc: 0.9204680323600769\n",
      "step: 15280 , time : 0.0\n",
      "train: loss: 11245.8046875 acc: 0.9797068238258362  val: loss: 530719.9375 acc: 0.951324462890625\n",
      "step: 15285 , time : 0.0\n",
      "train: loss: 17050.87890625 acc: 0.8891632556915283  val: loss: 1120335.875 acc: 0.8424088358879089\n",
      "step: 15290 , time : 0.0\n",
      "train: loss: 28902.890625 acc: 0.9337729215621948  val: loss: 1110743.75 acc: 0.9104173183441162\n",
      "step: 15295 , time : 0.0\n",
      "train: loss: 13550.494140625 acc: 0.9805618524551392  val: loss: 1010467.4375 acc: 0.82435142993927\n",
      "step: 15300 , time : 0.0\n",
      "train: loss: 14504.380859375 acc: 0.9690407514572144  val: loss: 1059725.75 acc: 0.8990670442581177\n",
      "step: 15305 , time : 0.0\n",
      "train: loss: 49642.78515625 acc: 0.9362612962722778  val: loss: 1201354.75 acc: 0.407040536403656\n",
      "step: 15310 , time : 0.0\n",
      "train: loss: 111503.8671875 acc: 0.9372846484184265  val: loss: 505422.4375 acc: 0.7867995500564575\n",
      "step: 15315 , time : 0.0010004043579101562\n",
      "train: loss: 33665.27734375 acc: 0.9799624085426331  val: loss: 533985.5 acc: 0.9373077154159546\n",
      "step: 15320 , time : 0.0010008811950683594\n",
      "train: loss: 18386.69140625 acc: 0.9846707582473755  val: loss: 664118.6875 acc: 0.7342668771743774\n",
      "step: 15325 , time : 0.0\n",
      "train: loss: 43297.90625 acc: 0.9681699275970459  val: loss: 826240.1875 acc: 0.7502775192260742\n",
      "step: 15330 , time : 0.0\n",
      "train: loss: 12152.392578125 acc: 0.9640409350395203  val: loss: 1027934.5625 acc: 0.7075854539871216\n",
      "step: 15335 , time : 0.0010006427764892578\n",
      "train: loss: 25225.81640625 acc: 0.9771071076393127  val: loss: 1749494.375 acc: -0.13628268241882324\n",
      "step: 15340 , time : 0.0010004043579101562\n",
      "train: loss: 40968.390625 acc: 0.9808858633041382  val: loss: 1229319.0 acc: 0.7583251595497131\n",
      "step: 15345 , time : 0.0\n",
      "train: loss: 85756.53125 acc: 0.9804723858833313  val: loss: 1787006.25 acc: 0.769785463809967\n",
      "step: 15350 , time : 0.0\n",
      "train: loss: 88165.5859375 acc: 0.9768422245979309  val: loss: 1777439.625 acc: 0.6805822849273682\n",
      "step: 15355 , time : 0.0\n",
      "train: loss: 46709.765625 acc: 0.9760405421257019  val: loss: 3540955.25 acc: -0.05746030807495117\n",
      "step: 15360 , time : 0.0010008811950683594\n",
      "train: loss: 71375.5078125 acc: 0.9784355759620667  val: loss: 2072048.875 acc: 0.6336609125137329\n",
      "step: 15365 , time : 0.0\n",
      "train: loss: 160562.375 acc: 0.9586994647979736  val: loss: 1712208.75 acc: 0.6578933596611023\n",
      "step: 15370 , time : 0.0\n",
      "train: loss: 358030.6875 acc: 0.910409688949585  val: loss: 2354400.5 acc: 0.7131300568580627\n",
      "step: 15375 , time : 0.0\n",
      "train: loss: 506224.46875 acc: 0.7912125587463379  val: loss: 755813.625 acc: 0.7646278738975525\n",
      "step: 15380 , time : 0.0\n",
      "train: loss: 173195.0 acc: 0.9722453355789185  val: loss: 2395232.75 acc: 0.523165225982666\n",
      "step: 15385 , time : 0.0\n",
      "train: loss: 85919.90625 acc: 0.9910019040107727  val: loss: 2542346.25 acc: 0.3950313925743103\n",
      "step: 15390 , time : 0.0\n",
      "train: loss: 56271.92578125 acc: 0.9927014112472534  val: loss: 356774.40625 acc: 0.9433757066726685\n",
      "step: 15395 , time : 0.0\n",
      "train: loss: 206898.53125 acc: 0.9695545434951782  val: loss: 1703236.0 acc: 0.5517687201499939\n",
      "step: 15400 , time : 0.0\n",
      "train: loss: 678150.5 acc: 0.8882611989974976  val: loss: 769396.0 acc: 0.8776775002479553\n",
      "step: 15405 , time : 0.0\n",
      "train: loss: 654197.75 acc: 0.9670621156692505  val: loss: 948505.6875 acc: 0.7739565372467041\n",
      "step: 15410 , time : 0.0\n",
      "train: loss: 353188.53125 acc: 0.9664130806922913  val: loss: 579197.0625 acc: 0.6670615673065186\n",
      "step: 15415 , time : 0.0\n",
      "train: loss: 270170.6875 acc: 0.9624236822128296  val: loss: 1654713.625 acc: 0.5675234794616699\n",
      "step: 15420 , time : 0.0\n",
      "train: loss: 722486.5 acc: 0.9650945067405701  val: loss: 770520.75 acc: 0.8306118249893188\n",
      "step: 15425 , time : 0.0\n",
      "train: loss: 958276.5 acc: 0.9699336290359497  val: loss: 1005122.8125 acc: 0.5454262495040894\n",
      "step: 15430 , time : 0.0010004043579101562\n",
      "train: loss: 1128936.5 acc: 0.9523701667785645  val: loss: 1835690.5 acc: 0.6503761410713196\n",
      "step: 15435 , time : 0.0\n",
      "train: loss: 611905.375 acc: 0.9675663709640503  val: loss: 359654.3125 acc: 0.8046400547027588\n",
      "step: 15440 , time : 0.0010004043579101562\n",
      "train: loss: 554861.4375 acc: 0.9307160377502441  val: loss: 603262.5625 acc: 0.8776661157608032\n",
      "step: 15445 , time : 0.0\n",
      "train: loss: 553121.125 acc: 0.9610899090766907  val: loss: 245814.09375 acc: 0.946963906288147\n",
      "step: 15450 , time : 0.0010004043579101562\n",
      "train: loss: 254232.90625 acc: 0.9713595509529114  val: loss: 800461.8125 acc: 0.7688292264938354\n",
      "step: 15455 , time : 0.0010006427764892578\n",
      "train: loss: 741682.6875 acc: 0.7808469533920288  val: loss: 513656.1875 acc: 0.9286102652549744\n",
      "step: 15460 , time : 0.0\n",
      "train: loss: 578001.1875 acc: 0.8804978132247925  val: loss: 1213453.5 acc: 0.3673818111419678\n",
      "step: 15465 , time : 0.0\n",
      "train: loss: 405740.5625 acc: 0.8004798889160156  val: loss: 1004327.0625 acc: 0.7649685144424438\n",
      "step: 15470 , time : 0.0\n",
      "train: loss: 332846.625 acc: 0.8119152784347534  val: loss: 1098828.375 acc: 0.8111950159072876\n",
      "step: 15475 , time : 0.0\n",
      "train: loss: 916835.375 acc: 0.7659820914268494  val: loss: 1486432.25 acc: 0.5740523338317871\n",
      "step: 15480 , time : 0.0\n",
      "train: loss: 1193985.375 acc: 0.6207642555236816  val: loss: 1363681.0 acc: 0.8177161812782288\n",
      "step: 15485 , time : 0.015625715255737305\n",
      "train: loss: 1046594.125 acc: 0.32710522413253784  val: loss: 1963249.375 acc: 0.4310417175292969\n",
      "step: 15490 , time : 0.0\n",
      "train: loss: 996577.625 acc: 0.44032591581344604  val: loss: 1444618.375 acc: 0.19480830430984497\n",
      "step: 15495 , time : 0.0\n",
      "train: loss: 699850.8125 acc: 0.38785284757614136  val: loss: 1079464.875 acc: 0.5344588160514832\n",
      "step: 15500 , time : 0.0\n",
      "train: loss: 340526.25 acc: 0.7138302326202393  val: loss: 891979.1875 acc: 0.5477241277694702\n",
      "step: 15505 , time : 0.0\n",
      "train: loss: 470463.59375 acc: 0.7938975691795349  val: loss: 617814.3125 acc: 0.8001006245613098\n",
      "step: 15510 , time : 0.0\n",
      "train: loss: 107322.140625 acc: 0.9112650156021118  val: loss: 717194.6875 acc: 0.8026422262191772\n",
      "step: 15515 , time : 0.0\n",
      "train: loss: 365384.09375 acc: 0.6365493535995483  val: loss: 1554065.25 acc: 0.8024590611457825\n",
      "step: 15520 , time : 0.0\n",
      "train: loss: 416379.84375 acc: 0.6953659057617188  val: loss: 1354676.875 acc: 0.6608173847198486\n",
      "step: 15525 , time : 0.0\n",
      "train: loss: 457573.65625 acc: 0.7906227111816406  val: loss: 532100.5 acc: 0.7903521656990051\n",
      "step: 15530 , time : 0.0\n",
      "train: loss: 195111.03125 acc: 0.8263567686080933  val: loss: 1513557.625 acc: 0.7324807643890381\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 15535 , time : 0.0\n",
      "train: loss: 108525.09375 acc: 0.9084055423736572  val: loss: 3812801.25 acc: 0.7831024527549744\n",
      "step: 15540 , time : 0.0\n",
      "train: loss: 221006.390625 acc: 0.8571630120277405  val: loss: 1263349.375 acc: 0.7584893703460693\n",
      "step: 15545 , time : 0.0\n",
      "train: loss: 526380.9375 acc: 0.6934821605682373  val: loss: 1871148.25 acc: 0.6658210158348083\n",
      "step: 15550 , time : 0.0009999275207519531\n",
      "train: loss: 185706.421875 acc: 0.7798082232475281  val: loss: 795304.3125 acc: 0.6560187339782715\n",
      "step: 15555 , time : 0.0\n",
      "train: loss: 343473.65625 acc: 0.7497484683990479  val: loss: 5030434.5 acc: 0.6129463911056519\n",
      "step: 15560 , time : 0.0\n",
      "train: loss: 144081.109375 acc: 0.8595581650733948  val: loss: 2051001.125 acc: 0.65034419298172\n",
      "step: 15565 , time : 0.0\n",
      "train: loss: 736879.125 acc: 0.7202968597412109  val: loss: 2860468.5 acc: 0.642974853515625\n",
      "step: 15570 , time : 0.001001119613647461\n",
      "train: loss: 1731862.5 acc: 0.7582948803901672  val: loss: 3302430.25 acc: 0.7891951203346252\n",
      "step: 15575 , time : 0.0010006427764892578\n",
      "train: loss: 1490466.125 acc: 0.7735744714736938  val: loss: 1094044.25 acc: 0.7025207281112671\n",
      "step: 15580 , time : 0.0010006427764892578\n",
      "train: loss: 1035806.3125 acc: 0.9005883932113647  val: loss: 1447397.875 acc: 0.7568087577819824\n",
      "step: 15585 , time : 0.0\n",
      "train: loss: 361213.0 acc: 0.9304345846176147  val: loss: 1169629.375 acc: 0.8264989852905273\n",
      "step: 15590 , time : 0.0\n",
      "train: loss: 453221.6875 acc: 0.9563106894493103  val: loss: 1272421.875 acc: 0.6219679117202759\n",
      "step: 15595 , time : 0.0\n",
      "train: loss: 309523.53125 acc: 0.9402562379837036  val: loss: 1054539.75 acc: 0.7884637117385864\n",
      "step: 15600 , time : 0.0\n",
      "train: loss: 196116.671875 acc: 0.9785841703414917  val: loss: 690226.0625 acc: 0.8928796052932739\n",
      "step: 15605 , time : 0.0\n",
      "train: loss: 124170.671875 acc: 0.9910317659378052  val: loss: 943844.8125 acc: 0.9016149640083313\n",
      "step: 15610 , time : 0.0\n",
      "train: loss: 137703.0 acc: 0.9905223250389099  val: loss: 2385789.5 acc: 0.7124993801116943\n",
      "step: 15615 , time : 0.0\n",
      "train: loss: 148331.53125 acc: 0.9828407168388367  val: loss: 1312239.375 acc: 0.5909081101417542\n",
      "step: 15620 , time : 0.0\n",
      "train: loss: 132053.96875 acc: 0.9809849858283997  val: loss: 320576.375 acc: 0.9472906589508057\n",
      "step: 15625 , time : 0.0\n",
      "train: loss: 25701.55859375 acc: 0.9903367757797241  val: loss: 1004978.375 acc: 0.8360693454742432\n",
      "step: 15630 , time : 0.015625476837158203\n",
      "train: loss: 21282.19921875 acc: 0.9920461773872375  val: loss: 772392.0625 acc: 0.7959379553794861\n",
      "step: 15635 , time : 0.0\n",
      "train: loss: 30879.0546875 acc: 0.9793063998222351  val: loss: 1730326.375 acc: 0.8024716973304749\n",
      "step: 15640 , time : 0.0\n",
      "train: loss: 16004.6171875 acc: 0.9651409983634949  val: loss: 3906838.25 acc: 0.05910569429397583\n",
      "step: 15645 , time : 0.0010004043579101562\n",
      "train: loss: 15766.826171875 acc: 0.9752584099769592  val: loss: 2048642.625 acc: 0.3156302571296692\n",
      "step: 15650 , time : 0.0\n",
      "train: loss: 19267.455078125 acc: 0.9509685635566711  val: loss: 1686479.5 acc: 0.39269256591796875\n",
      "step: 15655 , time : 0.0\n",
      "train: loss: 5530.60546875 acc: 0.9859613180160522  val: loss: 1010044.1875 acc: 0.6830786466598511\n",
      "step: 15660 , time : 0.0010006427764892578\n",
      "train: loss: 26573.205078125 acc: 0.9325917363166809  val: loss: 1002443.1875 acc: 0.8219518661499023\n",
      "step: 15665 , time : 0.0010004043579101562\n",
      "train: loss: 13861.634765625 acc: 0.9729759693145752  val: loss: 1037001.8125 acc: 0.8610485196113586\n",
      "step: 15670 , time : 0.0\n",
      "train: loss: 13080.2529296875 acc: 0.9771173000335693  val: loss: 297139.78125 acc: 0.9097850322723389\n",
      "step: 15675 , time : 0.0\n",
      "train: loss: 39932.40625 acc: 0.9740391373634338  val: loss: 1857775.75 acc: 0.8040298223495483\n",
      "step: 15680 , time : 0.0\n",
      "train: loss: 45296.4765625 acc: 0.965472936630249  val: loss: 277213.65625 acc: 0.9169245958328247\n",
      "step: 15685 , time : 0.0\n",
      "train: loss: 16056.8720703125 acc: 0.9720491766929626  val: loss: 462219.0625 acc: 0.9171864986419678\n",
      "step: 15690 , time : 0.0\n",
      "train: loss: 51694.140625 acc: 0.9726806879043579  val: loss: 683331.3125 acc: 0.8346543312072754\n",
      "step: 15695 , time : 0.0\n",
      "train: loss: 19342.080078125 acc: 0.967619001865387  val: loss: 1396499.375 acc: 0.42733460664749146\n",
      "step: 15700 , time : 0.0\n",
      "train: loss: 24954.005859375 acc: 0.9641042947769165  val: loss: 1337995.5 acc: 0.8425097465515137\n",
      "step: 15705 , time : 0.0\n",
      "train: loss: 60228.66015625 acc: 0.9685643315315247  val: loss: 1243305.375 acc: 0.7456690073013306\n",
      "step: 15710 , time : 0.0\n",
      "train: loss: 30583.673828125 acc: 0.9926750659942627  val: loss: 3015572.75 acc: 0.21846067905426025\n",
      "step: 15715 , time : 0.0\n",
      "train: loss: 40196.1484375 acc: 0.9899783730506897  val: loss: 1578513.0 acc: 0.5531362295150757\n",
      "step: 15720 , time : 0.0\n",
      "train: loss: 72705.046875 acc: 0.9716705083847046  val: loss: 1884263.5 acc: 0.7247564196586609\n",
      "step: 15725 , time : 0.0\n",
      "train: loss: 39968.8203125 acc: 0.9690964818000793  val: loss: 1355448.25 acc: 0.6337902545928955\n",
      "step: 15730 , time : 0.0\n",
      "train: loss: 186321.734375 acc: 0.9458319544792175  val: loss: 381433.46875 acc: 0.8951320648193359\n",
      "step: 15735 , time : 0.0\n",
      "train: loss: 243207.640625 acc: 0.9088779091835022  val: loss: 1362394.5 acc: 0.8695523738861084\n",
      "step: 15740 , time : 0.0\n",
      "train: loss: 85171.0546875 acc: 0.9572797417640686  val: loss: 482996.65625 acc: 0.9381362199783325\n",
      "step: 15745 , time : 0.0\n",
      "train: loss: 850294.0625 acc: 0.8642236590385437  val: loss: 1240156.625 acc: 0.4911070466041565\n",
      "step: 15750 , time : 0.0\n",
      "train: loss: 744169.4375 acc: 0.9152733683586121  val: loss: 775132.9375 acc: 0.8056944012641907\n",
      "step: 15755 , time : 0.0\n",
      "train: loss: 381259.21875 acc: 0.9445648193359375  val: loss: 385559.71875 acc: 0.9206639528274536\n",
      "step: 15760 , time : 0.0010006427764892578\n",
      "train: loss: 41668.6796875 acc: 0.9928783178329468  val: loss: 1979866.375 acc: 0.5932526588439941\n",
      "step: 15765 , time : 0.0\n",
      "train: loss: 285678.0 acc: 0.9781085848808289  val: loss: 640755.4375 acc: 0.8364098072052002\n",
      "step: 15770 , time : 0.0\n",
      "train: loss: 1797949.75 acc: 0.895851194858551  val: loss: 842264.4375 acc: 0.33276957273483276\n",
      "step: 15775 , time : 0.0\n",
      "train: loss: 542671.8125 acc: 0.9682864546775818  val: loss: 3017761.75 acc: 0.4500788450241089\n",
      "step: 15780 , time : 0.0\n",
      "train: loss: 265974.34375 acc: 0.8741140365600586  val: loss: 1474073.625 acc: 0.8355344533920288\n",
      "step: 15785 , time : 0.0\n",
      "train: loss: 928955.25 acc: 0.9627357721328735  val: loss: 601370.8125 acc: 0.9060882925987244\n",
      "step: 15790 , time : 0.0\n",
      "train: loss: 2834649.75 acc: 0.9208440184593201  val: loss: 1077398.625 acc: 0.8947542309761047\n",
      "step: 15795 , time : 0.0\n",
      "train: loss: 892159.625 acc: 0.9410662651062012  val: loss: 559931.9375 acc: 0.7985882759094238\n",
      "step: 15800 , time : 0.0\n",
      "train: loss: 1076539.75 acc: 0.9542727470397949  val: loss: 517665.28125 acc: 0.8474453091621399\n",
      "step: 15805 , time : 0.0\n",
      "train: loss: 220086.34375 acc: 0.9548069834709167  val: loss: 705385.75 acc: 0.6910040378570557\n",
      "step: 15810 , time : 0.0\n",
      "train: loss: 1141122.625 acc: 0.90046226978302  val: loss: 779314.0625 acc: 0.7509265542030334\n",
      "step: 15815 , time : 0.0\n",
      "train: loss: 203104.359375 acc: 0.9664245843887329  val: loss: 523464.84375 acc: 0.8623195886611938\n",
      "step: 15820 , time : 0.0\n",
      "train: loss: 382043.6875 acc: 0.9276043176651001  val: loss: 1248000.25 acc: 0.5878008604049683\n",
      "step: 15825 , time : 0.0\n",
      "train: loss: 309064.6875 acc: 0.867713451385498  val: loss: 1101084.5 acc: 0.7610154151916504\n",
      "step: 15830 , time : 0.0\n",
      "train: loss: 1458555.0 acc: 0.5725398063659668  val: loss: 1507360.375 acc: 0.807529866695404\n",
      "step: 15835 , time : 0.0\n",
      "train: loss: 633453.5625 acc: 0.7058911323547363  val: loss: 1382225.5 acc: 0.7273420095443726\n",
      "step: 15840 , time : 0.0\n",
      "train: loss: 557901.1875 acc: 0.8517372012138367  val: loss: 793503.1875 acc: 0.7815330028533936\n",
      "step: 15845 , time : 0.0\n",
      "train: loss: 1050843.625 acc: 0.7498296499252319  val: loss: 1442814.0 acc: 0.8694301843643188\n",
      "step: 15850 , time : 0.0\n",
      "train: loss: 1158189.5 acc: 0.48668843507766724  val: loss: 1741653.75 acc: 0.7272852659225464\n",
      "step: 15855 , time : 0.0\n",
      "train: loss: 644166.0625 acc: 0.6854077577590942  val: loss: 3111665.0 acc: 0.6925564408302307\n",
      "step: 15860 , time : 0.0\n",
      "train: loss: 663263.3125 acc: 0.5217312574386597  val: loss: 1689050.5 acc: 0.7381575107574463\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 15865 , time : 0.015625953674316406\n",
      "train: loss: 397529.75 acc: 0.6371798515319824  val: loss: 2210386.0 acc: 0.7003965973854065\n",
      "step: 15870 , time : 0.0\n",
      "train: loss: 223602.40625 acc: 0.8094699382781982  val: loss: 2131371.25 acc: 0.7449823617935181\n",
      "step: 15875 , time : 0.0\n",
      "train: loss: 351390.40625 acc: 0.7850717902183533  val: loss: 1663068.75 acc: 0.7031412124633789\n",
      "step: 15880 , time : 0.001001119613647461\n",
      "train: loss: 209808.890625 acc: 0.882279634475708  val: loss: 3631618.5 acc: 0.7160916328430176\n",
      "step: 15885 , time : 0.001001119613647461\n",
      "train: loss: 216817.21875 acc: 0.8760886192321777  val: loss: 2414656.75 acc: 0.7586179375648499\n",
      "step: 15890 , time : 0.0\n",
      "train: loss: 148192.875 acc: 0.8659894466400146  val: loss: 6309328.5 acc: 0.5583080649375916\n",
      "step: 15895 , time : 0.0\n",
      "train: loss: 70449.875 acc: 0.902999222278595  val: loss: 1804109.5 acc: 0.7840044498443604\n",
      "step: 15900 , time : 0.0\n",
      "train: loss: 35039.2265625 acc: 0.9500743746757507  val: loss: 581963.25 acc: 0.8080936670303345\n",
      "step: 15905 , time : 0.0\n",
      "train: loss: 151441.21875 acc: 0.8712404370307922  val: loss: 3039508.25 acc: 0.7621833086013794\n",
      "step: 15910 , time : 0.0\n",
      "train: loss: 1080691.75 acc: 0.6150688529014587  val: loss: 2812958.75 acc: 0.5806306600570679\n",
      "step: 15915 , time : 0.0\n",
      "train: loss: 225778.28125 acc: 0.8366177678108215  val: loss: 1035623.0625 acc: 0.7014371156692505\n",
      "step: 15920 , time : 0.0\n",
      "train: loss: 1010369.0625 acc: 0.7031781673431396  val: loss: 618148.75 acc: 0.7584381103515625\n",
      "step: 15925 , time : 0.0\n",
      "train: loss: 391953.5 acc: 0.6751483678817749  val: loss: 2191463.25 acc: 0.6815587282180786\n",
      "step: 15930 , time : 0.0010004043579101562\n",
      "train: loss: 181706.859375 acc: 0.8734211921691895  val: loss: 2032503.375 acc: 0.6651714444160461\n",
      "step: 15935 , time : 0.0\n",
      "train: loss: 1872068.625 acc: 0.7268409132957458  val: loss: 815407.8125 acc: 0.7772237062454224\n",
      "step: 15940 , time : 0.0\n",
      "train: loss: 1630885.875 acc: 0.7161626815795898  val: loss: 948344.5 acc: 0.5862725973129272\n",
      "step: 15945 , time : 0.0\n",
      "train: loss: 1202787.625 acc: 0.9189391136169434  val: loss: 1283878.625 acc: 0.7256534695625305\n",
      "step: 15950 , time : 0.0\n",
      "train: loss: 384646.25 acc: 0.9614169597625732  val: loss: 2059870.0 acc: 0.71148681640625\n",
      "step: 15955 , time : 0.0\n",
      "train: loss: 1143637.0 acc: 0.7271220684051514  val: loss: 1223388.25 acc: 0.4778478741645813\n",
      "step: 15960 , time : 0.0\n",
      "train: loss: 391032.5625 acc: 0.9170477986335754  val: loss: 890792.5 acc: 0.8431427478790283\n",
      "step: 15965 , time : 0.0\n",
      "train: loss: 87924.8984375 acc: 0.9917828440666199  val: loss: 2337406.75 acc: -0.4272589683532715\n",
      "step: 15970 , time : 0.0\n",
      "train: loss: 128877.3984375 acc: 0.990439772605896  val: loss: 2190820.25 acc: 0.6553716659545898\n",
      "step: 15975 , time : 0.0\n",
      "train: loss: 168054.25 acc: 0.9876191020011902  val: loss: 3379464.25 acc: 0.47134929895401\n",
      "step: 15980 , time : 0.0\n",
      "train: loss: 237538.734375 acc: 0.9814149737358093  val: loss: 1033385.5625 acc: 0.895603358745575\n",
      "step: 15985 , time : 0.0\n",
      "train: loss: 106702.5 acc: 0.982880711555481  val: loss: 2432715.75 acc: 0.5006463527679443\n",
      "step: 15990 , time : 0.0\n",
      "train: loss: 14777.56640625 acc: 0.9833690524101257  val: loss: 2062821.0 acc: 0.3864578604698181\n",
      "step: 15995 , time : 0.0\n",
      "train: loss: 52474.67578125 acc: 0.958922266960144  val: loss: 794887.4375 acc: 0.9025003910064697\n",
      "step: 16000 , time : 0.0010006427764892578\n",
      "train: loss: 22246.3984375 acc: 0.9848677515983582  val: loss: 624283.75 acc: 0.7335530519485474\n",
      "step: 16005 , time : 0.0\n",
      "train: loss: 22976.408203125 acc: 0.921177089214325  val: loss: 1065654.5 acc: 0.7616357803344727\n",
      "step: 16010 , time : 0.0010008811950683594\n",
      "train: loss: 50832.32421875 acc: 0.9054594039916992  val: loss: 392290.40625 acc: 0.9578791856765747\n",
      "step: 16015 , time : 0.0010008811950683594\n",
      "train: loss: 31356.841796875 acc: 0.9409213066101074  val: loss: 886133.5 acc: 0.9171122312545776\n",
      "step: 16020 , time : 0.0\n",
      "train: loss: 42706.328125 acc: 0.9520149827003479  val: loss: 1241163.875 acc: 0.8157762289047241\n",
      "step: 16025 , time : 0.0\n",
      "train: loss: 64521.47265625 acc: 0.948645293712616  val: loss: 519626.3125 acc: 0.881184458732605\n",
      "step: 16030 , time : 0.0\n",
      "train: loss: 10978.9638671875 acc: 0.9677407741546631  val: loss: 255181.140625 acc: 0.9563140273094177\n",
      "step: 16035 , time : 0.0\n",
      "train: loss: 15540.7744140625 acc: 0.9740638732910156  val: loss: 1486930.0 acc: 0.4640808701515198\n",
      "step: 16040 , time : 0.0\n",
      "train: loss: 17323.3828125 acc: 0.9814647436141968  val: loss: 886130.3125 acc: 0.8791922330856323\n",
      "step: 16045 , time : 0.0\n",
      "train: loss: 91086.53125 acc: 0.9589027762413025  val: loss: 362227.84375 acc: 0.8807169198989868\n",
      "step: 16050 , time : 0.0\n",
      "train: loss: 56010.6640625 acc: 0.9694406390190125  val: loss: 2636807.25 acc: 0.49560779333114624\n",
      "step: 16055 , time : 0.0\n",
      "train: loss: 44077.06640625 acc: 0.9753331542015076  val: loss: 471145.1875 acc: 0.8622888326644897\n",
      "step: 16060 , time : 0.0\n",
      "train: loss: 41340.2890625 acc: 0.9602622985839844  val: loss: 1087872.0 acc: 0.7265820503234863\n",
      "step: 16065 , time : 0.0\n",
      "train: loss: 25848.10546875 acc: 0.9768303632736206  val: loss: 293515.34375 acc: 0.86384117603302\n",
      "step: 16070 , time : 0.0\n",
      "train: loss: 36585.28515625 acc: 0.9847925305366516  val: loss: 776575.25 acc: 0.6484336256980896\n",
      "step: 16075 , time : 0.0\n",
      "train: loss: 45158.09765625 acc: 0.9844939708709717  val: loss: 2649036.0 acc: 0.44154292345046997\n",
      "step: 16080 , time : 0.0\n",
      "train: loss: 41267.546875 acc: 0.9893574118614197  val: loss: 702982.6875 acc: 0.6516692638397217\n",
      "step: 16085 , time : 0.0\n",
      "train: loss: 53248.1640625 acc: 0.9595545530319214  val: loss: 931465.75 acc: 0.5459023714065552\n",
      "step: 16090 , time : 0.0\n",
      "train: loss: 71824.8515625 acc: 0.9769212007522583  val: loss: 1037340.0625 acc: 0.3978090286254883\n",
      "step: 16095 , time : 0.0\n",
      "train: loss: 195294.265625 acc: 0.9408093094825745  val: loss: 869511.1875 acc: 0.688776433467865\n",
      "step: 16100 , time : 0.0\n",
      "train: loss: 132131.140625 acc: 0.9607262015342712  val: loss: 323015.84375 acc: 0.9381070137023926\n",
      "step: 16105 , time : 0.0\n",
      "train: loss: 86300.1328125 acc: 0.942071795463562  val: loss: 211604.578125 acc: 0.9428951144218445\n",
      "step: 16110 , time : 0.0\n",
      "train: loss: 142772.5625 acc: 0.9642344117164612  val: loss: 281215.8125 acc: 0.898594081401825\n",
      "step: 16115 , time : 0.0\n",
      "train: loss: 143080.484375 acc: 0.9844496846199036  val: loss: 2114118.25 acc: 0.40173768997192383\n",
      "step: 16120 , time : 0.0\n",
      "train: loss: 68243.96875 acc: 0.9933596253395081  val: loss: 453030.53125 acc: 0.9333801865577698\n",
      "step: 16125 , time : 0.0010013580322265625\n",
      "train: loss: 54858.390625 acc: 0.9921825528144836  val: loss: 1002524.8125 acc: 0.7631981372833252\n",
      "step: 16130 , time : 0.0010914802551269531\n",
      "train: loss: 201134.3125 acc: 0.9845218658447266  val: loss: 503929.90625 acc: 0.8987626433372498\n",
      "step: 16135 , time : 0.0\n",
      "train: loss: 796719.8125 acc: 0.9591482877731323  val: loss: 311183.65625 acc: 0.911369264125824\n",
      "step: 16140 , time : 0.015625\n",
      "train: loss: 511785.4375 acc: 0.9657022356987  val: loss: 458354.9375 acc: 0.877237856388092\n",
      "step: 16145 , time : 0.0\n",
      "train: loss: 505354.28125 acc: 0.8260302543640137  val: loss: 929256.375 acc: 0.7707914113998413\n",
      "step: 16150 , time : 0.0\n",
      "train: loss: 1153501.875 acc: 0.926543653011322  val: loss: 915153.125 acc: 0.9037668704986572\n",
      "step: 16155 , time : 0.0\n",
      "train: loss: 519864.09375 acc: 0.9772852659225464  val: loss: 434766.125 acc: 0.8843019008636475\n",
      "step: 16160 , time : 0.0\n",
      "train: loss: 1017768.25 acc: 0.9390184879302979  val: loss: 807216.5625 acc: 0.7048304080963135\n",
      "step: 16165 , time : 0.0\n",
      "train: loss: 1173227.0 acc: 0.9508973360061646  val: loss: 1260994.875 acc: 0.8798239231109619\n",
      "step: 16170 , time : 0.0\n",
      "train: loss: 1050730.375 acc: 0.8843597769737244  val: loss: 915422.75 acc: 0.9216065406799316\n",
      "step: 16175 , time : 0.0\n",
      "train: loss: 170637.296875 acc: 0.9824725389480591  val: loss: 296817.0 acc: 0.8920627236366272\n",
      "step: 16180 , time : 0.0\n",
      "train: loss: 422599.90625 acc: 0.8895849585533142  val: loss: 234995.796875 acc: 0.9418169260025024\n",
      "step: 16185 , time : 0.0\n",
      "train: loss: 472040.875 acc: 0.8596058487892151  val: loss: 1352044.375 acc: 0.9136350154876709\n",
      "step: 16190 , time : 0.0\n",
      "train: loss: 2376834.75 acc: 0.6615719199180603  val: loss: 957804.8125 acc: 0.8870320320129395\n",
      "step: 16195 , time : 0.0\n",
      "train: loss: 898619.0625 acc: 0.6329284906387329  val: loss: 1474671.875 acc: 0.6494647860527039\n",
      "step: 16200 , time : 0.0\n",
      "train: loss: 472324.0 acc: 0.7402708530426025  val: loss: 1398260.625 acc: 0.7531793117523193\n",
      "step: 16205 , time : 0.0\n",
      "train: loss: 754983.75 acc: 0.7856473922729492  val: loss: 1989084.25 acc: 0.8673075437545776\n",
      "step: 16210 , time : 0.0\n",
      "train: loss: 1109442.125 acc: 0.7671254277229309  val: loss: 392103.0 acc: 0.8078688979148865\n",
      "step: 16215 , time : 0.0010006427764892578\n",
      "train: loss: 1237344.0 acc: 0.5965790152549744  val: loss: 2206897.75 acc: 0.6813594102859497\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 16220 , time : 0.0\n",
      "train: loss: 372085.0 acc: 0.6892673969268799  val: loss: 2894820.75 acc: 0.5168927907943726\n",
      "step: 16225 , time : 0.0\n",
      "train: loss: 883245.9375 acc: 0.6145983934402466  val: loss: 1256315.875 acc: 0.8228714466094971\n",
      "step: 16230 , time : 0.001001119613647461\n",
      "train: loss: 183660.609375 acc: 0.8572664260864258  val: loss: 1205342.875 acc: 0.7392298579216003\n",
      "step: 16235 , time : 0.0010006427764892578\n",
      "train: loss: 116354.46875 acc: 0.8998661637306213  val: loss: 530662.5 acc: 0.8065831661224365\n",
      "step: 16240 , time : 0.0\n",
      "train: loss: 70486.90625 acc: 0.9380561709403992  val: loss: 3545320.5 acc: 0.6584615707397461\n",
      "step: 16245 , time : 0.0\n",
      "train: loss: 49655.15234375 acc: 0.9599343538284302  val: loss: 3609611.5 acc: 0.6566671133041382\n",
      "step: 16250 , time : 0.0010001659393310547\n",
      "train: loss: 40219.05859375 acc: 0.9671492576599121  val: loss: 2151520.25 acc: 0.6631416082382202\n",
      "step: 16255 , time : 0.0\n",
      "train: loss: 288326.875 acc: 0.8368551731109619  val: loss: 251348.734375 acc: 0.8416273593902588\n",
      "step: 16260 , time : 0.0\n",
      "train: loss: 207654.015625 acc: 0.8015190958976746  val: loss: 288405.25 acc: 0.8143672943115234\n",
      "step: 16265 , time : 0.0\n",
      "train: loss: 159529.34375 acc: 0.8609297275543213  val: loss: 581084.875 acc: 0.8156325817108154\n",
      "step: 16270 , time : 0.0020012855529785156\n",
      "train: loss: 158989.96875 acc: 0.8957586884498596  val: loss: 1237242.5 acc: 0.7483237385749817\n",
      "step: 16275 , time : 0.0\n",
      "train: loss: 112232.9296875 acc: 0.9104195833206177  val: loss: 3602659.0 acc: 0.26235073804855347\n",
      "step: 16280 , time : 0.0\n",
      "train: loss: 551089.4375 acc: 0.74018394947052  val: loss: 284915.0625 acc: 0.832544207572937\n",
      "step: 16285 , time : 0.0\n",
      "train: loss: 177457.921875 acc: 0.8309686183929443  val: loss: 282437.625 acc: 0.8172420859336853\n",
      "step: 16290 , time : 0.0010004043579101562\n",
      "train: loss: 80509.359375 acc: 0.9132784008979797  val: loss: 2484991.75 acc: 0.7090868949890137\n",
      "step: 16295 , time : 0.0\n",
      "train: loss: 356989.25 acc: 0.8007305860519409  val: loss: 1422087.625 acc: 0.6665078401565552\n",
      "step: 16300 , time : 0.0\n",
      "train: loss: 2688583.25 acc: 0.6914640665054321  val: loss: 682253.0 acc: 0.7968209981918335\n",
      "step: 16305 , time : 0.0\n",
      "train: loss: 1398120.875 acc: 0.818529486656189  val: loss: 864405.3125 acc: 0.8213801383972168\n",
      "step: 16310 , time : 0.0010006427764892578\n",
      "train: loss: 1042101.875 acc: 0.9223290085792542  val: loss: 1087521.625 acc: 0.5999385714530945\n",
      "step: 16315 , time : 0.0\n",
      "train: loss: 1243198.125 acc: 0.8396519422531128  val: loss: 1496156.125 acc: 0.8799407482147217\n",
      "step: 16320 , time : 0.0\n",
      "train: loss: 887085.6875 acc: 0.8391557931900024  val: loss: 1557279.5 acc: 0.8321270942687988\n",
      "step: 16325 , time : 0.0\n",
      "train: loss: 397468.25 acc: 0.9356447458267212  val: loss: 522587.53125 acc: 0.8571619987487793\n",
      "step: 16330 , time : 0.0\n",
      "train: loss: 356573.34375 acc: 0.9650903940200806  val: loss: 1833697.125 acc: 0.5874149799346924\n",
      "step: 16335 , time : 0.0010008811950683594\n",
      "train: loss: 148753.984375 acc: 0.988288938999176  val: loss: 1275788.25 acc: 0.6887317895889282\n",
      "step: 16340 , time : 0.0\n",
      "train: loss: 347038.96875 acc: 0.9684364199638367  val: loss: 1443340.5 acc: 0.45239728689193726\n",
      "step: 16345 , time : 0.0010004043579101562\n",
      "train: loss: 222913.09375 acc: 0.9740114808082581  val: loss: 1351486.875 acc: 0.634613037109375\n",
      "step: 16350 , time : 0.0010013580322265625\n",
      "train: loss: 149988.421875 acc: 0.9794253706932068  val: loss: 993276.4375 acc: 0.8498674035072327\n",
      "step: 16355 , time : 0.0\n",
      "train: loss: 108697.9921875 acc: 0.9797818660736084  val: loss: 1364692.125 acc: 0.5996421575546265\n",
      "step: 16360 , time : 0.0\n",
      "train: loss: 22114.25390625 acc: 0.9876953363418579  val: loss: 175493.09375 acc: 0.9556475281715393\n",
      "step: 16365 , time : 0.0\n",
      "train: loss: 46295.08203125 acc: 0.9656703472137451  val: loss: 800905.0625 acc: 0.7353565692901611\n",
      "step: 16370 , time : 0.015625953674316406\n",
      "train: loss: 34199.4375 acc: 0.9809067845344543  val: loss: 713941.1875 acc: 0.9113447070121765\n",
      "step: 16375 , time : 0.0\n",
      "train: loss: 17667.765625 acc: 0.982182502746582  val: loss: 429105.25 acc: 0.8849530816078186\n",
      "step: 16380 , time : 0.0\n",
      "train: loss: 31507.755859375 acc: 0.9427617192268372  val: loss: 109759.0625 acc: 0.9790285229682922\n",
      "step: 16385 , time : 0.0\n",
      "train: loss: 24417.59765625 acc: 0.9661579728126526  val: loss: 576580.3125 acc: 0.8372424244880676\n",
      "step: 16390 , time : 0.0\n",
      "train: loss: 20047.509765625 acc: 0.9692985415458679  val: loss: 690556.875 acc: 0.8520997166633606\n",
      "step: 16395 , time : 0.0\n",
      "train: loss: 39676.68359375 acc: 0.9439728856086731  val: loss: 1375345.625 acc: 0.7818655371665955\n",
      "step: 16400 , time : 0.0\n",
      "train: loss: 20769.501953125 acc: 0.9244800209999084  val: loss: 415792.21875 acc: 0.9140667915344238\n",
      "step: 16405 , time : 0.0\n",
      "train: loss: 85033.453125 acc: 0.955109179019928  val: loss: 873707.8125 acc: 0.5715768337249756\n",
      "step: 16410 , time : 0.0010008811950683594\n",
      "train: loss: 44437.78125 acc: 0.9789484143257141  val: loss: 274638.34375 acc: 0.8809940218925476\n",
      "step: 16415 , time : 0.001001119613647461\n",
      "train: loss: 17925.1171875 acc: 0.987798810005188  val: loss: 994764.1875 acc: 0.8614087104797363\n",
      "step: 16420 , time : 0.0010004043579101562\n",
      "train: loss: 36177.85546875 acc: 0.9807851910591125  val: loss: 1630797.625 acc: 0.8186954855918884\n",
      "step: 16425 , time : 0.0010006427764892578\n",
      "train: loss: 61886.1640625 acc: 0.9625903367996216  val: loss: 1314695.125 acc: 0.7268005609512329\n",
      "step: 16430 , time : 0.0010008811950683594\n",
      "train: loss: 21181.689453125 acc: 0.9731558561325073  val: loss: 1852506.0 acc: 0.22689521312713623\n",
      "step: 16435 , time : 0.0010006427764892578\n",
      "train: loss: 34046.01953125 acc: 0.984345018863678  val: loss: 503287.59375 acc: 0.9186499118804932\n",
      "step: 16440 , time : 0.0\n",
      "train: loss: 49589.41796875 acc: 0.9847636222839355  val: loss: 1192638.625 acc: 0.8084793090820312\n",
      "step: 16445 , time : 0.0\n",
      "train: loss: 72110.03125 acc: 0.9858261346817017  val: loss: 328655.65625 acc: 0.9077126979827881\n",
      "step: 16450 , time : 0.0\n",
      "train: loss: 110991.078125 acc: 0.9694533348083496  val: loss: 702251.625 acc: 0.689437747001648\n",
      "step: 16455 , time : 0.0\n",
      "train: loss: 73057.015625 acc: 0.9788153767585754  val: loss: 1679294.0 acc: 0.6718965768814087\n",
      "step: 16460 , time : 0.0\n",
      "train: loss: 70749.28125 acc: 0.9844628572463989  val: loss: 570312.25 acc: 0.9010862708091736\n",
      "step: 16465 , time : 0.0\n",
      "train: loss: 311459.125 acc: 0.8589213490486145  val: loss: 1058001.125 acc: 0.8324447274208069\n",
      "step: 16470 , time : 0.0\n",
      "train: loss: 213085.546875 acc: 0.9111545085906982  val: loss: 938356.875 acc: 0.7785495519638062\n",
      "step: 16475 , time : 0.0010008811950683594\n",
      "train: loss: 552331.3125 acc: 0.8900771737098694  val: loss: 239595.375 acc: 0.980891764163971\n",
      "step: 16480 , time : 0.001001119613647461\n",
      "train: loss: 176846.71875 acc: 0.9791621565818787  val: loss: 264672.65625 acc: 0.9565756916999817\n",
      "step: 16485 , time : 0.0\n",
      "train: loss: 136083.546875 acc: 0.9879997372627258  val: loss: 810877.125 acc: 0.7107443809509277\n",
      "step: 16490 , time : 0.0\n",
      "train: loss: 66415.2109375 acc: 0.9906901717185974  val: loss: 950548.25 acc: 0.7733154892921448\n",
      "step: 16495 , time : 0.0\n",
      "train: loss: 140549.515625 acc: 0.9771019220352173  val: loss: 238949.671875 acc: 0.9493826627731323\n",
      "step: 16500 , time : 0.0\n",
      "train: loss: 169343.53125 acc: 0.984310507774353  val: loss: 335963.0625 acc: 0.957317054271698\n",
      "step: 16505 , time : 0.0\n",
      "train: loss: 329401.375 acc: 0.9857603907585144  val: loss: 1226122.875 acc: 0.48820817470550537\n",
      "step: 16510 , time : 0.0\n",
      "train: loss: 445623.3125 acc: 0.9714813828468323  val: loss: 676781.625 acc: 0.9057450890541077\n",
      "step: 16515 , time : 0.0\n",
      "train: loss: 369826.3125 acc: 0.9706281423568726  val: loss: 968474.125 acc: 0.8820070624351501\n",
      "step: 16520 , time : 0.0\n",
      "train: loss: 878525.125 acc: 0.9655917286872864  val: loss: 1182926.125 acc: 0.862567126750946\n",
      "step: 16525 , time : 0.0\n",
      "train: loss: 1045575.6875 acc: 0.9608391523361206  val: loss: 324037.78125 acc: 0.937447726726532\n",
      "step: 16530 , time : 0.0\n",
      "train: loss: 1784847.375 acc: 0.9359439611434937  val: loss: 2168184.5 acc: 0.6145225763320923\n",
      "step: 16535 , time : 0.0\n",
      "train: loss: 493665.21875 acc: 0.9692712426185608  val: loss: 1833350.5 acc: 0.3876769542694092\n",
      "step: 16540 , time : 0.001001119613647461\n",
      "train: loss: 821737.0 acc: 0.9192538857460022  val: loss: 448651.5 acc: 0.9349426627159119\n",
      "step: 16545 , time : 0.0010008811950683594\n",
      "train: loss: 1167536.5 acc: 0.898795485496521  val: loss: 664849.4375 acc: 0.6366270780563354\n",
      "step: 16550 , time : 0.0\n",
      "train: loss: 308535.84375 acc: 0.9501806497573853  val: loss: 1178016.875 acc: 0.8840869665145874\n",
      "step: 16555 , time : 0.0\n",
      "train: loss: 1705137.0 acc: 0.500511646270752  val: loss: 1418486.5 acc: 0.1612059473991394\n",
      "step: 16560 , time : 0.0\n",
      "train: loss: 926844.375 acc: 0.645348072052002  val: loss: 1885421.75 acc: 0.8451087474822998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 16565 , time : 0.0\n",
      "train: loss: 1258073.375 acc: 0.5012773275375366  val: loss: 1404058.375 acc: 0.7560756206512451\n",
      "step: 16570 , time : 0.001001119613647461\n",
      "train: loss: 772438.125 acc: 0.5617250204086304  val: loss: 1231291.625 acc: 0.8521853685379028\n",
      "step: 16575 , time : 0.0\n",
      "train: loss: 1434372.125 acc: 0.6963372826576233  val: loss: 301417.21875 acc: 0.9002346992492676\n",
      "step: 16580 , time : 0.0010008811950683594\n",
      "train: loss: 923262.9375 acc: 0.22996699810028076  val: loss: 2082154.125 acc: 0.6818047761917114\n",
      "step: 16585 , time : 0.0\n",
      "train: loss: 356297.59375 acc: 0.7906564474105835  val: loss: 1094089.625 acc: 0.532457709312439\n",
      "step: 16590 , time : 0.0010006427764892578\n",
      "train: loss: 369926.15625 acc: 0.6987448930740356  val: loss: 1933038.375 acc: 0.6848291158676147\n",
      "step: 16595 , time : 0.0010008811950683594\n",
      "train: loss: 202902.65625 acc: 0.7983854413032532  val: loss: 1145004.125 acc: 0.6812057495117188\n",
      "step: 16600 , time : 0.0\n",
      "train: loss: 262966.9375 acc: 0.8074828386306763  val: loss: 3467208.0 acc: 0.6594160795211792\n",
      "step: 16605 , time : 0.0\n",
      "train: loss: 231294.109375 acc: 0.8308578133583069  val: loss: 1716802.625 acc: 0.7057656049728394\n",
      "step: 16610 , time : 0.0\n",
      "train: loss: 114797.7578125 acc: 0.9079018235206604  val: loss: 344283.90625 acc: 0.7733535170555115\n",
      "step: 16615 , time : 0.0\n",
      "train: loss: 215518.359375 acc: 0.814873456954956  val: loss: 2095660.125 acc: 0.629475474357605\n",
      "step: 16620 , time : 0.0\n",
      "train: loss: 286260.5625 acc: 0.833052396774292  val: loss: 746532.375 acc: 0.7184514999389648\n",
      "step: 16625 , time : 0.0010004043579101562\n",
      "train: loss: 109459.6953125 acc: 0.9182232618331909  val: loss: 673950.6875 acc: 0.8112709522247314\n",
      "step: 16630 , time : 0.0010006427764892578\n",
      "train: loss: 57731.3515625 acc: 0.9424898624420166  val: loss: 2984643.75 acc: 0.7059084177017212\n",
      "step: 16635 , time : 0.0\n",
      "train: loss: 169333.515625 acc: 0.8771347999572754  val: loss: 953338.0 acc: 0.7852555513381958\n",
      "step: 16640 , time : 0.0010004043579101562\n",
      "train: loss: 178755.5625 acc: 0.8047261238098145  val: loss: 891188.6875 acc: 0.6756172180175781\n",
      "step: 16645 , time : 0.0\n",
      "train: loss: 53616.078125 acc: 0.927493691444397  val: loss: 1689922.25 acc: 0.6874286532402039\n",
      "step: 16650 , time : 0.0\n",
      "train: loss: 420827.75 acc: 0.8135465383529663  val: loss: 1627984.125 acc: 0.7243517637252808\n",
      "step: 16655 , time : 0.0010004043579101562\n",
      "train: loss: 264128.59375 acc: 0.8148505091667175  val: loss: 481741.34375 acc: 0.796345055103302\n",
      "step: 16660 , time : 0.0010004043579101562\n",
      "train: loss: 249205.390625 acc: 0.7989763021469116  val: loss: 1351472.0 acc: 0.6715977191925049\n",
      "step: 16665 , time : 0.0\n",
      "train: loss: 1578204.375 acc: 0.6711329817771912  val: loss: 1580562.25 acc: 0.6932896375656128\n",
      "step: 16670 , time : 0.0\n",
      "train: loss: 1205365.25 acc: 0.5715364217758179  val: loss: 947471.5625 acc: 0.6333861351013184\n",
      "step: 16675 , time : 0.0\n",
      "train: loss: 1516317.5 acc: 0.8838644027709961  val: loss: 1313981.375 acc: 0.5120025873184204\n",
      "step: 16680 , time : 0.0\n",
      "train: loss: 868384.0 acc: 0.9206852316856384  val: loss: 1404591.625 acc: 0.21838313341140747\n",
      "step: 16685 , time : 0.0\n",
      "train: loss: 404678.5 acc: 0.9575099945068359  val: loss: 2375276.5 acc: 0.511939287185669\n",
      "step: 16690 , time : 0.0\n",
      "train: loss: 286129.8125 acc: 0.9609414339065552  val: loss: 2724273.25 acc: 0.6209241151809692\n",
      "step: 16695 , time : 0.0010006427764892578\n",
      "train: loss: 268719.53125 acc: 0.9629896283149719  val: loss: 1134272.875 acc: 0.7925143241882324\n",
      "step: 16700 , time : 0.0\n",
      "train: loss: 122134.6953125 acc: 0.990476667881012  val: loss: 226385.953125 acc: 0.8777225017547607\n",
      "step: 16705 , time : 0.0010006427764892578\n",
      "train: loss: 112915.921875 acc: 0.9919159412384033  val: loss: 684378.5 acc: 0.7123777866363525\n",
      "step: 16710 , time : 0.0010006427764892578\n",
      "train: loss: 213603.0 acc: 0.9775373935699463  val: loss: 960504.25 acc: 0.8046596050262451\n",
      "step: 16715 , time : 0.0\n",
      "train: loss: 164544.875 acc: 0.9762423634529114  val: loss: 1491735.125 acc: 0.8056314587593079\n",
      "step: 16720 , time : 0.0\n",
      "train: loss: 56936.359375 acc: 0.9829118251800537  val: loss: 189675.171875 acc: 0.9708496928215027\n",
      "step: 16725 , time : 0.0010006427764892578\n",
      "train: loss: 45193.89453125 acc: 0.9660711884498596  val: loss: 2040359.875 acc: 0.662314772605896\n",
      "step: 16730 , time : 0.001001119613647461\n",
      "train: loss: 43952.43359375 acc: 0.9712671041488647  val: loss: 165304.546875 acc: 0.9734007716178894\n",
      "step: 16735 , time : 0.0\n",
      "train: loss: 31715.19921875 acc: 0.9792300462722778  val: loss: 107754.15625 acc: 0.981518566608429\n",
      "step: 16740 , time : 0.0\n",
      "train: loss: 49566.90234375 acc: 0.9545010924339294  val: loss: 125224.0703125 acc: 0.9277551770210266\n",
      "step: 16745 , time : 0.0\n",
      "train: loss: 29275.240234375 acc: 0.9612722396850586  val: loss: 233101.71875 acc: 0.9765526056289673\n",
      "step: 16750 , time : 0.0010008811950683594\n",
      "train: loss: 22923.939453125 acc: 0.9416592121124268  val: loss: 391669.125 acc: 0.9489322304725647\n",
      "step: 16755 , time : 0.0010008811950683594\n",
      "train: loss: 18721.705078125 acc: 0.9580227136611938  val: loss: 1525288.875 acc: 0.2690660357475281\n",
      "step: 16760 , time : 0.0\n",
      "train: loss: 15607.9970703125 acc: 0.952918291091919  val: loss: 491918.28125 acc: 0.4368000030517578\n",
      "step: 16765 , time : 0.0\n",
      "train: loss: 5911.9072265625 acc: 0.991935133934021  val: loss: 467349.53125 acc: 0.8644945621490479\n",
      "step: 16770 , time : 0.001001119613647461\n",
      "train: loss: 81790.2890625 acc: 0.9500865936279297  val: loss: 152379.125 acc: 0.9708120226860046\n",
      "step: 16775 , time : 0.0010006427764892578\n",
      "train: loss: 21093.994140625 acc: 0.982524037361145  val: loss: 766148.875 acc: 0.7189573049545288\n",
      "step: 16780 , time : 0.0\n",
      "train: loss: 22448.681640625 acc: 0.9818730354309082  val: loss: 384176.78125 acc: 0.9040427207946777\n",
      "step: 16785 , time : 0.0\n",
      "train: loss: 43501.30859375 acc: 0.9707940816879272  val: loss: 609098.75 acc: 0.9081363677978516\n",
      "step: 16790 , time : 0.0\n",
      "train: loss: 21928.6171875 acc: 0.9822798371315002  val: loss: 1396933.0 acc: 0.7342423796653748\n",
      "step: 16795 , time : 0.0\n",
      "train: loss: 54005.7265625 acc: 0.9472780227661133  val: loss: 333788.0625 acc: 0.9349057674407959\n",
      "step: 16800 , time : 0.0\n",
      "train: loss: 32216.451171875 acc: 0.9444369077682495  val: loss: 717221.625 acc: 0.9453318119049072\n",
      "step: 16805 , time : 0.001001119613647461\n",
      "train: loss: 99093.0390625 acc: 0.9742560982704163  val: loss: 808882.375 acc: 0.8465396761894226\n",
      "step: 16810 , time : 0.0\n",
      "train: loss: 64242.98828125 acc: 0.9849384427070618  val: loss: 159413.640625 acc: 0.9896483421325684\n",
      "step: 16815 , time : 0.0\n",
      "train: loss: 60156.24609375 acc: 0.9851338267326355  val: loss: 699522.875 acc: 0.9119030833244324\n",
      "step: 16820 , time : 0.0\n",
      "train: loss: 61984.6796875 acc: 0.975638747215271  val: loss: 312101.15625 acc: 0.8998469114303589\n",
      "step: 16825 , time : 0.0\n",
      "train: loss: 94733.7109375 acc: 0.9657677412033081  val: loss: 1259499.75 acc: 0.2918371558189392\n",
      "step: 16830 , time : 0.0\n",
      "train: loss: 363754.65625 acc: 0.8885472416877747  val: loss: 1354733.625 acc: 0.8273226022720337\n",
      "step: 16835 , time : 0.0010006427764892578\n",
      "train: loss: 210304.96875 acc: 0.9358797669410706  val: loss: 1202604.25 acc: 0.9244149327278137\n",
      "step: 16840 , time : 0.0010006427764892578\n",
      "train: loss: 152078.265625 acc: 0.959274172782898  val: loss: 692041.625 acc: 0.9103835225105286\n",
      "step: 16845 , time : 0.0\n",
      "train: loss: 370641.875 acc: 0.9490023255348206  val: loss: 596539.1875 acc: 0.9350833296775818\n",
      "step: 16850 , time : 0.0\n",
      "train: loss: 494485.46875 acc: 0.9587950706481934  val: loss: 415659.4375 acc: 0.7194828987121582\n",
      "step: 16855 , time : 0.0\n",
      "train: loss: 69739.0390625 acc: 0.9932991862297058  val: loss: 464820.09375 acc: 0.9343035817146301\n",
      "step: 16860 , time : 0.0\n",
      "train: loss: 281979.8125 acc: 0.9655864238739014  val: loss: 246809.515625 acc: 0.9505659937858582\n",
      "step: 16865 , time : 0.0010006427764892578\n",
      "train: loss: 151130.203125 acc: 0.9729878306388855  val: loss: 1779462.875 acc: 0.696776807308197\n",
      "step: 16870 , time : 0.0\n",
      "train: loss: 461201.25 acc: 0.9763247966766357  val: loss: 4265897.0 acc: 0.5956976413726807\n",
      "step: 16875 , time : 0.0\n",
      "train: loss: 221284.3125 acc: 0.9521827101707458  val: loss: 580611.875 acc: 0.9176569581031799\n",
      "step: 16880 , time : 0.0\n",
      "train: loss: 655363.125 acc: 0.8189530372619629  val: loss: 4029339.0 acc: 0.635298490524292\n",
      "step: 16885 , time : 0.0010006427764892578\n",
      "train: loss: 799855.75 acc: 0.9702207446098328  val: loss: 1029485.375 acc: 0.830220103263855\n",
      "step: 16890 , time : 0.0\n",
      "train: loss: 758915.875 acc: 0.9683890342712402  val: loss: 1354178.25 acc: 0.647734522819519\n",
      "step: 16895 , time : 0.0\n",
      "train: loss: 1720424.25 acc: 0.8950075507164001  val: loss: 2594658.25 acc: 0.7546254992485046\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 16900 , time : 0.0\n",
      "train: loss: 1006533.4375 acc: 0.9418829679489136  val: loss: 156258.859375 acc: 0.9579299688339233\n",
      "step: 16905 , time : 0.0\n",
      "train: loss: 334239.78125 acc: 0.9614956378936768  val: loss: 2637486.0 acc: 0.46316373348236084\n",
      "step: 16910 , time : 0.0\n",
      "train: loss: 480499.0 acc: 0.9710112810134888  val: loss: 773305.5 acc: 0.7563134431838989\n",
      "step: 16915 , time : 0.0\n",
      "train: loss: 395112.09375 acc: 0.9114090204238892  val: loss: 1507894.0 acc: 0.8003107905387878\n",
      "step: 16920 , time : 0.0010004043579101562\n",
      "train: loss: 1258203.125 acc: 0.8610031604766846  val: loss: 1881555.875 acc: -0.24615538120269775\n",
      "step: 16925 , time : 0.0010006427764892578\n",
      "train: loss: 1141968.375 acc: 0.6946436762809753  val: loss: 1163848.625 acc: 0.7462412118911743\n",
      "step: 16930 , time : 0.0\n",
      "train: loss: 829253.4375 acc: 0.6929543018341064  val: loss: 1008097.6875 acc: 0.6832325458526611\n",
      "step: 16935 , time : 0.0\n",
      "train: loss: 441523.96875 acc: 0.7309545278549194  val: loss: 1267400.75 acc: 0.6728336215019226\n",
      "step: 16940 , time : 0.0010006427764892578\n",
      "train: loss: 357821.59375 acc: 0.7797373533248901  val: loss: 939978.125 acc: 0.6961436867713928\n",
      "step: 16945 , time : 0.0\n",
      "train: loss: 1445499.125 acc: 0.5147336721420288  val: loss: 1735566.5 acc: 0.7870346307754517\n",
      "step: 16950 , time : 0.0\n",
      "train: loss: 917393.0625 acc: 0.6428309679031372  val: loss: 1570974.625 acc: 0.6901540160179138\n",
      "step: 16955 , time : 0.001001119613647461\n",
      "train: loss: 629197.0625 acc: 0.6191099882125854  val: loss: 1762608.25 acc: 0.49903178215026855\n",
      "step: 16960 , time : 0.0\n",
      "train: loss: 461871.5625 acc: 0.5444762110710144  val: loss: 818494.0625 acc: 0.7104047536849976\n",
      "step: 16965 , time : 0.0\n",
      "train: loss: 145775.6875 acc: 0.8694229125976562  val: loss: 2086365.125 acc: 0.7066709995269775\n",
      "step: 16970 , time : 0.001001119613647461\n",
      "train: loss: 248247.328125 acc: 0.8033978343009949  val: loss: 1224146.375 acc: 0.7063987255096436\n",
      "step: 16975 , time : 0.001001119613647461\n",
      "train: loss: 251214.578125 acc: 0.8173834681510925  val: loss: 879950.9375 acc: 0.6651663184165955\n",
      "step: 16980 , time : 0.0\n",
      "train: loss: 133608.640625 acc: 0.8980457782745361  val: loss: 1812769.0 acc: 0.5469177961349487\n",
      "step: 16985 , time : 0.0\n",
      "train: loss: 126472.796875 acc: 0.8974008560180664  val: loss: 683467.375 acc: 0.7450975775718689\n",
      "step: 16990 , time : 0.0010006427764892578\n",
      "train: loss: 175850.03125 acc: 0.8556010723114014  val: loss: 2776125.25 acc: 0.7005172371864319\n",
      "step: 16995 , time : 0.001001119613647461\n",
      "train: loss: 213679.59375 acc: 0.8713716268539429  val: loss: 662742.5625 acc: 0.7639050483703613\n",
      "step: 17000 , time : 0.0\n",
      "train: loss: 50324.296875 acc: 0.9328708648681641  val: loss: 1676481.875 acc: 0.7492722272872925\n",
      "step: 17005 , time : 0.0\n",
      "train: loss: 320163.5 acc: 0.7478045225143433  val: loss: 164672.546875 acc: 0.8733174800872803\n",
      "step: 17010 , time : 0.0010008811950683594\n",
      "train: loss: 534432.625 acc: 0.7683343887329102  val: loss: 1331213.0 acc: 0.6759276390075684\n",
      "step: 17015 , time : 0.0\n",
      "train: loss: 240752.53125 acc: 0.7759857773780823  val: loss: 2641517.5 acc: 0.5681989192962646\n",
      "step: 17020 , time : 0.0010004043579101562\n",
      "train: loss: 46007.015625 acc: 0.9392642378807068  val: loss: 1396759.875 acc: 0.6764370799064636\n",
      "step: 17025 , time : 0.0\n",
      "train: loss: 232475.921875 acc: 0.8340592384338379  val: loss: 729840.5 acc: 0.7106655836105347\n",
      "step: 17030 , time : 0.0\n",
      "train: loss: 949324.1875 acc: 0.7649053335189819  val: loss: 1141031.75 acc: 0.7095671892166138\n",
      "step: 17035 , time : 0.0010008811950683594\n",
      "train: loss: 1659259.5 acc: 0.7563995122909546  val: loss: 396185.625 acc: 0.8056126832962036\n",
      "step: 17040 , time : 0.0\n",
      "train: loss: 1688069.125 acc: 0.8423740863800049  val: loss: 1140903.125 acc: 0.8031009435653687\n",
      "step: 17045 , time : 0.0010006427764892578\n",
      "train: loss: 476740.65625 acc: 0.9642737507820129  val: loss: 1330702.125 acc: 0.4088318347930908\n",
      "step: 17050 , time : 0.0\n",
      "train: loss: 559264.0625 acc: 0.9456183314323425  val: loss: 777203.8125 acc: 0.8367416858673096\n",
      "step: 17055 , time : 0.0010006427764892578\n",
      "train: loss: 273395.71875 acc: 0.9258168339729309  val: loss: 405910.53125 acc: 0.9090064167976379\n",
      "step: 17060 , time : 0.0\n",
      "train: loss: 202144.796875 acc: 0.9700122475624084  val: loss: 418902.28125 acc: 0.8348284959793091\n",
      "step: 17065 , time : 0.0010004043579101562\n",
      "train: loss: 129607.6328125 acc: 0.9891501665115356  val: loss: 1030349.25 acc: 0.7687847018241882\n",
      "step: 17070 , time : 0.0\n",
      "train: loss: 112317.2734375 acc: 0.9917998909950256  val: loss: 613132.6875 acc: 0.9150216579437256\n",
      "step: 17075 , time : 0.0\n",
      "train: loss: 269833.96875 acc: 0.9698356986045837  val: loss: 3154427.75 acc: 0.73719322681427\n",
      "step: 17080 , time : 0.0010004043579101562\n",
      "train: loss: 127601.9765625 acc: 0.9831352829933167  val: loss: 464600.75 acc: 0.7976959943771362\n",
      "step: 17085 , time : 0.0010004043579101562\n",
      "train: loss: 22195.8203125 acc: 0.9928940534591675  val: loss: 318254.125 acc: 0.9532803893089294\n",
      "step: 17090 , time : 0.0010008811950683594\n",
      "train: loss: 32558.76953125 acc: 0.9789494872093201  val: loss: 831643.875 acc: 0.7743730545043945\n",
      "step: 17095 , time : 0.0\n",
      "train: loss: 38337.87890625 acc: 0.9755496978759766  val: loss: 1015337.75 acc: 0.7968410849571228\n",
      "step: 17100 , time : 0.0\n",
      "train: loss: 42520.10546875 acc: 0.9811533689498901  val: loss: 446161.8125 acc: 0.9012481570243835\n",
      "step: 17105 , time : 0.0010006427764892578\n",
      "train: loss: 27205.40625 acc: 0.9698907732963562  val: loss: 892298.375 acc: 0.8213018178939819\n",
      "step: 17110 , time : 0.0\n",
      "train: loss: 78442.84375 acc: 0.8465572595596313  val: loss: 234519.46875 acc: 0.9473158121109009\n",
      "step: 17115 , time : 0.0\n",
      "train: loss: 15080.427734375 acc: 0.9568356275558472  val: loss: 535710.3125 acc: 0.7156176567077637\n",
      "step: 17120 , time : 0.0\n",
      "train: loss: 59427.66015625 acc: 0.9313127994537354  val: loss: 918899.0 acc: 0.9487814903259277\n",
      "step: 17125 , time : 0.0010008811950683594\n",
      "train: loss: 36029.31640625 acc: 0.8806338310241699  val: loss: 250919.3125 acc: 0.9173658490180969\n",
      "step: 17130 , time : 0.0\n",
      "train: loss: 22448.880859375 acc: 0.9633533358573914  val: loss: 1137633.25 acc: 0.7596268653869629\n",
      "step: 17135 , time : 0.0010004043579101562\n",
      "train: loss: 21063.609375 acc: 0.9820107221603394  val: loss: 479394.96875 acc: 0.9087145328521729\n",
      "step: 17140 , time : 0.0\n",
      "train: loss: 58424.234375 acc: 0.9690048098564148  val: loss: 556182.25 acc: 0.9498584270477295\n",
      "step: 17145 , time : 0.0\n",
      "train: loss: 52523.99609375 acc: 0.9646356105804443  val: loss: 593969.125 acc: 0.8649828433990479\n",
      "step: 17150 , time : 0.0\n",
      "train: loss: 131843.0625 acc: 0.9405878782272339  val: loss: 1038173.75 acc: 0.7852625250816345\n",
      "step: 17155 , time : 0.0\n",
      "train: loss: 48903.109375 acc: 0.9671283960342407  val: loss: 1711192.875 acc: 0.34763526916503906\n",
      "step: 17160 , time : 0.0\n",
      "train: loss: 23997.919921875 acc: 0.9719952940940857  val: loss: 2670166.25 acc: 0.5560770034790039\n",
      "step: 17165 , time : 0.0\n",
      "train: loss: 31221.48828125 acc: 0.9768873453140259  val: loss: 1699154.25 acc: 0.3994407653808594\n",
      "step: 17170 , time : 0.001001119613647461\n",
      "train: loss: 46868.18359375 acc: 0.985649824142456  val: loss: 1693115.125 acc: 0.8766032457351685\n",
      "step: 17175 , time : 0.0\n",
      "train: loss: 41159.40625 acc: 0.9871995449066162  val: loss: 2618487.0 acc: 0.03840935230255127\n",
      "step: 17180 , time : 0.0\n",
      "train: loss: 51747.16796875 acc: 0.9884337782859802  val: loss: 849257.0 acc: 0.7521287798881531\n",
      "step: 17185 , time : 0.0\n",
      "train: loss: 31109.896484375 acc: 0.9885294437408447  val: loss: 1375696.5 acc: 0.7487205266952515\n",
      "step: 17190 , time : 0.0010013580322265625\n",
      "train: loss: 118677.609375 acc: 0.9380530714988708  val: loss: 699510.25 acc: 0.5771199464797974\n",
      "step: 17195 , time : 0.0010008811950683594\n",
      "train: loss: 147796.046875 acc: 0.9596375823020935  val: loss: 775843.8125 acc: 0.8744622468948364\n",
      "step: 17200 , time : 0.0\n",
      "train: loss: 177614.109375 acc: 0.902955949306488  val: loss: 1145123.25 acc: 0.7560091018676758\n",
      "step: 17205 , time : 0.0\n",
      "train: loss: 375662.75 acc: 0.9398807883262634  val: loss: 1010025.625 acc: 0.7747052907943726\n",
      "step: 17210 , time : 0.0\n",
      "train: loss: 215163.625 acc: 0.9781739711761475  val: loss: 2803398.25 acc: 0.011266350746154785\n",
      "step: 17215 , time : 0.0\n",
      "train: loss: 126932.6796875 acc: 0.987317681312561  val: loss: 5529078.0 acc: 0.41148436069488525\n",
      "step: 17220 , time : 0.0\n",
      "train: loss: 118051.265625 acc: 0.9770617485046387  val: loss: 942534.75 acc: 0.8275964856147766\n",
      "step: 17225 , time : 0.0\n",
      "train: loss: 104493.4921875 acc: 0.9792087078094482  val: loss: 208417.90625 acc: 0.9423837065696716\n",
      "step: 17230 , time : 0.0\n",
      "train: loss: 451694.875 acc: 0.9503006339073181  val: loss: 3740935.25 acc: 0.05393856763839722\n",
      "step: 17235 , time : 0.015625953674316406\n",
      "train: loss: 582696.9375 acc: 0.9638026356697083  val: loss: 615255.4375 acc: 0.9184070825576782\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 17240 , time : 0.0\n",
      "train: loss: 715599.9375 acc: 0.9577951431274414  val: loss: 1065106.25 acc: 0.6295050382614136\n",
      "step: 17245 , time : 0.0\n",
      "train: loss: 636420.0 acc: 0.9307320713996887  val: loss: 522646.1875 acc: 0.8049279451370239\n",
      "step: 17250 , time : 0.0\n",
      "train: loss: 394704.90625 acc: 0.9685428738594055  val: loss: 1161888.125 acc: 0.8809223771095276\n",
      "step: 17255 , time : 0.0\n",
      "train: loss: 2795841.75 acc: 0.9295821785926819  val: loss: 938759.5 acc: 0.8889583945274353\n",
      "step: 17260 , time : 0.0010008811950683594\n",
      "train: loss: 1438186.375 acc: 0.9489067792892456  val: loss: 2204895.0 acc: 0.4330335855484009\n",
      "step: 17265 , time : 0.0010008811950683594\n",
      "train: loss: 903237.25 acc: 0.9597957134246826  val: loss: 1716566.5 acc: 0.5193487405776978\n",
      "step: 17270 , time : 0.0010004043579101562\n",
      "train: loss: 1188141.125 acc: 0.9033547043800354  val: loss: 1761714.875 acc: 0.8339011669158936\n",
      "step: 17275 , time : 0.0\n",
      "train: loss: 179902.046875 acc: 0.9844493865966797  val: loss: 1142751.625 acc: 0.8293783664703369\n",
      "step: 17280 , time : 0.0\n",
      "train: loss: 343595.4375 acc: 0.9717047810554504  val: loss: 971707.375 acc: 0.7854517698287964\n",
      "step: 17285 , time : 0.0\n",
      "train: loss: 2362570.25 acc: 0.7478618621826172  val: loss: 672549.5 acc: 0.8589605689048767\n",
      "step: 17290 , time : 0.0\n",
      "train: loss: 588846.0 acc: 0.731415331363678  val: loss: 1436305.875 acc: 0.8186461925506592\n",
      "step: 17295 , time : 0.0\n",
      "train: loss: 430237.15625 acc: 0.7592496871948242  val: loss: 952143.125 acc: 0.49157559871673584\n",
      "step: 17300 , time : 0.0\n",
      "train: loss: 435096.15625 acc: 0.8343420624732971  val: loss: 1352421.375 acc: 0.6352260708808899\n",
      "step: 17305 , time : 0.0\n",
      "train: loss: 854308.25 acc: 0.7270317077636719  val: loss: 1004593.5 acc: 0.6908172369003296\n",
      "step: 17310 , time : 0.0\n",
      "train: loss: 1047434.375 acc: 0.6114053726196289  val: loss: 1439860.375 acc: 0.7241370677947998\n",
      "step: 17315 , time : 0.0\n",
      "train: loss: 732406.5 acc: 0.5559494495391846  val: loss: 6002552.5 acc: 0.5425938963890076\n",
      "step: 17320 , time : 0.0010006427764892578\n",
      "train: loss: 664539.625 acc: 0.725612998008728  val: loss: 1288375.375 acc: 0.5744361877441406\n",
      "step: 17325 , time : 0.0010004043579101562\n",
      "train: loss: 383846.46875 acc: 0.7652431130409241  val: loss: 940988.125 acc: 0.7312437295913696\n",
      "step: 17330 , time : 0.001001119613647461\n",
      "train: loss: 167542.59375 acc: 0.8498609662055969  val: loss: 644216.375 acc: 0.7101796865463257\n",
      "step: 17335 , time : 0.0\n",
      "train: loss: 343130.15625 acc: 0.8005573749542236  val: loss: 1197022.125 acc: 0.678734302520752\n",
      "step: 17340 , time : 0.0\n",
      "train: loss: 315730.21875 acc: 0.8330309391021729  val: loss: 1285160.25 acc: 0.704868495464325\n",
      "step: 17345 , time : 0.0\n",
      "train: loss: 147097.9375 acc: 0.8875815272331238  val: loss: 1027595.625 acc: 0.6712170839309692\n",
      "step: 17350 , time : 0.0\n",
      "train: loss: 348069.0 acc: 0.7821495532989502  val: loss: 980497.5625 acc: 0.7498599290847778\n",
      "step: 17355 , time : 0.0\n",
      "train: loss: 205008.0625 acc: 0.8747503757476807  val: loss: 886678.1875 acc: 0.7297696471214294\n",
      "step: 17360 , time : 0.0\n",
      "train: loss: 49227.0625 acc: 0.9450854063034058  val: loss: 1406204.625 acc: 0.6268041729927063\n",
      "step: 17365 , time : 0.0\n",
      "train: loss: 64029.66796875 acc: 0.9010986089706421  val: loss: 817138.25 acc: 0.6908241510391235\n",
      "step: 17370 , time : 0.0010008811950683594\n",
      "train: loss: 184643.921875 acc: 0.8604145050048828  val: loss: 2358501.75 acc: 0.6559935808181763\n",
      "step: 17375 , time : 0.0\n",
      "train: loss: 289125.25 acc: 0.7589203119277954  val: loss: 1504897.75 acc: 0.6080337166786194\n",
      "step: 17380 , time : 0.0\n",
      "train: loss: 722255.1875 acc: 0.6751466989517212  val: loss: 1608959.625 acc: 0.6020574569702148\n",
      "step: 17385 , time : 0.0009999275207519531\n",
      "train: loss: 381536.4375 acc: 0.7939764857292175  val: loss: 964781.6875 acc: 0.6521744728088379\n",
      "step: 17390 , time : 0.0\n",
      "train: loss: 428625.3125 acc: 0.777235746383667  val: loss: 3539583.0 acc: 0.5814220905303955\n",
      "step: 17395 , time : 0.0\n",
      "train: loss: 411246.8125 acc: 0.7769557237625122  val: loss: 1388537.875 acc: 0.6755989789962769\n",
      "step: 17400 , time : 0.0\n",
      "train: loss: 1489139.0 acc: 0.7501858472824097  val: loss: 824609.75 acc: 0.7175928354263306\n",
      "step: 17405 , time : 0.0\n",
      "train: loss: 1234454.75 acc: 0.8024961352348328  val: loss: 637942.25 acc: 0.8428328037261963\n",
      "step: 17410 , time : 0.0\n",
      "train: loss: 733558.25 acc: 0.9112484455108643  val: loss: 1847005.125 acc: 0.7803824543952942\n",
      "step: 17415 , time : 0.0\n",
      "train: loss: 395471.9375 acc: 0.9635547995567322  val: loss: 1131285.625 acc: 0.9244146943092346\n",
      "step: 17420 , time : 0.0\n",
      "train: loss: 278506.875 acc: 0.9531481266021729  val: loss: 1368848.625 acc: 0.8131644129753113\n",
      "step: 17425 , time : 0.0\n",
      "train: loss: 238219.421875 acc: 0.9568488001823425  val: loss: 567981.25 acc: 0.9323828816413879\n",
      "step: 17430 , time : 0.0\n",
      "train: loss: 162963.453125 acc: 0.9833218455314636  val: loss: 827951.9375 acc: 0.8783708214759827\n",
      "step: 17435 , time : 0.015626192092895508\n",
      "train: loss: 151066.53125 acc: 0.9892594814300537  val: loss: 1200789.25 acc: 0.6604518890380859\n",
      "step: 17440 , time : 0.0\n",
      "train: loss: 290274.875 acc: 0.9708288908004761  val: loss: 833141.0 acc: 0.8513216972351074\n",
      "step: 17445 , time : 0.0\n",
      "train: loss: 99331.5859375 acc: 0.981725811958313  val: loss: 997331.6875 acc: 0.945511519908905\n",
      "step: 17450 , time : 0.0\n",
      "train: loss: 63466.26953125 acc: 0.9835361242294312  val: loss: 303885.625 acc: 0.9616265892982483\n",
      "step: 17455 , time : 0.0010004043579101562\n",
      "train: loss: 46239.22265625 acc: 0.9746110439300537  val: loss: 1811444.125 acc: 0.7247527837753296\n",
      "step: 17460 , time : 0.0010008811950683594\n",
      "train: loss: 45915.4921875 acc: 0.9524555802345276  val: loss: 1685282.25 acc: 0.8758734464645386\n",
      "step: 17465 , time : 0.0\n",
      "train: loss: 42318.40625 acc: 0.9022999405860901  val: loss: 826056.4375 acc: 0.8524258136749268\n",
      "step: 17470 , time : 0.001001119613647461\n",
      "train: loss: 54978.2109375 acc: 0.9351110458374023  val: loss: 1878533.625 acc: 0.8152943849563599\n",
      "step: 17475 , time : 0.0\n",
      "train: loss: 22592.63671875 acc: 0.9546050429344177  val: loss: 1899738.875 acc: 0.8192768096923828\n",
      "step: 17480 , time : 0.0\n",
      "train: loss: 24420.669921875 acc: 0.9310476779937744  val: loss: 1199677.125 acc: 0.7689576148986816\n",
      "step: 17485 , time : 0.0010006427764892578\n",
      "train: loss: 28666.318359375 acc: 0.9470168352127075  val: loss: 2058083.875 acc: 0.45719194412231445\n",
      "step: 17490 , time : 0.0010008811950683594\n",
      "train: loss: 26572.31640625 acc: 0.9585785865783691  val: loss: 1347305.75 acc: 0.6865291595458984\n",
      "step: 17495 , time : 0.0\n",
      "train: loss: 16255.056640625 acc: 0.9754146933555603  val: loss: 646597.25 acc: 0.7907947897911072\n",
      "step: 17500 , time : 0.0010008811950683594\n",
      "train: loss: 161003.53125 acc: 0.8646690249443054  val: loss: 1129108.0 acc: 0.8142758011817932\n",
      "step: 17505 , time : 0.0010004043579101562\n",
      "train: loss: 71581.5859375 acc: 0.9543265700340271  val: loss: 1875295.5 acc: -0.505459189414978\n",
      "step: 17510 , time : 0.0\n",
      "train: loss: 29067.388671875 acc: 0.9805805087089539  val: loss: 606220.1875 acc: 0.6393008232116699\n",
      "step: 17515 , time : 0.0\n",
      "train: loss: 36207.34375 acc: 0.9758076667785645  val: loss: 1165389.125 acc: 0.7602866888046265\n",
      "step: 17520 , time : 0.0010004043579101562\n",
      "train: loss: 115524.5546875 acc: 0.9417169094085693  val: loss: 1586023.625 acc: 0.479902446269989\n",
      "step: 17525 , time : 0.0010004043579101562\n",
      "train: loss: 39178.63671875 acc: 0.9627119898796082  val: loss: 1277004.25 acc: 0.7034302949905396\n",
      "step: 17530 , time : 0.0\n",
      "train: loss: 34186.2734375 acc: 0.9658793807029724  val: loss: 1267214.0 acc: -0.19043588638305664\n",
      "step: 17535 , time : 0.0\n",
      "train: loss: 86450.4609375 acc: 0.9783654808998108  val: loss: 1686507.625 acc: 0.42843949794769287\n",
      "step: 17540 , time : 0.0010006427764892578\n",
      "train: loss: 33786.26953125 acc: 0.9885123372077942  val: loss: 1155180.875 acc: 0.7923812866210938\n",
      "step: 17545 , time : 0.0\n",
      "train: loss: 57075.55859375 acc: 0.9806985259056091  val: loss: 2467571.0 acc: 0.7329163551330566\n",
      "step: 17550 , time : 0.0010006427764892578\n",
      "train: loss: 87436.5 acc: 0.976415753364563  val: loss: 905465.5 acc: 0.8480339050292969\n",
      "step: 17555 , time : 0.0\n",
      "train: loss: 72284.8828125 acc: 0.9806660413742065  val: loss: 898961.5 acc: 0.9204963445663452\n",
      "step: 17560 , time : 0.0\n",
      "train: loss: 183028.125 acc: 0.9656226634979248  val: loss: 1349950.875 acc: 0.7077163457870483\n",
      "step: 17565 , time : 0.0\n",
      "train: loss: 164140.59375 acc: 0.9486159682273865  val: loss: 254169.6875 acc: 0.9364849328994751\n",
      "step: 17570 , time : 0.0\n",
      "train: loss: 67116.3203125 acc: 0.9644784331321716  val: loss: 320778.75 acc: 0.9535014629364014\n",
      "step: 17575 , time : 0.0010006427764892578\n",
      "train: loss: 286437.0 acc: 0.9592854976654053  val: loss: 1421726.75 acc: 0.6750025749206543\n",
      "step: 17580 , time : 0.0\n",
      "train: loss: 79994.125 acc: 0.9913315773010254  val: loss: 694487.8125 acc: 0.9057217836380005\n",
      "step: 17585 , time : 0.0\n",
      "train: loss: 594513.125 acc: 0.9145220518112183  val: loss: 1329403.125 acc: 0.24606144428253174\n",
      "step: 17590 , time : 0.0010008811950683594\n",
      "train: loss: 69304.1015625 acc: 0.9915560483932495  val: loss: 431486.6875 acc: 0.886775553226471\n",
      "step: 17595 , time : 0.0\n",
      "train: loss: 106887.7265625 acc: 0.9898050427436829  val: loss: 488958.625 acc: 0.8421329259872437\n",
      "step: 17600 , time : 0.0\n",
      "train: loss: 460053.1875 acc: 0.9760888814926147  val: loss: 888246.25 acc: 0.89011549949646\n",
      "step: 17605 , time : 0.0\n",
      "train: loss: 867841.9375 acc: 0.8901326060295105  val: loss: 982786.3125 acc: 0.7681941986083984\n",
      "step: 17610 , time : 0.0010013580322265625\n",
      "train: loss: 496114.71875 acc: 0.9332306981086731  val: loss: 534690.3125 acc: 0.46742701530456543\n",
      "step: 17615 , time : 0.0\n",
      "train: loss: 1671416.375 acc: 0.9427103400230408  val: loss: 1771913.125 acc: 0.4291236996650696\n",
      "step: 17620 , time : 0.0010008811950683594\n",
      "train: loss: 2724641.75 acc: 0.922149121761322  val: loss: 415168.0 acc: 0.8485255241394043\n",
      "step: 17625 , time : 0.0\n",
      "train: loss: 1154875.375 acc: 0.9540449380874634  val: loss: 268571.625 acc: 0.9041381478309631\n",
      "step: 17630 , time : 0.0\n",
      "train: loss: 623421.125 acc: 0.9635095596313477  val: loss: 282409.5 acc: 0.9358572363853455\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 17635 , time : 0.0010008811950683594\n",
      "train: loss: 638248.125 acc: 0.942920446395874  val: loss: 280738.53125 acc: 0.9563390016555786\n",
      "step: 17640 , time : 0.0\n",
      "train: loss: 1228417.25 acc: 0.9192180633544922  val: loss: 1724647.0 acc: 0.6090550422668457\n",
      "step: 17645 , time : 0.0\n",
      "train: loss: 508553.375 acc: 0.9584397673606873  val: loss: 715731.3125 acc: 0.8470647931098938\n",
      "step: 17650 , time : 0.0\n",
      "train: loss: 885226.5 acc: 0.7815385460853577  val: loss: 593419.75 acc: 0.7640901803970337\n",
      "step: 17655 , time : 0.0010008811950683594\n",
      "train: loss: 693333.8125 acc: 0.7895200252532959  val: loss: 992401.625 acc: 0.5607078075408936\n",
      "step: 17660 , time : 0.0\n",
      "train: loss: 1569813.375 acc: 0.5418061017990112  val: loss: 1742749.75 acc: 0.7690470218658447\n",
      "step: 17665 , time : 0.0010004043579101562\n",
      "train: loss: 879443.375 acc: 0.7227991819381714  val: loss: 5259528.5 acc: 0.16240453720092773\n",
      "step: 17670 , time : 0.0010013580322265625\n",
      "train: loss: 259344.65625 acc: 0.8777257204055786  val: loss: 1403305.125 acc: 0.5811625719070435\n",
      "step: 17675 , time : 0.0\n",
      "train: loss: 856929.25 acc: 0.3910074234008789  val: loss: 1240705.625 acc: 0.8075277209281921\n",
      "step: 17680 , time : 0.0010008811950683594\n",
      "train: loss: 913588.9375 acc: 0.6491345167160034  val: loss: 1707106.125 acc: 0.29801493883132935\n",
      "step: 17685 , time : 0.0010008811950683594\n",
      "train: loss: 375210.71875 acc: 0.7872968316078186  val: loss: 1755621.375 acc: 0.5846356749534607\n",
      "step: 17690 , time : 0.001001119613647461\n",
      "train: loss: 362556.6875 acc: 0.7222180366516113  val: loss: 1295141.125 acc: 0.623737096786499\n",
      "step: 17695 , time : 0.0\n",
      "train: loss: 119950.1953125 acc: 0.8969582319259644  val: loss: 2090990.0 acc: 0.6436086297035217\n",
      "step: 17700 , time : 0.0\n",
      "train: loss: 295636.34375 acc: 0.8094102144241333  val: loss: 1036517.5625 acc: 0.6272915601730347\n",
      "step: 17705 , time : 0.0\n",
      "train: loss: 291569.21875 acc: 0.8636670112609863  val: loss: 1302619.375 acc: 0.6282721757888794\n",
      "step: 17710 , time : 0.0010006427764892578\n",
      "train: loss: 306277.84375 acc: 0.8025177717208862  val: loss: 1865216.75 acc: 0.5888487100601196\n",
      "step: 17715 , time : 0.0010008811950683594\n",
      "train: loss: 51303.1796875 acc: 0.9602979421615601  val: loss: 1796791.25 acc: 0.6011872291564941\n",
      "step: 17720 , time : 0.0010004043579101562\n",
      "train: loss: 133944.859375 acc: 0.8782068490982056  val: loss: 959603.6875 acc: 0.7179904580116272\n",
      "step: 17725 , time : 0.001001119613647461\n",
      "train: loss: 82538.34375 acc: 0.9105004668235779  val: loss: 2158601.75 acc: 0.6331108808517456\n",
      "step: 17730 , time : 0.0010004043579101562\n",
      "train: loss: 122645.3359375 acc: 0.9130141735076904  val: loss: 2156428.5 acc: 0.6603538990020752\n",
      "step: 17735 , time : 0.0010006427764892578\n",
      "train: loss: 323781.34375 acc: 0.8165265321731567  val: loss: 2041002.75 acc: 0.7041217088699341\n",
      "step: 17740 , time : 0.0\n",
      "train: loss: 784095.6875 acc: 0.709160327911377  val: loss: 3644613.25 acc: 0.6337767839431763\n",
      "step: 17745 , time : 0.0010008811950683594\n",
      "train: loss: 126275.7265625 acc: 0.8866116404533386  val: loss: 2694932.75 acc: 0.5554376840591431\n",
      "step: 17750 , time : 0.0\n",
      "train: loss: 270904.1875 acc: 0.8036055564880371  val: loss: 1086300.125 acc: 0.7412238121032715\n",
      "step: 17755 , time : 0.0010006427764892578\n",
      "train: loss: 212951.546875 acc: 0.8565894365310669  val: loss: 2148701.25 acc: 0.6275495886802673\n",
      "step: 17760 , time : 0.0010006427764892578\n",
      "train: loss: 339635.125 acc: 0.8456062078475952  val: loss: 5531725.0 acc: 0.5990753173828125\n",
      "step: 17765 , time : 0.0010004043579101562\n",
      "train: loss: 1602487.625 acc: 0.7462905049324036  val: loss: 647848.375 acc: 0.8103243112564087\n",
      "step: 17770 , time : 0.0\n",
      "train: loss: 1059947.625 acc: 0.7942110300064087  val: loss: 760736.25 acc: 0.8306334614753723\n",
      "step: 17775 , time : 0.0\n",
      "train: loss: 1041735.1875 acc: 0.9137356877326965  val: loss: 2219936.5 acc: 0.7416294813156128\n",
      "step: 17780 , time : 0.0\n",
      "train: loss: 360718.625 acc: 0.9702293276786804  val: loss: 728642.875 acc: 0.8844926953315735\n",
      "step: 17785 , time : 0.0\n",
      "train: loss: 394251.0625 acc: 0.9499431848526001  val: loss: 1498809.0 acc: 0.5658872127532959\n",
      "step: 17790 , time : 0.0010008811950683594\n",
      "train: loss: 196521.078125 acc: 0.9690065383911133  val: loss: 1565401.25 acc: 0.7384521961212158\n",
      "step: 17795 , time : 0.0010006427764892578\n",
      "train: loss: 216074.4375 acc: 0.9818504452705383  val: loss: 703363.5 acc: 0.8996617794036865\n",
      "step: 17800 , time : 0.0\n",
      "train: loss: 131684.90625 acc: 0.9870062470436096  val: loss: 2764083.75 acc: 0.6096361875534058\n",
      "step: 17805 , time : 0.0\n",
      "train: loss: 114250.6796875 acc: 0.9900345802307129  val: loss: 1925694.875 acc: 0.7841017246246338\n",
      "step: 17810 , time : 0.0010004043579101562\n",
      "train: loss: 161256.5 acc: 0.9769530296325684  val: loss: 2254056.75 acc: 0.6201636791229248\n",
      "step: 17815 , time : 0.0010008811950683594\n",
      "train: loss: 32810.72265625 acc: 0.9945394396781921  val: loss: 1245959.125 acc: 0.8604834079742432\n",
      "step: 17820 , time : 0.0\n",
      "train: loss: 56674.52734375 acc: 0.9768085479736328  val: loss: 948297.0 acc: 0.7489955425262451\n",
      "step: 17825 , time : 0.0\n",
      "train: loss: 38356.40625 acc: 0.9789025783538818  val: loss: 1381174.0 acc: 0.1833106279373169\n",
      "step: 17830 , time : 0.0010004043579101562\n",
      "train: loss: 44297.5859375 acc: 0.9812712669372559  val: loss: 561516.1875 acc: 0.8571944236755371\n",
      "step: 17835 , time : 0.0\n",
      "train: loss: 34844.98828125 acc: 0.9312852621078491  val: loss: 2790816.25 acc: 0.4875919818878174\n",
      "step: 17840 , time : 0.0010004043579101562\n",
      "train: loss: 50447.0859375 acc: 0.9615800380706787  val: loss: 975259.125 acc: 0.8156194686889648\n",
      "step: 17845 , time : 0.0\n",
      "train: loss: 41723.390625 acc: 0.8088498115539551  val: loss: 889711.8125 acc: 0.48912888765335083\n",
      "step: 17850 , time : 0.0\n",
      "train: loss: 35974.85546875 acc: 0.8523222804069519  val: loss: 1239559.125 acc: 0.7639898061752319\n",
      "step: 17855 , time : 0.0\n",
      "train: loss: 28988.033203125 acc: 0.9499912261962891  val: loss: 905282.5625 acc: 0.8912384510040283\n",
      "step: 17860 , time : 0.0\n",
      "train: loss: 27091.1015625 acc: 0.9489343762397766  val: loss: 1317396.875 acc: 0.898002028465271\n",
      "step: 17865 , time : 0.0010006427764892578\n",
      "train: loss: 19398.998046875 acc: 0.9886552095413208  val: loss: 1041008.0625 acc: 0.5188958644866943\n",
      "step: 17870 , time : 0.0\n",
      "train: loss: 59391.375 acc: 0.9582356810569763  val: loss: 1070065.75 acc: 0.749089241027832\n",
      "step: 17875 , time : 0.0\n",
      "train: loss: 51802.4296875 acc: 0.961927592754364  val: loss: 1957850.375 acc: 0.8497307896614075\n",
      "step: 17880 , time : 0.0010006427764892578\n",
      "train: loss: 23604.048828125 acc: 0.9828504920005798  val: loss: 1346057.75 acc: 0.6999095678329468\n",
      "step: 17885 , time : 0.0010008811950683594\n",
      "train: loss: 32592.97265625 acc: 0.9807031154632568  val: loss: 1078425.25 acc: 0.7045544385910034\n",
      "step: 17890 , time : 0.0\n",
      "train: loss: 80026.28125 acc: 0.8878670930862427  val: loss: 708343.8125 acc: 0.9161332845687866\n",
      "step: 17895 , time : 0.0\n",
      "train: loss: 21949.146484375 acc: 0.9619468450546265  val: loss: 1294467.875 acc: 0.5206786394119263\n",
      "step: 17900 , time : 0.0010004043579101562\n",
      "train: loss: 66108.703125 acc: 0.9740654826164246  val: loss: 938823.5 acc: 0.6931571960449219\n",
      "step: 17905 , time : 0.0010004043579101562\n",
      "train: loss: 91726.90625 acc: 0.9864800572395325  val: loss: 1854562.25 acc: 0.3209967017173767\n",
      "step: 17910 , time : 0.0\n",
      "train: loss: 39021.203125 acc: 0.9837196469306946  val: loss: 749907.6875 acc: 0.7596517205238342\n",
      "step: 17915 , time : 0.0\n",
      "train: loss: 43485.48046875 acc: 0.9876282811164856  val: loss: 2178951.25 acc: 0.5293987989425659\n",
      "step: 17920 , time : 0.0\n",
      "train: loss: 99454.421875 acc: 0.9689009189605713  val: loss: 2698755.0 acc: 0.6445515155792236\n",
      "step: 17925 , time : 0.0\n",
      "train: loss: 361121.9375 acc: 0.8893218636512756  val: loss: 169828.015625 acc: 0.9333975315093994\n",
      "step: 17930 , time : 0.0\n",
      "train: loss: 104286.8046875 acc: 0.9620389342308044  val: loss: 1220519.5 acc: 0.773452639579773\n",
      "step: 17935 , time : 0.0010006427764892578\n",
      "train: loss: 136218.03125 acc: 0.9405665993690491  val: loss: 1431444.75 acc: 0.7913150191307068\n",
      "step: 17940 , time : 0.0\n",
      "train: loss: 95641.6953125 acc: 0.9832310676574707  val: loss: 365291.8125 acc: 0.9391071796417236\n",
      "step: 17945 , time : 0.0\n",
      "train: loss: 348658.78125 acc: 0.9542210102081299  val: loss: 293410.28125 acc: 0.9326704740524292\n",
      "step: 17950 , time : 0.0010008811950683594\n",
      "train: loss: 89198.828125 acc: 0.9875845313072205  val: loss: 742442.125 acc: 0.6119272708892822\n",
      "step: 17955 , time : 0.0\n",
      "train: loss: 137965.328125 acc: 0.9752114415168762  val: loss: 1211653.375 acc: 0.8168631792068481\n",
      "step: 17960 , time : 0.001001596450805664\n",
      "train: loss: 416150.34375 acc: 0.883338451385498  val: loss: 853546.3125 acc: 0.8802090883255005\n",
      "step: 17965 , time : 0.0010004043579101562\n",
      "train: loss: 410991.1875 acc: 0.9806512594223022  val: loss: 321944.40625 acc: 0.9519249796867371\n",
      "step: 17970 , time : 0.0\n",
      "train: loss: 233475.953125 acc: 0.9774585366249084  val: loss: 4889510.5 acc: 0.5082175731658936\n",
      "step: 17975 , time : 0.0\n",
      "train: loss: 336846.59375 acc: 0.8230327367782593  val: loss: 1328491.5 acc: 0.6791089773178101\n",
      "step: 17980 , time : 0.0\n",
      "train: loss: 553708.5 acc: 0.9563121795654297  val: loss: 1537733.875 acc: 0.24701225757598877\n",
      "step: 17985 , time : 0.0\n",
      "train: loss: 1227724.875 acc: 0.9658556580543518  val: loss: 852104.25 acc: 0.46457159519195557\n",
      "step: 17990 , time : 0.0\n",
      "train: loss: 1428998.125 acc: 0.9457409977912903  val: loss: 768832.125 acc: 0.7021514177322388\n",
      "step: 17995 , time : 0.015626192092895508\n",
      "train: loss: 316061.09375 acc: 0.9633021950721741  val: loss: 627157.4375 acc: 0.7657533884048462\n",
      "step: 18000 , time : 0.0\n",
      "train: loss: 831314.4375 acc: 0.955007016658783  val: loss: 540038.3125 acc: 0.8845201134681702\n",
      "step: 18005 , time : 0.0\n",
      "train: loss: 569443.25 acc: 0.9608835577964783  val: loss: 852076.1875 acc: 0.8245162963867188\n",
      "step: 18010 , time : 0.0\n",
      "train: loss: 327198.0 acc: 0.9619634747505188  val: loss: 1006122.1875 acc: 0.24724143743515015\n",
      "step: 18015 , time : 0.0\n",
      "train: loss: 715060.75 acc: 0.9501090049743652  val: loss: 377545.375 acc: 0.9406290650367737\n",
      "step: 18020 , time : 0.0\n",
      "train: loss: 1032704.5 acc: 0.49359750747680664  val: loss: 1496744.875 acc: 0.7790375351905823\n",
      "step: 18025 , time : 0.0\n",
      "train: loss: 672618.75 acc: 0.633337140083313  val: loss: 881421.125 acc: 0.736601710319519\n",
      "step: 18030 , time : 0.0\n",
      "train: loss: 553215.0 acc: 0.8130289316177368  val: loss: 2012573.125 acc: 0.7789528369903564\n",
      "step: 18035 , time : 0.0\n",
      "train: loss: 590914.5625 acc: 0.8875944018363953  val: loss: 2951694.75 acc: 0.842200517654419\n",
      "step: 18040 , time : 0.0\n",
      "train: loss: 1027315.625 acc: 0.5405274629592896  val: loss: 1502017.375 acc: 0.858952522277832\n",
      "step: 18045 , time : 0.0\n",
      "train: loss: 808274.6875 acc: 0.5624296069145203  val: loss: 992522.25 acc: 0.5819114446640015\n",
      "step: 18050 , time : 0.0\n",
      "train: loss: 487859.9375 acc: 0.7291425466537476  val: loss: 4421901.0 acc: 0.5898573398590088\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 18055 , time : 0.0\n",
      "train: loss: 242848.609375 acc: 0.778904378414154  val: loss: 4083219.75 acc: 0.5718694925308228\n",
      "step: 18060 , time : 0.0010006427764892578\n",
      "train: loss: 218589.21875 acc: 0.8600138425827026  val: loss: 1472905.0 acc: 0.6712933778762817\n",
      "step: 18065 , time : 0.0\n",
      "train: loss: 222423.765625 acc: 0.8405137062072754  val: loss: 4455168.5 acc: 0.6591070890426636\n",
      "step: 18070 , time : 0.0\n",
      "train: loss: 113464.328125 acc: 0.9192501902580261  val: loss: 2516958.75 acc: 0.6659430265426636\n",
      "step: 18075 , time : 0.0\n",
      "train: loss: 248991.203125 acc: 0.8700848817825317  val: loss: 1745587.625 acc: 0.6977841258049011\n",
      "step: 18080 , time : 0.0010006427764892578\n",
      "train: loss: 351690.875 acc: 0.8195757865905762  val: loss: 1161108.625 acc: 0.6234284043312073\n",
      "step: 18085 , time : 0.0\n",
      "train: loss: 160096.1875 acc: 0.8950903415679932  val: loss: 7445119.0 acc: 0.46590203046798706\n",
      "step: 18090 , time : 0.0\n",
      "train: loss: 33506.140625 acc: 0.9540368318557739  val: loss: 4048969.0 acc: 0.6275681257247925\n",
      "step: 18095 , time : 0.0010006427764892578\n",
      "train: loss: 41736.234375 acc: 0.9614686965942383  val: loss: 356937.0625 acc: 0.7655818462371826\n",
      "step: 18100 , time : 0.0\n",
      "train: loss: 239237.21875 acc: 0.7238924503326416  val: loss: 1446075.375 acc: 0.7119934558868408\n",
      "step: 18105 , time : 0.0\n",
      "train: loss: 110921.890625 acc: 0.87260502576828  val: loss: 2909084.0 acc: 0.565048098564148\n",
      "step: 18110 , time : 0.0\n",
      "train: loss: 986841.25 acc: 0.6908774375915527  val: loss: 2482923.0 acc: 0.6483129262924194\n",
      "step: 18115 , time : 0.0\n",
      "train: loss: 131669.515625 acc: 0.8927818536758423  val: loss: 1075688.375 acc: 0.7590498328208923\n",
      "step: 18120 , time : 0.0\n",
      "train: loss: 379339.5 acc: 0.80841064453125  val: loss: 1661122.25 acc: 0.6869527101516724\n",
      "step: 18125 , time : 0.0\n",
      "train: loss: 366614.0625 acc: 0.8118047714233398  val: loss: 2598438.0 acc: 0.6145325899124146\n",
      "step: 18130 , time : 0.0\n",
      "train: loss: 1855312.5 acc: 0.6998645067214966  val: loss: 951167.625 acc: 0.8423179388046265\n",
      "step: 18135 , time : 0.0\n",
      "train: loss: 785228.75 acc: 0.6659976243972778  val: loss: 788017.5625 acc: 0.853446364402771\n",
      "step: 18140 , time : 0.0\n",
      "train: loss: 1255458.625 acc: 0.8704791069030762  val: loss: 1445378.375 acc: 0.6779348254203796\n",
      "step: 18145 , time : 0.0\n",
      "train: loss: 854378.0 acc: 0.9217444062232971  val: loss: 1301311.25 acc: 0.5578115582466125\n",
      "step: 18150 , time : 0.0\n",
      "train: loss: 545756.8125 acc: 0.8638534545898438  val: loss: 1308986.0 acc: 0.7518942356109619\n",
      "step: 18155 , time : 0.0\n",
      "train: loss: 1102165.125 acc: 0.7424682378768921  val: loss: 969516.875 acc: 0.7257315516471863\n",
      "step: 18160 , time : 0.0\n",
      "train: loss: 342934.875 acc: 0.9684814810752869  val: loss: 565941.5 acc: 0.7226763963699341\n",
      "step: 18165 , time : 0.0\n",
      "train: loss: 633129.625 acc: 0.9553315043449402  val: loss: 479292.625 acc: 0.8720763921737671\n",
      "step: 18170 , time : 0.0\n",
      "train: loss: 146281.375 acc: 0.9872778654098511  val: loss: 1327342.125 acc: 0.8088021278381348\n",
      "step: 18175 , time : 0.0010006427764892578\n",
      "train: loss: 134844.90625 acc: 0.9877124428749084  val: loss: 1851519.625 acc: -0.033503174781799316\n",
      "step: 18180 , time : 0.0010004043579101562\n",
      "train: loss: 51302.3125 acc: 0.9820334911346436  val: loss: 472854.4375 acc: 0.896754264831543\n",
      "step: 18185 , time : 0.0\n",
      "train: loss: 48160.74609375 acc: 0.9843195676803589  val: loss: 845171.8125 acc: 0.7908727526664734\n",
      "step: 18190 , time : 0.0\n",
      "train: loss: 87598.6171875 acc: 0.9729536771774292  val: loss: 2102363.0 acc: 0.18117237091064453\n",
      "step: 18195 , time : 0.0\n",
      "train: loss: 16666.3125 acc: 0.9781478643417358  val: loss: 1168485.625 acc: 0.8526164293289185\n",
      "step: 18200 , time : 0.0010006427764892578\n",
      "train: loss: 14294.5146484375 acc: 0.9804062843322754  val: loss: 924504.125 acc: 0.7964777946472168\n",
      "step: 18205 , time : 0.0\n",
      "train: loss: 49247.12109375 acc: 0.9684486389160156  val: loss: 1147173.75 acc: -0.18128585815429688\n",
      "step: 18210 , time : 0.0\n",
      "train: loss: 48580.5390625 acc: 0.9517029523849487  val: loss: 1561777.375 acc: 0.45223021507263184\n",
      "step: 18215 , time : 0.0\n",
      "train: loss: 32450.76171875 acc: 0.9283055663108826  val: loss: 708570.375 acc: 0.7269872426986694\n",
      "step: 18220 , time : 0.0\n",
      "train: loss: 12975.4375 acc: 0.9688552618026733  val: loss: 993200.625 acc: 0.855869710445404\n",
      "step: 18225 , time : 0.0\n",
      "train: loss: 18682.76171875 acc: 0.944091796875  val: loss: 848286.5 acc: 0.32519322633743286\n",
      "step: 18230 , time : 0.0\n",
      "train: loss: 9439.126953125 acc: 0.9870047569274902  val: loss: 1487634.125 acc: 0.6462535858154297\n",
      "step: 18235 , time : 0.0\n",
      "train: loss: 59814.9921875 acc: 0.9654067754745483  val: loss: 381560.625 acc: 0.9479873776435852\n",
      "step: 18240 , time : 0.015625715255737305\n",
      "train: loss: 68214.625 acc: 0.9671427607536316  val: loss: 861963.4375 acc: 0.12523967027664185\n",
      "step: 18245 , time : 0.0\n",
      "train: loss: 100835.0859375 acc: 0.9051573276519775  val: loss: 300701.84375 acc: 0.9570662975311279\n",
      "step: 18250 , time : 0.0\n",
      "train: loss: 47482.875 acc: 0.974206268787384  val: loss: 1244303.5 acc: 0.7954047322273254\n",
      "step: 18255 , time : 0.0\n",
      "train: loss: 32827.8671875 acc: 0.9640994071960449  val: loss: 352406.28125 acc: 0.9286948442459106\n",
      "step: 18260 , time : 0.0\n",
      "train: loss: 79901.8203125 acc: 0.8729373812675476  val: loss: 793108.625 acc: 0.6736475229263306\n",
      "step: 18265 , time : 0.0010008811950683594\n",
      "train: loss: 23781.259765625 acc: 0.9859982132911682  val: loss: 852397.625 acc: 0.877703845500946\n",
      "step: 18270 , time : 0.0010006427764892578\n",
      "train: loss: 61724.2109375 acc: 0.9811618328094482  val: loss: 961247.625 acc: 0.37732255458831787\n",
      "step: 18275 , time : 0.0010008811950683594\n",
      "train: loss: 39809.95703125 acc: 0.9830899238586426  val: loss: 741562.8125 acc: 0.7810764312744141\n",
      "step: 18280 , time : 0.0010004043579101562\n",
      "train: loss: 64611.796875 acc: 0.9752284288406372  val: loss: 296470.53125 acc: 0.797661542892456\n",
      "step: 18285 , time : 0.0\n",
      "train: loss: 83164.703125 acc: 0.9723986983299255  val: loss: 187837.078125 acc: 0.8036293983459473\n",
      "step: 18290 , time : 0.0\n",
      "train: loss: 141758.953125 acc: 0.9719171524047852  val: loss: 617350.375 acc: 0.8676301836967468\n",
      "step: 18295 , time : 0.0010004043579101562\n",
      "train: loss: 134343.078125 acc: 0.970196545124054  val: loss: 105558.6875 acc: 0.8844472169876099\n",
      "step: 18300 , time : 0.0\n",
      "train: loss: 154912.140625 acc: 0.7985488176345825  val: loss: 863333.3125 acc: 0.7737468481063843\n",
      "step: 18305 , time : 0.0\n",
      "train: loss: 604654.25 acc: 0.9108445048332214  val: loss: 2871672.25 acc: 0.4904100298881531\n",
      "step: 18310 , time : 0.0\n",
      "train: loss: 78604.5078125 acc: 0.991764485836029  val: loss: 1835002.375 acc: 0.528450608253479\n",
      "step: 18315 , time : 0.0\n",
      "train: loss: 602523.1875 acc: 0.8874481320381165  val: loss: 339412.5 acc: 0.9327138662338257\n",
      "step: 18320 , time : 0.0\n",
      "train: loss: 137003.484375 acc: 0.9812350273132324  val: loss: 344664.28125 acc: 0.894974410533905\n",
      "step: 18325 , time : 0.0\n",
      "train: loss: 276603.09375 acc: 0.9726816415786743  val: loss: 337490.15625 acc: 0.9144875407218933\n",
      "step: 18330 , time : 0.0\n",
      "train: loss: 356886.71875 acc: 0.9767224192619324  val: loss: 360415.40625 acc: 0.9175125956535339\n",
      "step: 18335 , time : 0.0\n",
      "train: loss: 429413.78125 acc: 0.963361918926239  val: loss: 1598066.875 acc: 0.571263313293457\n",
      "step: 18340 , time : 0.0\n",
      "train: loss: 571263.8125 acc: 0.9415847659111023  val: loss: 1138431.25 acc: 0.6525932550430298\n",
      "step: 18345 , time : 0.0\n",
      "train: loss: 1466945.875 acc: 0.9278014302253723  val: loss: 853744.125 acc: 0.8980004787445068\n",
      "step: 18350 , time : 0.0\n",
      "train: loss: 596546.75 acc: 0.9724211096763611  val: loss: 354411.4375 acc: 0.9517188668251038\n",
      "step: 18355 , time : 0.0\n",
      "train: loss: 1654768.0 acc: 0.9518318176269531  val: loss: 1357524.75 acc: 0.9024326801300049\n",
      "step: 18360 , time : 0.0\n",
      "train: loss: 795428.75 acc: 0.9561110734939575  val: loss: 559458.125 acc: 0.8925536274909973\n",
      "step: 18365 , time : 0.0\n",
      "train: loss: 424981.15625 acc: 0.9664710164070129  val: loss: 575731.625 acc: 0.9562106728553772\n",
      "step: 18370 , time : 0.0\n",
      "train: loss: 224088.625 acc: 0.9691745042800903  val: loss: 298611.53125 acc: 0.9729405045509338\n",
      "step: 18375 , time : 0.0\n",
      "train: loss: 822396.8125 acc: 0.9208031296730042  val: loss: 3021202.5 acc: 0.6619508266448975\n",
      "step: 18380 , time : 0.0\n",
      "train: loss: 522710.0625 acc: 0.9335281848907471  val: loss: 662440.1875 acc: 0.880806028842926\n",
      "step: 18385 , time : 0.0\n",
      "train: loss: 1667766.125 acc: 0.519267737865448  val: loss: 1511327.875 acc: 0.6389451622962952\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 18390 , time : 0.0\n",
      "train: loss: 845065.0625 acc: 0.582526683807373  val: loss: 1137666.0 acc: 0.9115220904350281\n",
      "step: 18395 , time : 0.0\n",
      "train: loss: 496805.53125 acc: 0.7857406139373779  val: loss: 1316825.5 acc: 0.789562463760376\n",
      "step: 18400 , time : 0.0010008811950683594\n",
      "train: loss: 1164945.75 acc: 0.730697751045227  val: loss: 1388822.125 acc: 0.8273469805717468\n",
      "step: 18405 , time : 0.0\n",
      "train: loss: 1100825.0 acc: 0.5944437980651855  val: loss: 593480.0 acc: 0.8860107064247131\n",
      "step: 18410 , time : 0.0010008811950683594\n",
      "train: loss: 593447.0625 acc: 0.7545502185821533  val: loss: 2907709.25 acc: 0.6956415772438049\n",
      "step: 18415 , time : 0.0\n",
      "train: loss: 535671.875 acc: 0.6463125944137573  val: loss: 3590764.0 acc: 0.5181721448898315\n",
      "step: 18420 , time : 0.0\n",
      "train: loss: 685120.4375 acc: 0.8080448508262634  val: loss: 2670098.0 acc: 0.6547572612762451\n",
      "step: 18425 , time : 0.0010004043579101562\n",
      "train: loss: 195880.25 acc: 0.8322128057479858  val: loss: 1722960.875 acc: 0.7148734331130981\n",
      "step: 18430 , time : 0.0\n",
      "train: loss: 87923.8671875 acc: 0.9261561632156372  val: loss: 1805538.375 acc: 0.7650706768035889\n",
      "step: 18435 , time : 0.0\n",
      "train: loss: 108779.4296875 acc: 0.9097474217414856  val: loss: 2927081.5 acc: 0.6068542003631592\n",
      "step: 18440 , time : 0.0\n",
      "train: loss: 108953.2421875 acc: 0.9230575561523438  val: loss: 357585.4375 acc: 0.7758249044418335\n",
      "step: 18445 , time : 0.0\n",
      "train: loss: 82303.328125 acc: 0.9430722594261169  val: loss: 814895.75 acc: 0.7558754682540894\n",
      "step: 18450 , time : 0.0\n",
      "train: loss: 145320.421875 acc: 0.8802056908607483  val: loss: 731135.625 acc: 0.7083297967910767\n",
      "step: 18455 , time : 0.0\n",
      "train: loss: 63441.4453125 acc: 0.9459911584854126  val: loss: 782436.1875 acc: 0.7765225172042847\n",
      "step: 18460 , time : 0.0010006427764892578\n",
      "train: loss: 23202.884765625 acc: 0.9588474035263062  val: loss: 1380260.125 acc: 0.6552890539169312\n",
      "step: 18465 , time : 0.0\n",
      "train: loss: 61397.609375 acc: 0.9191349148750305  val: loss: 1059513.375 acc: 0.7136185169219971\n",
      "step: 18470 , time : 0.0\n",
      "train: loss: 361349.09375 acc: 0.8147077560424805  val: loss: 1498694.125 acc: 0.6616536378860474\n",
      "step: 18475 , time : 0.0\n",
      "train: loss: 648301.625 acc: 0.7419122457504272  val: loss: 2157268.75 acc: 0.607867956161499\n",
      "step: 18480 , time : 0.0\n",
      "train: loss: 762092.9375 acc: 0.7429504990577698  val: loss: 459699.875 acc: 0.7995186448097229\n",
      "step: 18485 , time : 0.0\n",
      "train: loss: 85354.1328125 acc: 0.9223688840866089  val: loss: 4761462.5 acc: 0.5889629125595093\n",
      "step: 18490 , time : 0.0\n",
      "train: loss: 161739.5 acc: 0.8388407230377197  val: loss: 2940513.25 acc: 0.29550397396087646\n",
      "step: 18495 , time : 0.0\n",
      "train: loss: 2435406.5 acc: 0.6508275270462036  val: loss: 560103.375 acc: 0.7868698835372925\n",
      "step: 18500 , time : 0.0\n",
      "train: loss: 1004316.5 acc: 0.691055953502655  val: loss: 507710.0 acc: 0.8337593674659729\n",
      "step: 18505 , time : 0.0\n",
      "train: loss: 750688.125 acc: 0.8625204563140869  val: loss: 1352521.375 acc: 0.796153724193573\n",
      "step: 18510 , time : 0.0010008811950683594\n",
      "train: loss: 590861.625 acc: 0.9002538323402405  val: loss: 1080712.625 acc: 0.8125669956207275\n",
      "step: 18515 , time : 0.0010004043579101562\n",
      "train: loss: 735714.0625 acc: 0.8723484873771667  val: loss: 1446376.625 acc: 0.8316195011138916\n",
      "step: 18520 , time : 0.0\n",
      "train: loss: 451324.8125 acc: 0.9229356646537781  val: loss: 1522874.375 acc: 0.8004031777381897\n",
      "step: 18525 , time : 0.001001119613647461\n",
      "train: loss: 188508.3125 acc: 0.9823360443115234  val: loss: 391285.375 acc: 0.8751845359802246\n",
      "step: 18530 , time : 0.0\n",
      "train: loss: 178621.109375 acc: 0.9876366257667542  val: loss: 2355397.5 acc: 0.34831881523132324\n",
      "step: 18535 , time : 0.0\n",
      "train: loss: 117374.9296875 acc: 0.9923539757728577  val: loss: 907639.9375 acc: 0.8555681109428406\n",
      "step: 18540 , time : 0.0\n",
      "train: loss: 92317.4921875 acc: 0.9902670979499817  val: loss: 1521141.75 acc: 0.6307902336120605\n",
      "step: 18545 , time : 0.0\n",
      "train: loss: 69908.25 acc: 0.9859358072280884  val: loss: 1245111.875 acc: 0.7417771816253662\n",
      "step: 18550 , time : 0.0\n",
      "train: loss: 64746.703125 acc: 0.9880294799804688  val: loss: 779513.4375 acc: 0.7314023375511169\n",
      "step: 18555 , time : 0.0\n",
      "train: loss: 21441.8125 acc: 0.8948618769645691  val: loss: 156313.9375 acc: 0.9646849632263184\n",
      "step: 18560 , time : 0.0\n",
      "train: loss: 16214.701171875 acc: 0.9588011503219604  val: loss: 778037.0 acc: 0.9196488261222839\n",
      "step: 18565 , time : 0.0\n",
      "train: loss: 11970.4072265625 acc: 0.9927645921707153  val: loss: 884499.0 acc: 0.45804500579833984\n",
      "step: 18570 , time : 0.0\n",
      "train: loss: 32652.943359375 acc: 0.9784854054450989  val: loss: 879217.375 acc: 0.8029628396034241\n",
      "step: 18575 , time : 0.0\n",
      "train: loss: 13226.408203125 acc: 0.9746927618980408  val: loss: 1544531.25 acc: 0.798037588596344\n",
      "step: 18580 , time : 0.0\n",
      "train: loss: 36831.23046875 acc: 0.9518488049507141  val: loss: 289587.40625 acc: 0.947163999080658\n",
      "step: 18585 , time : 0.0\n",
      "train: loss: 15232.3662109375 acc: 0.9811822175979614  val: loss: 454028.28125 acc: 0.9271751642227173\n",
      "step: 18590 , time : 0.0\n",
      "train: loss: 19852.84375 acc: 0.9606781005859375  val: loss: 736491.125 acc: 0.9319079518318176\n",
      "step: 18595 , time : 0.0\n",
      "train: loss: 19689.650390625 acc: 0.9463682174682617  val: loss: 501019.5 acc: 0.9201363325119019\n",
      "step: 18600 , time : 0.0\n",
      "train: loss: 71226.390625 acc: 0.9464704394340515  val: loss: 752009.9375 acc: 0.9293413162231445\n",
      "step: 18605 , time : 0.0\n",
      "train: loss: 31286.59375 acc: 0.981835126876831  val: loss: 1544727.5 acc: 0.1617799997329712\n",
      "step: 18610 , time : 0.0\n",
      "train: loss: 10450.830078125 acc: 0.994002640247345  val: loss: 922200.75 acc: 0.7289879322052002\n",
      "step: 18615 , time : 0.0\n",
      "train: loss: 27787.84765625 acc: 0.9821328520774841  val: loss: 2373382.0 acc: -1.738673210144043\n",
      "step: 18620 , time : 0.0\n",
      "train: loss: 20040.115234375 acc: 0.9683489799499512  val: loss: 413197.25 acc: 0.9286066293716431\n",
      "step: 18625 , time : 0.0010004043579101562\n",
      "train: loss: 21475.3828125 acc: 0.9866281151771545  val: loss: 331010.28125 acc: 0.9273955225944519\n",
      "step: 18630 , time : 0.001001119613647461\n",
      "train: loss: 19447.326171875 acc: 0.9932392239570618  val: loss: 801507.0 acc: 0.699799120426178\n",
      "step: 18635 , time : 0.0010008811950683594\n",
      "train: loss: 63486.67578125 acc: 0.9767477512359619  val: loss: 184101.5625 acc: 0.9627910852432251\n",
      "step: 18640 , time : 0.0010008811950683594\n",
      "train: loss: 46332.63671875 acc: 0.9891906976699829  val: loss: 2418056.0 acc: 0.17537081241607666\n",
      "step: 18645 , time : 0.0\n",
      "train: loss: 34101.13671875 acc: 0.9899588227272034  val: loss: 708877.25 acc: 0.7244895696640015\n",
      "step: 18650 , time : 0.0\n",
      "train: loss: 48134.0859375 acc: 0.9777477383613586  val: loss: 263196.90625 acc: 0.9524480700492859\n",
      "step: 18655 , time : 0.0\n",
      "train: loss: 66215.5703125 acc: 0.9597586393356323  val: loss: 1727418.875 acc: 0.5513384342193604\n",
      "step: 18660 , time : 0.0\n",
      "train: loss: 193564.1875 acc: 0.9533120393753052  val: loss: 236025.390625 acc: 0.9699329137802124\n",
      "step: 18665 , time : 0.0\n",
      "train: loss: 130647.84375 acc: 0.8524397611618042  val: loss: 868442.5 acc: 0.9079746603965759\n",
      "step: 18670 , time : 0.0\n",
      "train: loss: 217526.359375 acc: 0.9637190699577332  val: loss: 365409.15625 acc: 0.9485424160957336\n",
      "step: 18675 , time : 0.0\n",
      "train: loss: 234498.84375 acc: 0.9750840067863464  val: loss: 1440902.375 acc: 0.8447100520133972\n",
      "step: 18680 , time : 0.0\n",
      "train: loss: 47661.375 acc: 0.9948946833610535  val: loss: 847204.0625 acc: 0.8797053098678589\n",
      "step: 18685 , time : 0.0\n",
      "train: loss: 133691.375 acc: 0.984104573726654  val: loss: 796986.875 acc: 0.8145639896392822\n",
      "step: 18690 , time : 0.0\n",
      "train: loss: 79893.0 acc: 0.9870627522468567  val: loss: 460764.1875 acc: 0.8451423048973083\n",
      "step: 18695 , time : 0.0\n",
      "train: loss: 183911.671875 acc: 0.9835206866264343  val: loss: 526854.3125 acc: 0.8989856839179993\n",
      "step: 18700 , time : 0.0\n",
      "train: loss: 750252.125 acc: 0.9647340774536133  val: loss: 1170313.5 acc: 0.8028583526611328\n",
      "step: 18705 , time : 0.0\n",
      "train: loss: 285799.5625 acc: 0.9508112668991089  val: loss: 1645007.875 acc: 0.8010438680648804\n",
      "step: 18710 , time : 0.0\n",
      "train: loss: 1317178.75 acc: 0.9360338449478149  val: loss: 1728568.875 acc: 0.7010931968688965\n",
      "step: 18715 , time : 0.0\n",
      "train: loss: 613709.9375 acc: 0.9790093302726746  val: loss: 896045.9375 acc: 0.7653475403785706\n",
      "step: 18720 , time : 0.0010004043579101562\n",
      "train: loss: 1436030.125 acc: 0.9592762589454651  val: loss: 1042268.375 acc: 0.8413366079330444\n",
      "step: 18725 , time : 0.0010013580322265625\n",
      "train: loss: 1943922.125 acc: 0.9129791259765625  val: loss: 596745.875 acc: 0.8912256360054016\n",
      "step: 18730 , time : 0.001001119613647461\n",
      "train: loss: 787795.0625 acc: 0.9633222222328186  val: loss: 2079404.75 acc: -0.10804986953735352\n",
      "step: 18735 , time : 0.0010013580322265625\n",
      "train: loss: 58332.08203125 acc: 0.9938467144966125  val: loss: 427865.0625 acc: 0.9248385429382324\n",
      "step: 18740 , time : 0.0\n",
      "train: loss: 168906.140625 acc: 0.9797515869140625  val: loss: 408277.28125 acc: 0.9566863179206848\n",
      "step: 18745 , time : 0.0010006427764892578\n",
      "train: loss: 301298.21875 acc: 0.9445320963859558  val: loss: 1771344.0 acc: 0.8579022884368896\n",
      "step: 18750 , time : 0.0010006427764892578\n",
      "train: loss: 2294764.75 acc: 0.36114245653152466  val: loss: 852150.9375 acc: 0.8738959431648254\n",
      "step: 18755 , time : 0.0010004043579101562\n",
      "train: loss: 479382.34375 acc: 0.6880984306335449  val: loss: 848293.3125 acc: 0.5267657041549683\n",
      "step: 18760 , time : 0.0010006427764892578\n",
      "train: loss: 642457.9375 acc: 0.7888588309288025  val: loss: 896529.375 acc: 0.6900009512901306\n",
      "step: 18765 , time : 0.0010008811950683594\n",
      "train: loss: 490736.375 acc: 0.7976322174072266  val: loss: 1010958.75 acc: 0.2957920432090759\n",
      "step: 18770 , time : 0.0\n",
      "train: loss: 1056361.875 acc: 0.7872186899185181  val: loss: 959525.0 acc: 0.8577179908752441\n",
      "step: 18775 , time : 0.0\n",
      "train: loss: 813992.0 acc: 0.5907009840011597  val: loss: 1803001.0 acc: 0.6345641613006592\n",
      "step: 18780 , time : 0.0\n",
      "train: loss: 278922.6875 acc: 0.7702208757400513  val: loss: 1790094.75 acc: 0.6492282152175903\n",
      "step: 18785 , time : 0.0\n",
      "train: loss: 325309.6875 acc: 0.7612825632095337  val: loss: 2555378.0 acc: 0.607595682144165\n",
      "step: 18790 , time : 0.0\n",
      "train: loss: 233199.578125 acc: 0.8051300048828125  val: loss: 841830.875 acc: 0.77984619140625\n",
      "step: 18795 , time : 0.0\n",
      "train: loss: 198956.796875 acc: 0.8731417655944824  val: loss: 2202555.0 acc: 0.6282188892364502\n",
      "step: 18800 , time : 0.0\n",
      "train: loss: 113026.125 acc: 0.8928172588348389  val: loss: 1261229.5 acc: 0.7014798521995544\n",
      "step: 18805 , time : 0.0\n",
      "train: loss: 220880.578125 acc: 0.8662971258163452  val: loss: 1617243.625 acc: 0.6521518230438232\n",
      "step: 18810 , time : 0.0\n",
      "train: loss: 368903.78125 acc: 0.7973277568817139  val: loss: 1932235.75 acc: 0.5725592970848083\n",
      "step: 18815 , time : 0.0\n",
      "train: loss: 267986.0 acc: 0.8468261361122131  val: loss: 1239618.875 acc: 0.6950299739837646\n",
      "step: 18820 , time : 0.0\n",
      "train: loss: 69458.1171875 acc: 0.9368738532066345  val: loss: 3268207.5 acc: 0.6067277193069458\n",
      "step: 18825 , time : 0.0\n",
      "train: loss: 127250.421875 acc: 0.8660678267478943  val: loss: 2168793.0 acc: 0.6294411420822144\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 18830 , time : 0.0\n",
      "train: loss: 235872.5 acc: 0.8620038032531738  val: loss: 2125086.25 acc: 0.6637303233146667\n",
      "step: 18835 , time : 0.0010006427764892578\n",
      "train: loss: 208793.109375 acc: 0.8267043232917786  val: loss: 1565663.0 acc: 0.6139086484909058\n",
      "step: 18840 , time : 0.0010004043579101562\n",
      "train: loss: 878602.0 acc: 0.7414556741714478  val: loss: 2010462.375 acc: 0.6869300603866577\n",
      "step: 18845 , time : 0.0\n",
      "train: loss: 259379.828125 acc: 0.8570366501808167  val: loss: 3156103.0 acc: 0.6434464454650879\n",
      "step: 18850 , time : 0.0\n",
      "train: loss: 207701.09375 acc: 0.7938826084136963  val: loss: 135568.109375 acc: 0.8791998028755188\n",
      "step: 18855 , time : 0.0\n",
      "train: loss: 347067.0625 acc: 0.7770692110061646  val: loss: 1849777.125 acc: 0.6184332370758057\n",
      "step: 18860 , time : 0.0010008811950683594\n",
      "train: loss: 2125267.0 acc: 0.6733862161636353  val: loss: 1458033.75 acc: 0.7580409646034241\n",
      "step: 18865 , time : 0.0\n",
      "train: loss: 1169019.25 acc: 0.7015480995178223  val: loss: 679905.0 acc: 0.6676304340362549\n",
      "step: 18870 , time : 0.0\n",
      "train: loss: 1029890.0 acc: 0.872523307800293  val: loss: 936383.625 acc: 0.4912383556365967\n",
      "step: 18875 , time : 0.0\n",
      "train: loss: 895672.0 acc: 0.9054945111274719  val: loss: 872054.0 acc: 0.7210806608200073\n",
      "step: 18880 , time : 0.0010008811950683594\n",
      "train: loss: 214771.328125 acc: 0.9659439921379089  val: loss: 1088945.75 acc: 0.8878796100616455\n",
      "step: 18885 , time : 0.0010008811950683594\n",
      "train: loss: 109538.078125 acc: 0.9804635047912598  val: loss: 725154.25 acc: 0.7126470804214478\n",
      "step: 18890 , time : 0.0\n",
      "train: loss: 233358.609375 acc: 0.9736155271530151  val: loss: 324209.84375 acc: 0.9393628239631653\n",
      "step: 18895 , time : 0.0\n",
      "train: loss: 179334.03125 acc: 0.9859704971313477  val: loss: 157997.359375 acc: 0.9455912709236145\n",
      "step: 18900 , time : 0.0\n",
      "train: loss: 141362.21875 acc: 0.9895715117454529  val: loss: 245999.484375 acc: 0.974346935749054\n",
      "step: 18905 , time : 0.0\n",
      "train: loss: 141047.078125 acc: 0.985686182975769  val: loss: 468720.46875 acc: 0.7082686424255371\n",
      "step: 18910 , time : 0.0\n",
      "train: loss: 118540.5078125 acc: 0.9868544340133667  val: loss: 1164585.875 acc: 0.5890545845031738\n",
      "step: 18915 , time : 0.0\n",
      "train: loss: 55244.9921875 acc: 0.9748083353042603  val: loss: 1692180.875 acc: -0.02365553379058838\n",
      "step: 18920 , time : 0.0\n",
      "train: loss: 26912.640625 acc: 0.9801346659660339  val: loss: 1094664.5 acc: 0.6712000370025635\n",
      "step: 18925 , time : 0.0\n",
      "train: loss: 15453.8828125 acc: 0.9271219968795776  val: loss: 658814.75 acc: 0.9150549173355103\n",
      "step: 18930 , time : 0.0\n",
      "train: loss: 13409.6748046875 acc: 0.9490484595298767  val: loss: 526780.625 acc: 0.9454436898231506\n",
      "step: 18935 , time : 0.0\n",
      "train: loss: 27693.650390625 acc: 0.9839498400688171  val: loss: 1693434.375 acc: 0.522544264793396\n",
      "step: 18940 , time : 0.0\n",
      "train: loss: 28544.916015625 acc: 0.9805068373680115  val: loss: 622607.3125 acc: 0.9042479991912842\n",
      "step: 18945 , time : 0.0\n",
      "train: loss: 42717.65625 acc: 0.9730409383773804  val: loss: 572096.0 acc: 0.8167503476142883\n",
      "step: 18950 , time : 0.0010006427764892578\n",
      "train: loss: 15551.6171875 acc: 0.971575915813446  val: loss: 269435.53125 acc: 0.9532574415206909\n",
      "step: 18955 , time : 0.0\n",
      "train: loss: 20470.53125 acc: 0.9092182517051697  val: loss: 676634.5 acc: 0.815405011177063\n",
      "step: 18960 , time : 0.0\n",
      "train: loss: 11734.029296875 acc: 0.9608757495880127  val: loss: 640531.5 acc: 0.817439079284668\n",
      "step: 18965 , time : 0.0\n",
      "train: loss: 87252.890625 acc: 0.9739722609519958  val: loss: 467892.625 acc: 0.9117832183837891\n",
      "step: 18970 , time : 0.0010006427764892578\n",
      "train: loss: 39080.16796875 acc: 0.9696192145347595  val: loss: 468326.59375 acc: 0.8031147718429565\n",
      "step: 18975 , time : 0.001001119613647461\n",
      "train: loss: 36082.390625 acc: 0.982118546962738  val: loss: 312429.875 acc: 0.9100300669670105\n",
      "step: 18980 , time : 0.0\n",
      "train: loss: 27995.19140625 acc: 0.984840452671051  val: loss: 903972.0625 acc: 0.9145556688308716\n",
      "step: 18985 , time : 0.0010008811950683594\n",
      "train: loss: 39872.40234375 acc: 0.975188136100769  val: loss: 4163230.75 acc: 0.5819946527481079\n",
      "step: 18990 , time : 0.0\n",
      "train: loss: 14124.873046875 acc: 0.9789172410964966  val: loss: 465842.78125 acc: 0.9462710022926331\n",
      "step: 18995 , time : 0.0\n",
      "train: loss: 113762.3359375 acc: 0.9514070153236389  val: loss: 1262663.5 acc: 0.6285473704338074\n",
      "step: 19000 , time : 0.0\n",
      "train: loss: 58797.1015625 acc: 0.9836871027946472  val: loss: 412886.40625 acc: 0.9227676391601562\n",
      "step: 19005 , time : 0.0\n",
      "train: loss: 65593.09375 acc: 0.9863131046295166  val: loss: 2539653.5 acc: 0.595740556716919\n",
      "step: 19010 , time : 0.0\n",
      "train: loss: 22421.099609375 acc: 0.9917302131652832  val: loss: 678705.0625 acc: 0.9586414098739624\n",
      "step: 19015 , time : 0.0\n",
      "train: loss: 43074.69921875 acc: 0.9732169508934021  val: loss: 1642674.375 acc: 0.7402437925338745\n",
      "step: 19020 , time : 0.0\n",
      "train: loss: 173887.953125 acc: 0.9616256952285767  val: loss: 395377.75 acc: 0.8047081232070923\n",
      "step: 19025 , time : 0.0\n",
      "train: loss: 155229.890625 acc: 0.9350717067718506  val: loss: 1019321.25 acc: 0.8678469657897949\n",
      "step: 19030 , time : 0.0\n",
      "train: loss: 74214.71875 acc: 0.9489380121231079  val: loss: 730258.8125 acc: 0.8454244136810303\n",
      "step: 19035 , time : 0.0\n",
      "train: loss: 145733.34375 acc: 0.9680891036987305  val: loss: 268827.34375 acc: 0.9567774534225464\n",
      "step: 19040 , time : 0.0\n",
      "train: loss: 115291.796875 acc: 0.9891253113746643  val: loss: 289005.90625 acc: 0.9587005376815796\n",
      "step: 19045 , time : 0.0\n",
      "train: loss: 116237.8828125 acc: 0.9877296686172485  val: loss: 2747047.0 acc: 0.4215250611305237\n",
      "step: 19050 , time : 0.0\n",
      "train: loss: 98864.2421875 acc: 0.9895831942558289  val: loss: 244749.625 acc: 0.9498446583747864\n",
      "step: 19055 , time : 0.0\n",
      "train: loss: 688671.6875 acc: 0.8341698050498962  val: loss: 1200738.0 acc: 0.8029084801673889\n",
      "step: 19060 , time : 0.0\n",
      "train: loss: 289239.0625 acc: 0.9444922804832458  val: loss: 1427490.125 acc: 0.8671952486038208\n",
      "step: 19065 , time : 0.0\n",
      "train: loss: 553389.625 acc: 0.963424563407898  val: loss: 671362.0 acc: 0.8951784372329712\n",
      "step: 19070 , time : 0.0010004043579101562\n",
      "train: loss: 673867.4375 acc: 0.9585691690444946  val: loss: 1789920.875 acc: 0.48495250940322876\n",
      "step: 19075 , time : 0.0\n",
      "train: loss: 366542.96875 acc: 0.945533037185669  val: loss: 1670926.875 acc: 0.6448304653167725\n",
      "step: 19080 , time : 0.0\n",
      "train: loss: 311199.34375 acc: 0.9871971011161804  val: loss: 789141.4375 acc: 0.8991937041282654\n",
      "step: 19085 , time : 0.0\n",
      "train: loss: 3073444.5 acc: 0.8792980313301086  val: loss: 1250267.125 acc: 0.8361278772354126\n",
      "step: 19090 , time : 0.0\n",
      "train: loss: 2049582.625 acc: 0.9208781719207764  val: loss: 3891890.0 acc: 0.6358912587165833\n",
      "step: 19095 , time : 0.0\n",
      "train: loss: 715962.875 acc: 0.9657760262489319  val: loss: 669474.5625 acc: 0.8683953285217285\n",
      "step: 19100 , time : 0.0\n",
      "train: loss: 306534.34375 acc: 0.9289135336875916  val: loss: 1233790.625 acc: 0.34882646799087524\n",
      "step: 19105 , time : 0.0\n",
      "train: loss: 488862.21875 acc: 0.9641066193580627  val: loss: 1609502.125 acc: 0.7779529690742493\n",
      "step: 19110 , time : 0.0\n",
      "train: loss: 248050.921875 acc: 0.9684946537017822  val: loss: 1287835.75 acc: 0.7923445701599121\n",
      "step: 19115 , time : 0.0\n",
      "train: loss: 3760633.75 acc: -0.90155029296875  val: loss: 1233777.375 acc: 0.8062244653701782\n",
      "step: 19120 , time : 0.0\n",
      "train: loss: 1123066.75 acc: 0.6911720037460327  val: loss: 1166018.75 acc: 0.4065161347389221\n",
      "step: 19125 , time : 0.0\n",
      "train: loss: 769660.125 acc: 0.6812785863876343  val: loss: 1544754.25 acc: 0.7880396246910095\n",
      "step: 19130 , time : 0.0\n",
      "train: loss: 486259.375 acc: 0.8340587019920349  val: loss: 800143.5 acc: 0.8336300849914551\n",
      "step: 19135 , time : 0.0\n",
      "train: loss: 941642.375 acc: 0.8211742639541626  val: loss: 735754.125 acc: 0.6598159074783325\n",
      "step: 19140 , time : 0.0\n",
      "train: loss: 1327694.875 acc: 0.616317629814148  val: loss: 709058.625 acc: 0.6595857739448547\n",
      "step: 19145 , time : 0.0\n",
      "train: loss: 745007.25 acc: 0.6710035800933838  val: loss: 2580503.25 acc: 0.6305253505706787\n",
      "step: 19150 , time : 0.0\n",
      "train: loss: 316635.03125 acc: 0.7544566988945007  val: loss: 510945.4375 acc: 0.7304908037185669\n",
      "step: 19155 , time : 0.0\n",
      "train: loss: 317193.90625 acc: 0.8071161508560181  val: loss: 586776.625 acc: 0.7679290175437927\n",
      "step: 19160 , time : 0.0\n",
      "train: loss: 105458.046875 acc: 0.8965546488761902  val: loss: 1379317.5 acc: 0.7039207220077515\n",
      "step: 19165 , time : 0.0\n",
      "train: loss: 68658.6328125 acc: 0.948724091053009  val: loss: 1076922.25 acc: 0.7324057817459106\n",
      "step: 19170 , time : 0.0010006427764892578\n",
      "train: loss: 99719.3203125 acc: 0.9088642597198486  val: loss: 1961920.0 acc: 0.5809837579727173\n",
      "step: 19175 , time : 0.0010006427764892578\n",
      "train: loss: 249535.21875 acc: 0.8688147068023682  val: loss: 1179856.375 acc: 0.6346253752708435\n",
      "step: 19180 , time : 0.0\n",
      "train: loss: 66505.2890625 acc: 0.948176920413971  val: loss: 1244167.375 acc: 0.6692627668380737\n",
      "step: 19185 , time : 0.0\n",
      "train: loss: 84135.1640625 acc: 0.9352310299873352  val: loss: 2133016.75 acc: 0.6563061475753784\n",
      "step: 19190 , time : 0.0\n",
      "train: loss: 330962.78125 acc: 0.7819581031799316  val: loss: 114786.4296875 acc: 0.8863197565078735\n",
      "step: 19195 , time : 0.0\n",
      "train: loss: 248901.859375 acc: 0.7685922980308533  val: loss: 2163435.0 acc: 0.6180129647254944\n",
      "step: 19200 , time : 0.0\n",
      "train: loss: 155986.96875 acc: 0.8763639330863953  val: loss: 997592.5 acc: 0.6709893345832825\n",
      "step: 19205 , time : 0.0\n",
      "train: loss: 208156.5625 acc: 0.8655976057052612  val: loss: 2886512.0 acc: 0.6153067946434021\n",
      "step: 19210 , time : 0.0\n",
      "train: loss: 613988.25 acc: 0.797174334526062  val: loss: 1563459.75 acc: 0.6801362037658691\n",
      "step: 19215 , time : 0.0\n",
      "train: loss: 841366.8125 acc: 0.6558851003646851  val: loss: 681926.375 acc: 0.7053878903388977\n",
      "step: 19220 , time : 0.0\n",
      "train: loss: 67381.625 acc: 0.9350206851959229  val: loss: 1888036.0 acc: 0.5929995775222778\n",
      "step: 19225 , time : 0.0\n",
      "train: loss: 137560.828125 acc: 0.8756927251815796  val: loss: 1314045.5 acc: 0.6847277879714966\n",
      "step: 19230 , time : 0.0\n",
      "train: loss: 1247718.625 acc: 0.7641564607620239  val: loss: 744772.9375 acc: 0.8078587055206299\n",
      "step: 19235 , time : 0.0\n",
      "train: loss: 1182908.125 acc: 0.8583391904830933  val: loss: 769484.5 acc: 0.8216621279716492\n",
      "step: 19240 , time : 0.0\n",
      "train: loss: 470888.125 acc: 0.9605087637901306  val: loss: 1392764.5 acc: 0.8886126279830933\n",
      "step: 19245 , time : 0.0\n",
      "train: loss: 344757.34375 acc: 0.964028537273407  val: loss: 1149777.0 acc: 0.7642606496810913\n",
      "step: 19250 , time : 0.0\n",
      "train: loss: 128292.71875 acc: 0.980185866355896  val: loss: 1181743.875 acc: 0.8416616320610046\n",
      "step: 19255 , time : 0.0\n",
      "train: loss: 137726.0 acc: 0.9822322130203247  val: loss: 898050.625 acc: 0.590323805809021\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 19260 , time : 0.0\n",
      "train: loss: 156654.203125 acc: 0.9855015277862549  val: loss: 1449751.375 acc: 0.8195555210113525\n",
      "step: 19265 , time : 0.0\n",
      "train: loss: 111355.875 acc: 0.9921393394470215  val: loss: 1237058.125 acc: 0.791580855846405\n",
      "step: 19270 , time : 0.0\n",
      "train: loss: 157428.875 acc: 0.9872204661369324  val: loss: 425239.65625 acc: 0.8859981298446655\n",
      "step: 19275 , time : 0.002001523971557617\n",
      "train: loss: 82649.171875 acc: 0.9881170988082886  val: loss: 853315.3125 acc: 0.6995214223861694\n",
      "step: 19280 , time : 0.0\n",
      "train: loss: 100961.5859375 acc: 0.9797317385673523  val: loss: 341392.5625 acc: 0.8782367706298828\n",
      "step: 19285 , time : 0.0\n",
      "train: loss: 32437.625 acc: 0.9755228161811829  val: loss: 954871.6875 acc: 0.8767445087432861\n",
      "step: 19290 , time : 0.0\n",
      "train: loss: 23138.220703125 acc: 0.946038544178009  val: loss: 486525.875 acc: 0.9318783283233643\n",
      "step: 19295 , time : 0.0010006427764892578\n",
      "train: loss: 40001.94921875 acc: 0.9724236726760864  val: loss: 1549795.125 acc: 0.6681684255599976\n",
      "step: 19300 , time : 0.0010008811950683594\n",
      "train: loss: 10912.638671875 acc: 0.982458770275116  val: loss: 935757.25 acc: 0.7853581309318542\n",
      "step: 19305 , time : 0.0\n",
      "train: loss: 20854.337890625 acc: 0.9710346460342407  val: loss: 955361.125 acc: 0.9071049690246582\n",
      "step: 19310 , time : 0.0\n",
      "train: loss: 9387.361328125 acc: 0.9795814752578735  val: loss: 3683537.0 acc: 0.6812209486961365\n",
      "step: 19315 , time : 0.0\n",
      "train: loss: 24991.359375 acc: 0.9378226399421692  val: loss: 1186193.0 acc: 0.8176126480102539\n",
      "step: 19320 , time : 0.0\n",
      "train: loss: 24180.20703125 acc: 0.9020830392837524  val: loss: 1189233.875 acc: 0.816533625125885\n",
      "step: 19325 , time : 0.0\n",
      "train: loss: 8208.2392578125 acc: 0.9831029772758484  val: loss: 388156.625 acc: 0.9433249831199646\n",
      "step: 19330 , time : 0.0\n",
      "train: loss: 41303.9296875 acc: 0.9747916460037231  val: loss: 98031.28125 acc: 0.9617725610733032\n",
      "step: 19335 , time : 0.0\n",
      "train: loss: 32159.927734375 acc: 0.9738141894340515  val: loss: 759341.4375 acc: 0.7302217483520508\n",
      "step: 19340 , time : 0.0\n",
      "train: loss: 24455.50390625 acc: 0.9860434532165527  val: loss: 1095165.0 acc: 0.8813629150390625\n",
      "step: 19345 , time : 0.0\n",
      "train: loss: 18227.0 acc: 0.9876426458358765  val: loss: 2308018.75 acc: 0.6873648166656494\n",
      "step: 19350 , time : 0.0\n",
      "train: loss: 28418.134765625 acc: 0.9692390561103821  val: loss: 1193398.625 acc: 0.8703005313873291\n",
      "step: 19355 , time : 0.0\n",
      "train: loss: 11852.2197265625 acc: 0.98512202501297  val: loss: 1128181.0 acc: 0.8886104822158813\n",
      "step: 19360 , time : 0.0\n",
      "train: loss: 14985.9375 acc: 0.9878239035606384  val: loss: 2376631.5 acc: 0.4912996292114258\n",
      "step: 19365 , time : 0.0\n",
      "train: loss: 45561.49609375 acc: 0.9861811399459839  val: loss: 4822236.0 acc: 0.10085105895996094\n",
      "step: 19370 , time : 0.0\n",
      "train: loss: 53487.97265625 acc: 0.9864697456359863  val: loss: 1206793.375 acc: 0.8119453191757202\n",
      "step: 19375 , time : 0.0\n",
      "train: loss: 36306.70703125 acc: 0.9901174306869507  val: loss: 1005860.9375 acc: 0.8091163039207458\n",
      "step: 19380 , time : 0.0\n",
      "train: loss: 53548.35546875 acc: 0.9823417663574219  val: loss: 2154864.0 acc: 0.8054219484329224\n",
      "step: 19385 , time : 0.0\n",
      "train: loss: 39422.25390625 acc: 0.9835620522499084  val: loss: 1152362.875 acc: 0.6905170679092407\n",
      "step: 19390 , time : 0.0\n",
      "train: loss: 142080.453125 acc: 0.9570090174674988  val: loss: 2155305.25 acc: 0.7556993961334229\n",
      "step: 19395 , time : 0.0010008811950683594\n",
      "train: loss: 63007.71484375 acc: 0.9493674039840698  val: loss: 1699329.25 acc: 0.7525664567947388\n",
      "step: 19400 , time : 0.0\n",
      "train: loss: 512616.5625 acc: 0.922225296497345  val: loss: 2727567.25 acc: -0.11194169521331787\n",
      "step: 19405 , time : 0.0010008811950683594\n",
      "train: loss: 191162.265625 acc: 0.9720184206962585  val: loss: 1750345.75 acc: 0.3346298336982727\n",
      "step: 19410 , time : 0.0\n",
      "train: loss: 59566.73828125 acc: 0.9934271574020386  val: loss: 2772593.0 acc: -0.9651371240615845\n",
      "step: 19415 , time : 0.0\n",
      "train: loss: 97202.9765625 acc: 0.9879204630851746  val: loss: 2824716.75 acc: 0.18662256002426147\n",
      "step: 19420 , time : 0.001001119613647461\n",
      "train: loss: 143233.921875 acc: 0.9743517637252808  val: loss: 2265097.25 acc: 0.612140417098999\n",
      "step: 19425 , time : 0.0\n",
      "train: loss: 318492.1875 acc: 0.9562373161315918  val: loss: 861948.5 acc: 0.48179614543914795\n",
      "step: 19430 , time : 0.015624523162841797\n",
      "train: loss: 749066.625 acc: 0.9623070955276489  val: loss: 1543276.125 acc: 0.791674017906189\n",
      "step: 19435 , time : 0.0\n",
      "train: loss: 631567.5625 acc: 0.9586051106452942  val: loss: 1062327.625 acc: 0.8352956175804138\n",
      "step: 19440 , time : 0.0\n",
      "train: loss: 436731.75 acc: 0.9273910522460938  val: loss: 2017711.375 acc: 0.6625565886497498\n",
      "step: 19445 , time : 0.0\n",
      "train: loss: 1417023.375 acc: 0.9307361841201782  val: loss: 2059421.125 acc: 0.6608527898788452\n",
      "step: 19450 , time : 0.0\n",
      "train: loss: 1218126.875 acc: 0.9549822807312012  val: loss: 1031484.8125 acc: 0.20147734880447388\n",
      "step: 19455 , time : 0.0\n",
      "train: loss: 1614106.125 acc: 0.9358013272285461  val: loss: 1895572.5 acc: 0.4793206453323364\n",
      "step: 19460 , time : 0.0\n",
      "train: loss: 1305807.0 acc: 0.8689652681350708  val: loss: 653126.0625 acc: 0.8375266790390015\n",
      "step: 19465 , time : 0.0\n",
      "train: loss: 179910.859375 acc: 0.984962522983551  val: loss: 2142306.75 acc: 0.04146921634674072\n",
      "step: 19470 , time : 0.0\n",
      "train: loss: 627919.0 acc: 0.9544901251792908  val: loss: 1187498.0 acc: 0.8975456953048706\n",
      "step: 19475 , time : 0.0\n",
      "train: loss: 287313.46875 acc: 0.958814263343811  val: loss: 314706.4375 acc: 0.9652696847915649\n",
      "step: 19480 , time : 0.0\n",
      "train: loss: 1180284.375 acc: 0.7712743878364563  val: loss: 569075.125 acc: 0.9036070108413696\n",
      "step: 19485 , time : 0.0\n",
      "train: loss: 417606.5 acc: 0.8231236934661865  val: loss: 894552.3125 acc: 0.7794210910797119\n",
      "step: 19490 , time : 0.0010006427764892578\n",
      "train: loss: 1156175.375 acc: 0.8012354969978333  val: loss: 1780043.375 acc: 0.8231727480888367\n",
      "step: 19495 , time : 0.001001119613647461\n",
      "train: loss: 320559.5625 acc: 0.7714768648147583  val: loss: 879108.8125 acc: 0.8848248720169067\n",
      "step: 19500 , time : 0.0010004043579101562\n",
      "train: loss: 855060.0625 acc: 0.837637722492218  val: loss: 1327311.375 acc: 0.5675649642944336\n",
      "step: 19505 , time : 0.0\n",
      "train: loss: 1527523.0 acc: 0.4703291058540344  val: loss: 1356461.625 acc: 0.7552405595779419\n",
      "step: 19510 , time : 0.0010006427764892578\n",
      "train: loss: 350825.25 acc: 0.7771170139312744  val: loss: 1205408.125 acc: 0.4688338041305542\n",
      "step: 19515 , time : 0.0\n",
      "train: loss: 248167.3125 acc: 0.7991806268692017  val: loss: 2266339.5 acc: 0.5730922222137451\n",
      "step: 19520 , time : 0.0\n",
      "train: loss: 148308.34375 acc: 0.8747339248657227  val: loss: 1566840.125 acc: 0.6602563261985779\n",
      "step: 19525 , time : 0.0\n",
      "train: loss: 109932.609375 acc: 0.9094640612602234  val: loss: 4353986.5 acc: 0.6373494863510132\n",
      "step: 19530 , time : 0.0\n",
      "train: loss: 48666.75390625 acc: 0.959677517414093  val: loss: 559106.4375 acc: 0.7376605272293091\n",
      "step: 19535 , time : 0.0\n",
      "train: loss: 90215.2421875 acc: 0.9255794286727905  val: loss: 1276708.125 acc: 0.6833773851394653\n",
      "step: 19540 , time : 0.0\n",
      "train: loss: 325184.59375 acc: 0.8255817890167236  val: loss: 1809939.625 acc: 0.597284197807312\n",
      "step: 19545 , time : 0.0\n",
      "train: loss: 140393.8125 acc: 0.8592355251312256  val: loss: 1030642.625 acc: 0.7047092914581299\n",
      "step: 19550 , time : 0.0\n",
      "train: loss: 551900.5625 acc: 0.7596753239631653  val: loss: 407880.15625 acc: 0.7738980054855347\n",
      "step: 19555 , time : 0.0\n",
      "train: loss: 26916.455078125 acc: 0.9623594284057617  val: loss: 773912.5 acc: 0.7159924507141113\n",
      "step: 19560 , time : 0.0\n",
      "train: loss: 44793.41796875 acc: 0.9564594030380249  val: loss: 1036044.4375 acc: 0.6797573566436768\n",
      "step: 19565 , time : 0.0\n",
      "train: loss: 411129.5 acc: 0.7883135080337524  val: loss: 1222166.125 acc: 0.6889793872833252\n",
      "step: 19570 , time : 0.0\n",
      "train: loss: 86928.4375 acc: 0.9007665514945984  val: loss: 429568.75 acc: 0.7253209352493286\n",
      "step: 19575 , time : 0.0\n",
      "train: loss: 163800.1875 acc: 0.8577624559402466  val: loss: 2230480.5 acc: 0.603830099105835\n",
      "step: 19580 , time : 0.0\n",
      "train: loss: 152571.734375 acc: 0.8912870287895203  val: loss: 505281.5 acc: 0.7589724063873291\n",
      "step: 19585 , time : 0.0\n",
      "train: loss: 653490.1875 acc: 0.7337847948074341  val: loss: 658989.375 acc: 0.7404210567474365\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 19590 , time : 0.0\n",
      "train: loss: 724162.9375 acc: 0.7335633039474487  val: loss: 1833921.875 acc: 0.6255910396575928\n",
      "step: 19595 , time : 0.0010004043579101562\n",
      "train: loss: 670229.5625 acc: 0.6985071897506714  val: loss: 449333.71875 acc: 0.8755115270614624\n",
      "step: 19600 , time : 0.0010008811950683594\n",
      "train: loss: 1446004.875 acc: 0.857117772102356  val: loss: 633941.125 acc: 0.9161145091056824\n",
      "step: 19605 , time : 0.0010008811950683594\n",
      "train: loss: 364765.53125 acc: 0.9708171486854553  val: loss: 650904.75 acc: 0.8614670038223267\n",
      "step: 19610 , time : 0.0\n",
      "train: loss: 755533.625 acc: 0.8887457251548767  val: loss: 445845.21875 acc: 0.9419744610786438\n",
      "step: 19615 , time : 0.0\n",
      "train: loss: 267730.84375 acc: 0.9662087559700012  val: loss: 610671.0 acc: 0.9128713607788086\n",
      "step: 19620 , time : 0.0010008811950683594\n",
      "train: loss: 171669.984375 acc: 0.9680519104003906  val: loss: 874351.6875 acc: 0.9237232804298401\n",
      "step: 19625 , time : 0.0010008811950683594\n",
      "train: loss: 91647.1640625 acc: 0.9904337525367737  val: loss: 1848592.125 acc: 0.7568351030349731\n",
      "step: 19630 , time : 0.0\n",
      "train: loss: 81309.3984375 acc: 0.9946262240409851  val: loss: 1446901.125 acc: 0.8098207116127014\n",
      "step: 19635 , time : 0.0\n",
      "train: loss: 207347.546875 acc: 0.9829844832420349  val: loss: 1492416.25 acc: -0.004066944122314453\n",
      "step: 19640 , time : 0.0\n",
      "train: loss: 80071.078125 acc: 0.9854281544685364  val: loss: 512665.4375 acc: 0.9416718482971191\n",
      "step: 19645 , time : 0.0\n",
      "train: loss: 108364.140625 acc: 0.9785239100456238  val: loss: 531245.5 acc: 0.8604117631912231\n",
      "step: 19650 , time : 0.0\n",
      "train: loss: 26586.62109375 acc: 0.9829456806182861  val: loss: 1137044.625 acc: 0.6720060110092163\n",
      "step: 19655 , time : 0.0\n",
      "train: loss: 17653.9296875 acc: 0.9737176299095154  val: loss: 1745899.375 acc: 0.6896562576293945\n",
      "step: 19660 , time : 0.0\n",
      "train: loss: 33714.85546875 acc: 0.9775009155273438  val: loss: 2394723.25 acc: 0.2162070870399475\n",
      "step: 19665 , time : 0.0\n",
      "train: loss: 10513.552734375 acc: 0.9842087626457214  val: loss: 144343.046875 acc: 0.9770992994308472\n",
      "step: 19670 , time : 0.0\n",
      "train: loss: 30086.064453125 acc: 0.9338885545730591  val: loss: 2115316.0 acc: 0.8421601057052612\n",
      "step: 19675 , time : 0.0\n",
      "train: loss: 9717.728515625 acc: 0.9854017496109009  val: loss: 811011.8125 acc: 0.606660008430481\n",
      "step: 19680 , time : 0.0\n",
      "train: loss: 19708.279296875 acc: 0.9871965050697327  val: loss: 1565425.25 acc: 0.7854605913162231\n",
      "step: 19685 , time : 0.0\n",
      "train: loss: 19645.283203125 acc: 0.9065230488777161  val: loss: 431211.90625 acc: 0.725396990776062\n",
      "step: 19690 , time : 0.0\n",
      "train: loss: 13239.1484375 acc: 0.9742686152458191  val: loss: 1431644.125 acc: 0.8848517537117004\n",
      "step: 19695 , time : 0.0\n",
      "train: loss: 34350.45703125 acc: 0.9706111550331116  val: loss: 1623686.125 acc: 0.6187059879302979\n",
      "step: 19700 , time : 0.0\n",
      "train: loss: 62929.84375 acc: 0.9769148230552673  val: loss: 1820186.125 acc: 0.5544944405555725\n",
      "step: 19705 , time : 0.0\n",
      "train: loss: 24884.0390625 acc: 0.986811637878418  val: loss: 1382906.5 acc: 0.742892861366272\n",
      "step: 19710 , time : 0.0010008811950683594\n",
      "train: loss: 11517.6376953125 acc: 0.9904318451881409  val: loss: 1128783.75 acc: 0.8557208180427551\n",
      "step: 19715 , time : 0.0\n",
      "train: loss: 22817.8125 acc: 0.9835766553878784  val: loss: 9182861.0 acc: 0.11014962196350098\n",
      "step: 19720 , time : 0.0\n",
      "train: loss: 24419.396484375 acc: 0.9818646907806396  val: loss: 1527085.625 acc: 0.10958206653594971\n",
      "step: 19725 , time : 0.0\n",
      "train: loss: 10951.6455078125 acc: 0.9887968301773071  val: loss: 1820730.0 acc: 0.8088260293006897\n",
      "step: 19730 , time : 0.0\n",
      "train: loss: 55707.76953125 acc: 0.9882308840751648  val: loss: 2483372.5 acc: 0.41641366481781006\n",
      "step: 19735 , time : 0.0\n",
      "train: loss: 52485.30859375 acc: 0.985980749130249  val: loss: 2422547.5 acc: 0.6704029440879822\n",
      "step: 19740 , time : 0.0\n",
      "train: loss: 42779.9140625 acc: 0.9875956773757935  val: loss: 1610594.5 acc: 0.8577267527580261\n",
      "step: 19745 , time : 0.0\n",
      "train: loss: 29614.263671875 acc: 0.9889801144599915  val: loss: 2884504.0 acc: 0.06443846225738525\n",
      "step: 19750 , time : 0.0\n",
      "train: loss: 53478.734375 acc: 0.9780946969985962  val: loss: 538214.0 acc: 0.8067253828048706\n",
      "step: 19755 , time : 0.0\n",
      "train: loss: 114341.9375 acc: 0.9791204333305359  val: loss: 2112591.25 acc: 0.5933658480644226\n",
      "step: 19760 , time : 0.0\n",
      "train: loss: 92580.921875 acc: 0.966140627861023  val: loss: 849397.4375 acc: 0.8801866769790649\n",
      "step: 19765 , time : 0.0\n",
      "train: loss: 575998.0 acc: 0.8571665287017822  val: loss: 448013.96875 acc: 0.8874000906944275\n",
      "step: 19770 , time : 0.0\n",
      "train: loss: 116859.9375 acc: 0.9805328845977783  val: loss: 627483.1875 acc: 0.8858972787857056\n",
      "step: 19775 , time : 0.0\n",
      "train: loss: 153845.109375 acc: 0.9819543361663818  val: loss: 1125022.5 acc: 0.6416928768157959\n",
      "step: 19780 , time : 0.0\n",
      "train: loss: 176409.671875 acc: 0.9757904410362244  val: loss: 1108254.25 acc: 0.7406713962554932\n",
      "step: 19785 , time : 0.0\n",
      "train: loss: 197335.625 acc: 0.954017162322998  val: loss: 1594346.125 acc: 0.6553894281387329\n",
      "step: 19790 , time : 0.0\n",
      "train: loss: 463100.6875 acc: 0.9474897384643555  val: loss: 1006704.625 acc: 0.7344926595687866\n",
      "step: 19795 , time : 0.0\n",
      "train: loss: 548039.4375 acc: 0.969979465007782  val: loss: 479626.3125 acc: 0.8818167448043823\n",
      "step: 19800 , time : 0.0\n",
      "train: loss: 525115.625 acc: 0.9544016122817993  val: loss: 2113409.75 acc: 0.3187216520309448\n",
      "step: 19805 , time : 0.0\n",
      "train: loss: 332189.25 acc: 0.9501599669456482  val: loss: 731652.4375 acc: 0.85050368309021\n",
      "step: 19810 , time : 0.0\n",
      "train: loss: 1191172.125 acc: 0.9643778204917908  val: loss: 559565.5625 acc: 0.9510369896888733\n",
      "step: 19815 , time : 0.0\n",
      "train: loss: 1712164.75 acc: 0.9474374651908875  val: loss: 388938.21875 acc: 0.9354336857795715\n",
      "step: 19820 , time : 0.0\n",
      "train: loss: 613452.1875 acc: 0.9772359132766724  val: loss: 1373950.625 acc: 0.6912345290184021\n",
      "step: 19825 , time : 0.0\n",
      "train: loss: 842196.3125 acc: 0.9423078298568726  val: loss: 1447994.25 acc: 0.1204291582107544\n",
      "step: 19830 , time : 0.0\n",
      "train: loss: 1458478.125 acc: 0.779690682888031  val: loss: 914874.6875 acc: 0.7169454097747803\n",
      "step: 19835 , time : 0.0010006427764892578\n",
      "train: loss: 382265.875 acc: 0.9510477781295776  val: loss: 327200.1875 acc: 0.9134829640388489\n",
      "step: 19840 , time : 0.0\n",
      "train: loss: 716349.375 acc: 0.9088701009750366  val: loss: 404997.6875 acc: 0.9305455684661865\n",
      "step: 19845 , time : 0.0\n",
      "train: loss: 775860.3125 acc: 0.7452598214149475  val: loss: 782558.4375 acc: 0.8139042854309082\n",
      "step: 19850 , time : 0.0\n",
      "train: loss: 1330770.625 acc: 0.4787006974220276  val: loss: 765671.8125 acc: 0.7007063031196594\n",
      "step: 19855 , time : 0.0\n",
      "train: loss: 575965.625 acc: 0.6756281852722168  val: loss: 979925.9375 acc: 0.7549706101417542\n",
      "step: 19860 , time : 0.0\n",
      "train: loss: 266345.21875 acc: 0.7834371328353882  val: loss: 1003718.1875 acc: 0.8171790242195129\n",
      "step: 19865 , time : 0.0\n",
      "train: loss: 364113.78125 acc: 0.7488821744918823  val: loss: 789296.6875 acc: 0.7653915882110596\n",
      "step: 19870 , time : 0.0\n",
      "train: loss: 833109.75 acc: 0.5995112061500549  val: loss: 1033227.875 acc: 0.7330019474029541\n",
      "step: 19875 , time : 0.0010006427764892578\n",
      "train: loss: 468412.125 acc: 0.7467902302742004  val: loss: 2290930.5 acc: 0.5695935487747192\n",
      "step: 19880 , time : 0.0\n",
      "train: loss: 640642.5625 acc: 0.7254366278648376  val: loss: 3294558.25 acc: 0.6693545579910278\n",
      "step: 19885 , time : 0.0\n",
      "train: loss: 263983.25 acc: 0.7665085196495056  val: loss: 1289766.375 acc: 0.6965973973274231\n",
      "step: 19890 , time : 0.0\n",
      "train: loss: 132240.40625 acc: 0.8948028683662415  val: loss: 436905.40625 acc: 0.779318630695343\n",
      "step: 19895 , time : 0.0010008811950683594\n",
      "train: loss: 184436.046875 acc: 0.870556116104126  val: loss: 2225241.25 acc: 0.6389666199684143\n",
      "step: 19900 , time : 0.0\n",
      "train: loss: 88762.359375 acc: 0.9371076822280884  val: loss: 579243.3125 acc: 0.750145673751831\n",
      "step: 19905 , time : 0.0\n",
      "train: loss: 212907.328125 acc: 0.8582717180252075  val: loss: 4069231.0 acc: 0.6146619915962219\n",
      "step: 19910 , time : 0.0010006427764892578\n",
      "train: loss: 112768.3671875 acc: 0.9166103601455688  val: loss: 2882530.5 acc: 0.6010241508483887\n",
      "step: 19915 , time : 0.0\n",
      "train: loss: 64663.37109375 acc: 0.9372524619102478  val: loss: 1905088.25 acc: 0.6906687021255493\n",
      "step: 19920 , time : 0.0\n",
      "train: loss: 49078.77734375 acc: 0.9466550350189209  val: loss: 2307488.25 acc: 0.7343915104866028\n",
      "step: 19925 , time : 0.0\n",
      "train: loss: 25458.396484375 acc: 0.9596174955368042  val: loss: 5355308.0 acc: 0.602424144744873\n",
      "step: 19930 , time : 0.0\n",
      "train: loss: 67733.015625 acc: 0.9224845170974731  val: loss: 3026520.0 acc: 0.6189330816268921\n",
      "step: 19935 , time : 0.0\n",
      "train: loss: 512822.21875 acc: 0.7618375420570374  val: loss: 4744402.5 acc: 0.6175891757011414\n",
      "step: 19940 , time : 0.0\n",
      "train: loss: 133840.578125 acc: 0.8793213367462158  val: loss: 3291485.5 acc: 0.685242772102356\n",
      "step: 19945 , time : 0.0010001659393310547\n",
      "train: loss: 848783.875 acc: 0.6515508890151978  val: loss: 1726974.875 acc: 0.6381782293319702\n",
      "step: 19950 , time : 0.0010008811950683594\n",
      "train: loss: 176685.328125 acc: 0.8599503040313721  val: loss: 744384.1875 acc: 0.7683120369911194\n",
      "step: 19955 , time : 0.0\n",
      "train: loss: 349167.28125 acc: 0.7392183542251587  val: loss: 2650764.5 acc: 0.5864464044570923\n",
      "step: 19960 , time : 0.0\n",
      "train: loss: 1059475.875 acc: 0.7414135336875916  val: loss: 1157628.125 acc: 0.7947732210159302\n",
      "step: 19965 , time : 0.0\n",
      "train: loss: 941599.3125 acc: 0.8919320106506348  val: loss: 1003783.5 acc: 0.8136018514633179\n",
      "step: 19970 , time : 0.0010008811950683594\n",
      "train: loss: 477643.5 acc: 0.9619787931442261  val: loss: 3570271.5 acc: 0.7284859418869019\n",
      "step: 19975 , time : 0.0010004043579101562\n",
      "train: loss: 220916.015625 acc: 0.9777506589889526  val: loss: 1613071.75 acc: 0.711682915687561\n",
      "step: 19980 , time : 0.0\n",
      "train: loss: 1057869.625 acc: 0.710760235786438  val: loss: 1584977.875 acc: 0.7786356210708618\n",
      "step: 19985 , time : 0.0010006427764892578\n",
      "train: loss: 243062.984375 acc: 0.971636950969696  val: loss: 1112269.625 acc: 0.45421111583709717\n",
      "step: 19990 , time : 0.0\n",
      "train: loss: 136548.953125 acc: 0.9831206798553467  val: loss: 1529796.375 acc: 0.8549176454544067\n",
      "step: 19995 , time : 0.0\n",
      "train: loss: 138986.015625 acc: 0.9904859066009521  val: loss: 1069217.875 acc: 0.8793530464172363\n",
      "step: 20000 , time : 0.0\n",
      "train: loss: 107782.890625 acc: 0.9907245635986328  val: loss: 1693736.625 acc: 0.7515262365341187\n",
      "step: 20005 , time : 0.0\n",
      "train: loss: 126925.4609375 acc: 0.9836304187774658  val: loss: 1617612.875 acc: 0.7757101058959961\n",
      "step: 20010 , time : 0.0\n",
      "train: loss: 60736.05859375 acc: 0.9782251119613647  val: loss: 1200424.75 acc: 0.7425170540809631\n",
      "step: 20015 , time : 0.0\n",
      "train: loss: 43355.875 acc: 0.9714252352714539  val: loss: 1207210.625 acc: 0.8256688714027405\n",
      "step: 20020 , time : 0.0010008811950683594\n",
      "train: loss: 30068.607421875 acc: 0.9864099621772766  val: loss: 1455516.75 acc: 0.8246533274650574\n",
      "step: 20025 , time : 0.0010006427764892578\n",
      "train: loss: 18939.869140625 acc: 0.9861919283866882  val: loss: 3008415.5 acc: -0.7545773983001709\n",
      "step: 20030 , time : 0.0\n",
      "train: loss: 7777.99072265625 acc: 0.9861425161361694  val: loss: 2724592.75 acc: 0.5862511396408081\n",
      "step: 20035 , time : 0.001001119613647461\n",
      "train: loss: 176450.8125 acc: 0.7799785137176514  val: loss: 2048463.875 acc: 0.4043461084365845\n",
      "step: 20040 , time : 0.0010008811950683594\n",
      "train: loss: 13565.7373046875 acc: 0.9787512421607971  val: loss: 495207.84375 acc: 0.8361295461654663\n",
      "step: 20045 , time : 0.0010004043579101562\n",
      "train: loss: 12400.3134765625 acc: 0.9650518298149109  val: loss: 1643595.25 acc: 0.492628812789917\n",
      "step: 20050 , time : 0.0\n",
      "train: loss: 18220.36328125 acc: 0.9343972206115723  val: loss: 1440888.125 acc: 0.7444977760314941\n",
      "step: 20055 , time : 0.0010008811950683594\n",
      "train: loss: 22279.921875 acc: 0.9641340374946594  val: loss: 1028277.875 acc: -0.33880186080932617\n",
      "step: 20060 , time : 0.0\n",
      "train: loss: 51454.67578125 acc: 0.968359112739563  val: loss: 1547590.25 acc: 0.5902185440063477\n",
      "step: 20065 , time : 0.001001119613647461\n",
      "train: loss: 23748.537109375 acc: 0.9686422944068909  val: loss: 2083329.5 acc: 0.45078617334365845\n",
      "step: 20070 , time : 0.0010008811950683594\n",
      "train: loss: 15164.220703125 acc: 0.9899129867553711  val: loss: 1117653.5 acc: 0.7660079598426819\n",
      "step: 20075 , time : 0.0\n",
      "train: loss: 40317.08203125 acc: 0.9840012788772583  val: loss: 753538.4375 acc: 0.8709497451782227\n",
      "step: 20080 , time : 0.0\n",
      "train: loss: 25472.765625 acc: 0.9694678783416748  val: loss: 752769.625 acc: 0.8881924748420715\n",
      "step: 20085 , time : 0.0\n",
      "train: loss: 28471.09375 acc: 0.97076416015625  val: loss: 1694692.25 acc: 0.7778646945953369\n",
      "step: 20090 , time : 0.0\n",
      "train: loss: 17667.671875 acc: 0.97364342212677  val: loss: 1101255.25 acc: 0.8500258922576904\n",
      "step: 20095 , time : 0.0\n",
      "train: loss: 40982.01171875 acc: 0.9886991381645203  val: loss: 3262031.0 acc: 0.31439900398254395\n",
      "step: 20100 , time : 0.0\n",
      "train: loss: 48519.05859375 acc: 0.9883186221122742  val: loss: 1036829.0 acc: 0.8034054636955261\n",
      "step: 20105 , time : 0.0\n",
      "train: loss: 72152.5078125 acc: 0.9806962013244629  val: loss: 1712127.375 acc: 0.6122907400131226\n",
      "step: 20110 , time : 0.0\n",
      "train: loss: 37129.5546875 acc: 0.9920454621315002  val: loss: 2242925.5 acc: -0.08600616455078125\n",
      "step: 20115 , time : 0.0\n",
      "train: loss: 66097.296875 acc: 0.9860554337501526  val: loss: 1064629.625 acc: 0.6361857652664185\n",
      "step: 20120 , time : 0.0010008811950683594\n",
      "train: loss: 136119.734375 acc: 0.9707822203636169  val: loss: 2345448.5 acc: 0.5871614813804626\n",
      "step: 20125 , time : 0.0\n",
      "train: loss: 285789.0 acc: 0.9395762085914612  val: loss: 1792306.625 acc: -0.5061118602752686\n",
      "step: 20130 , time : 0.0\n",
      "train: loss: 770938.5625 acc: 0.6553807258605957  val: loss: 1047632.375 acc: 0.7548435926437378\n",
      "step: 20135 , time : 0.0010004043579101562\n",
      "train: loss: 458918.25 acc: 0.8965667486190796  val: loss: 454633.25 acc: 0.8015032410621643\n",
      "step: 20140 , time : 0.0\n",
      "train: loss: 104899.7265625 acc: 0.9869115352630615  val: loss: 190777.546875 acc: 0.9694123864173889\n",
      "step: 20145 , time : 0.0010004043579101562\n",
      "train: loss: 45563.1015625 acc: 0.9942979216575623  val: loss: 166429.859375 acc: 0.8414479494094849\n",
      "step: 20150 , time : 0.0010006427764892578\n",
      "train: loss: 269477.71875 acc: 0.9641762375831604  val: loss: 165394.828125 acc: 0.8690513372421265\n",
      "step: 20155 , time : 0.0\n",
      "train: loss: 253536.9375 acc: 0.975127637386322  val: loss: 470547.5 acc: 0.8957436084747314\n",
      "step: 20160 , time : 0.0010006427764892578\n",
      "train: loss: 500016.90625 acc: 0.9787537455558777  val: loss: 1253203.875 acc: 0.8607578873634338\n",
      "step: 20165 , time : 0.0\n",
      "train: loss: 501223.375 acc: 0.9689283967018127  val: loss: 969860.25 acc: 0.7799150347709656\n",
      "step: 20170 , time : 0.0010006427764892578\n",
      "train: loss: 1918918.25 acc: 0.7949207425117493  val: loss: 952562.0625 acc: 0.5361962914466858\n",
      "step: 20175 , time : 0.0010004043579101562\n",
      "train: loss: 1150905.0 acc: 0.9535936713218689  val: loss: 675639.9375 acc: 0.7525174617767334\n",
      "step: 20180 , time : 0.0\n",
      "train: loss: 2911163.0 acc: 0.9223242402076721  val: loss: 366499.625 acc: 0.8499244451522827\n",
      "step: 20185 , time : 0.0010008811950683594\n",
      "train: loss: 1744022.5 acc: 0.931423544883728  val: loss: 390080.625 acc: 0.9302176237106323\n",
      "step: 20190 , time : 0.0010008811950683594\n",
      "train: loss: 810753.4375 acc: 0.9445488452911377  val: loss: 512939.4375 acc: 0.7656468152999878\n",
      "step: 20195 , time : 0.0\n",
      "train: loss: 775160.5625 acc: 0.9394720792770386  val: loss: 193739.15625 acc: 0.9522561430931091\n",
      "step: 20200 , time : 0.0010004043579101562\n",
      "train: loss: 614947.6875 acc: 0.9273622035980225  val: loss: 646650.5625 acc: 0.8685076236724854\n",
      "step: 20205 , time : 0.0\n",
      "train: loss: 578485.5625 acc: 0.92542964220047  val: loss: 220836.34375 acc: 0.9550122022628784\n",
      "step: 20210 , time : 0.0\n",
      "train: loss: 2659546.5 acc: 0.532773494720459  val: loss: 705500.125 acc: 0.8681573867797852\n",
      "step: 20215 , time : 0.0\n",
      "train: loss: 1738374.125 acc: 0.5380550622940063  val: loss: 871887.8125 acc: 0.7521659731864929\n",
      "step: 20220 , time : 0.0\n",
      "train: loss: 1183850.125 acc: 0.7169892191886902  val: loss: 915735.1875 acc: 0.6511111259460449\n",
      "step: 20225 , time : 0.0010008811950683594\n",
      "train: loss: 1306950.0 acc: 0.7817245721817017  val: loss: 1085006.25 acc: 0.9003044366836548\n",
      "step: 20230 , time : 0.0\n",
      "train: loss: 694438.1875 acc: 0.7364344596862793  val: loss: 670896.75 acc: 0.8553423285484314\n",
      "step: 20235 , time : 0.0\n",
      "train: loss: 1369469.625 acc: 0.6747496724128723  val: loss: 1296170.75 acc: 0.814249575138092\n",
      "step: 20240 , time : 0.0\n",
      "train: loss: 875020.5 acc: 0.7012059688568115  val: loss: 4851302.5 acc: 0.35621482133865356\n",
      "step: 20245 , time : 0.001001119613647461\n",
      "train: loss: 262341.0 acc: 0.7916688919067383  val: loss: 5499174.5 acc: 0.6267211437225342\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 20250 , time : 0.0\n",
      "train: loss: 224239.9375 acc: 0.8054152727127075  val: loss: 3455063.0 acc: 0.6764075756072998\n",
      "step: 20255 , time : 0.0010008811950683594\n",
      "train: loss: 493500.34375 acc: 0.7964414954185486  val: loss: 3306185.5 acc: 0.6545262336730957\n",
      "step: 20260 , time : 0.0010006427764892578\n",
      "train: loss: 425754.59375 acc: 0.8110784292221069  val: loss: 1187545.5 acc: 0.7882072925567627\n",
      "step: 20265 , time : 0.0\n",
      "train: loss: 48808.13671875 acc: 0.9602089524269104  val: loss: 4100657.5 acc: 0.5664762258529663\n",
      "step: 20270 , time : 0.0010004043579101562\n",
      "train: loss: 125027.5625 acc: 0.9075704216957092  val: loss: 2135701.5 acc: 0.6395315527915955\n",
      "step: 20275 , time : 0.0\n",
      "train: loss: 345011.8125 acc: 0.8096977472305298  val: loss: 2915968.5 acc: 0.6114001274108887\n",
      "step: 20280 , time : 0.001001119613647461\n",
      "train: loss: 114395.515625 acc: 0.8975232839584351  val: loss: 1280313.625 acc: 0.7019805908203125\n",
      "step: 20285 , time : 0.0010008811950683594\n",
      "train: loss: 13043.0703125 acc: 0.9873838424682617  val: loss: 657506.8125 acc: 0.7941827178001404\n",
      "step: 20290 , time : 0.0\n",
      "train: loss: 336240.75 acc: 0.7634931206703186  val: loss: 1607461.125 acc: 0.5959593057632446\n",
      "step: 20295 , time : 0.0010006427764892578\n",
      "train: loss: 184030.375 acc: 0.8682008385658264  val: loss: 1204089.5 acc: 0.6435647010803223\n",
      "step: 20300 , time : 0.0\n",
      "train: loss: 493654.03125 acc: 0.7611293792724609  val: loss: 315995.125 acc: 0.7821517586708069\n",
      "step: 20305 , time : 0.0010006427764892578\n",
      "train: loss: 584498.625 acc: 0.7410157918930054  val: loss: 5026552.0 acc: 0.5615368485450745\n",
      "step: 20310 , time : 0.0\n",
      "train: loss: 413735.84375 acc: 0.7663270235061646  val: loss: 1648446.875 acc: 0.6490867137908936\n",
      "step: 20315 , time : 0.0\n",
      "train: loss: 69832.1640625 acc: 0.9386820793151855  val: loss: 1119962.75 acc: 0.6856217384338379\n",
      "step: 20320 , time : 0.0010006427764892578\n",
      "train: loss: 527435.625 acc: 0.7833291888237  val: loss: 392276.0625 acc: 0.8164663910865784\n",
      "step: 20325 , time : 0.0\n",
      "train: loss: 1736501.5 acc: 0.7242680788040161  val: loss: 2805282.0 acc: 0.7433197498321533\n",
      "step: 20330 , time : 0.0010008811950683594\n",
      "train: loss: 1150631.5 acc: 0.8758720755577087  val: loss: 676139.5 acc: 0.7298394441604614\n",
      "step: 20335 , time : 0.0\n",
      "train: loss: 548407.3125 acc: 0.9619913697242737  val: loss: 1488820.75 acc: 0.5872822403907776\n",
      "step: 20340 , time : 0.0010004043579101562\n",
      "train: loss: 460624.875 acc: 0.938076376914978  val: loss: 1306013.75 acc: 0.7833337783813477\n",
      "step: 20345 , time : 0.0\n",
      "train: loss: 238342.65625 acc: 0.9565549492835999  val: loss: 916513.4375 acc: 0.9064827561378479\n",
      "step: 20350 , time : 0.0010008811950683594\n",
      "train: loss: 196473.625 acc: 0.9666817784309387  val: loss: 1759460.0 acc: 0.4490935802459717\n",
      "step: 20355 , time : 0.0\n",
      "train: loss: 309892.09375 acc: 0.9732070565223694  val: loss: 1732878.5 acc: 0.6548457741737366\n",
      "step: 20360 , time : 0.0\n",
      "train: loss: 179606.28125 acc: 0.9871184229850769  val: loss: 2032095.5 acc: 0.628881573677063\n",
      "step: 20365 , time : 0.0010004043579101562\n",
      "train: loss: 213274.875 acc: 0.9831663966178894  val: loss: 1568020.125 acc: 0.34699153900146484\n",
      "step: 20370 , time : 0.0\n",
      "train: loss: 168218.21875 acc: 0.9794986248016357  val: loss: 1626411.0 acc: 0.6823215484619141\n",
      "step: 20375 , time : 0.0\n",
      "train: loss: 63789.04296875 acc: 0.9754130840301514  val: loss: 1315953.875 acc: 0.6749309301376343\n",
      "step: 20380 , time : 0.0\n",
      "train: loss: 21124.564453125 acc: 0.9882960915565491  val: loss: 1211716.0 acc: 0.8285118341445923\n",
      "step: 20385 , time : 0.0010008811950683594\n",
      "train: loss: 94512.9921875 acc: 0.9797826409339905  val: loss: 1092795.25 acc: 0.8792450428009033\n",
      "step: 20390 , time : 0.001001119613647461\n",
      "train: loss: 17887.701171875 acc: 0.9870574474334717  val: loss: 1310672.25 acc: 0.6184552311897278\n",
      "step: 20395 , time : 0.0010004043579101562\n",
      "train: loss: 17154.693359375 acc: 0.9810050129890442  val: loss: 1860481.0 acc: 0.5011736750602722\n",
      "step: 20400 , time : 0.0\n",
      "train: loss: 166198.328125 acc: 0.9205465316772461  val: loss: 310913.84375 acc: 0.8634709715843201\n",
      "step: 20405 , time : 0.0010008811950683594\n",
      "train: loss: 10715.51171875 acc: 0.9750975966453552  val: loss: 1870959.25 acc: 0.4693838357925415\n",
      "step: 20410 , time : 0.0\n",
      "train: loss: 15054.67578125 acc: 0.9604105353355408  val: loss: 959732.5625 acc: 0.8572487235069275\n",
      "step: 20415 , time : 0.0\n",
      "train: loss: 13036.8642578125 acc: 0.949126124382019  val: loss: 779630.1875 acc: 0.7910746335983276\n",
      "step: 20420 , time : 0.0\n",
      "train: loss: 19315.505859375 acc: 0.9579865336418152  val: loss: 789917.875 acc: 0.8721334934234619\n",
      "step: 20425 , time : 0.001001119613647461\n",
      "train: loss: 18494.443359375 acc: 0.9754815697669983  val: loss: 1068525.0 acc: 0.7459357976913452\n",
      "step: 20430 , time : 0.0\n",
      "train: loss: 62140.546875 acc: 0.9703831076622009  val: loss: 398626.65625 acc: 0.8802705407142639\n",
      "step: 20435 , time : 0.0\n",
      "train: loss: 67417.328125 acc: 0.9687409996986389  val: loss: 677212.9375 acc: 0.8801367878913879\n",
      "step: 20440 , time : 0.0\n",
      "train: loss: 44442.51953125 acc: 0.9843664169311523  val: loss: 1177038.25 acc: 0.7007777690887451\n",
      "step: 20445 , time : 0.0\n",
      "train: loss: 17409.712890625 acc: 0.980281412601471  val: loss: 3816351.25 acc: -0.6648755073547363\n",
      "step: 20450 , time : 0.015625953674316406\n",
      "train: loss: 56792.03515625 acc: 0.9287274479866028  val: loss: 2228234.25 acc: 0.6403735280036926\n",
      "step: 20455 , time : 0.0\n",
      "train: loss: 4575.65380859375 acc: 0.9896634817123413  val: loss: 804777.4375 acc: 0.6659377813339233\n",
      "step: 20460 , time : 0.0\n",
      "train: loss: 22683.79296875 acc: 0.99070143699646  val: loss: 1341405.5 acc: 0.2837316393852234\n",
      "step: 20465 , time : 0.0\n",
      "train: loss: 46267.9140625 acc: 0.9901644587516785  val: loss: 1212962.875 acc: 0.49154436588287354\n",
      "step: 20470 , time : 0.0010006427764892578\n",
      "train: loss: 27676.029296875 acc: 0.9889664649963379  val: loss: 484641.59375 acc: 0.9332441091537476\n",
      "step: 20475 , time : 0.0\n",
      "train: loss: 35277.3671875 acc: 0.9873718023300171  val: loss: 44381.4609375 acc: 0.9927344918251038\n",
      "step: 20480 , time : 0.0\n",
      "train: loss: 47054.7890625 acc: 0.9843722581863403  val: loss: 349311.46875 acc: 0.8316969871520996\n",
      "step: 20485 , time : 0.001001119613647461\n",
      "train: loss: 121005.7265625 acc: 0.9740509390830994  val: loss: 723380.4375 acc: 0.9082753658294678\n",
      "step: 20490 , time : 0.0010008811950683594\n",
      "train: loss: 263742.03125 acc: 0.9215981364250183  val: loss: 409017.03125 acc: 0.8678080439567566\n",
      "step: 20495 , time : 0.0\n",
      "train: loss: 44552.24609375 acc: 0.9788828492164612  val: loss: 489423.34375 acc: 0.8981863260269165\n",
      "step: 20500 , time : 0.0\n",
      "train: loss: 181677.390625 acc: 0.980381190776825  val: loss: 339303.0 acc: 0.9077072143554688\n",
      "step: 20505 , time : 0.0\n",
      "train: loss: 299494.28125 acc: 0.9706250429153442  val: loss: 433520.15625 acc: 0.8592149019241333\n",
      "step: 20510 , time : 0.0\n",
      "train: loss: 56909.23046875 acc: 0.9944306015968323  val: loss: 293379.53125 acc: 0.9030452966690063\n",
      "step: 20515 , time : 0.015625\n",
      "train: loss: 117190.3203125 acc: 0.9863965511322021  val: loss: 206265.875 acc: 0.9666332602500916\n",
      "step: 20520 , time : 0.0\n",
      "train: loss: 105711.4921875 acc: 0.9818336367607117  val: loss: 388902.71875 acc: 0.8927696347236633\n",
      "step: 20525 , time : 0.0\n",
      "train: loss: 695192.8125 acc: 0.9572235345840454  val: loss: 588272.4375 acc: 0.9204403162002563\n",
      "step: 20530 , time : 0.0010004043579101562\n",
      "train: loss: 765385.4375 acc: 0.9697608351707458  val: loss: 799056.375 acc: 0.638963520526886\n",
      "step: 20535 , time : 0.0\n",
      "train: loss: 347797.28125 acc: 0.9564400911331177  val: loss: 1522182.75 acc: 0.5851800441741943\n",
      "step: 20540 , time : 0.0010004043579101562\n",
      "train: loss: 576859.0 acc: 0.981117308139801  val: loss: 2391623.0 acc: 0.6983851194381714\n",
      "step: 20545 , time : 0.0011892318725585938\n",
      "train: loss: 770462.0 acc: 0.9720174074172974  val: loss: 1022002.3125 acc: 0.9590622186660767\n",
      "step: 20550 , time : 0.0\n",
      "train: loss: 2036587.75 acc: 0.9237027168273926  val: loss: 401501.75 acc: 0.9352413415908813\n",
      "step: 20555 , time : 0.0\n",
      "train: loss: 901989.1875 acc: 0.961460292339325  val: loss: 802426.375 acc: 0.8500068187713623\n",
      "step: 20560 , time : 0.0\n",
      "train: loss: 724637.875 acc: 0.9561679363250732  val: loss: 555525.75 acc: 0.959027886390686\n",
      "step: 20565 , time : 0.0\n",
      "train: loss: 104894.0390625 acc: 0.985774040222168  val: loss: 1296959.25 acc: 0.7134703993797302\n",
      "step: 20570 , time : 0.0\n",
      "train: loss: 338543.78125 acc: 0.9442190527915955  val: loss: 447255.46875 acc: 0.9055708050727844\n",
      "step: 20575 , time : 0.0\n",
      "train: loss: 823610.3125 acc: 0.802160382270813  val: loss: 767945.625 acc: 0.8951793909072876\n",
      "step: 20580 , time : 0.0\n",
      "train: loss: 1622869.0 acc: 0.551780104637146  val: loss: 2388814.25 acc: 0.8044435977935791\n",
      "step: 20585 , time : 0.0\n",
      "train: loss: 630543.9375 acc: 0.7881350517272949  val: loss: 574125.4375 acc: 0.7613840103149414\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 20590 , time : 0.0\n",
      "train: loss: 948674.25 acc: 0.8041483163833618  val: loss: 850226.5625 acc: 0.6656268835067749\n",
      "step: 20595 , time : 0.0\n",
      "train: loss: 878955.8125 acc: 0.6905380487442017  val: loss: 351573.5625 acc: 0.8869966864585876\n",
      "step: 20600 , time : 0.0010006427764892578\n",
      "train: loss: 896240.25 acc: 0.6553058624267578  val: loss: 2710643.0 acc: 0.8051425218582153\n",
      "step: 20605 , time : 0.0\n",
      "train: loss: 258467.765625 acc: 0.8372225761413574  val: loss: 2330712.5 acc: 0.6254791021347046\n",
      "step: 20610 , time : 0.0\n",
      "train: loss: 356245.34375 acc: 0.7631212472915649  val: loss: 6791785.0 acc: 0.573644757270813\n",
      "step: 20615 , time : 0.0\n",
      "train: loss: 98041.109375 acc: 0.9273622632026672  val: loss: 1296121.375 acc: 0.751928448677063\n",
      "step: 20620 , time : 0.0\n",
      "train: loss: 99955.6640625 acc: 0.9233375191688538  val: loss: 3531828.25 acc: 0.625868558883667\n",
      "step: 20625 , time : 0.0\n",
      "train: loss: 404788.71875 acc: 0.7975184917449951  val: loss: 780961.375 acc: 0.7868142127990723\n",
      "step: 20630 , time : 0.0\n",
      "train: loss: 364313.4375 acc: 0.8121389746665955  val: loss: 2777394.5 acc: 0.6346261501312256\n",
      "step: 20635 , time : 0.0\n",
      "train: loss: 56627.828125 acc: 0.952848494052887  val: loss: 1483785.75 acc: 0.6407326459884644\n",
      "step: 20640 , time : 0.0\n",
      "train: loss: 113775.796875 acc: 0.930153489112854  val: loss: 860153.3125 acc: 0.7333067059516907\n",
      "step: 20645 , time : 0.0\n",
      "train: loss: 102795.953125 acc: 0.914504885673523  val: loss: 657146.5625 acc: 0.7677761316299438\n",
      "step: 20650 , time : 0.0\n",
      "train: loss: 117042.203125 acc: 0.8866865038871765  val: loss: 378656.75 acc: 0.7926236391067505\n",
      "step: 20655 , time : 0.0\n",
      "train: loss: 57234.0546875 acc: 0.9498600363731384  val: loss: 3718771.0 acc: 0.6342254281044006\n",
      "step: 20660 , time : 0.0\n",
      "train: loss: 214288.421875 acc: 0.8546833395957947  val: loss: 2015513.375 acc: 0.613930344581604\n",
      "step: 20665 , time : 0.0\n",
      "train: loss: 576084.4375 acc: 0.7639949321746826  val: loss: 1188366.875 acc: 0.6535505652427673\n",
      "step: 20670 , time : 0.0\n",
      "train: loss: 33122.2109375 acc: 0.9686970710754395  val: loss: 1073357.25 acc: 0.6166973114013672\n",
      "step: 20675 , time : 0.0\n",
      "train: loss: 1424113.0 acc: 0.6732273101806641  val: loss: 782144.0625 acc: 0.7850543260574341\n",
      "step: 20680 , time : 0.0\n",
      "train: loss: 176522.015625 acc: 0.8525679707527161  val: loss: 464363.21875 acc: 0.7067093849182129\n",
      "step: 20685 , time : 0.0\n",
      "train: loss: 661837.1875 acc: 0.7767934799194336  val: loss: 1760614.0 acc: 0.6620683670043945\n",
      "step: 20690 , time : 0.0\n",
      "train: loss: 1476039.625 acc: 0.7239213585853577  val: loss: 948254.3125 acc: 0.8455905914306641\n",
      "step: 20695 , time : 0.0\n",
      "train: loss: 1165612.0 acc: 0.870502233505249  val: loss: 1673468.375 acc: 0.8597851395606995\n",
      "step: 20700 , time : 0.0\n",
      "train: loss: 965631.4375 acc: 0.9066231846809387  val: loss: 1282958.25 acc: 0.6514147520065308\n",
      "step: 20705 , time : 0.0\n",
      "train: loss: 651957.8125 acc: 0.9076645374298096  val: loss: 2580428.5 acc: 0.490151584148407\n",
      "step: 20710 , time : 0.0010004043579101562\n",
      "train: loss: 147882.796875 acc: 0.9815022945404053  val: loss: 1352631.5 acc: 0.8830407857894897\n",
      "step: 20715 , time : 0.0\n",
      "train: loss: 176402.6875 acc: 0.9748790860176086  val: loss: 704518.9375 acc: 0.6265537738800049\n",
      "step: 20720 , time : 0.001001119613647461\n",
      "train: loss: 111935.6640625 acc: 0.989914059638977  val: loss: 529590.8125 acc: 0.7757445573806763\n",
      "step: 20725 , time : 0.0010006427764892578\n",
      "train: loss: 112142.453125 acc: 0.990679919719696  val: loss: 384156.28125 acc: 0.9291316270828247\n",
      "step: 20730 , time : 0.0\n",
      "train: loss: 115240.75 acc: 0.9927728772163391  val: loss: 2119571.0 acc: 0.16559338569641113\n",
      "step: 20735 , time : 0.0\n",
      "train: loss: 112291.796875 acc: 0.985181450843811  val: loss: 1242698.75 acc: 0.7425354719161987\n",
      "step: 20740 , time : 0.0\n",
      "train: loss: 39145.48828125 acc: 0.9905796051025391  val: loss: 646578.3125 acc: 0.889091968536377\n",
      "step: 20745 , time : 0.0\n",
      "train: loss: 36749.03515625 acc: 0.9902079105377197  val: loss: 571994.5 acc: 0.8963186144828796\n",
      "step: 20750 , time : 0.0\n",
      "train: loss: 43006.19140625 acc: 0.9292012453079224  val: loss: 1039185.25 acc: 0.6911033391952515\n",
      "step: 20755 , time : 0.0\n",
      "train: loss: 17888.6953125 acc: 0.9679158329963684  val: loss: 615914.9375 acc: 0.9010326266288757\n",
      "step: 20760 , time : 0.0\n",
      "train: loss: 8765.251953125 acc: 0.9809790849685669  val: loss: 541258.375 acc: 0.8319340944290161\n",
      "step: 20765 , time : 0.0\n",
      "train: loss: 9922.5107421875 acc: 0.9856061935424805  val: loss: 210102.609375 acc: 0.5950185060501099\n",
      "step: 20770 , time : 0.0\n",
      "train: loss: 9748.771484375 acc: 0.9608792662620544  val: loss: 438878.0 acc: 0.9136082530021667\n",
      "step: 20775 , time : 0.0\n",
      "train: loss: 27456.765625 acc: 0.9076995253562927  val: loss: 252231.125 acc: 0.9252611994743347\n",
      "step: 20780 , time : 0.0\n",
      "train: loss: 16479.2578125 acc: 0.9617137908935547  val: loss: 1245519.75 acc: 0.6482893228530884\n",
      "step: 20785 , time : 0.0\n",
      "train: loss: 15843.1396484375 acc: 0.9805741310119629  val: loss: 143058.25 acc: 0.9527537822723389\n",
      "step: 20790 , time : 0.0\n",
      "train: loss: 11819.2216796875 acc: 0.9822092652320862  val: loss: 543623.0 acc: 0.8536662459373474\n",
      "step: 20795 , time : 0.015626192092895508\n",
      "train: loss: 42280.09375 acc: 0.9740954041481018  val: loss: 291808.625 acc: 0.8820221424102783\n",
      "step: 20800 , time : 0.0\n",
      "train: loss: 31033.95703125 acc: 0.9630535840988159  val: loss: 2295026.5 acc: 0.29674237966537476\n",
      "step: 20805 , time : 0.0\n",
      "train: loss: 77149.1171875 acc: 0.9352904558181763  val: loss: 366080.65625 acc: 0.8277244567871094\n",
      "step: 20810 , time : 0.0\n",
      "train: loss: 28183.162109375 acc: 0.9827527403831482  val: loss: 547853.125 acc: 0.7397115230560303\n",
      "step: 20815 , time : 0.0\n",
      "train: loss: 13808.2548828125 acc: 0.98149573802948  val: loss: 2295686.25 acc: 0.6101827621459961\n",
      "step: 20820 , time : 0.0\n",
      "train: loss: 24626.166015625 acc: 0.9857637882232666  val: loss: 138332.8125 acc: 0.975792407989502\n",
      "step: 20825 , time : 0.0010013580322265625\n",
      "train: loss: 35199.49609375 acc: 0.973786473274231  val: loss: 704602.875 acc: 0.7669053077697754\n",
      "step: 20830 , time : 0.0\n",
      "train: loss: 42320.80859375 acc: 0.9891868829727173  val: loss: 1021663.0 acc: 0.6361255645751953\n",
      "step: 20835 , time : 0.0\n",
      "train: loss: 27363.8125 acc: 0.9931709170341492  val: loss: 1703524.25 acc: 0.8122242093086243\n",
      "step: 20840 , time : 0.0\n",
      "train: loss: 23470.68359375 acc: 0.9902810454368591  val: loss: 434300.59375 acc: 0.7635642290115356\n",
      "step: 20845 , time : 0.0\n",
      "train: loss: 46976.328125 acc: 0.9847953915596008  val: loss: 303324.65625 acc: 0.9494792819023132\n",
      "step: 20850 , time : 0.0\n",
      "train: loss: 92011.703125 acc: 0.9541594386100769  val: loss: 760997.8125 acc: 0.9252521991729736\n",
      "step: 20855 , time : 0.0\n",
      "train: loss: 198443.640625 acc: 0.9184149503707886  val: loss: 915239.625 acc: 0.9029014706611633\n",
      "step: 20860 , time : 0.0\n",
      "train: loss: 46202.37890625 acc: 0.9699757099151611  val: loss: 690581.0 acc: 0.905779242515564\n",
      "step: 20865 , time : 0.0\n",
      "train: loss: 185281.65625 acc: 0.9650524258613586  val: loss: 1087031.25 acc: 0.8466949462890625\n",
      "step: 20870 , time : 0.0\n",
      "train: loss: 411775.4375 acc: 0.9494775533676147  val: loss: 697430.5 acc: 0.9286245703697205\n",
      "step: 20875 , time : 0.0\n",
      "train: loss: 56330.42578125 acc: 0.9944055080413818  val: loss: 636258.125 acc: 0.8483664989471436\n",
      "step: 20880 , time : 0.0010006427764892578\n",
      "train: loss: 29951.1328125 acc: 0.9958315491676331  val: loss: 820447.3125 acc: 0.9392078518867493\n",
      "step: 20885 , time : 0.0010006427764892578\n",
      "train: loss: 99468.953125 acc: 0.9907174706459045  val: loss: 303065.96875 acc: 0.9674787521362305\n",
      "step: 20890 , time : 0.0\n",
      "train: loss: 923009.125 acc: 0.9664046764373779  val: loss: 1421572.75 acc: 0.6891606450080872\n",
      "step: 20895 , time : 0.0\n",
      "train: loss: 441884.59375 acc: 0.9435306787490845  val: loss: 891792.1875 acc: 0.8292550444602966\n",
      "step: 20900 , time : 0.0\n",
      "train: loss: 589853.75 acc: 0.9650430679321289  val: loss: 1178146.375 acc: 0.9070250988006592\n",
      "step: 20905 , time : 0.0\n",
      "train: loss: 322775.75 acc: 0.936149001121521  val: loss: 2355876.5 acc: 0.8405016660690308\n",
      "step: 20910 , time : 0.0\n",
      "train: loss: 563273.6875 acc: 0.9749496579170227  val: loss: 3256774.75 acc: 0.6680580973625183\n",
      "step: 20915 , time : 0.0\n",
      "train: loss: 1243822.125 acc: 0.9620779156684875  val: loss: 927661.375 acc: 0.8484745025634766\n",
      "step: 20920 , time : 0.0\n",
      "train: loss: 1936142.5 acc: 0.9005952477455139  val: loss: 595835.9375 acc: 0.9058046340942383\n",
      "step: 20925 , time : 0.0\n",
      "train: loss: 451522.0625 acc: 0.9624048471450806  val: loss: 1534216.75 acc: 0.6523289680480957\n",
      "step: 20930 , time : 0.0\n",
      "train: loss: 856244.1875 acc: 0.9391087293624878  val: loss: 511826.625 acc: 0.9392089247703552\n",
      "step: 20935 , time : 0.0\n",
      "train: loss: 340845.84375 acc: 0.9078095555305481  val: loss: 714897.9375 acc: 0.7064140439033508\n",
      "step: 20940 , time : 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: loss: 393224.53125 acc: 0.9369152188301086  val: loss: 280021.03125 acc: 0.9570698738098145\n",
      "step: 20945 , time : 0.0\n",
      "train: loss: 1548409.25 acc: 0.5616854429244995  val: loss: 229038.765625 acc: 0.94693922996521\n",
      "step: 20950 , time : 0.0\n",
      "train: loss: 1592138.5 acc: -0.11764335632324219  val: loss: 773760.6875 acc: 0.1841985583305359\n",
      "step: 20955 , time : 0.0\n",
      "train: loss: 681335.75 acc: 0.7150341272354126  val: loss: 2596526.5 acc: 0.8237279653549194\n",
      "step: 20960 , time : 0.0\n",
      "train: loss: 574904.75 acc: 0.6511991024017334  val: loss: 1118012.75 acc: 0.7223917841911316\n",
      "step: 20965 , time : 0.0010008811950683594\n",
      "train: loss: 762890.8125 acc: 0.7044584155082703  val: loss: 740139.9375 acc: 0.6614272594451904\n",
      "step: 20970 , time : 0.0012607574462890625\n",
      "train: loss: 875895.0 acc: 0.6880028247833252  val: loss: 992262.625 acc: 0.699691653251648\n",
      "step: 20975 , time : 0.0\n",
      "train: loss: 327918.6875 acc: 0.7986064553260803  val: loss: 313842.3125 acc: 0.7273684740066528\n",
      "step: 20980 , time : 0.0\n",
      "train: loss: 394033.875 acc: 0.7284290790557861  val: loss: 1001320.4375 acc: 0.6201531291007996\n",
      "step: 20985 , time : 0.0\n",
      "train: loss: 142213.578125 acc: 0.8627781271934509  val: loss: 3911663.0 acc: 0.6511585712432861\n",
      "step: 20990 , time : 0.0\n",
      "train: loss: 137288.421875 acc: 0.8970437049865723  val: loss: 701746.125 acc: 0.7583097815513611\n",
      "step: 20995 , time : 0.0\n",
      "train: loss: 27576.37890625 acc: 0.978498101234436  val: loss: 1631268.875 acc: 0.7099765539169312\n",
      "step: 21000 , time : 0.0\n",
      "train: loss: 221128.25 acc: 0.8753063678741455  val: loss: 1207437.75 acc: 0.6219862103462219\n",
      "step: 21005 , time : 0.0\n",
      "train: loss: 39891.67578125 acc: 0.968605101108551  val: loss: 1854523.0 acc: 0.5974780321121216\n",
      "step: 21010 , time : 0.0\n",
      "train: loss: 97322.234375 acc: 0.9256517887115479  val: loss: 678214.25 acc: 0.7109772562980652\n",
      "step: 21015 , time : 0.0\n",
      "train: loss: 101509.9140625 acc: 0.9140681028366089  val: loss: 3283944.0 acc: 0.6023370027542114\n",
      "step: 21020 , time : 0.0\n",
      "train: loss: 224204.828125 acc: 0.8284487724304199  val: loss: 1122110.0 acc: 0.696162760257721\n",
      "step: 21025 , time : 0.0\n",
      "train: loss: 63934.55859375 acc: 0.932021975517273  val: loss: 1486771.0 acc: 0.6581878662109375\n",
      "step: 21030 , time : 0.0\n",
      "train: loss: 676191.4375 acc: 0.6523890495300293  val: loss: 1401288.75 acc: 0.6706757545471191\n",
      "step: 21035 , time : 0.0\n",
      "train: loss: 861412.3125 acc: 0.6702228784561157  val: loss: 104735.15625 acc: 0.9129284024238586\n",
      "step: 21040 , time : 0.0\n",
      "train: loss: 123572.109375 acc: 0.9221231937408447  val: loss: 2567811.75 acc: 0.5539227724075317\n",
      "step: 21045 , time : 0.0\n",
      "train: loss: 335519.3125 acc: 0.8161272406578064  val: loss: 1061664.25 acc: 0.7222508192062378\n",
      "step: 21050 , time : 0.0\n",
      "train: loss: 635383.6875 acc: 0.7613627314567566  val: loss: 884800.1875 acc: 0.6674422025680542\n",
      "step: 21055 , time : 0.0\n",
      "train: loss: 2130112.0 acc: 0.6766892075538635  val: loss: 1157309.125 acc: 0.7121337652206421\n",
      "step: 21060 , time : 0.0\n",
      "train: loss: 653488.375 acc: 0.8658251166343689  val: loss: 532513.375 acc: 0.8731805086135864\n",
      "step: 21065 , time : 0.0\n",
      "train: loss: 1321933.5 acc: 0.858391523361206  val: loss: 868636.0625 acc: 0.8973154425621033\n",
      "step: 21070 , time : 0.0010008811950683594\n",
      "train: loss: 853458.125 acc: 0.9337242841720581  val: loss: 612570.625 acc: 0.8515212535858154\n",
      "step: 21075 , time : 0.0010008811950683594\n",
      "train: loss: 360938.96875 acc: 0.8860127925872803  val: loss: 4118830.5 acc: 0.6006988883018494\n",
      "step: 21080 , time : 0.0\n",
      "train: loss: 247339.125 acc: 0.9703251123428345  val: loss: 579579.1875 acc: 0.7213554382324219\n",
      "step: 21085 , time : 0.001001119613647461\n",
      "train: loss: 241426.453125 acc: 0.9767805337905884  val: loss: 330964.78125 acc: 0.8817408084869385\n",
      "step: 21090 , time : 0.0010004043579101562\n",
      "train: loss: 96484.3671875 acc: 0.9907289743423462  val: loss: 529853.875 acc: 0.8403045535087585\n",
      "step: 21095 , time : 0.001001119613647461\n",
      "train: loss: 168627.9375 acc: 0.9884628057479858  val: loss: 645755.9375 acc: 0.7650681734085083\n",
      "step: 21100 , time : 0.0\n",
      "train: loss: 94766.7734375 acc: 0.9879166483879089  val: loss: 1691124.375 acc: 0.35942304134368896\n",
      "step: 21105 , time : 0.0\n",
      "train: loss: 176980.703125 acc: 0.9795364737510681  val: loss: 138370.65625 acc: 0.9564626216888428\n",
      "step: 21110 , time : 0.0\n",
      "train: loss: 48232.53515625 acc: 0.9879288673400879  val: loss: 1333929.5 acc: 0.7377432584762573\n",
      "step: 21115 , time : 0.0\n",
      "train: loss: 30360.18359375 acc: 0.9866619110107422  val: loss: 330339.75 acc: 0.9440404176712036\n",
      "step: 21120 , time : 0.0\n",
      "train: loss: 18871.177734375 acc: 0.986527681350708  val: loss: 502839.78125 acc: 0.8917046785354614\n",
      "step: 21125 , time : 0.0\n",
      "train: loss: 27786.380859375 acc: 0.9563331604003906  val: loss: 423688.59375 acc: 0.9478526711463928\n",
      "step: 21130 , time : 0.0\n",
      "train: loss: 18901.005859375 acc: 0.9660488367080688  val: loss: 859544.125 acc: 0.6708939671516418\n",
      "step: 21135 , time : 0.0\n",
      "train: loss: 30588.689453125 acc: 0.9479073882102966  val: loss: 935617.6875 acc: 0.6571210622787476\n",
      "step: 21140 , time : 0.0\n",
      "train: loss: 10810.7900390625 acc: 0.9522747993469238  val: loss: 340808.8125 acc: 0.777949333190918\n",
      "step: 21145 , time : 0.0\n",
      "train: loss: 31950.544921875 acc: 0.890564501285553  val: loss: 2149104.0 acc: 0.418520987033844\n",
      "step: 21150 , time : 0.0\n",
      "train: loss: 19673.603515625 acc: 0.9648464918136597  val: loss: 377506.90625 acc: 0.8985075950622559\n",
      "step: 21155 , time : 0.0\n",
      "train: loss: 11456.3076171875 acc: 0.9367882013320923  val: loss: 314060.0625 acc: 0.9475991725921631\n",
      "step: 21160 , time : 0.0\n",
      "train: loss: 65031.421875 acc: 0.9593898057937622  val: loss: 415540.3125 acc: 0.9017111659049988\n",
      "step: 21165 , time : 0.0\n",
      "train: loss: 43980.6171875 acc: 0.9695772528648376  val: loss: 825357.0 acc: 0.9323348999023438\n",
      "step: 21170 , time : 0.0010001659393310547\n",
      "train: loss: 22106.017578125 acc: 0.9872931838035583  val: loss: 1429048.375 acc: 0.8833584785461426\n",
      "step: 21175 , time : 0.0\n",
      "train: loss: 20945.07421875 acc: 0.9761402010917664  val: loss: 952264.875 acc: 0.6099798679351807\n",
      "step: 21180 , time : 0.0010004043579101562\n",
      "train: loss: 21952.146484375 acc: 0.9800668358802795  val: loss: 796692.8125 acc: 0.9380277395248413\n",
      "step: 21185 , time : 0.0\n",
      "train: loss: 12989.388671875 acc: 0.9926515817642212  val: loss: 1881887.5 acc: 0.872410237789154\n",
      "step: 21190 , time : 0.0010008811950683594\n",
      "train: loss: 21716.587890625 acc: 0.9902908802032471  val: loss: 1612216.625 acc: 0.7117083668708801\n",
      "step: 21195 , time : 0.001001119613647461\n",
      "train: loss: 39500.07421875 acc: 0.9867601990699768  val: loss: 829568.875 acc: 0.8903225064277649\n",
      "step: 21200 , time : 0.0\n",
      "train: loss: 34250.5 acc: 0.9930499792098999  val: loss: 1058457.375 acc: 0.7337186336517334\n",
      "step: 21205 , time : 0.0\n",
      "train: loss: 41718.12109375 acc: 0.9890977740287781  val: loss: 1165049.625 acc: 0.5706250667572021\n",
      "step: 21210 , time : 0.0\n",
      "train: loss: 53271.8828125 acc: 0.9805376529693604  val: loss: 1746828.75 acc: 0.8649299144744873\n",
      "step: 21215 , time : 0.0\n",
      "train: loss: 265591.1875 acc: 0.9191200137138367  val: loss: 1474981.25 acc: 0.7933075428009033\n",
      "step: 21220 , time : 0.0\n",
      "train: loss: 120803.0703125 acc: 0.9572358727455139  val: loss: 625715.375 acc: 0.9073053598403931\n",
      "step: 21225 , time : 0.0\n",
      "train: loss: 120989.0703125 acc: 0.9547731876373291  val: loss: 855050.8125 acc: 0.8230642080307007\n",
      "step: 21230 , time : 0.0\n",
      "train: loss: 147464.109375 acc: 0.9797776341438293  val: loss: 1438033.875 acc: 0.5975059866905212\n",
      "step: 21235 , time : 0.0\n",
      "train: loss: 122730.5390625 acc: 0.9860252737998962  val: loss: 1491121.0 acc: 0.7663260698318481\n",
      "step: 21240 , time : 0.0\n",
      "train: loss: 81680.453125 acc: 0.9908990263938904  val: loss: 451272.625 acc: 0.9353731274604797\n",
      "step: 21245 , time : 0.0\n",
      "train: loss: 81078.515625 acc: 0.9872872233390808  val: loss: 2662013.0 acc: -0.11391711235046387\n",
      "step: 21250 , time : 0.0\n",
      "train: loss: 199110.0 acc: 0.976101279258728  val: loss: 440310.53125 acc: 0.9524020552635193\n",
      "step: 21255 , time : 0.0\n",
      "train: loss: 424924.75 acc: 0.9731341600418091  val: loss: 2668125.25 acc: 0.2838659882545471\n",
      "step: 21260 , time : 0.0\n",
      "train: loss: 492470.8125 acc: 0.9286029934883118  val: loss: 2714842.75 acc: 0.7205939292907715\n",
      "step: 21265 , time : 0.0010006427764892578\n",
      "train: loss: 465340.5625 acc: 0.965216338634491  val: loss: 1269456.25 acc: 0.6863278150558472\n",
      "step: 21270 , time : 0.0\n",
      "train: loss: 352511.03125 acc: 0.9755759835243225  val: loss: 934172.125 acc: 0.6979533433914185\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 21275 , time : 0.0\n",
      "train: loss: 1285694.375 acc: 0.9568899273872375  val: loss: 702347.875 acc: 0.8818507194519043\n",
      "step: 21280 , time : 0.0\n",
      "train: loss: 1009202.6875 acc: 0.9603329300880432  val: loss: 2047565.75 acc: 0.5269256234169006\n",
      "step: 21285 , time : 0.0010006427764892578\n",
      "train: loss: 741418.75 acc: 0.9505054354667664  val: loss: 1901349.0 acc: 0.6244635581970215\n",
      "step: 21290 , time : 0.001001119613647461\n",
      "train: loss: 871484.4375 acc: 0.9556120038032532  val: loss: 591200.625 acc: 0.9112998843193054\n",
      "step: 21295 , time : 0.0\n",
      "train: loss: 109375.5078125 acc: 0.9865375757217407  val: loss: 1493118.5 acc: 0.4650765657424927\n",
      "step: 21300 , time : 0.0\n",
      "train: loss: 473829.125 acc: 0.9513822197914124  val: loss: 811615.0625 acc: 0.7282225489616394\n",
      "step: 21305 , time : 0.0\n",
      "train: loss: 443713.6875 acc: 0.9550063014030457  val: loss: 3408200.75 acc: 0.6757615804672241\n",
      "step: 21310 , time : 0.0010006427764892578\n",
      "train: loss: 1695217.875 acc: 0.649124264717102  val: loss: 926930.4375 acc: 0.7629826664924622\n",
      "step: 21315 , time : 0.0\n",
      "train: loss: 1137941.375 acc: 0.6262346506118774  val: loss: 1309931.625 acc: 0.8232953548431396\n",
      "step: 21320 , time : 0.0\n",
      "train: loss: 548770.1875 acc: 0.7509726881980896  val: loss: 544256.0625 acc: 0.8861492276191711\n",
      "step: 21325 , time : 0.0\n",
      "train: loss: 329670.875 acc: 0.8798971176147461  val: loss: 1585266.625 acc: 0.8053457736968994\n",
      "step: 21330 , time : 0.0\n",
      "train: loss: 1165089.625 acc: -0.012665748596191406  val: loss: 1048284.375 acc: 0.7527400851249695\n",
      "step: 21335 , time : 0.0\n",
      "train: loss: 937604.125 acc: 0.5383073091506958  val: loss: 2336685.5 acc: 0.6550143957138062\n",
      "step: 21340 , time : 0.0\n",
      "train: loss: 429511.34375 acc: 0.7499040365219116  val: loss: 1214539.875 acc: 0.7163193225860596\n",
      "step: 21345 , time : 0.0\n",
      "train: loss: 399844.0 acc: 0.7686818838119507  val: loss: 208422.921875 acc: 0.8183482885360718\n",
      "step: 21350 , time : 0.0\n",
      "train: loss: 148402.3125 acc: 0.8704723119735718  val: loss: 1723215.375 acc: 0.7238026857376099\n",
      "step: 21355 , time : 0.0\n",
      "train: loss: 51850.390625 acc: 0.9559076428413391  val: loss: 4680235.0 acc: 0.6468559503555298\n",
      "step: 21360 , time : 0.0\n",
      "train: loss: 22443.1328125 acc: 0.9810998439788818  val: loss: 1180667.625 acc: 0.6830300688743591\n",
      "step: 21365 , time : 0.0\n",
      "train: loss: 142400.671875 acc: 0.9069229960441589  val: loss: 1691117.0 acc: 0.6946396827697754\n",
      "step: 21370 , time : 0.0\n",
      "train: loss: 75248.3671875 acc: 0.94332355260849  val: loss: 1672995.5 acc: 0.620885968208313\n",
      "step: 21375 , time : 0.0\n",
      "train: loss: 72364.125 acc: 0.9414486885070801  val: loss: 1331648.375 acc: 0.6443123817443848\n",
      "step: 21380 , time : 0.0009996891021728516\n",
      "train: loss: 226512.640625 acc: 0.8722451329231262  val: loss: 1017649.8125 acc: 0.6824473142623901\n",
      "step: 21385 , time : 0.0010006427764892578\n",
      "train: loss: 139955.359375 acc: 0.8203712105751038  val: loss: 4041026.5 acc: 0.4807767868041992\n",
      "step: 21390 , time : 0.0\n",
      "train: loss: 117352.5 acc: 0.8917939066886902  val: loss: 1548452.25 acc: 0.6888443231582642\n",
      "step: 21395 , time : 0.0\n",
      "train: loss: 536845.1875 acc: 0.7824990153312683  val: loss: 1106548.0 acc: 0.6753453612327576\n",
      "step: 21400 , time : 0.0\n",
      "train: loss: 575731.5 acc: 0.7152487635612488  val: loss: 2382137.25 acc: 0.6311228275299072\n",
      "step: 21405 , time : 0.0\n",
      "train: loss: 917595.0625 acc: 0.6976755857467651  val: loss: 768939.5625 acc: 0.7466566562652588\n",
      "step: 21410 , time : 0.001001119613647461\n",
      "train: loss: 176777.8125 acc: 0.830952525138855  val: loss: 671501.9375 acc: 0.7313490509986877\n",
      "step: 21415 , time : 0.0\n",
      "train: loss: 711464.625 acc: 0.6965755820274353  val: loss: 440143.53125 acc: 0.6958062648773193\n",
      "step: 21420 , time : 0.0\n",
      "train: loss: 1310926.75 acc: 0.6889415979385376  val: loss: 920630.125 acc: 0.7009779810905457\n",
      "step: 21425 , time : 0.0\n",
      "train: loss: 959988.5 acc: 0.7811588048934937  val: loss: 550363.125 acc: 0.8348119258880615\n",
      "step: 21430 , time : 0.0\n",
      "train: loss: 1010450.8125 acc: 0.9271227717399597  val: loss: 1484976.75 acc: -0.10121536254882812\n",
      "step: 21435 , time : 0.0\n",
      "train: loss: 259478.640625 acc: 0.9789837002754211  val: loss: 1191617.0 acc: 0.6618716716766357\n",
      "step: 21440 , time : 0.0\n",
      "train: loss: 178850.421875 acc: 0.9693336486816406  val: loss: 1490953.75 acc: 0.5519921779632568\n",
      "step: 21445 , time : 0.0\n",
      "train: loss: 229879.984375 acc: 0.9676045179367065  val: loss: 700423.5 acc: 0.8263422250747681\n",
      "step: 21450 , time : 0.0010001659393310547\n",
      "train: loss: 62090.140625 acc: 0.9931517243385315  val: loss: 470745.84375 acc: 0.8709508776664734\n",
      "step: 21455 , time : 0.0010006427764892578\n",
      "train: loss: 80166.140625 acc: 0.9921911358833313  val: loss: 269629.28125 acc: 0.9416487812995911\n",
      "step: 21460 , time : 0.0010004043579101562\n",
      "train: loss: 821771.4375 acc: 0.9482476115226746  val: loss: 418055.6875 acc: 0.8380997776985168\n",
      "step: 21465 , time : 0.0\n",
      "train: loss: 80568.015625 acc: 0.9881075024604797  val: loss: 1329356.75 acc: 0.2801380753517151\n",
      "step: 21470 , time : 0.0\n",
      "train: loss: 57143.2734375 acc: 0.9910257458686829  val: loss: 503835.1875 acc: 0.9239137768745422\n",
      "step: 21475 , time : 0.0\n",
      "train: loss: 44600.515625 acc: 0.9878170490264893  val: loss: 1039941.5625 acc: 0.914459764957428\n",
      "step: 21480 , time : 0.0\n",
      "train: loss: 13909.162109375 acc: 0.9875331521034241  val: loss: 1670516.625 acc: 0.8324564695358276\n",
      "step: 21485 , time : 0.0\n",
      "train: loss: 10310.0263671875 acc: 0.9956586956977844  val: loss: 1687416.375 acc: 0.8712828159332275\n",
      "step: 21490 , time : 0.0\n",
      "train: loss: 20607.259765625 acc: 0.9894275069236755  val: loss: 616501.375 acc: 0.9410501718521118\n",
      "step: 21495 , time : 0.0\n",
      "train: loss: 45622.0625 acc: 0.9824410080909729  val: loss: 1171010.75 acc: 0.6212102174758911\n",
      "step: 21500 , time : 0.0010004043579101562\n",
      "train: loss: 35475.9140625 acc: 0.9750570058822632  val: loss: 603501.75 acc: 0.9293112754821777\n",
      "step: 21505 , time : 0.0010006427764892578\n",
      "train: loss: 31439.83984375 acc: 0.9819987416267395  val: loss: 390308.375 acc: 0.9649893045425415\n",
      "step: 21510 , time : 0.0\n",
      "train: loss: 9552.9599609375 acc: 0.9785375595092773  val: loss: 969567.0 acc: 0.8644243478775024\n",
      "step: 21515 , time : 0.0\n",
      "train: loss: 13331.158203125 acc: 0.9622815251350403  val: loss: 735150.25 acc: 0.9270044565200806\n",
      "step: 21520 , time : 0.0\n",
      "train: loss: 19303.9140625 acc: 0.9654688239097595  val: loss: 1389253.625 acc: 0.5226696729660034\n",
      "step: 21525 , time : 0.001001119613647461\n",
      "train: loss: 48559.9453125 acc: 0.9768668413162231  val: loss: 2311651.0 acc: 0.5718210935592651\n",
      "step: 21530 , time : 0.0010006427764892578\n",
      "train: loss: 26074.05859375 acc: 0.9833682179450989  val: loss: 2126234.25 acc: -0.027286887168884277\n",
      "step: 21535 , time : 0.0\n",
      "train: loss: 80491.46875 acc: 0.9359824657440186  val: loss: 1241108.75 acc: 0.7894600629806519\n",
      "step: 21540 , time : 0.0010006427764892578\n",
      "train: loss: 20683.5703125 acc: 0.9842429757118225  val: loss: 613998.75 acc: 0.9354416131973267\n",
      "step: 21545 , time : 0.0010006427764892578\n",
      "train: loss: 36767.234375 acc: 0.974078357219696  val: loss: 2971977.0 acc: 0.3692757487297058\n",
      "step: 21550 , time : 0.0010004043579101562\n",
      "train: loss: 12273.0625 acc: 0.9614364504814148  val: loss: 622818.125 acc: 0.9196907877922058\n",
      "step: 21555 , time : 0.0\n",
      "train: loss: 20741.40234375 acc: 0.9922178387641907  val: loss: 1732765.5 acc: 0.7492243647575378\n",
      "step: 21560 , time : 0.0\n",
      "train: loss: 36880.203125 acc: 0.9919178485870361  val: loss: 2605769.0 acc: 0.6514869928359985\n",
      "step: 21565 , time : 0.0\n",
      "train: loss: 32502.16015625 acc: 0.9907515645027161  val: loss: 2023395.5 acc: 0.26419633626937866\n",
      "step: 21570 , time : 0.0\n",
      "train: loss: 32968.78125 acc: 0.9918513298034668  val: loss: 1076091.5 acc: 0.8594279885292053\n",
      "step: 21575 , time : 0.0\n",
      "train: loss: 41031.5390625 acc: 0.9725357294082642  val: loss: 1656280.75 acc: 0.6936557292938232\n",
      "step: 21580 , time : 0.0\n",
      "train: loss: 151550.53125 acc: 0.932646632194519  val: loss: 1171202.25 acc: 0.5213158130645752\n",
      "step: 21585 , time : 0.0010006427764892578\n",
      "train: loss: 154346.9375 acc: 0.9641447067260742  val: loss: 2756721.5 acc: -0.9716439247131348\n",
      "step: 21590 , time : 0.0010004043579101562\n",
      "train: loss: 119864.0546875 acc: 0.9626759886741638  val: loss: 598211.9375 acc: 0.9224834442138672\n",
      "step: 21595 , time : 0.0010006427764892578\n",
      "train: loss: 157551.6875 acc: 0.9611015915870667  val: loss: 2412780.0 acc: 0.2566187381744385\n",
      "step: 21600 , time : 0.0\n",
      "train: loss: 782736.375 acc: 0.897879958152771  val: loss: 589059.4375 acc: 0.715607762336731\n",
      "step: 21605 , time : 0.0\n",
      "train: loss: 191386.65625 acc: 0.9776389002799988  val: loss: 1083315.25 acc: 0.590869665145874\n",
      "step: 21610 , time : 0.0\n",
      "train: loss: 159602.078125 acc: 0.9842690825462341  val: loss: 3023438.0 acc: 0.006635546684265137\n",
      "step: 21615 , time : 0.0\n",
      "train: loss: 87416.09375 acc: 0.9918360114097595  val: loss: 1194212.25 acc: 0.6608203053474426\n",
      "step: 21620 , time : 0.0\n",
      "train: loss: 141257.515625 acc: 0.9873668551445007  val: loss: 2463963.75 acc: 0.33678948879241943\n",
      "step: 21625 , time : 0.0\n",
      "train: loss: 1242894.25 acc: 0.9120664596557617  val: loss: 486920.03125 acc: 0.9136888980865479\n",
      "step: 21630 , time : 0.0\n",
      "train: loss: 443127.09375 acc: 0.9613529443740845  val: loss: 2378934.0 acc: 0.35915809869766235\n",
      "step: 21635 , time : 0.0\n",
      "train: loss: 447886.25 acc: 0.9709043502807617  val: loss: 1992037.75 acc: 0.7006409168243408\n",
      "step: 21640 , time : 0.0\n",
      "train: loss: 791303.625 acc: 0.940083384513855  val: loss: 1452351.0 acc: 0.7415540218353271\n",
      "step: 21645 , time : 0.0\n",
      "train: loss: 2198913.75 acc: 0.9421460032463074  val: loss: 472375.90625 acc: 0.9208728075027466\n",
      "step: 21650 , time : 0.0010006427764892578\n",
      "train: loss: 486493.8125 acc: 0.9800583720207214  val: loss: 2233516.0 acc: 0.6332710981369019\n",
      "step: 21655 , time : 0.0\n",
      "train: loss: 743673.0 acc: 0.9334813356399536  val: loss: 842300.25 acc: 0.8406676650047302\n",
      "step: 21660 , time : 8.606910705566406e-05\n",
      "train: loss: 468008.6875 acc: 0.9583203792572021  val: loss: 1014198.6875 acc: 0.7548413276672363\n",
      "step: 21665 , time : 0.0\n",
      "train: loss: 247152.34375 acc: 0.9779041409492493  val: loss: 1320079.0 acc: 0.8117601275444031\n",
      "step: 21670 , time : 0.0\n",
      "train: loss: 340361.5 acc: 0.9309283494949341  val: loss: 169083.96875 acc: 0.9411633014678955\n",
      "step: 21675 , time : 0.0\n",
      "train: loss: 2033114.625 acc: 0.6401376724243164  val: loss: 510270.3125 acc: 0.8643035888671875\n",
      "step: 21680 , time : 0.0\n",
      "train: loss: 2529220.0 acc: 0.792515754699707  val: loss: 963643.625 acc: 0.7606282234191895\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 21685 , time : 0.0\n",
      "train: loss: 832962.125 acc: 0.6892349720001221  val: loss: 591381.875 acc: 0.8388495445251465\n",
      "step: 21690 , time : 0.001001119613647461\n",
      "train: loss: 510713.4375 acc: 0.8158890008926392  val: loss: 1150255.0 acc: 0.8554122447967529\n",
      "step: 21695 , time : 0.0\n",
      "train: loss: 828601.0 acc: 0.7102852463722229  val: loss: 990504.1875 acc: 0.6877188682556152\n",
      "step: 21700 , time : 0.0\n",
      "train: loss: 1230708.125 acc: 0.6012060046195984  val: loss: 1283039.75 acc: 0.45806360244750977\n",
      "step: 21705 , time : 0.001001119613647461\n",
      "train: loss: 402501.1875 acc: 0.7192167043685913  val: loss: 2086549.25 acc: 0.5259177088737488\n",
      "step: 21710 , time : 0.0010004043579101562\n",
      "train: loss: 169677.59375 acc: 0.8839262127876282  val: loss: 2010895.75 acc: 0.6516669988632202\n",
      "step: 21715 , time : 0.0010006427764892578\n",
      "train: loss: 684014.125 acc: 0.7195119857788086  val: loss: 849538.625 acc: 0.6371806859970093\n",
      "step: 21720 , time : 0.001001119613647461\n",
      "train: loss: 238014.859375 acc: 0.8215509653091431  val: loss: 2300275.25 acc: 0.5941148996353149\n",
      "step: 21725 , time : 0.0010006427764892578\n",
      "train: loss: 82846.2734375 acc: 0.9290628433227539  val: loss: 1175637.0 acc: 0.664085865020752\n",
      "step: 21730 , time : 0.0\n",
      "train: loss: 74820.53125 acc: 0.938411295413971  val: loss: 443790.5 acc: 0.7087604999542236\n",
      "step: 21735 , time : 0.0\n",
      "train: loss: 224992.71875 acc: 0.7998303771018982  val: loss: 4776561.5 acc: 0.5208220481872559\n",
      "step: 21740 , time : 0.0\n",
      "train: loss: 90155.0625 acc: 0.9259163737297058  val: loss: 3580983.75 acc: 0.5338162183761597\n",
      "step: 21745 , time : 0.0\n",
      "train: loss: 93105.671875 acc: 0.9188889861106873  val: loss: 874715.5 acc: 0.7437942028045654\n",
      "step: 21750 , time : 0.0010013580322265625\n",
      "train: loss: 110667.4375 acc: 0.8842030167579651  val: loss: 1855634.75 acc: 0.5412189960479736\n",
      "step: 21755 , time : 0.0010006427764892578\n",
      "train: loss: 181890.8125 acc: 0.867607831954956  val: loss: 801077.0 acc: 0.6888197064399719\n",
      "step: 21760 , time : 0.0010006427764892578\n",
      "train: loss: 86999.6796875 acc: 0.9125797748565674  val: loss: 4114323.0 acc: 0.5614104270935059\n",
      "step: 21765 , time : 0.0\n",
      "train: loss: 433114.3125 acc: 0.7931420803070068  val: loss: 1258882.875 acc: 0.6460083723068237\n",
      "step: 21770 , time : 0.0010013580322265625\n",
      "train: loss: 869748.625 acc: 0.6921840906143188  val: loss: 2539995.5 acc: 0.5898107290267944\n",
      "step: 21775 , time : 0.0010001659393310547\n",
      "train: loss: 356468.84375 acc: 0.8138521313667297  val: loss: 3031494.5 acc: 0.5481406450271606\n",
      "step: 21780 , time : 0.001001119613647461\n",
      "train: loss: 258822.53125 acc: 0.8290414214134216  val: loss: 2509844.5 acc: 0.6099399328231812\n",
      "step: 21785 , time : 0.0\n",
      "train: loss: 1746781.0 acc: 0.6586627960205078  val: loss: 3547442.75 acc: 0.5627692937850952\n",
      "step: 21790 , time : 0.0010006427764892578\n",
      "train: loss: 1773149.5 acc: 0.7393019199371338  val: loss: 1242309.75 acc: 0.7044367790222168\n",
      "step: 21795 , time : 0.0\n",
      "train: loss: 1836178.375 acc: 0.815656304359436  val: loss: 2028309.375 acc: 0.8199210166931152\n",
      "step: 21800 , time : 0.0\n",
      "train: loss: 531712.75 acc: 0.9482111930847168  val: loss: 1455320.25 acc: 0.8530479669570923\n",
      "step: 21805 , time : 0.0\n",
      "train: loss: 201344.296875 acc: 0.9749742150306702  val: loss: 1000876.25 acc: 0.8721591830253601\n",
      "step: 21810 , time : 0.0\n",
      "train: loss: 258097.0625 acc: 0.9626168608665466  val: loss: 3537571.25 acc: 0.7352864742279053\n",
      "step: 21815 , time : 0.0009999275207519531\n",
      "train: loss: 185158.328125 acc: 0.9666007161140442  val: loss: 438983.6875 acc: 0.9660761952400208\n",
      "step: 21820 , time : 0.0\n",
      "train: loss: 91042.2421875 acc: 0.9926803708076477  val: loss: 733358.9375 acc: 0.9601783752441406\n",
      "step: 21825 , time : 0.0\n",
      "train: loss: 106992.7734375 acc: 0.9926190376281738  val: loss: 329464.3125 acc: 0.9371069669723511\n",
      "step: 21830 , time : 0.0\n",
      "train: loss: 153415.34375 acc: 0.9873799681663513  val: loss: 583902.5 acc: 0.7850670218467712\n",
      "step: 21835 , time : 0.0010006427764892578\n",
      "train: loss: 54503.06640625 acc: 0.9900001883506775  val: loss: 1656120.375 acc: 0.3249073028564453\n",
      "step: 21840 , time : 0.0\n",
      "train: loss: 88003.328125 acc: 0.9732403755187988  val: loss: 1819143.75 acc: -0.9793353080749512\n",
      "step: 21845 , time : 0.001001119613647461\n",
      "train: loss: 13457.7119140625 acc: 0.9944925904273987  val: loss: 1231656.75 acc: 0.8750324249267578\n",
      "step: 21850 , time : 0.0\n",
      "train: loss: 12073.4111328125 acc: 0.9663422703742981  val: loss: 1749517.0 acc: 0.12246567010879517\n",
      "step: 21855 , time : 0.0\n",
      "train: loss: 12337.0517578125 acc: 0.9670928716659546  val: loss: 2570710.0 acc: 0.16988646984100342\n",
      "step: 21860 , time : 0.0\n",
      "train: loss: 12122.36328125 acc: 0.9877749085426331  val: loss: 531473.8125 acc: 0.8709216117858887\n",
      "step: 21865 , time : 0.0010008811950683594\n",
      "train: loss: 18538.162109375 acc: 0.961832582950592  val: loss: 1111688.75 acc: 0.871552586555481\n",
      "step: 21870 , time : 0.0\n",
      "train: loss: 16065.9267578125 acc: 0.966564416885376  val: loss: 911678.5 acc: 0.7865920662879944\n",
      "step: 21875 , time : 0.0010004043579101562\n",
      "train: loss: 19007.396484375 acc: 0.9534804821014404  val: loss: 1759654.0 acc: 0.7725667953491211\n",
      "step: 21880 , time : 0.0010006427764892578\n",
      "train: loss: 7501.33740234375 acc: 0.9777810573577881  val: loss: 678645.6875 acc: 0.8359699845314026\n",
      "step: 21885 , time : 0.0\n",
      "train: loss: 9325.251953125 acc: 0.9785347580909729  val: loss: 711824.6875 acc: 0.8426891565322876\n",
      "step: 21890 , time : 0.0\n",
      "train: loss: 20267.41796875 acc: 0.9894144535064697  val: loss: 1734919.875 acc: 0.03361016511917114\n",
      "step: 21895 , time : 0.0\n",
      "train: loss: 28783.03125 acc: 0.9745866060256958  val: loss: 1648644.75 acc: 0.7240614891052246\n",
      "step: 21900 , time : 0.0\n",
      "train: loss: 21204.939453125 acc: 0.9840403199195862  val: loss: 2060217.75 acc: 0.545782744884491\n",
      "step: 21905 , time : 0.0\n",
      "train: loss: 19493.7890625 acc: 0.9917179942131042  val: loss: 1096455.875 acc: 0.5703034996986389\n",
      "step: 21910 , time : 0.0\n",
      "train: loss: 17229.767578125 acc: 0.9866420030593872  val: loss: 2291906.5 acc: 0.5509580373764038\n",
      "step: 21915 , time : 0.0\n",
      "train: loss: 39752.25 acc: 0.9638650417327881  val: loss: 757320.75 acc: 0.8044284582138062\n",
      "step: 21920 , time : 0.0\n",
      "train: loss: 28946.572265625 acc: 0.9867421388626099  val: loss: 2211758.0 acc: 0.7584288120269775\n",
      "step: 21925 , time : 0.0\n",
      "train: loss: 53144.16796875 acc: 0.9849302768707275  val: loss: 2427951.0 acc: 0.41151392459869385\n",
      "step: 21930 , time : 0.0\n",
      "train: loss: 41960.2421875 acc: 0.9890354871749878  val: loss: 838708.0 acc: 0.7522700428962708\n",
      "step: 21935 , time : 0.0\n",
      "train: loss: 44950.54296875 acc: 0.9805040955543518  val: loss: 2771690.75 acc: 0.29583919048309326\n",
      "step: 21940 , time : 0.0\n",
      "train: loss: 46269.2890625 acc: 0.9881726503372192  val: loss: 1023349.125 acc: 0.7742018699645996\n",
      "step: 21945 , time : 0.0\n",
      "train: loss: 67012.359375 acc: 0.974033534526825  val: loss: 2752124.5 acc: 0.26921868324279785\n",
      "step: 21950 , time : 0.0\n",
      "train: loss: 364468.375 acc: 0.8691726326942444  val: loss: 2274580.0 acc: -0.06089472770690918\n",
      "step: 21955 , time : 0.0\n",
      "train: loss: 87875.953125 acc: 0.9270344376564026  val: loss: 1823886.375 acc: 0.7187420129776001\n",
      "step: 21960 , time : 0.0\n",
      "train: loss: 178560.59375 acc: 0.9722898602485657  val: loss: 871044.125 acc: 0.8973823189735413\n",
      "step: 21965 , time : 0.0\n",
      "train: loss: 138298.46875 acc: 0.9739911556243896  val: loss: 3970385.75 acc: 0.5733504295349121\n",
      "step: 21970 , time : 0.0\n",
      "train: loss: 161852.84375 acc: 0.9879812598228455  val: loss: 1152506.75 acc: 0.8384511470794678\n",
      "step: 21975 , time : 0.0\n",
      "train: loss: 253064.03125 acc: 0.9627094268798828  val: loss: 1384519.5 acc: 0.8170008659362793\n",
      "step: 21980 , time : 0.0\n",
      "train: loss: 261651.15625 acc: 0.9607605934143066  val: loss: 724689.875 acc: 0.686528205871582\n",
      "step: 21985 , time : 0.0\n",
      "train: loss: 261283.171875 acc: 0.967124879360199  val: loss: 811054.25 acc: 0.8432359099388123\n",
      "step: 21990 , time : 0.0\n",
      "train: loss: 680903.9375 acc: 0.9542384147644043  val: loss: 1186285.875 acc: 0.783683180809021\n",
      "step: 21995 , time : 0.0010006427764892578\n",
      "train: loss: 214777.109375 acc: 0.9822221398353577  val: loss: 1070442.25 acc: 0.48443347215652466\n",
      "step: 22000 , time : 0.0\n",
      "train: loss: 487198.0 acc: 0.930670976638794  val: loss: 2799823.75 acc: 0.24309885501861572\n",
      "step: 22005 , time : 0.0010004043579101562\n",
      "train: loss: 723703.0 acc: 0.9592966437339783  val: loss: 789894.6875 acc: 0.832449197769165\n",
      "step: 22010 , time : 0.0\n",
      "train: loss: 2086722.375 acc: 0.9447271823883057  val: loss: 913730.375 acc: 0.7790026068687439\n",
      "step: 22015 , time : 0.0\n",
      "train: loss: 1425909.75 acc: 0.9078903198242188  val: loss: 297586.6875 acc: 0.9234739542007446\n",
      "step: 22020 , time : 0.0\n",
      "train: loss: 503009.3125 acc: 0.970570981502533  val: loss: 1072663.25 acc: 0.7850117683410645\n",
      "step: 22025 , time : 0.0\n",
      "train: loss: 102983.7265625 acc: 0.9911885857582092  val: loss: 1114402.75 acc: 0.6130862236022949\n",
      "step: 22030 , time : 0.0\n",
      "train: loss: 284950.0625 acc: 0.9710496664047241  val: loss: 1905827.5 acc: -0.11983072757720947\n",
      "step: 22035 , time : 0.0\n",
      "train: loss: 505087.4375 acc: 0.9430680871009827  val: loss: 477365.8125 acc: 0.8784307837486267\n",
      "step: 22040 , time : 0.0\n",
      "train: loss: 2465850.25 acc: 0.4957273602485657  val: loss: 306138.96875 acc: 0.812025785446167\n",
      "step: 22045 , time : 0.0010008811950683594\n",
      "train: loss: 1788950.25 acc: 0.5019420981407166  val: loss: 1444652.5 acc: 0.8280264139175415\n",
      "step: 22050 , time : 0.0\n",
      "train: loss: 948206.6875 acc: 0.6764433979988098  val: loss: 957726.5625 acc: 0.7755115032196045\n",
      "step: 22055 , time : 0.0010008811950683594\n",
      "train: loss: 272292.78125 acc: 0.9214543700218201  val: loss: 829757.625 acc: 0.654842734336853\n",
      "step: 22060 , time : 0.0010013580322265625\n",
      "train: loss: 653129.9375 acc: 0.8250662684440613  val: loss: 626571.75 acc: 0.867912232875824\n",
      "step: 22065 , time : 0.001001119613647461\n",
      "train: loss: 1643065.5 acc: -0.13235712051391602  val: loss: 592417.9375 acc: 0.7549189925193787\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 22070 , time : 0.0010008811950683594\n",
      "train: loss: 643970.25 acc: 0.6506073474884033  val: loss: 1257025.75 acc: 0.7022109031677246\n",
      "step: 22075 , time : 0.0\n",
      "train: loss: 200478.84375 acc: 0.8370859026908875  val: loss: 4246050.5 acc: 0.6252915859222412\n",
      "step: 22080 , time : 0.0010004043579101562\n",
      "train: loss: 295466.9375 acc: 0.7665464282035828  val: loss: 1852035.125 acc: 0.628921627998352\n",
      "step: 22085 , time : 0.0\n",
      "train: loss: 59575.66015625 acc: 0.9490088820457458  val: loss: 1983817.5 acc: 0.6407428979873657\n",
      "step: 22090 , time : 0.0\n",
      "train: loss: 23219.53125 acc: 0.9790798425674438  val: loss: 1220476.75 acc: 0.6700918674468994\n",
      "step: 22095 , time : 0.0010004043579101562\n",
      "train: loss: 175661.171875 acc: 0.8931357860565186  val: loss: 960800.0 acc: 0.6761860847473145\n",
      "step: 22100 , time : 0.0010006427764892578\n",
      "train: loss: 156720.171875 acc: 0.8834881782531738  val: loss: 977932.1875 acc: 0.7372319102287292\n",
      "step: 22105 , time : 0.0010004043579101562\n",
      "train: loss: 131134.34375 acc: 0.9145911335945129  val: loss: 180048.390625 acc: 0.8377688527107239\n",
      "step: 22110 , time : 0.0\n",
      "train: loss: 337500.53125 acc: 0.824689507484436  val: loss: 1916663.5 acc: 0.6161702275276184\n",
      "step: 22115 , time : 0.0010004043579101562\n",
      "train: loss: 291069.125 acc: 0.793590247631073  val: loss: 4866145.0 acc: 0.5267758369445801\n",
      "step: 22120 , time : 0.0010006427764892578\n",
      "train: loss: 44402.21875 acc: 0.9278912544250488  val: loss: 2980954.25 acc: 0.6576551795005798\n",
      "step: 22125 , time : 0.001001119613647461\n",
      "train: loss: 83807.5234375 acc: 0.909968376159668  val: loss: 1438032.5 acc: 0.6191593408584595\n",
      "step: 22130 , time : 0.0\n",
      "train: loss: 257266.09375 acc: 0.8270459771156311  val: loss: 4162682.75 acc: 0.5147314071655273\n",
      "step: 22135 , time : 0.0\n",
      "train: loss: 511096.0625 acc: 0.7721754312515259  val: loss: 3880604.5 acc: 0.537917971611023\n",
      "step: 22140 , time : 0.0010006427764892578\n",
      "train: loss: 680222.6875 acc: 0.7408855557441711  val: loss: 2548182.5 acc: 0.6364222764968872\n",
      "step: 22145 , time : 0.0010008811950683594\n",
      "train: loss: 171692.5625 acc: 0.8854749202728271  val: loss: 2140508.0 acc: 0.5947762727737427\n",
      "step: 22150 , time : 0.0\n",
      "train: loss: 627925.9375 acc: 0.7468019723892212  val: loss: 3320408.5 acc: 0.5580576658248901\n",
      "step: 22155 , time : 0.0\n",
      "train: loss: 1247779.375 acc: 0.7390933632850647  val: loss: 972193.0625 acc: 0.6745479106903076\n",
      "step: 22160 , time : 0.0020020008087158203\n",
      "train: loss: 1402516.25 acc: 0.8135787844657898  val: loss: 473121.5625 acc: 0.8535329103469849\n",
      "step: 22165 , time : 0.0\n",
      "train: loss: 943729.0625 acc: 0.8933095335960388  val: loss: 1909610.0 acc: 0.5506480932235718\n",
      "step: 22170 , time : 0.0010004043579101562\n",
      "train: loss: 471975.09375 acc: 0.9660513997077942  val: loss: 1911280.25 acc: 0.6730462312698364\n",
      "step: 22175 , time : 0.001001119613647461\n",
      "train: loss: 526788.0 acc: 0.8823145031929016  val: loss: 915218.8125 acc: 0.9258577227592468\n",
      "step: 22180 , time : 0.0\n",
      "train: loss: 149547.984375 acc: 0.9805651307106018  val: loss: 1306761.0 acc: 0.7996065020561218\n",
      "step: 22185 , time : 0.0010006427764892578\n",
      "train: loss: 114021.421875 acc: 0.9888179898262024  val: loss: 946878.0 acc: 0.630979061126709\n",
      "step: 22190 , time : 0.0\n",
      "train: loss: 116624.890625 acc: 0.9924046397209167  val: loss: 1695447.0 acc: 0.708511233329773\n",
      "step: 22195 , time : 0.0010006427764892578\n",
      "train: loss: 83397.4375 acc: 0.9923627376556396  val: loss: 869954.9375 acc: 0.8605636358261108\n",
      "step: 22200 , time : 0.0010004043579101562\n",
      "train: loss: 60492.953125 acc: 0.98993319272995  val: loss: 442291.03125 acc: 0.9256531000137329\n",
      "step: 22205 , time : 0.0\n",
      "train: loss: 6295.74267578125 acc: 0.9968159198760986  val: loss: 1441870.375 acc: 0.7793256044387817\n",
      "step: 22210 , time : 0.0\n",
      "train: loss: 22865.095703125 acc: 0.991996169090271  val: loss: 195846.46875 acc: 0.9529889822006226\n",
      "step: 22215 , time : 0.0\n",
      "train: loss: 23017.994140625 acc: 0.9862001538276672  val: loss: 1457594.75 acc: 0.8494347333908081\n",
      "step: 22220 , time : 0.0\n",
      "train: loss: 17113.958984375 acc: 0.9591386318206787  val: loss: 691222.0625 acc: 0.9255850911140442\n",
      "step: 22225 , time : 0.0\n",
      "train: loss: 5934.35546875 acc: 0.9922172427177429  val: loss: 2614124.5 acc: 0.7346171140670776\n",
      "step: 22230 , time : 0.0\n",
      "train: loss: 172593.53125 acc: 0.7382827997207642  val: loss: 938813.8125 acc: 0.7248495817184448\n",
      "step: 22235 , time : 0.0\n",
      "train: loss: 16587.857421875 acc: 0.9465420246124268  val: loss: 2072921.75 acc: 0.5933851599693298\n",
      "step: 22240 , time : 0.0009999275207519531\n",
      "train: loss: 25423.044921875 acc: 0.9782440066337585  val: loss: 1656510.625 acc: 0.6468218564987183\n",
      "step: 22245 , time : 0.0\n",
      "train: loss: 11013.875 acc: 0.9700229167938232  val: loss: 2375737.5 acc: 0.5790446400642395\n",
      "step: 22250 , time : 0.0\n",
      "train: loss: 10421.099609375 acc: 0.9541743397712708  val: loss: 1483247.375 acc: 0.6333557367324829\n",
      "step: 22255 , time : 0.0\n",
      "train: loss: 59584.515625 acc: 0.971394419670105  val: loss: 1588861.0 acc: 0.8329243659973145\n",
      "step: 22260 , time : 0.015624761581420898\n",
      "train: loss: 20586.35546875 acc: 0.9869161248207092  val: loss: 2097811.5 acc: 0.4575977921485901\n",
      "step: 22265 , time : 0.0\n",
      "train: loss: 24950.869140625 acc: 0.9754434823989868  val: loss: 854118.9375 acc: 0.8581814169883728\n",
      "step: 22270 , time : 0.0\n",
      "train: loss: 27751.923828125 acc: 0.9903777241706848  val: loss: 2080611.75 acc: 0.7631904482841492\n",
      "step: 22275 , time : 0.0\n",
      "train: loss: 18748.984375 acc: 0.9852186441421509  val: loss: 776244.5625 acc: 0.8233807682991028\n",
      "step: 22280 , time : 0.0010004043579101562\n",
      "train: loss: 21153.7109375 acc: 0.9859357476234436  val: loss: 772776.3125 acc: 0.8205481767654419\n",
      "step: 22285 , time : 0.0\n",
      "train: loss: 28649.640625 acc: 0.9753987193107605  val: loss: 2544908.25 acc: 0.1865476369857788\n",
      "step: 22290 , time : 0.0\n",
      "train: loss: 51012.109375 acc: 0.9843067526817322  val: loss: 1801982.0 acc: 0.632758617401123\n",
      "step: 22295 , time : 0.0010006427764892578\n",
      "train: loss: 28023.947265625 acc: 0.992085337638855  val: loss: 498617.03125 acc: 0.7662054300308228\n",
      "step: 22300 , time : 0.0010006427764892578\n",
      "train: loss: 22688.474609375 acc: 0.9915310740470886  val: loss: 1902844.125 acc: 0.2963746190071106\n",
      "step: 22305 , time : 0.0\n",
      "train: loss: 32265.720703125 acc: 0.9713901281356812  val: loss: 1188781.75 acc: 0.8238797187805176\n",
      "step: 22310 , time : 0.0\n",
      "train: loss: 49432.78515625 acc: 0.9675695300102234  val: loss: 807355.75 acc: 0.7653435468673706\n",
      "step: 22315 , time : 0.0\n",
      "train: loss: 227514.96875 acc: 0.9411899447441101  val: loss: 655936.25 acc: 0.8690259456634521\n",
      "step: 22320 , time : 0.0\n",
      "train: loss: 165914.515625 acc: 0.9379861354827881  val: loss: 1188421.5 acc: 0.6986731886863708\n",
      "step: 22325 , time : 0.0\n",
      "train: loss: 212832.625 acc: 0.8479838371276855  val: loss: 1271136.375 acc: 0.7472829818725586\n",
      "step: 22330 , time : 0.0\n",
      "train: loss: 1231207.5 acc: 0.8396323919296265  val: loss: 2132344.5 acc: 0.4249175190925598\n",
      "step: 22335 , time : 0.0\n",
      "train: loss: 141583.484375 acc: 0.9896934628486633  val: loss: 143724.734375 acc: 0.9412954449653625\n",
      "step: 22340 , time : 0.0\n",
      "train: loss: 57031.328125 acc: 0.992291271686554  val: loss: 711243.4375 acc: 0.8954684138298035\n",
      "step: 22345 , time : 0.0\n",
      "train: loss: 137755.109375 acc: 0.9794155955314636  val: loss: 461697.28125 acc: 0.8680867552757263\n",
      "step: 22350 , time : 0.0\n",
      "train: loss: 144723.328125 acc: 0.9844802618026733  val: loss: 837271.1875 acc: 0.6970313787460327\n",
      "step: 22355 , time : 0.0\n",
      "train: loss: 541494.1875 acc: 0.9726666808128357  val: loss: 400792.625 acc: 0.9105260372161865\n",
      "step: 22360 , time : 0.0010008811950683594\n",
      "train: loss: 314279.3125 acc: 0.9710502624511719  val: loss: 829095.6875 acc: 0.34315985441207886\n",
      "step: 22365 , time : 0.0010006427764892578\n",
      "train: loss: 264683.0 acc: 0.97347092628479  val: loss: 508043.9375 acc: 0.939774215221405\n",
      "step: 22370 , time : 0.0010004043579101562\n",
      "train: loss: 641202.125 acc: 0.9743111729621887  val: loss: 875889.0625 acc: 0.8217887878417969\n",
      "step: 22375 , time : 0.0\n",
      "train: loss: 418700.59375 acc: 0.9839807152748108  val: loss: 1574815.75 acc: 0.09612494707107544\n",
      "step: 22380 , time : 0.0\n",
      "train: loss: 2181722.5 acc: 0.9192415475845337  val: loss: 490219.15625 acc: 0.8883966207504272\n",
      "step: 22385 , time : 0.0\n",
      "train: loss: 867429.0625 acc: 0.9667759537696838  val: loss: 208826.609375 acc: 0.9582251906394958\n",
      "step: 22390 , time : 0.0010006427764892578\n",
      "train: loss: 363208.9375 acc: 0.9692842364311218  val: loss: 207225.890625 acc: 0.9657657742500305\n",
      "step: 22395 , time : 0.001001119613647461\n",
      "train: loss: 549725.375 acc: 0.8856236934661865  val: loss: 264279.5 acc: 0.9377701878547668\n",
      "step: 22400 , time : 0.0\n",
      "train: loss: 1001280.5625 acc: 0.9013493061065674  val: loss: 776247.5625 acc: 0.8662064671516418\n",
      "step: 22405 , time : 0.0\n",
      "train: loss: 969629.6875 acc: 0.6473877429962158  val: loss: 997173.0625 acc: 0.8326284289360046\n",
      "step: 22410 , time : 0.0\n",
      "train: loss: 719035.5 acc: 0.6474838256835938  val: loss: 742231.0 acc: 0.8865038156509399\n",
      "step: 22415 , time : 0.0\n",
      "train: loss: 563988.0625 acc: 0.8047271370887756  val: loss: 1109157.0 acc: 0.8209380507469177\n",
      "step: 22420 , time : 0.0010004043579101562\n",
      "train: loss: 129912.609375 acc: 0.8787110447883606  val: loss: 1276710.375 acc: 0.7565209865570068\n",
      "step: 22425 , time : 0.0010006427764892578\n",
      "train: loss: 716732.75 acc: 0.8781481981277466  val: loss: 1699838.625 acc: 0.8512128591537476\n",
      "step: 22430 , time : 0.0\n",
      "train: loss: 1242060.0 acc: 0.34371817111968994  val: loss: 1828945.25 acc: 0.8255232572555542\n",
      "step: 22435 , time : 0.0010006427764892578\n",
      "train: loss: 440246.4375 acc: 0.7142332792282104  val: loss: 4013379.5 acc: 0.6504663228988647\n",
      "step: 22440 , time : 0.0\n",
      "train: loss: 177167.15625 acc: 0.8871556520462036  val: loss: 1482419.0 acc: 0.6822507381439209\n",
      "step: 22445 , time : 0.0\n",
      "train: loss: 403875.59375 acc: 0.7886195182800293  val: loss: 1725709.75 acc: 0.6871323585510254\n",
      "step: 22450 , time : 0.0\n",
      "train: loss: 275876.34375 acc: 0.8459356427192688  val: loss: 4195744.0 acc: 0.5909883975982666\n",
      "step: 22455 , time : 0.0\n",
      "train: loss: 33010.0078125 acc: 0.9665660858154297  val: loss: 2606983.75 acc: 0.6580405831336975\n",
      "step: 22460 , time : 0.0010008811950683594\n",
      "train: loss: 139583.34375 acc: 0.8784172534942627  val: loss: 2188516.5 acc: 0.6623712182044983\n",
      "step: 22465 , time : 0.0\n",
      "train: loss: 165247.15625 acc: 0.8937482833862305  val: loss: 263186.03125 acc: 0.8052542805671692\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 22470 , time : 0.0010008811950683594\n",
      "train: loss: 66120.25 acc: 0.9479182958602905  val: loss: 2503978.5 acc: 0.623614490032196\n",
      "step: 22475 , time : 0.0\n",
      "train: loss: 84525.0703125 acc: 0.930350661277771  val: loss: 8054670.5 acc: 0.46644043922424316\n",
      "step: 22480 , time : 0.0\n",
      "train: loss: 157740.484375 acc: 0.7276501655578613  val: loss: 2769347.5 acc: 0.6263441443443298\n",
      "step: 22485 , time : 0.0010006427764892578\n",
      "train: loss: 66824.1953125 acc: 0.923385500907898  val: loss: 918296.25 acc: 0.7036248445510864\n",
      "step: 22490 , time : 0.0\n",
      "train: loss: 219828.421875 acc: 0.7302840948104858  val: loss: 4748057.5 acc: 0.5654883980751038\n",
      "step: 22495 , time : 0.0010006427764892578\n",
      "train: loss: 533004.875 acc: 0.7703292369842529  val: loss: 1609603.875 acc: 0.6112180948257446\n",
      "step: 22500 , time : 0.0\n",
      "train: loss: 215991.890625 acc: 0.8502219915390015  val: loss: 3417422.75 acc: 0.5930442810058594\n",
      "step: 22505 , time : 0.0010004043579101562\n",
      "train: loss: 469717.78125 acc: 0.7479521632194519  val: loss: 1051528.5 acc: 0.7425481677055359\n",
      "step: 22510 , time : 0.0\n",
      "train: loss: 132388.6875 acc: 0.8884293437004089  val: loss: 631475.9375 acc: 0.7070826888084412\n",
      "step: 22515 , time : 0.0010008811950683594\n",
      "train: loss: 150380.9375 acc: 0.8324790596961975  val: loss: 160860.140625 acc: 0.8927782773971558\n",
      "step: 22520 , time : 0.0\n",
      "train: loss: 1095241.75 acc: 0.7327654361724854  val: loss: 252947.03125 acc: 0.8946225643157959\n",
      "step: 22525 , time : 0.0010006427764892578\n",
      "train: loss: 1442885.5 acc: 0.8051633834838867  val: loss: 1547786.0 acc: 0.7296805381774902\n",
      "step: 22530 , time : 0.0\n",
      "train: loss: 344500.375 acc: 0.9758841395378113  val: loss: 1537608.125 acc: 0.6760034561157227\n",
      "step: 22535 , time : 0.0010008811950683594\n",
      "train: loss: 246201.53125 acc: 0.9651171565055847  val: loss: 1608269.5 acc: 0.47317826747894287\n",
      "step: 22540 , time : 0.0\n",
      "train: loss: 99607.5625 acc: 0.9724797010421753  val: loss: 2429913.5 acc: 0.3981534242630005\n",
      "step: 22545 , time : 0.0\n",
      "train: loss: 83168.125 acc: 0.9877613186836243  val: loss: 585340.75 acc: 0.7429535388946533\n",
      "step: 22550 , time : 0.0010006427764892578\n",
      "train: loss: 104465.984375 acc: 0.9892301559448242  val: loss: 2170555.75 acc: 0.21003299951553345\n",
      "step: 22555 , time : 0.0\n",
      "train: loss: 63521.484375 acc: 0.9953171610832214  val: loss: 1885740.0 acc: 0.5638140439987183\n",
      "step: 22560 , time : 0.0\n",
      "train: loss: 62662.75390625 acc: 0.9949259161949158  val: loss: 565156.875 acc: 0.8471392393112183\n",
      "step: 22565 , time : 0.001001119613647461\n",
      "train: loss: 93266.3359375 acc: 0.9924001693725586  val: loss: 1768543.625 acc: 0.7935147285461426\n",
      "step: 22570 , time : 0.0\n",
      "train: loss: 41514.77734375 acc: 0.9939882755279541  val: loss: 1168397.625 acc: 0.7089755535125732\n",
      "step: 22575 , time : 0.0\n",
      "train: loss: 47983.3984375 acc: 0.9815254807472229  val: loss: 1382377.875 acc: 0.7960137724876404\n",
      "step: 22580 , time : 0.0010006427764892578\n",
      "train: loss: 14259.3037109375 acc: 0.9897483587265015  val: loss: 340896.21875 acc: 0.9211697578430176\n",
      "step: 22585 , time : 0.0\n",
      "train: loss: 20054.60546875 acc: 0.9862619638442993  val: loss: 1524176.0 acc: 0.6667760014533997\n",
      "step: 22590 , time : 0.0010004043579101562\n",
      "train: loss: 21172.220703125 acc: 0.980208158493042  val: loss: 710901.625 acc: 0.8723667860031128\n",
      "step: 22595 , time : 0.0010008811950683594\n",
      "train: loss: 17672.28125 acc: 0.9695706963539124  val: loss: 561887.9375 acc: 0.9130765795707703\n",
      "step: 22600 , time : 0.0\n",
      "train: loss: 8697.8212890625 acc: 0.9697387218475342  val: loss: 186699.078125 acc: 0.9108133316040039\n",
      "step: 22605 , time : 0.0\n",
      "train: loss: 16827.380859375 acc: 0.9663786888122559  val: loss: 448972.875 acc: 0.8347655534744263\n",
      "step: 22610 , time : 0.0\n",
      "train: loss: 9283.830078125 acc: 0.9826747179031372  val: loss: 673538.5625 acc: 0.8388617634773254\n",
      "step: 22615 , time : 0.0010008811950683594\n",
      "train: loss: 15493.583984375 acc: 0.9654569625854492  val: loss: 740336.3125 acc: 0.887356698513031\n",
      "step: 22620 , time : 0.0\n",
      "train: loss: 16280.349609375 acc: 0.9823758602142334  val: loss: 1163706.875 acc: 0.7778733968734741\n",
      "step: 22625 , time : 0.0\n",
      "train: loss: 33921.15625 acc: 0.9741374254226685  val: loss: 1406031.375 acc: 0.4885282516479492\n",
      "step: 22630 , time : 0.0\n",
      "train: loss: 30380.48046875 acc: 0.9739657044410706  val: loss: 660718.875 acc: 0.8596462607383728\n",
      "step: 22635 , time : 0.0\n",
      "train: loss: 13027.0546875 acc: 0.984260082244873  val: loss: 1550484.125 acc: 0.6816813945770264\n",
      "step: 22640 , time : 0.0010006427764892578\n",
      "train: loss: 46057.40234375 acc: 0.9820864200592041  val: loss: 762582.4375 acc: 0.9087461829185486\n",
      "step: 22645 , time : 0.0\n",
      "train: loss: 9943.73828125 acc: 0.988609790802002  val: loss: 216667.984375 acc: 0.9444087743759155\n",
      "step: 22650 , time : 0.0\n",
      "train: loss: 100760.9296875 acc: 0.6500189304351807  val: loss: 1792738.625 acc: 0.5518002510070801\n",
      "step: 22655 , time : 0.0010008811950683594\n",
      "train: loss: 24125.138671875 acc: 0.9925158619880676  val: loss: 396494.09375 acc: 0.9489829540252686\n",
      "step: 22660 , time : 0.0\n",
      "train: loss: 36447.86328125 acc: 0.9848323464393616  val: loss: 244707.96875 acc: 0.9448983073234558\n",
      "step: 22665 , time : 0.0\n",
      "train: loss: 50885.8359375 acc: 0.977967381477356  val: loss: 367407.78125 acc: 0.8648175001144409\n",
      "step: 22670 , time : 0.0010008811950683594\n",
      "train: loss: 35937.37109375 acc: 0.9900118112564087  val: loss: 417489.8125 acc: 0.7419436573982239\n",
      "step: 22675 , time : 0.001001119613647461\n",
      "train: loss: 26602.53125 acc: 0.9937509298324585  val: loss: 1171888.625 acc: 0.4822084903717041\n",
      "step: 22680 , time : 0.0\n",
      "train: loss: 334093.28125 acc: 0.8611124157905579  val: loss: 749399.125 acc: 0.6759653091430664\n",
      "step: 22685 , time : 0.0010008811950683594\n",
      "train: loss: 61058.390625 acc: 0.9685564637184143  val: loss: 161322.703125 acc: 0.9802467823028564\n",
      "step: 22690 , time : 0.0010008811950683594\n",
      "train: loss: 260059.625 acc: 0.9151594638824463  val: loss: 99614.8984375 acc: 0.969404935836792\n",
      "step: 22695 , time : 0.0\n",
      "train: loss: 475773.59375 acc: 0.8814189434051514  val: loss: 767858.5 acc: 0.789147138595581\n",
      "step: 22700 , time : 0.0010006427764892578\n",
      "train: loss: 90229.2890625 acc: 0.9878935217857361  val: loss: 2217671.25 acc: 0.10998654365539551\n",
      "step: 22705 , time : 0.0\n",
      "train: loss: 67458.40625 acc: 0.991866409778595  val: loss: 1055449.625 acc: 0.8036957383155823\n",
      "step: 22710 , time : 0.0\n",
      "train: loss: 78354.6875 acc: 0.9854711294174194  val: loss: 958007.625 acc: 0.8490565419197083\n",
      "step: 22715 , time : 0.0\n",
      "train: loss: 252982.65625 acc: 0.9501010179519653  val: loss: 1652513.375 acc: -0.2474883794784546\n",
      "step: 22720 , time : 0.0\n",
      "train: loss: 593707.0625 acc: 0.9443882703781128  val: loss: 897745.5 acc: 0.7344580888748169\n",
      "step: 22725 , time : 0.0\n",
      "train: loss: 589614.0625 acc: 0.943342924118042  val: loss: 486906.59375 acc: 0.703255295753479\n",
      "step: 22730 , time : 0.0\n",
      "train: loss: 186279.71875 acc: 0.9781968593597412  val: loss: 683710.1875 acc: 0.9120961427688599\n",
      "step: 22735 , time : 0.0\n",
      "train: loss: 907919.5625 acc: 0.9433833956718445  val: loss: 1789152.375 acc: 0.5651363730430603\n",
      "step: 22740 , time : 0.0\n",
      "train: loss: 1159462.375 acc: 0.9545815587043762  val: loss: 899502.375 acc: 0.7583506107330322\n",
      "step: 22745 , time : 0.0010008811950683594\n",
      "train: loss: 1449699.625 acc: 0.9438731670379639  val: loss: 559181.8125 acc: 0.9667327404022217\n",
      "step: 22750 , time : 0.0\n",
      "train: loss: 982391.9375 acc: 0.9498215317726135  val: loss: 862173.9375 acc: 0.9123359322547913\n",
      "step: 22755 , time : 0.0\n",
      "train: loss: 935561.9375 acc: 0.9423583745956421  val: loss: 1588246.75 acc: 0.7354304790496826\n",
      "step: 22760 , time : 0.0010008811950683594\n",
      "train: loss: 244492.6875 acc: 0.9712681174278259  val: loss: 471375.21875 acc: 0.9521970152854919\n",
      "step: 22765 , time : 0.0010006427764892578\n",
      "train: loss: 205617.3125 acc: 0.9703681468963623  val: loss: 787153.8125 acc: 0.91764897108078\n",
      "step: 22770 , time : 0.0\n",
      "train: loss: 297838.8125 acc: 0.9611954689025879  val: loss: 782390.8125 acc: 0.8686105012893677\n",
      "step: 22775 , time : 0.0010008811950683594\n",
      "train: loss: 1566975.5 acc: 0.4509333372116089  val: loss: 694989.6875 acc: 0.8540149331092834\n",
      "step: 22780 , time : 0.0\n",
      "train: loss: 867735.375 acc: 0.8913991451263428  val: loss: 1535188.5 acc: 0.6781256198883057\n",
      "step: 22785 , time : 0.0\n",
      "train: loss: 589170.375 acc: 0.7726249098777771  val: loss: 681163.75 acc: 0.9212559461593628\n",
      "step: 22790 , time : 0.0\n",
      "train: loss: 1036045.0 acc: 0.6991701126098633  val: loss: 570194.75 acc: 0.8232837319374084\n",
      "step: 22795 , time : 0.0\n",
      "train: loss: 1137251.75 acc: 0.49892812967300415  val: loss: 1477748.75 acc: 0.7762165069580078\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 22800 , time : 0.0\n",
      "train: loss: 865768.0625 acc: 0.7203184366226196  val: loss: 2501019.5 acc: 0.6632190942764282\n",
      "step: 22805 , time : 0.001001119613647461\n",
      "train: loss: 218708.0625 acc: 0.8675058484077454  val: loss: 3532358.25 acc: 0.5962703227996826\n",
      "step: 22810 , time : 0.0010004043579101562\n",
      "train: loss: 82177.640625 acc: 0.9254065752029419  val: loss: 3283436.5 acc: 0.6301039457321167\n",
      "step: 22815 , time : 0.0\n",
      "train: loss: 97735.4765625 acc: 0.9160139560699463  val: loss: 477388.75 acc: 0.7874791026115417\n",
      "step: 22820 , time : 0.0\n",
      "train: loss: 62830.3828125 acc: 0.9463495016098022  val: loss: 906414.1875 acc: 0.6963589191436768\n",
      "step: 22825 , time : 0.0\n",
      "train: loss: 68948.296875 acc: 0.9529533386230469  val: loss: 2020335.75 acc: 0.663623571395874\n",
      "step: 22830 , time : 0.0\n",
      "train: loss: 92961.984375 acc: 0.9162744283676147  val: loss: 5071611.0 acc: 0.549483060836792\n",
      "step: 22835 , time : 0.0\n",
      "train: loss: 655918.125 acc: 0.7612109184265137  val: loss: 2940639.5 acc: 0.5346975326538086\n",
      "step: 22840 , time : 0.0\n",
      "train: loss: 62179.296875 acc: 0.9546968340873718  val: loss: 1299335.25 acc: 0.6659564971923828\n",
      "step: 22845 , time : 0.015626192092895508\n",
      "train: loss: 70768.5625 acc: 0.9458287954330444  val: loss: 373178.53125 acc: 0.8026200532913208\n",
      "step: 22850 , time : 0.0\n",
      "train: loss: 283685.5625 acc: 0.8025606870651245  val: loss: 520872.71875 acc: 0.7509051561355591\n",
      "step: 22855 , time : 0.0\n",
      "train: loss: 173648.984375 acc: 0.8577508330345154  val: loss: 1405280.5 acc: 0.6842440366744995\n",
      "step: 22860 , time : 0.0\n",
      "train: loss: 520481.21875 acc: 0.7495541572570801  val: loss: 640348.5 acc: 0.7547605037689209\n",
      "step: 22865 , time : 0.0010006427764892578\n",
      "train: loss: 185023.5625 acc: 0.8612433671951294  val: loss: 4192446.25 acc: 0.5451005697250366\n",
      "step: 22870 , time : 0.0\n",
      "train: loss: 100140.0078125 acc: 0.883610725402832  val: loss: 2300230.5 acc: 0.6358706951141357\n",
      "step: 22875 , time : 0.0\n",
      "train: loss: 328036.125 acc: 0.8212869763374329  val: loss: 4053029.25 acc: 0.5966618657112122\n",
      "step: 22880 , time : 0.0\n",
      "train: loss: 281393.96875 acc: 0.821622371673584  val: loss: 2638661.25 acc: 0.6133779883384705\n",
      "step: 22885 , time : 0.0\n",
      "train: loss: 1462535.75 acc: 0.7026223540306091  val: loss: 1077717.0 acc: 0.7234483957290649\n",
      "step: 22890 , time : 0.0\n",
      "train: loss: 1315986.75 acc: 0.816653311252594  val: loss: 1127266.5 acc: 0.7529742121696472\n",
      "step: 22895 , time : 0.0\n",
      "train: loss: 1155071.125 acc: 0.8788090348243713  val: loss: 1230665.875 acc: 0.5393253564834595\n",
      "step: 22900 , time : 0.0\n",
      "train: loss: 213914.59375 acc: 0.9806529879570007  val: loss: 1952646.875 acc: 0.6431435346603394\n",
      "step: 22905 , time : 0.0\n",
      "train: loss: 284810.4375 acc: 0.970172643661499  val: loss: 2739708.5 acc: 0.4042810797691345\n",
      "step: 22910 , time : 0.0010004043579101562\n",
      "train: loss: 81091.5234375 acc: 0.981390118598938  val: loss: 834882.625 acc: 0.7810214757919312\n",
      "step: 22915 , time : 0.0009999275207519531\n",
      "train: loss: 60769.25 acc: 0.9948142766952515  val: loss: 1045904.8125 acc: 0.884548544883728\n",
      "step: 22920 , time : 0.0\n",
      "train: loss: 80023.296875 acc: 0.9936957359313965  val: loss: 733730.6875 acc: 0.7783219814300537\n",
      "step: 22925 , time : 0.0\n",
      "train: loss: 102467.7109375 acc: 0.9920028448104858  val: loss: 280260.0625 acc: 0.9269112348556519\n",
      "step: 22930 , time : 0.0\n",
      "train: loss: 67107.359375 acc: 0.9926367998123169  val: loss: 446856.40625 acc: 0.8708916902542114\n",
      "step: 22935 , time : 0.0\n",
      "train: loss: 38644.02734375 acc: 0.9944728016853333  val: loss: 1231424.75 acc: 0.46050578355789185\n",
      "step: 22940 , time : 0.0\n",
      "train: loss: 42228.31640625 acc: 0.9903044700622559  val: loss: 1194414.75 acc: 0.8299882411956787\n",
      "step: 22945 , time : 0.0\n",
      "train: loss: 13661.9931640625 acc: 0.9900916218757629  val: loss: 511258.53125 acc: 0.7826851606369019\n",
      "step: 22950 , time : 0.0\n",
      "train: loss: 21815.326171875 acc: 0.991270124912262  val: loss: 1257574.375 acc: 0.6820038557052612\n",
      "step: 22955 , time : 0.0\n",
      "train: loss: 15711.6298828125 acc: 0.9902130365371704  val: loss: 413084.34375 acc: 0.8622755408287048\n",
      "step: 22960 , time : 0.0\n",
      "train: loss: 14962.888671875 acc: 0.9920880198478699  val: loss: 1032952.4375 acc: 0.5216715335845947\n",
      "step: 22965 , time : 0.0\n",
      "train: loss: 35752.29296875 acc: 0.9710797071456909  val: loss: 384209.21875 acc: 0.9245581030845642\n",
      "step: 22970 , time : 0.0\n",
      "train: loss: 15923.0498046875 acc: 0.9535475969314575  val: loss: 795834.375 acc: 0.4035228490829468\n",
      "step: 22975 , time : 0.011600017547607422\n",
      "train: loss: 14258.458984375 acc: 0.9734275341033936  val: loss: 276633.9375 acc: 0.9596046209335327\n",
      "step: 22980 , time : 0.001001119613647461\n",
      "train: loss: 54684.94140625 acc: 0.9650911092758179  val: loss: 1422277.0 acc: 0.8681744337081909\n",
      "step: 22985 , time : 0.0010001659393310547\n",
      "train: loss: 23189.578125 acc: 0.9620662927627563  val: loss: 343098.25 acc: 0.9430099725723267\n",
      "step: 22990 , time : 0.0\n",
      "train: loss: 28950.447265625 acc: 0.9620929956436157  val: loss: 1282574.5 acc: 0.7291140556335449\n",
      "step: 22995 , time : 0.0010008811950683594\n",
      "train: loss: 13050.5625 acc: 0.9863997101783752  val: loss: 882556.9375 acc: 0.8918646574020386\n",
      "step: 23000 , time : 0.0010006427764892578\n",
      "train: loss: 110434.0625 acc: 0.9076434373855591  val: loss: 249985.25 acc: 0.9542832970619202\n",
      "step: 23005 , time : 0.0010004043579101562\n",
      "train: loss: 31334.908203125 acc: 0.9824391007423401  val: loss: 400968.96875 acc: 0.8089585304260254\n",
      "step: 23010 , time : 0.0010004043579101562\n",
      "train: loss: 21969.2578125 acc: 0.9840993881225586  val: loss: 309733.6875 acc: 0.9408926963806152\n",
      "step: 23015 , time : 0.0\n",
      "train: loss: 12559.298828125 acc: 0.9536091685295105  val: loss: 691965.125 acc: 0.8676305413246155\n",
      "step: 23020 , time : 0.0\n",
      "train: loss: 36873.12890625 acc: 0.9865779876708984  val: loss: 639268.75 acc: 0.8032327890396118\n",
      "step: 23025 , time : 0.0\n",
      "train: loss: 40617.90234375 acc: 0.9917118549346924  val: loss: 1329784.875 acc: 0.6758290529251099\n",
      "step: 23030 , time : 0.001001119613647461\n",
      "train: loss: 41056.15234375 acc: 0.990824282169342  val: loss: 2634276.5 acc: -1.5973083972930908\n",
      "step: 23035 , time : 0.0\n",
      "train: loss: 21891.45703125 acc: 0.9940817952156067  val: loss: 282802.71875 acc: 0.9602253437042236\n",
      "step: 23040 , time : 0.0010008811950683594\n",
      "train: loss: 70643.5234375 acc: 0.981621503829956  val: loss: 280456.34375 acc: 0.9663300514221191\n",
      "step: 23045 , time : 0.0\n",
      "train: loss: 101772.703125 acc: 0.9600471258163452  val: loss: 856091.0625 acc: 0.9037519097328186\n",
      "step: 23050 , time : 0.0\n",
      "train: loss: 230199.28125 acc: 0.937892496585846  val: loss: 672696.6875 acc: 0.9045184850692749\n",
      "step: 23055 , time : 0.0\n",
      "train: loss: 102419.5234375 acc: 0.9575167298316956  val: loss: 377268.28125 acc: 0.9758249521255493\n",
      "step: 23060 , time : 0.001001119613647461\n",
      "train: loss: 128523.4375 acc: 0.9707955121994019  val: loss: 659031.3125 acc: 0.8243487477302551\n",
      "step: 23065 , time : 0.0\n",
      "train: loss: 55264.0 acc: 0.9941840171813965  val: loss: 1754915.25 acc: 0.5243731737136841\n",
      "step: 23070 , time : 0.0\n",
      "train: loss: 124100.2421875 acc: 0.9871665835380554  val: loss: 1063340.875 acc: 0.8610048294067383\n",
      "step: 23075 , time : 0.0010008811950683594\n",
      "train: loss: 64535.1796875 acc: 0.9940666556358337  val: loss: 827127.375 acc: 0.8830304741859436\n",
      "step: 23080 , time : 0.0\n",
      "train: loss: 226495.40625 acc: 0.9656313061714172  val: loss: 460135.59375 acc: 0.8953572511672974\n",
      "step: 23085 , time : 0.001001119613647461\n",
      "train: loss: 851301.4375 acc: 0.9540166258811951  val: loss: 546289.5625 acc: 0.9152887463569641\n",
      "step: 23090 , time : 0.0010006427764892578\n",
      "train: loss: 563561.875 acc: 0.9431257843971252  val: loss: 1957335.375 acc: 0.7786902189254761\n",
      "step: 23095 , time : 0.0010008811950683594\n",
      "train: loss: 489370.25 acc: 0.9621163010597229  val: loss: 594839.0 acc: 0.9085213541984558\n",
      "step: 23100 , time : 0.0\n",
      "train: loss: 908978.0625 acc: 0.9661704301834106  val: loss: 648098.4375 acc: 0.892160952091217\n",
      "step: 23105 , time : 0.0\n",
      "train: loss: 651890.3125 acc: 0.9627977013587952  val: loss: 3929373.0 acc: 0.6895591616630554\n",
      "step: 23110 , time : 0.0\n",
      "train: loss: 402334.28125 acc: 0.9839171171188354  val: loss: 356318.65625 acc: 0.9590250849723816\n",
      "step: 23115 , time : 0.0\n",
      "train: loss: 1085594.875 acc: 0.9276615381240845  val: loss: 2258772.75 acc: 0.5262343883514404\n",
      "step: 23120 , time : 0.0\n",
      "train: loss: 388196.21875 acc: 0.9804419279098511  val: loss: 1073635.5 acc: 0.7654714584350586\n",
      "step: 23125 , time : 0.0010008811950683594\n",
      "train: loss: 686651.3125 acc: 0.9595762491226196  val: loss: 1366508.375 acc: 0.8860154151916504\n",
      "step: 23130 , time : 0.0010004043579101562\n",
      "train: loss: 339928.28125 acc: 0.9755536913871765  val: loss: 418454.21875 acc: 0.895820677280426\n",
      "step: 23135 , time : 0.0\n",
      "train: loss: 1481279.125 acc: 0.9083126783370972  val: loss: 751923.875 acc: 0.9150383472442627\n",
      "step: 23140 , time : 0.0\n",
      "train: loss: 1648471.375 acc: 0.18283289670944214  val: loss: 826492.5625 acc: 0.9080145955085754\n",
      "step: 23145 , time : 0.0010008811950683594\n",
      "train: loss: 1197273.125 acc: 0.190482497215271  val: loss: 1806518.125 acc: 0.7826098799705505\n",
      "step: 23150 , time : 0.0010013580322265625\n",
      "train: loss: 829978.25 acc: 0.8110643625259399  val: loss: 3885366.0 acc: 0.43879038095474243\n",
      "step: 23155 , time : 0.0010006427764892578\n",
      "train: loss: 465322.0625 acc: 0.8051451444625854  val: loss: 1159398.375 acc: 0.6842561364173889\n",
      "step: 23160 , time : 0.0010008811950683594\n",
      "train: loss: 1066620.875 acc: 0.6392165422439575  val: loss: 1186690.5 acc: 0.8652750253677368\n",
      "step: 23165 , time : 0.0\n",
      "train: loss: 960818.3125 acc: 0.6555255651473999  val: loss: 1447496.5 acc: 0.5358981490135193\n",
      "step: 23170 , time : 0.0010006427764892578\n",
      "train: loss: 608626.3125 acc: 0.7017836570739746  val: loss: 2791641.0 acc: 0.5881679654121399\n",
      "step: 23175 , time : 0.0010008811950683594\n",
      "train: loss: 524321.0 acc: 0.705575704574585  val: loss: 1733440.125 acc: 0.6687660813331604\n",
      "step: 23180 , time : 0.0\n",
      "train: loss: 288591.4375 acc: 0.8230127692222595  val: loss: 1250491.5 acc: 0.6546452045440674\n",
      "step: 23185 , time : 0.0\n",
      "train: loss: 85706.4453125 acc: 0.9275035262107849  val: loss: 535175.8125 acc: 0.7667896151542664\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 23190 , time : 0.0\n",
      "train: loss: 129765.9296875 acc: 0.9154710173606873  val: loss: 3171882.0 acc: 0.5764812231063843\n",
      "step: 23195 , time : 0.001001119613647461\n",
      "train: loss: 132316.6875 acc: 0.9200409650802612  val: loss: 1288858.375 acc: 0.7014980912208557\n",
      "step: 23200 , time : 0.0010004043579101562\n",
      "train: loss: 356891.875 acc: 0.8037030696868896  val: loss: 1711135.625 acc: 0.5884888768196106\n",
      "step: 23205 , time : 0.0\n",
      "train: loss: 87526.3984375 acc: 0.9292396306991577  val: loss: 1850662.125 acc: 0.640007495880127\n",
      "step: 23210 , time : 0.0\n",
      "train: loss: 96206.4921875 acc: 0.8454164862632751  val: loss: 912774.6875 acc: 0.6365998983383179\n",
      "step: 23215 , time : 0.0\n",
      "train: loss: 66984.1484375 acc: 0.9359013438224792  val: loss: 1020187.9375 acc: 0.6733571887016296\n",
      "step: 23220 , time : 0.0\n",
      "train: loss: 110421.1796875 acc: 0.8623961210250854  val: loss: 727784.9375 acc: 0.6820996999740601\n",
      "step: 23225 , time : 0.0\n",
      "train: loss: 210999.890625 acc: 0.8684068918228149  val: loss: 1555346.75 acc: 0.6675870418548584\n",
      "step: 23230 , time : 0.0010008811950683594\n",
      "train: loss: 747916.9375 acc: 0.7032959461212158  val: loss: 735410.0 acc: 0.6654469966888428\n",
      "step: 23235 , time : 0.0010008811950683594\n",
      "train: loss: 53447.27734375 acc: 0.9330561757087708  val: loss: 1885888.375 acc: 0.5943479537963867\n",
      "step: 23240 , time : 0.0010013580322265625\n",
      "train: loss: 348181.90625 acc: 0.6712965965270996  val: loss: 587291.4375 acc: 0.6363629102706909\n",
      "step: 23245 , time : 0.0\n",
      "train: loss: 823891.4375 acc: 0.7261066436767578  val: loss: 2326047.0 acc: 0.6442862153053284\n",
      "step: 23250 , time : 0.0\n",
      "train: loss: 1358750.625 acc: 0.6426138877868652  val: loss: 985940.6875 acc: 0.7982936501502991\n",
      "step: 23255 , time : 0.015625476837158203\n",
      "train: loss: 801689.1875 acc: 0.8270247578620911  val: loss: 867201.0625 acc: 0.8256083726882935\n",
      "step: 23260 , time : 0.0\n",
      "train: loss: 813823.25 acc: 0.9229235053062439  val: loss: 827986.0625 acc: 0.7820104956626892\n",
      "step: 23265 , time : 0.0\n",
      "train: loss: 745335.0 acc: 0.9238759279251099  val: loss: 1215313.5 acc: 0.8359373807907104\n",
      "step: 23270 , time : 0.0\n",
      "train: loss: 385844.125 acc: 0.9446858763694763  val: loss: 1050825.875 acc: 0.843906819820404\n",
      "step: 23275 , time : 0.0\n",
      "train: loss: 121180.109375 acc: 0.9800865650177002  val: loss: 340774.03125 acc: 0.9558784365653992\n",
      "step: 23280 , time : 0.0\n",
      "train: loss: 122805.9921875 acc: 0.9866201281547546  val: loss: 91195.5 acc: 0.9820539355278015\n",
      "step: 23285 , time : 0.0\n",
      "train: loss: 60129.8671875 acc: 0.9958533048629761  val: loss: 677730.6875 acc: 0.7344293594360352\n",
      "step: 23290 , time : 0.0\n",
      "train: loss: 101331.953125 acc: 0.9929317235946655  val: loss: 204663.203125 acc: 0.8424755930900574\n",
      "step: 23295 , time : 0.0\n",
      "train: loss: 127445.859375 acc: 0.9869005680084229  val: loss: 735075.6875 acc: 0.8661296963691711\n",
      "step: 23300 , time : 0.015625715255737305\n",
      "train: loss: 79972.75 acc: 0.9872061014175415  val: loss: 361567.53125 acc: 0.849997341632843\n",
      "step: 23305 , time : 0.0010008811950683594\n",
      "train: loss: 34942.5234375 acc: 0.9901154041290283  val: loss: 332308.09375 acc: 0.9266628623008728\n",
      "step: 23310 , time : 0.0010006427764892578\n",
      "train: loss: 26206.109375 acc: 0.9833494424819946  val: loss: 677870.9375 acc: 0.7797691822052002\n",
      "step: 23315 , time : 0.0010006427764892578\n",
      "train: loss: 12257.716796875 acc: 0.9910498857498169  val: loss: 906655.6875 acc: 0.7243481278419495\n",
      "step: 23320 , time : 0.0010006427764892578\n",
      "train: loss: 10543.4765625 acc: 0.9693077206611633  val: loss: 137150.828125 acc: 0.9822252988815308\n",
      "step: 23325 , time : 0.0010004043579101562\n",
      "train: loss: 36296.203125 acc: 0.9679248332977295  val: loss: 552369.9375 acc: 0.7208837270736694\n",
      "step: 23330 , time : 0.0010006427764892578\n",
      "train: loss: 15766.7265625 acc: 0.9878115057945251  val: loss: 331117.03125 acc: 0.9006558060646057\n",
      "step: 23335 , time : 0.0\n",
      "train: loss: 18659.43359375 acc: 0.9791573882102966  val: loss: 565726.0 acc: 0.8796651363372803\n",
      "step: 23340 , time : 0.0\n",
      "train: loss: 8478.12890625 acc: 0.9747253656387329  val: loss: 1035067.0 acc: 0.6487617492675781\n",
      "step: 23345 , time : 0.0\n",
      "train: loss: 16532.4765625 acc: 0.9728711247444153  val: loss: 671447.6875 acc: 0.8791670203208923\n",
      "step: 23350 , time : 0.0\n",
      "train: loss: 11262.5146484375 acc: 0.97356116771698  val: loss: 1288731.125 acc: 0.8950464129447937\n",
      "step: 23355 , time : 0.0\n",
      "train: loss: 33365.22265625 acc: 0.9763679504394531  val: loss: 972107.9375 acc: 0.8191261291503906\n",
      "step: 23360 , time : 0.0\n",
      "train: loss: 28555.85546875 acc: 0.9864607453346252  val: loss: 625549.75 acc: 0.8855794668197632\n",
      "step: 23365 , time : 0.0\n",
      "train: loss: 15649.3916015625 acc: 0.9875210523605347  val: loss: 1181281.375 acc: 0.8535139560699463\n",
      "step: 23370 , time : 0.0\n",
      "train: loss: 23612.06640625 acc: 0.9883038401603699  val: loss: 634529.8125 acc: 0.797627329826355\n",
      "step: 23375 , time : 0.0\n",
      "train: loss: 14172.58203125 acc: 0.9925752878189087  val: loss: 153569.3125 acc: 0.9554495811462402\n",
      "step: 23380 , time : 0.0\n",
      "train: loss: 21155.14453125 acc: 0.978227436542511  val: loss: 1668555.625 acc: 0.7694046497344971\n",
      "step: 23385 , time : 0.0\n",
      "train: loss: 4417.744140625 acc: 0.997292697429657  val: loss: 836304.0 acc: 0.9013381600379944\n",
      "step: 23390 , time : 0.0\n",
      "train: loss: 41565.19921875 acc: 0.981529176235199  val: loss: 665490.875 acc: 0.8474522829055786\n",
      "step: 23395 , time : 0.0010008811950683594\n",
      "train: loss: 21644.794921875 acc: 0.9935286641120911  val: loss: 1567183.5 acc: 0.4684072732925415\n",
      "step: 23400 , time : 0.0\n",
      "train: loss: 23307.947265625 acc: 0.9932519197463989  val: loss: 1684190.625 acc: 0.8243476152420044\n",
      "step: 23405 , time : 0.0010008811950683594\n",
      "train: loss: 44365.953125 acc: 0.9867910146713257  val: loss: 839784.6875 acc: 0.9112244248390198\n",
      "step: 23410 , time : 0.0010004043579101562\n",
      "train: loss: 293062.71875 acc: 0.9196752309799194  val: loss: 1700229.875 acc: 0.23035448789596558\n",
      "step: 23415 , time : 0.0\n",
      "train: loss: 182658.6875 acc: 0.9032375812530518  val: loss: 1299259.375 acc: 0.8499644994735718\n",
      "step: 23420 , time : 0.0\n",
      "train: loss: 95424.0703125 acc: 0.9422546029090881  val: loss: 239727.9375 acc: 0.9358487129211426\n",
      "step: 23425 , time : 0.0\n",
      "train: loss: 1116122.0 acc: 0.7925191521644592  val: loss: 4664160.0 acc: 0.2617194652557373\n",
      "step: 23430 , time : 0.0\n",
      "train: loss: 533307.375 acc: 0.9424712061882019  val: loss: 1221103.375 acc: 0.672869086265564\n",
      "step: 23435 , time : 0.0\n",
      "train: loss: 323499.78125 acc: 0.9690779447555542  val: loss: 1227976.875 acc: 0.8871921300888062\n",
      "step: 23440 , time : 0.0\n",
      "train: loss: 50823.328125 acc: 0.9924514293670654  val: loss: 693340.0 acc: 0.8965159058570862\n",
      "step: 23445 , time : 0.0\n",
      "train: loss: 302957.59375 acc: 0.9742320775985718  val: loss: 635809.9375 acc: 0.9208612442016602\n",
      "step: 23450 , time : 0.0\n",
      "train: loss: 356534.4375 acc: 0.9667753577232361  val: loss: 1372831.5 acc: 0.7021709680557251\n",
      "step: 23455 , time : 0.0\n",
      "train: loss: 311771.71875 acc: 0.9676692485809326  val: loss: 2597421.0 acc: 0.5988709330558777\n",
      "step: 23460 , time : 0.0\n",
      "train: loss: 398609.75 acc: 0.9585357308387756  val: loss: 1334267.375 acc: 0.5660368204116821\n",
      "step: 23465 , time : 0.0\n",
      "train: loss: 1051170.25 acc: 0.9249261617660522  val: loss: 537432.8125 acc: 0.7935515642166138\n",
      "step: 23470 , time : 0.0\n",
      "train: loss: 2919405.0 acc: 0.906121551990509  val: loss: 980482.5 acc: 0.7884963154792786\n",
      "step: 23475 , time : 0.0\n",
      "train: loss: 1270355.875 acc: 0.9647210836410522  val: loss: 942216.375 acc: 0.8006076812744141\n",
      "step: 23480 , time : 0.0\n",
      "train: loss: 729545.8125 acc: 0.965568482875824  val: loss: 296517.15625 acc: 0.9549707770347595\n",
      "step: 23485 , time : 0.0\n",
      "train: loss: 373210.09375 acc: 0.9727084040641785  val: loss: 6245230.0 acc: -0.1445615291595459\n",
      "step: 23490 , time : 0.0\n",
      "train: loss: 420437.3125 acc: 0.9692725539207458  val: loss: 1771924.25 acc: 0.5226336717605591\n",
      "step: 23495 , time : 0.0\n",
      "train: loss: 475996.21875 acc: 0.9360726475715637  val: loss: 2047050.375 acc: 0.3473284840583801\n",
      "step: 23500 , time : 0.0\n",
      "train: loss: 132358.453125 acc: 0.9681987762451172  val: loss: 613449.5625 acc: 0.844100832939148\n",
      "step: 23505 , time : 0.0010004043579101562\n",
      "train: loss: 1118922.5 acc: 0.71039879322052  val: loss: 945440.875 acc: 0.8544238209724426\n",
      "step: 23510 , time : 0.0\n",
      "train: loss: 1348924.875 acc: 0.587428629398346  val: loss: 1408529.25 acc: 0.8409552574157715\n",
      "step: 23515 , time : 0.0\n",
      "train: loss: 355456.125 acc: 0.8570377230644226  val: loss: 1212049.125 acc: 0.847200334072113\n",
      "step: 23520 , time : 0.0010001659393310547\n",
      "train: loss: 963239.4375 acc: 0.5045189261436462  val: loss: 1663218.625 acc: 0.8156006336212158\n",
      "step: 23525 , time : 0.0009996891021728516\n",
      "train: loss: 807883.125 acc: 0.7991058826446533  val: loss: 1868119.25 acc: 0.7846112847328186\n",
      "step: 23530 , time : 0.0010008811950683594\n",
      "train: loss: 1214813.875 acc: 0.6433389186859131  val: loss: 2612884.5 acc: 0.718304455280304\n",
      "step: 23535 , time : 0.001001119613647461\n",
      "train: loss: 551912.8125 acc: 0.7300046682357788  val: loss: 2508511.5 acc: 0.6491044759750366\n",
      "step: 23540 , time : 0.0010004043579101562\n",
      "train: loss: 436672.125 acc: 0.748760998249054  val: loss: 1393213.875 acc: 0.6223878860473633\n",
      "step: 23545 , time : 0.0\n",
      "train: loss: 225651.09375 acc: 0.8454853296279907  val: loss: 1128560.875 acc: 0.6712238788604736\n",
      "step: 23550 , time : 0.0\n",
      "train: loss: 85288.7421875 acc: 0.9253802299499512  val: loss: 665081.25 acc: 0.6709716320037842\n",
      "step: 23555 , time : 0.0\n",
      "train: loss: 112058.5078125 acc: 0.9088531136512756  val: loss: 683464.3125 acc: 0.6279804706573486\n",
      "step: 23560 , time : 0.0\n",
      "train: loss: 146584.0 acc: 0.907158613204956  val: loss: 398357.96875 acc: 0.7731786370277405\n",
      "step: 23565 , time : 0.0\n",
      "train: loss: 45225.2109375 acc: 0.961514949798584  val: loss: 1528731.0 acc: 0.5428972244262695\n",
      "step: 23570 , time : 0.0\n",
      "train: loss: 276883.21875 acc: 0.8408476114273071  val: loss: 1049374.125 acc: 0.6660950183868408\n",
      "step: 23575 , time : 0.0\n",
      "train: loss: 355603.5625 acc: 0.7663559913635254  val: loss: 1758000.875 acc: 0.5901690721511841\n",
      "step: 23580 , time : 0.0\n",
      "train: loss: 38460.8359375 acc: 0.9669176936149597  val: loss: 2831331.25 acc: 0.6021422147750854\n",
      "step: 23585 , time : 0.0\n",
      "train: loss: 58785.2578125 acc: 0.9378563761711121  val: loss: 1518611.375 acc: 0.6534762382507324\n",
      "step: 23590 , time : 0.0\n",
      "train: loss: 519771.46875 acc: 0.6773494482040405  val: loss: 745636.3125 acc: 0.6996442079544067\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 23595 , time : 0.015625715255737305\n",
      "train: loss: 127166.9140625 acc: 0.8882440328598022  val: loss: 2370348.0 acc: 0.5959986448287964\n",
      "step: 23600 , time : 0.0010006427764892578\n",
      "train: loss: 306722.6875 acc: 0.8327370882034302  val: loss: 2021281.625 acc: 0.552725076675415\n",
      "step: 23605 , time : 0.0\n",
      "train: loss: 276187.0625 acc: 0.7417089343070984  val: loss: 3346721.25 acc: 0.619804859161377\n",
      "step: 23610 , time : 0.0\n",
      "train: loss: 566520.3125 acc: 0.7815523147583008  val: loss: 942015.6875 acc: 0.6743373870849609\n",
      "step: 23615 , time : 0.0\n",
      "train: loss: 1337092.75 acc: 0.6879454851150513  val: loss: 1413953.875 acc: 0.7271005511283875\n",
      "step: 23620 , time : 0.0\n",
      "train: loss: 925260.1875 acc: 0.801067590713501  val: loss: 1549378.875 acc: 0.7024956941604614\n",
      "step: 23625 , time : 0.0010008811950683594\n",
      "train: loss: 891914.25 acc: 0.908605694770813  val: loss: 1537837.375 acc: 0.6082958579063416\n",
      "step: 23630 , time : 0.0\n",
      "train: loss: 569357.625 acc: 0.9164369702339172  val: loss: 408098.78125 acc: 0.8590190410614014\n",
      "step: 23635 , time : 0.0\n",
      "train: loss: 223690.390625 acc: 0.9750384092330933  val: loss: 1268831.75 acc: -0.13380897045135498\n",
      "step: 23640 , time : 0.0\n",
      "train: loss: 257387.296875 acc: 0.9433203935623169  val: loss: 725335.6875 acc: 0.9116719961166382\n",
      "step: 23645 , time : 0.0\n",
      "train: loss: 119893.4140625 acc: 0.9864670634269714  val: loss: 176525.984375 acc: 0.9717205166816711\n",
      "step: 23650 , time : 0.0\n",
      "train: loss: 89868.265625 acc: 0.9932035803794861  val: loss: 337404.28125 acc: 0.902397632598877\n",
      "step: 23655 , time : 0.0\n",
      "train: loss: 87931.96875 acc: 0.9920288920402527  val: loss: 836450.25 acc: 0.8371620774269104\n",
      "step: 23660 , time : 0.0\n",
      "train: loss: 83676.203125 acc: 0.9878327250480652  val: loss: 1162944.875 acc: 0.8826490044593811\n",
      "step: 23665 , time : 0.0\n",
      "train: loss: 25459.908203125 acc: 0.9962669610977173  val: loss: 378604.0 acc: 0.8518059253692627\n",
      "step: 23670 , time : 0.0\n",
      "train: loss: 55685.74609375 acc: 0.9878315329551697  val: loss: 376593.8125 acc: 0.8796591758728027\n",
      "step: 23675 , time : 0.0\n",
      "train: loss: 26918.302734375 acc: 0.9916512966156006  val: loss: 961616.125 acc: 0.9262729287147522\n",
      "step: 23680 , time : 0.0\n",
      "train: loss: 22542.25390625 acc: 0.986017107963562  val: loss: 595721.4375 acc: 0.8875957727432251\n",
      "step: 23685 , time : 0.0\n",
      "train: loss: 11169.28125 acc: 0.9932999014854431  val: loss: 322896.90625 acc: 0.9603249430656433\n",
      "step: 23690 , time : 0.0\n",
      "train: loss: 22871.6015625 acc: 0.975507915019989  val: loss: 139051.640625 acc: 0.9675679802894592\n",
      "step: 23695 , time : 0.0\n",
      "train: loss: 15124.8115234375 acc: 0.9807412028312683  val: loss: 1030304.5 acc: 0.8504234552383423\n",
      "step: 23700 , time : 0.0\n",
      "train: loss: 16464.1171875 acc: 0.9783928394317627  val: loss: 484203.84375 acc: 0.9379759430885315\n",
      "step: 23705 , time : 0.0\n",
      "train: loss: 19651.078125 acc: 0.9273979067802429  val: loss: 1095485.5 acc: 0.8430731296539307\n",
      "step: 23710 , time : 0.0\n",
      "train: loss: 16971.203125 acc: 0.9751524925231934  val: loss: 1161598.0 acc: 0.8873559832572937\n",
      "step: 23715 , time : 0.0\n",
      "train: loss: 8343.986328125 acc: 0.987983226776123  val: loss: 287838.21875 acc: 0.9540706276893616\n",
      "step: 23720 , time : 0.02301478385925293\n",
      "train: loss: 46871.80859375 acc: 0.9454290866851807  val: loss: 3880161.0 acc: 0.558330774307251\n",
      "step: 23725 , time : 0.0010006427764892578\n",
      "train: loss: 37675.56640625 acc: 0.976170539855957  val: loss: 2409388.0 acc: 0.6354339122772217\n",
      "step: 23730 , time : 0.0\n",
      "train: loss: 13683.1279296875 acc: 0.9894028306007385  val: loss: 1839297.75 acc: 0.6151732206344604\n",
      "step: 23735 , time : 0.0\n",
      "train: loss: 30719.05078125 acc: 0.9784698486328125  val: loss: 3031021.5 acc: 0.4603455662727356\n",
      "step: 23740 , time : 0.0\n",
      "train: loss: 7478.7060546875 acc: 0.9927729368209839  val: loss: 1242879.875 acc: 0.5738053917884827\n",
      "step: 23745 , time : 0.0\n",
      "train: loss: 18243.68359375 acc: 0.9884002208709717  val: loss: 1821729.875 acc: 0.20497065782546997\n",
      "step: 23750 , time : 0.0\n",
      "train: loss: 23112.451171875 acc: 0.9855632185935974  val: loss: 1629448.25 acc: 0.42881453037261963\n",
      "step: 23755 , time : 0.0\n",
      "train: loss: 49918.30078125 acc: 0.9844626188278198  val: loss: 1824051.875 acc: 0.5948739647865295\n",
      "step: 23760 , time : 0.0\n",
      "train: loss: 49498.44140625 acc: 0.9857360124588013  val: loss: 3947751.5 acc: -0.043805718421936035\n",
      "step: 23765 , time : 0.0\n",
      "train: loss: 26717.06640625 acc: 0.990342915058136  val: loss: 980832.125 acc: 0.4897139072418213\n",
      "step: 23770 , time : 0.0\n",
      "train: loss: 51091.79296875 acc: 0.9856716394424438  val: loss: 1803217.25 acc: 0.5479220151901245\n",
      "step: 23775 , time : 0.0\n",
      "train: loss: 49320.52734375 acc: 0.9808809161186218  val: loss: 2538233.0 acc: 0.5505427122116089\n",
      "step: 23780 , time : 0.0\n",
      "train: loss: 201105.578125 acc: 0.9479217529296875  val: loss: 1392072.375 acc: 0.7259702682495117\n",
      "step: 23785 , time : 0.015625476837158203\n",
      "train: loss: 74147.2578125 acc: 0.9464409947395325  val: loss: 1654469.625 acc: 0.7883585095405579\n",
      "step: 23790 , time : 0.0\n",
      "train: loss: 133536.40625 acc: 0.9745166301727295  val: loss: 2187726.5 acc: 0.7685375809669495\n",
      "step: 23795 , time : 0.0\n",
      "train: loss: 136444.15625 acc: 0.9844496846199036  val: loss: 1403554.0 acc: 0.5590198636054993\n",
      "step: 23800 , time : 0.0\n",
      "train: loss: 58936.421875 acc: 0.9950305223464966  val: loss: 1208915.125 acc: -0.09021341800689697\n",
      "step: 23805 , time : 0.0\n",
      "train: loss: 76284.7734375 acc: 0.9906919598579407  val: loss: 1461223.0 acc: 0.6388769149780273\n",
      "step: 23810 , time : 0.0\n",
      "train: loss: 189422.015625 acc: 0.9651179313659668  val: loss: 1203158.0 acc: 0.6097667217254639\n",
      "step: 23815 , time : 0.0\n",
      "train: loss: 141442.859375 acc: 0.9833742380142212  val: loss: 1620920.125 acc: 0.8253144025802612\n",
      "step: 23820 , time : 0.0\n",
      "train: loss: 528111.4375 acc: 0.9713907241821289  val: loss: 653484.875 acc: 0.9137783050537109\n",
      "step: 23825 , time : 0.0\n",
      "train: loss: 869176.375 acc: 0.9064338803291321  val: loss: 928588.875 acc: 0.8748694658279419\n",
      "step: 23830 , time : 0.0010006427764892578\n",
      "train: loss: 587381.1875 acc: 0.974082887172699  val: loss: 1767431.125 acc: 0.5433883666992188\n",
      "step: 23835 , time : 0.0\n",
      "train: loss: 1975232.875 acc: 0.9229554533958435  val: loss: 994997.4375 acc: 0.7127001881599426\n",
      "step: 23840 , time : 0.001001119613647461\n",
      "train: loss: 2746093.75 acc: 0.917509138584137  val: loss: 679748.4375 acc: 0.8763384819030762\n",
      "step: 23845 , time : 0.0010004043579101562\n",
      "train: loss: 649230.75 acc: 0.9695965051651001  val: loss: 352032.59375 acc: 0.9365535378456116\n",
      "step: 23850 , time : 0.0\n",
      "train: loss: 562155.5625 acc: 0.9565889835357666  val: loss: 421870.71875 acc: 0.903931200504303\n",
      "step: 23855 , time : 0.0\n",
      "train: loss: 128177.6484375 acc: 0.9813427925109863  val: loss: 429559.96875 acc: 0.9457768797874451\n",
      "step: 23860 , time : 0.0\n",
      "train: loss: 471482.46875 acc: 0.9318885207176208  val: loss: 188504.1875 acc: 0.9796656966209412\n",
      "step: 23865 , time : 0.0\n",
      "train: loss: 270412.96875 acc: 0.9710736274719238  val: loss: 343858.15625 acc: 0.9485307335853577\n",
      "step: 23870 , time : 0.0\n",
      "train: loss: 1068825.625 acc: 0.8766083717346191  val: loss: 495253.6875 acc: 0.8974399566650391\n",
      "step: 23875 , time : 0.0\n",
      "train: loss: 478641.9375 acc: 0.839319109916687  val: loss: 1310015.625 acc: 0.7358798980712891\n",
      "step: 23880 , time : 0.015625953674316406\n",
      "train: loss: 477691.1875 acc: 0.7963535189628601  val: loss: 734001.4375 acc: 0.6030771136283875\n",
      "step: 23885 , time : 0.0\n",
      "train: loss: 927658.1875 acc: 0.6222108602523804  val: loss: 692981.0625 acc: 0.8373796939849854\n",
      "step: 23890 , time : 0.0\n",
      "train: loss: 1418611.625 acc: 0.4979580044746399  val: loss: 361162.375 acc: 0.7866379618644714\n",
      "step: 23895 , time : 0.0\n",
      "train: loss: 749034.1875 acc: 0.5545918941497803  val: loss: 2040639.375 acc: 0.7148160934448242\n",
      "step: 23900 , time : 0.0\n",
      "train: loss: 282082.71875 acc: 0.8190966248512268  val: loss: 2588467.25 acc: 0.5935208797454834\n",
      "step: 23905 , time : 0.0\n",
      "train: loss: 264184.9375 acc: 0.795857846736908  val: loss: 1663643.375 acc: 0.6225336790084839\n",
      "step: 23910 , time : 0.0\n",
      "train: loss: 147519.234375 acc: 0.860295295715332  val: loss: 2075075.375 acc: 0.6475645303726196\n",
      "step: 23915 , time : 0.0010006427764892578\n",
      "train: loss: 49497.57421875 acc: 0.9582753777503967  val: loss: 584659.5625 acc: 0.6325367093086243\n",
      "step: 23920 , time : 0.0\n",
      "train: loss: 81530.8125 acc: 0.9301548004150391  val: loss: 836827.5 acc: 0.7193970680236816\n",
      "step: 23925 , time : 0.0\n",
      "train: loss: 252300.953125 acc: 0.8475366830825806  val: loss: 749478.375 acc: 0.7415480613708496\n",
      "step: 23930 , time : 0.0\n",
      "train: loss: 161853.921875 acc: 0.9016897678375244  val: loss: 1379044.625 acc: 0.650214433670044\n",
      "step: 23935 , time : 0.0\n",
      "train: loss: 98113.4609375 acc: 0.9284888505935669  val: loss: 626005.75 acc: 0.6799668073654175\n",
      "step: 23940 , time : 0.0\n",
      "train: loss: 23669.568359375 acc: 0.9731706976890564  val: loss: 1318002.625 acc: 0.5990766286849976\n",
      "step: 23945 , time : 0.0\n",
      "train: loss: 35329.49609375 acc: 0.9625718593597412  val: loss: 931285.875 acc: 0.6511316895484924\n",
      "step: 23950 , time : 0.0\n",
      "train: loss: 240328.390625 acc: 0.8030233979225159  val: loss: 1783126.625 acc: 0.6198755502700806\n",
      "step: 23955 , time : 0.0\n",
      "train: loss: 360390.8125 acc: 0.8029450178146362  val: loss: 1430944.25 acc: 0.5614274740219116\n",
      "step: 23960 , time : 0.0\n",
      "train: loss: 181940.078125 acc: 0.8845092058181763  val: loss: 590864.0625 acc: 0.7424559593200684\n",
      "step: 23965 , time : 0.015625953674316406\n",
      "train: loss: 197160.0 acc: 0.8402947187423706  val: loss: 3338539.75 acc: 0.6047707200050354\n",
      "step: 23970 , time : 0.0\n",
      "train: loss: 980733.375 acc: 0.6351009011268616  val: loss: 3987258.0 acc: 0.5894256830215454\n",
      "step: 23975 , time : 0.0\n",
      "train: loss: 75706.0078125 acc: 0.9372632503509521  val: loss: 1672796.375 acc: 0.6077648401260376\n",
      "step: 23980 , time : 0.0\n",
      "train: loss: 2211380.0 acc: 0.6443808078765869  val: loss: 1224133.125 acc: 0.7630870342254639\n",
      "step: 23985 , time : 0.0\n",
      "train: loss: 1245083.875 acc: 0.6528887748718262  val: loss: 738582.5625 acc: 0.8454220294952393\n",
      "step: 23990 , time : 0.0\n",
      "train: loss: 1221945.125 acc: 0.7242234945297241  val: loss: 618283.8125 acc: 0.8331400752067566\n",
      "step: 23995 , time : 0.0\n",
      "train: loss: 419808.21875 acc: 0.9582457542419434  val: loss: 372181.8125 acc: 0.8994178771972656\n",
      "step: 24000 , time : 0.0\n",
      "train: loss: 217877.96875 acc: 0.9617422819137573  val: loss: 1770368.625 acc: 0.8479318022727966\n",
      "step: 24005 , time : 0.0\n",
      "train: loss: 512457.4375 acc: 0.8951682448387146  val: loss: 647631.0625 acc: 0.9047282338142395\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 24010 , time : 0.0\n",
      "train: loss: 313698.21875 acc: 0.9492354989051819  val: loss: 1807164.0 acc: 0.7933157682418823\n",
      "step: 24015 , time : 0.0010004043579101562\n",
      "train: loss: 84479.4453125 acc: 0.990023672580719  val: loss: 998827.75 acc: 0.887195885181427\n",
      "step: 24020 , time : 0.0010008811950683594\n",
      "train: loss: 112782.1328125 acc: 0.9921088218688965  val: loss: 513089.78125 acc: 0.9267885088920593\n",
      "step: 24025 , time : 0.0010006427764892578\n",
      "train: loss: 184695.1875 acc: 0.9775333404541016  val: loss: 1248579.125 acc: 0.6204499006271362\n",
      "step: 24030 , time : 0.0\n",
      "train: loss: 66374.90625 acc: 0.9915934801101685  val: loss: 1892797.125 acc: 0.7076340913772583\n",
      "step: 24035 , time : 0.0010008811950683594\n",
      "train: loss: 70075.0 acc: 0.9840884208679199  val: loss: 2736900.0 acc: 0.84743332862854\n",
      "step: 24040 , time : 0.0\n",
      "train: loss: 47204.0234375 acc: 0.9777551889419556  val: loss: 999735.125 acc: 0.8702619075775146\n",
      "step: 24045 , time : 0.0\n",
      "train: loss: 12611.0234375 acc: 0.99318927526474  val: loss: 544717.375 acc: 0.6828662753105164\n",
      "step: 24050 , time : 0.0010006427764892578\n",
      "train: loss: 11779.2939453125 acc: 0.9929094314575195  val: loss: 641235.875 acc: 0.9185229539871216\n",
      "step: 24055 , time : 0.0\n",
      "train: loss: 30309.541015625 acc: 0.9783244132995605  val: loss: 3033796.5 acc: 0.7232263088226318\n",
      "step: 24060 , time : 0.0\n",
      "train: loss: 9942.00390625 acc: 0.9808547496795654  val: loss: 1359092.125 acc: 0.743110179901123\n",
      "step: 24065 , time : 0.0010006427764892578\n",
      "train: loss: 21791.453125 acc: 0.9878242015838623  val: loss: 1363910.75 acc: 0.8073524236679077\n",
      "step: 24070 , time : 0.0\n",
      "train: loss: 6535.6923828125 acc: 0.9816839098930359  val: loss: 1215189.0 acc: 0.8536190390586853\n",
      "step: 24075 , time : 0.0\n",
      "train: loss: 14475.4375 acc: 0.9548819065093994  val: loss: 1584053.875 acc: -0.04567563533782959\n",
      "step: 24080 , time : 0.0\n",
      "train: loss: 9934.44921875 acc: 0.964703381061554  val: loss: 1224818.75 acc: 0.21040785312652588\n",
      "step: 24085 , time : 0.0\n",
      "train: loss: 25935.568359375 acc: 0.9630526304244995  val: loss: 2130302.0 acc: 0.7332519292831421\n",
      "step: 24090 , time : 0.0\n",
      "train: loss: 29876.669921875 acc: 0.9827859997749329  val: loss: 3053884.5 acc: -1.5334064960479736\n",
      "step: 24095 , time : 0.0\n",
      "train: loss: 36569.51953125 acc: 0.9764193296432495  val: loss: 3512694.75 acc: 0.4716351628303528\n",
      "step: 24100 , time : 0.0\n",
      "train: loss: 24286.189453125 acc: 0.9902811646461487  val: loss: 1303144.625 acc: 0.3480914235115051\n",
      "step: 24105 , time : 0.0\n",
      "train: loss: 14932.8681640625 acc: 0.9867111444473267  val: loss: 957876.875 acc: 0.8307493925094604\n",
      "step: 24110 , time : 0.0\n",
      "train: loss: 28402.150390625 acc: 0.9761918783187866  val: loss: 1410482.875 acc: 0.6045432090759277\n",
      "step: 24115 , time : 0.0\n",
      "train: loss: 13705.376953125 acc: 0.992175817489624  val: loss: 716187.0 acc: 0.8633359670639038\n",
      "step: 24120 , time : 0.0\n",
      "train: loss: 31737.44140625 acc: 0.990040123462677  val: loss: 2121138.0 acc: 0.2183709740638733\n",
      "step: 24125 , time : 0.0\n",
      "train: loss: 54950.73046875 acc: 0.9888370633125305  val: loss: 1286118.0 acc: 0.751530110836029\n",
      "step: 24130 , time : 0.0\n",
      "train: loss: 55201.36328125 acc: 0.9834255576133728  val: loss: 1833982.625 acc: 0.5742532014846802\n",
      "step: 24135 , time : 0.0\n",
      "train: loss: 43852.4921875 acc: 0.9766659140586853  val: loss: 203040.703125 acc: 0.9489792585372925\n",
      "step: 24140 , time : 0.0010006427764892578\n",
      "train: loss: 29413.23046875 acc: 0.9908037185668945  val: loss: 1686138.875 acc: 0.8459597826004028\n",
      "step: 24145 , time : 0.0\n",
      "train: loss: 90735.7578125 acc: 0.9610387086868286  val: loss: 1769093.625 acc: 0.8489539623260498\n",
      "step: 24150 , time : 0.0010004043579101562\n",
      "train: loss: 72970.484375 acc: 0.9645298719406128  val: loss: 3056278.0 acc: -0.4668375253677368\n",
      "step: 24155 , time : 0.0\n",
      "train: loss: 306816.6875 acc: 0.8529974818229675  val: loss: 1463112.375 acc: 0.6278976798057556\n",
      "step: 24160 , time : 0.0\n",
      "train: loss: 118481.875 acc: 0.9793843030929565  val: loss: 553723.5 acc: 0.8769923448562622\n",
      "step: 24165 , time : 0.0\n",
      "train: loss: 502926.40625 acc: 0.958695113658905  val: loss: 1346774.0 acc: 0.4657434821128845\n",
      "step: 24170 , time : 0.0\n",
      "train: loss: 187506.90625 acc: 0.9811072945594788  val: loss: 687671.125 acc: 0.8941031694412231\n",
      "step: 24175 , time : 0.0\n",
      "train: loss: 231736.21875 acc: 0.9572764039039612  val: loss: 1232535.75 acc: 0.7210837006568909\n",
      "step: 24180 , time : 0.0\n",
      "train: loss: 180635.71875 acc: 0.9774996638298035  val: loss: 456065.40625 acc: 0.8315147161483765\n",
      "step: 24185 , time : 0.0\n",
      "train: loss: 682545.75 acc: 0.9601253271102905  val: loss: 1083755.625 acc: 0.5641632080078125\n",
      "step: 24190 , time : 0.0\n",
      "train: loss: 313419.65625 acc: 0.9692156910896301  val: loss: 970277.5 acc: 0.7986162304878235\n",
      "step: 24195 , time : 0.0\n",
      "train: loss: 376087.8125 acc: 0.9528986811637878  val: loss: 1941690.125 acc: 0.1605321764945984\n",
      "step: 24200 , time : 0.0\n",
      "train: loss: 665791.625 acc: 0.9592699408531189  val: loss: 948328.6875 acc: 0.9079841375350952\n",
      "step: 24205 , time : 0.0\n",
      "train: loss: 869285.0625 acc: 0.9714454412460327  val: loss: 2020921.625 acc: 0.3381209373474121\n",
      "step: 24210 , time : 0.0\n",
      "train: loss: 1181933.75 acc: 0.9610292911529541  val: loss: 457024.5625 acc: 0.7980228066444397\n",
      "step: 24215 , time : 0.0\n",
      "train: loss: 293731.34375 acc: 0.9856035113334656  val: loss: 620699.625 acc: 0.8984541893005371\n",
      "step: 24220 , time : 0.0\n",
      "train: loss: 1152870.75 acc: 0.910394012928009  val: loss: 340509.125 acc: 0.9070043563842773\n",
      "step: 24225 , time : 0.0\n",
      "train: loss: 264644.03125 acc: 0.9673026204109192  val: loss: 389394.90625 acc: 0.9015432000160217\n",
      "step: 24230 , time : 0.0\n",
      "train: loss: 2602436.5 acc: 0.03334188461303711  val: loss: 192308.140625 acc: 0.9037633538246155\n",
      "step: 24235 , time : 0.0010006427764892578\n",
      "train: loss: 1102076.0 acc: 0.6704376935958862  val: loss: 532091.9375 acc: 0.8272622227668762\n",
      "step: 24240 , time : 0.0010008811950683594\n",
      "train: loss: 621875.75 acc: 0.8426333665847778  val: loss: 1372855.75 acc: 0.6763205528259277\n",
      "step: 24245 , time : 0.0\n",
      "train: loss: 1264129.625 acc: 0.7415170669555664  val: loss: 486407.9375 acc: 0.8482796549797058\n",
      "step: 24250 , time : 0.0\n",
      "train: loss: 885440.125 acc: 0.8094183206558228  val: loss: 556571.9375 acc: 0.8189995288848877\n",
      "step: 24255 , time : 0.0010006427764892578\n",
      "train: loss: 785901.0625 acc: 0.7309805154800415  val: loss: 316447.0625 acc: 0.8840639591217041\n",
      "step: 24260 , time : 0.0010006427764892578\n",
      "train: loss: 1345918.625 acc: 0.5675421357154846  val: loss: 2855732.25 acc: 0.6725316047668457\n",
      "step: 24265 , time : 0.0\n",
      "train: loss: 539824.5625 acc: 0.7071585655212402  val: loss: 1643263.375 acc: 0.5329744815826416\n",
      "step: 24270 , time : 0.0\n",
      "train: loss: 388071.65625 acc: 0.7625842094421387  val: loss: 1845419.25 acc: 0.5813183188438416\n",
      "step: 24275 , time : 0.0\n",
      "train: loss: 201037.015625 acc: 0.8078991174697876  val: loss: 1815350.75 acc: 0.602617621421814\n",
      "step: 24280 , time : 0.0\n",
      "train: loss: 104175.875 acc: 0.9126002192497253  val: loss: 388281.0 acc: 0.796013593673706\n",
      "step: 24285 , time : 0.0\n",
      "train: loss: 58318.15625 acc: 0.946641206741333  val: loss: 1772787.625 acc: 0.676893949508667\n",
      "step: 24290 , time : 0.0\n",
      "train: loss: 307523.625 acc: 0.8489694595336914  val: loss: 3625851.5 acc: 0.5996902585029602\n",
      "step: 24295 , time : 0.0\n",
      "train: loss: 239514.578125 acc: 0.8650261759757996  val: loss: 2542705.75 acc: 0.6298446655273438\n",
      "step: 24300 , time : 0.0\n",
      "train: loss: 44818.3828125 acc: 0.9621493816375732  val: loss: 891436.25 acc: 0.7128469944000244\n",
      "step: 24305 , time : 0.0\n",
      "train: loss: 206693.390625 acc: 0.7749820947647095  val: loss: 7059552.0 acc: 0.5325678586959839\n",
      "step: 24310 , time : 0.0\n",
      "train: loss: 41452.3515625 acc: 0.937716543674469  val: loss: 1610408.625 acc: 0.735405683517456\n",
      "step: 24315 , time : 0.0\n",
      "train: loss: 95620.921875 acc: 0.9003761410713196  val: loss: 1453227.125 acc: 0.6622010469436646\n",
      "step: 24320 , time : 0.0\n",
      "train: loss: 130563.375 acc: 0.9057464599609375  val: loss: 4686114.0 acc: 0.5748814344406128\n",
      "step: 24325 , time : 0.0\n",
      "train: loss: 921910.875 acc: 0.704354465007782  val: loss: 4200461.5 acc: 0.5731685161590576\n",
      "step: 24330 , time : 0.0\n",
      "train: loss: 366298.28125 acc: 0.8192498683929443  val: loss: 3253067.5 acc: 0.606715977191925\n",
      "step: 24335 , time : 0.015625715255737305\n",
      "train: loss: 433781.25 acc: 0.7748682498931885  val: loss: 2701084.25 acc: 0.566764235496521\n",
      "step: 24340 , time : 0.0010008811950683594\n",
      "train: loss: 1005194.5625 acc: 0.7105857133865356  val: loss: 893502.1875 acc: 0.7526662349700928\n",
      "step: 24345 , time : 0.0010006427764892578\n",
      "train: loss: 770985.1875 acc: 0.710611879825592  val: loss: 1171793.125 acc: 0.7479139566421509\n",
      "step: 24350 , time : 0.0\n",
      "train: loss: 1283889.375 acc: 0.7668805718421936  val: loss: 2286218.0 acc: 0.7779659032821655\n",
      "step: 24355 , time : 0.0010006427764892578\n",
      "train: loss: 1484724.125 acc: 0.837272047996521  val: loss: 1265581.125 acc: 0.8964223861694336\n",
      "step: 24360 , time : 0.0\n",
      "train: loss: 502376.15625 acc: 0.9388880729675293  val: loss: 1373291.625 acc: 0.765285849571228\n",
      "step: 24365 , time : 0.0010008811950683594\n",
      "train: loss: 871663.5625 acc: 0.9127575159072876  val: loss: 1991270.625 acc: 0.8129685521125793\n",
      "step: 24370 , time : 0.0010008811950683594\n",
      "train: loss: 92569.84375 acc: 0.9866559505462646  val: loss: 3928763.0 acc: 0.07757532596588135\n",
      "step: 24375 , time : 0.0\n",
      "train: loss: 294222.4375 acc: 0.9768176078796387  val: loss: 1760040.125 acc: 0.54621422290802\n",
      "step: 24380 , time : 0.0\n",
      "train: loss: 104619.0390625 acc: 0.9920223355293274  val: loss: 1375001.875 acc: 0.8799753189086914\n",
      "step: 24385 , time : 0.0\n",
      "train: loss: 162734.203125 acc: 0.988404393196106  val: loss: 1411429.75 acc: 0.6360226273536682\n",
      "step: 24390 , time : 0.0\n",
      "train: loss: 121870.640625 acc: 0.9888553023338318  val: loss: 1632532.75 acc: 0.46110087633132935\n",
      "step: 24395 , time : 0.0\n",
      "train: loss: 78850.4921875 acc: 0.9865446090698242  val: loss: 779291.5 acc: 0.4635513424873352\n",
      "step: 24400 , time : 0.0\n",
      "train: loss: 32702.54296875 acc: 0.9908468127250671  val: loss: 2496835.25 acc: 0.6964157819747925\n",
      "step: 24405 , time : 0.0\n",
      "train: loss: 16438.982421875 acc: 0.9938689470291138  val: loss: 2111975.5 acc: 0.7227728366851807\n",
      "step: 24410 , time : 0.0\n",
      "train: loss: 15463.33203125 acc: 0.9895699620246887  val: loss: 1673279.375 acc: 0.23595499992370605\n",
      "step: 24415 , time : 0.0\n",
      "train: loss: 21786.654296875 acc: 0.9858151078224182  val: loss: 1843395.5 acc: 0.4029308557510376\n",
      "step: 24420 , time : 0.0\n",
      "train: loss: 11248.8486328125 acc: 0.9860869646072388  val: loss: 958819.8125 acc: 0.7301534414291382\n",
      "step: 24425 , time : 0.0\n",
      "train: loss: 10380.509765625 acc: 0.9731734395027161  val: loss: 1379867.25 acc: 0.6760103702545166\n",
      "step: 24430 , time : 0.0\n",
      "train: loss: 14875.99609375 acc: 0.9220263361930847  val: loss: 1285965.5 acc: 0.8569082021713257\n",
      "step: 24435 , time : 0.0010008811950683594\n",
      "train: loss: 17538.830078125 acc: 0.9894992709159851  val: loss: 2057765.0 acc: 0.19237637519836426\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 24440 , time : 0.0\n",
      "train: loss: 14907.921875 acc: 0.9636483788490295  val: loss: 659556.8125 acc: 0.9381251335144043\n",
      "step: 24445 , time : 0.0\n",
      "train: loss: 7889.54931640625 acc: 0.9739456176757812  val: loss: 203013.328125 acc: 0.9256789088249207\n",
      "step: 24450 , time : 0.0\n",
      "train: loss: 17531.109375 acc: 0.9813825488090515  val: loss: 1409863.375 acc: 0.6001266837120056\n",
      "step: 24455 , time : 0.0010013580322265625\n",
      "train: loss: 37932.29296875 acc: 0.9714763760566711  val: loss: 1104721.125 acc: 0.7841581106185913\n",
      "step: 24460 , time : 0.0\n",
      "train: loss: 40549.484375 acc: 0.9767162799835205  val: loss: 2674532.5 acc: 0.46715861558914185\n",
      "step: 24465 , time : 0.0010006427764892578\n",
      "train: loss: 77608.640625 acc: 0.9791521430015564  val: loss: 2624977.5 acc: 0.4910488724708557\n",
      "step: 24470 , time : 0.0010006427764892578\n",
      "train: loss: 26471.439453125 acc: 0.9784316420555115  val: loss: 1245096.875 acc: 0.809425950050354\n",
      "step: 24475 , time : 0.0010006427764892578\n",
      "train: loss: 26836.064453125 acc: 0.9582599997520447  val: loss: 1408397.0 acc: 0.6917325258255005\n",
      "step: 24480 , time : 0.0\n",
      "train: loss: 10744.7421875 acc: 0.9911055564880371  val: loss: 2947631.0 acc: 0.6876258850097656\n",
      "step: 24485 , time : 0.0012173652648925781\n",
      "train: loss: 23284.857421875 acc: 0.9940747022628784  val: loss: 1052125.125 acc: 0.791658878326416\n",
      "step: 24490 , time : 0.0\n",
      "train: loss: 17944.427734375 acc: 0.9955991506576538  val: loss: 920738.375 acc: 0.8698118925094604\n",
      "step: 24495 , time : 0.0\n",
      "train: loss: 35969.2109375 acc: 0.9941288828849792  val: loss: 1524990.125 acc: 0.7153239250183105\n",
      "step: 24500 , time : 0.0\n",
      "train: loss: 51270.21484375 acc: 0.9848043322563171  val: loss: 1115673.875 acc: 0.7838891744613647\n",
      "step: 24505 , time : 0.0\n",
      "train: loss: 34786.18359375 acc: 0.9900268912315369  val: loss: 1508311.625 acc: 0.77635657787323\n",
      "step: 24510 , time : 0.0\n",
      "train: loss: 81189.0625 acc: 0.9659972786903381  val: loss: 477713.1875 acc: 0.8387281894683838\n",
      "step: 24515 , time : 0.0\n",
      "train: loss: 160548.53125 acc: 0.9456499218940735  val: loss: 2028900.25 acc: 0.18655163049697876\n",
      "step: 24520 , time : 0.0\n",
      "train: loss: 531795.25 acc: 0.8521756529808044  val: loss: 897718.5 acc: 0.8509857058525085\n",
      "step: 24525 , time : 0.0\n",
      "train: loss: 106106.578125 acc: 0.9843288660049438  val: loss: 1769824.625 acc: 0.7915593981742859\n",
      "step: 24530 , time : 0.0\n",
      "train: loss: 75824.4375 acc: 0.9909489154815674  val: loss: 671168.125 acc: 0.8607004284858704\n",
      "step: 24535 , time : 0.0\n",
      "train: loss: 62053.05859375 acc: 0.9903737902641296  val: loss: 1465678.625 acc: 0.8226954936981201\n",
      "step: 24540 , time : 0.0\n",
      "train: loss: 288590.6875 acc: 0.9659406542778015  val: loss: 193581.3125 acc: 0.9631909132003784\n",
      "step: 24545 , time : 0.0\n",
      "train: loss: 231772.578125 acc: 0.9828954339027405  val: loss: 588861.125 acc: 0.7239242196083069\n",
      "step: 24550 , time : 0.0\n",
      "train: loss: 952962.75 acc: 0.955074667930603  val: loss: 1789631.5 acc: 0.26719915866851807\n",
      "step: 24555 , time : 0.0\n",
      "train: loss: 466109.65625 acc: 0.9720925092697144  val: loss: 1330000.0 acc: 0.6806129217147827\n",
      "step: 24560 , time : 0.0\n",
      "train: loss: 294490.1875 acc: 0.9423367977142334  val: loss: 943396.4375 acc: 0.7558740377426147\n",
      "step: 24565 , time : 0.0\n",
      "train: loss: 749789.5625 acc: 0.9702522158622742  val: loss: 473579.34375 acc: 0.8824640512466431\n",
      "step: 24570 , time : 0.0\n",
      "train: loss: 880264.0625 acc: 0.9714291095733643  val: loss: 1417880.625 acc: 0.46577388048171997\n",
      "step: 24575 , time : 0.001001119613647461\n",
      "train: loss: 923701.3125 acc: 0.9650174975395203  val: loss: 432949.8125 acc: 0.8702483773231506\n",
      "step: 24580 , time : 0.0\n",
      "train: loss: 1421146.375 acc: 0.9435389637947083  val: loss: 796016.9375 acc: 0.8930422067642212\n",
      "step: 24585 , time : 0.0010006427764892578\n",
      "train: loss: 733658.5625 acc: 0.9335581660270691  val: loss: 305030.75 acc: 0.9133516550064087\n",
      "step: 24590 , time : 0.0\n",
      "train: loss: 473713.34375 acc: 0.9215654730796814  val: loss: 233121.0 acc: 0.9480200409889221\n",
      "step: 24595 , time : 0.0\n",
      "train: loss: 367754.96875 acc: 0.9322050213813782  val: loss: 482725.03125 acc: 0.9203962087631226\n",
      "step: 24600 , time : 0.0\n",
      "train: loss: 1341643.75 acc: 0.7227100729942322  val: loss: 3385033.5 acc: 0.3936081528663635\n",
      "step: 24605 , time : 0.0\n",
      "train: loss: 1424086.625 acc: 0.7607969641685486  val: loss: 2033000.75 acc: 0.8184706568717957\n",
      "step: 24610 , time : 0.0\n",
      "train: loss: 529873.8125 acc: 0.6526618599891663  val: loss: 2285184.25 acc: 0.8227339386940002\n",
      "step: 24615 , time : 0.0\n",
      "train: loss: 529256.1875 acc: 0.7453126311302185  val: loss: 2026201.625 acc: 0.7729915976524353\n",
      "step: 24620 , time : 0.0\n",
      "train: loss: 685930.6875 acc: 0.8241174221038818  val: loss: 939819.9375 acc: 0.8588651418685913\n",
      "step: 24625 , time : 0.0\n",
      "train: loss: 1258981.125 acc: 0.5863593816757202  val: loss: 2580408.75 acc: 0.6914325952529907\n",
      "step: 24630 , time : 0.0\n",
      "train: loss: 505645.03125 acc: 0.7329410314559937  val: loss: 932489.625 acc: 0.6177681684494019\n",
      "step: 24635 , time : 0.0\n",
      "train: loss: 250665.71875 acc: 0.8039987087249756  val: loss: 1357632.375 acc: 0.7212426662445068\n",
      "step: 24640 , time : 0.0\n",
      "train: loss: 96352.515625 acc: 0.9084113240242004  val: loss: 564171.6875 acc: 0.7156732082366943\n",
      "step: 24645 , time : 0.0\n",
      "train: loss: 118554.4296875 acc: 0.8990485668182373  val: loss: 4861806.5 acc: 0.6528475880622864\n",
      "step: 24650 , time : 0.0\n",
      "train: loss: 33193.375 acc: 0.9723239541053772  val: loss: 1748607.125 acc: 0.6725075244903564\n",
      "step: 24655 , time : 0.0\n",
      "train: loss: 149321.90625 acc: 0.8959303498268127  val: loss: 1679834.125 acc: 0.6569266319274902\n",
      "step: 24660 , time : 0.0\n",
      "train: loss: 35002.3203125 acc: 0.9718752503395081  val: loss: 1717408.25 acc: 0.6738905310630798\n",
      "step: 24665 , time : 0.0010008811950683594\n",
      "train: loss: 284585.1875 acc: 0.821712076663971  val: loss: 1913689.5 acc: 0.7036839127540588\n",
      "step: 24670 , time : 0.0010008811950683594\n",
      "train: loss: 115704.8125 acc: 0.9098305106163025  val: loss: 3255298.75 acc: 0.6219546794891357\n",
      "step: 24675 , time : 0.0010006427764892578\n",
      "train: loss: 52641.41015625 acc: 0.951775074005127  val: loss: 1722531.875 acc: 0.6366971135139465\n",
      "step: 24680 , time : 0.0\n",
      "train: loss: 91212.6953125 acc: 0.8882217407226562  val: loss: 2877311.25 acc: 0.5897372364997864\n",
      "step: 24685 , time : 0.0\n",
      "train: loss: 365431.625 acc: 0.8589710593223572  val: loss: 462388.6875 acc: 0.7717224955558777\n",
      "step: 24690 , time : 0.0\n",
      "train: loss: 519178.3125 acc: 0.7378588914871216  val: loss: 999670.1875 acc: 0.6718625426292419\n",
      "step: 24695 , time : 0.0\n",
      "train: loss: 581830.875 acc: 0.7595837712287903  val: loss: 2574992.25 acc: 0.6463121771812439\n",
      "step: 24700 , time : 0.0\n",
      "train: loss: 664071.9375 acc: 0.7401005029678345  val: loss: 568081.6875 acc: 0.7455159425735474\n",
      "step: 24705 , time : 0.0\n",
      "train: loss: 171315.65625 acc: 0.8979384303092957  val: loss: 2714301.75 acc: 0.5513684153556824\n",
      "step: 24710 , time : 0.0\n",
      "train: loss: 614786.6875 acc: 0.7260448932647705  val: loss: 498359.75 acc: 0.7501952648162842\n",
      "step: 24715 , time : 0.0\n",
      "train: loss: 1909374.75 acc: 0.7631678581237793  val: loss: 1520670.875 acc: 0.6760517358779907\n",
      "step: 24720 , time : 0.0\n",
      "train: loss: 767465.3125 acc: 0.8543037176132202  val: loss: 1072992.0 acc: 0.6792431473731995\n",
      "step: 24725 , time : 0.0\n",
      "train: loss: 985700.125 acc: 0.9244918823242188  val: loss: 1778155.375 acc: 0.6340373754501343\n",
      "step: 24730 , time : 0.0\n",
      "train: loss: 722669.3125 acc: 0.9059951901435852  val: loss: 767243.25 acc: 0.8999822735786438\n",
      "step: 24735 , time : 0.0\n",
      "train: loss: 462664.5625 acc: 0.9214686155319214  val: loss: 889034.1875 acc: 0.8845738172531128\n",
      "step: 24740 , time : 0.0\n",
      "train: loss: 86070.2265625 acc: 0.9837043881416321  val: loss: 1935539.5 acc: 0.7437141537666321\n",
      "step: 24745 , time : 0.0\n",
      "train: loss: 124530.5703125 acc: 0.9878610372543335  val: loss: 546742.25 acc: 0.8649420738220215\n",
      "step: 24750 , time : 0.0\n",
      "train: loss: 144866.546875 acc: 0.9900780320167542  val: loss: 1721801.125 acc: 0.6034778356552124\n",
      "step: 24755 , time : 0.0\n",
      "train: loss: 136186.75 acc: 0.988506019115448  val: loss: 934261.5625 acc: 0.7661100625991821\n",
      "step: 24760 , time : 0.0\n",
      "train: loss: 90029.2578125 acc: 0.9891356229782104  val: loss: 844091.0 acc: 0.7023608684539795\n",
      "step: 24765 , time : 0.0\n",
      "train: loss: 33826.45703125 acc: 0.9940604567527771  val: loss: 2121750.75 acc: 0.664260745048523\n",
      "step: 24770 , time : 0.001001119613647461\n",
      "train: loss: 15946.59375 acc: 0.9869688153266907  val: loss: 1830284.375 acc: 0.7272149324417114\n",
      "step: 24775 , time : 0.0010006427764892578\n",
      "train: loss: 23621.970703125 acc: 0.9919350743293762  val: loss: 969555.625 acc: 0.6313133239746094\n",
      "step: 24780 , time : 0.0\n",
      "train: loss: 12175.6435546875 acc: 0.9767462015151978  val: loss: 526718.75 acc: 0.9382759928703308\n",
      "step: 24785 , time : 0.0\n",
      "train: loss: 16116.37109375 acc: 0.9362550377845764  val: loss: 624761.6875 acc: 0.8261192440986633\n",
      "step: 24790 , time : 0.001001119613647461\n",
      "train: loss: 8228.09765625 acc: 0.9809942245483398  val: loss: 1208489.125 acc: 0.35280168056488037\n",
      "step: 24795 , time : 0.0010004043579101562\n",
      "train: loss: 29426.646484375 acc: 0.9804601073265076  val: loss: 1656022.375 acc: 0.4545133709907532\n",
      "step: 24800 , time : 0.0\n",
      "train: loss: 16452.61328125 acc: 0.9708306193351746  val: loss: 1253303.125 acc: -1.2477824687957764\n",
      "step: 24805 , time : 0.0\n",
      "train: loss: 3928.135009765625 acc: 0.9889835715293884  val: loss: 875478.3125 acc: 0.871874213218689\n",
      "step: 24810 , time : 0.0\n",
      "train: loss: 11917.7138671875 acc: 0.9855386018753052  val: loss: 163944.625 acc: 0.9572328925132751\n",
      "step: 24815 , time : 0.0\n",
      "train: loss: 30756.990234375 acc: 0.9795504212379456  val: loss: 1952029.75 acc: -1.1849324703216553\n",
      "step: 24820 , time : 0.0\n",
      "train: loss: 14025.4228515625 acc: 0.9857516288757324  val: loss: 1860382.625 acc: 0.13542133569717407\n",
      "step: 24825 , time : 0.015625\n",
      "train: loss: 11630.15234375 acc: 0.9896703362464905  val: loss: 1247755.625 acc: 0.6321391463279724\n",
      "step: 24830 , time : 0.0\n",
      "train: loss: 83614.046875 acc: 0.9105463624000549  val: loss: 442771.03125 acc: 0.834069550037384\n",
      "step: 24835 , time : 0.0\n",
      "train: loss: 17652.62109375 acc: 0.9903210997581482  val: loss: 1821025.375 acc: 0.32743358612060547\n",
      "step: 24840 , time : 0.0\n",
      "train: loss: 10849.7587890625 acc: 0.9853329062461853  val: loss: 447256.5625 acc: 0.8433671593666077\n",
      "step: 24845 , time : 0.0\n",
      "train: loss: 10931.21875 acc: 0.9873566031455994  val: loss: 1413172.75 acc: 0.7564641237258911\n",
      "step: 24850 , time : 0.0\n",
      "train: loss: 26399.357421875 acc: 0.9906381964683533  val: loss: 1045485.25 acc: 0.689315915107727\n",
      "step: 24855 , time : 0.0\n",
      "train: loss: 34255.734375 acc: 0.9915145635604858  val: loss: 1082568.625 acc: 0.8297613263130188\n",
      "step: 24860 , time : 0.0\n",
      "train: loss: 37005.53125 acc: 0.9905993938446045  val: loss: 1629245.125 acc: 0.7457082271575928\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 24865 , time : 0.0\n",
      "train: loss: 55828.44921875 acc: 0.983893632888794  val: loss: 839202.75 acc: 0.8713569045066833\n",
      "step: 24870 , time : 0.0\n",
      "train: loss: 75426.484375 acc: 0.9838399291038513  val: loss: 609797.5 acc: 0.8352780342102051\n",
      "step: 24875 , time : 0.0010008811950683594\n",
      "train: loss: 72867.25 acc: 0.9718420505523682  val: loss: 89523.1953125 acc: 0.9877828359603882\n",
      "step: 24880 , time : 0.0\n",
      "train: loss: 92060.0546875 acc: 0.9788156151771545  val: loss: 1099343.125 acc: 0.6278880834579468\n",
      "step: 24885 , time : 0.001001119613647461\n",
      "train: loss: 319033.1875 acc: 0.8945858478546143  val: loss: 405606.71875 acc: 0.8548250198364258\n",
      "step: 24890 , time : 0.0\n",
      "train: loss: 214946.171875 acc: 0.9731953740119934  val: loss: 375985.28125 acc: 0.9521352648735046\n",
      "step: 24895 , time : 0.0010004043579101562\n",
      "train: loss: 117220.8203125 acc: 0.9902095794677734  val: loss: 130455.296875 acc: 0.9616943597793579\n",
      "step: 24900 , time : 0.0010006427764892578\n",
      "train: loss: 45271.91015625 acc: 0.9937825202941895  val: loss: 174566.890625 acc: 0.9469843506813049\n",
      "step: 24905 , time : 0.001001119613647461\n",
      "train: loss: 230002.046875 acc: 0.9655623435974121  val: loss: 1250468.75 acc: 0.7037075757980347\n",
      "step: 24910 , time : 0.0\n",
      "train: loss: 241725.640625 acc: 0.985194742679596  val: loss: 818681.5 acc: 0.9402042627334595\n",
      "step: 24915 , time : 0.0\n",
      "train: loss: 439676.03125 acc: 0.9668210744857788  val: loss: 498541.15625 acc: 0.9515017867088318\n",
      "step: 24920 , time : 0.001001119613647461\n",
      "train: loss: 515654.875 acc: 0.9707189798355103  val: loss: 1676923.375 acc: 0.7453931570053101\n",
      "step: 24925 , time : 0.0\n",
      "train: loss: 349746.625 acc: 0.96009761095047  val: loss: 805438.5 acc: 0.9249618649482727\n",
      "step: 24930 , time : 0.015624523162841797\n",
      "train: loss: 796047.5 acc: 0.9619993567466736  val: loss: 674569.0625 acc: 0.9244385361671448\n",
      "step: 24935 , time : 0.0\n",
      "train: loss: 1109291.5 acc: 0.969379723072052  val: loss: 1211890.625 acc: 0.8458950519561768\n",
      "step: 24940 , time : 0.0010008811950683594\n",
      "train: loss: 2432700.25 acc: 0.9382402896881104  val: loss: 205226.8125 acc: 0.9756321310997009\n",
      "step: 24945 , time : 0.001001119613647461\n",
      "train: loss: 1062550.875 acc: 0.9556905627250671  val: loss: 1115803.625 acc: 0.6435626745223999\n",
      "step: 24950 , time : 0.0\n",
      "train: loss: 211493.90625 acc: 0.9706087112426758  val: loss: 170255.875 acc: 0.9526878595352173\n",
      "step: 24955 , time : 0.0\n",
      "train: loss: 381168.78125 acc: 0.9489559531211853  val: loss: 462133.4375 acc: 0.9284922480583191\n",
      "step: 24960 , time : 0.0\n",
      "train: loss: 295054.78125 acc: 0.9557369351387024  val: loss: 2082935.375 acc: 0.7224577069282532\n",
      "step: 24965 , time : 0.0\n",
      "train: loss: 1709437.25 acc: 0.6522419452667236  val: loss: 976856.8125 acc: 0.8978745937347412\n",
      "step: 24970 , time : 0.0\n",
      "train: loss: 862268.625 acc: 0.7303463220596313  val: loss: 1613486.375 acc: 0.8710492253303528\n",
      "step: 24975 , time : 0.0\n",
      "train: loss: 481391.0 acc: 0.9082798957824707  val: loss: 1764926.375 acc: 0.8036580085754395\n",
      "step: 24980 , time : 0.0010008811950683594\n",
      "train: loss: 383885.71875 acc: 0.7835107445716858  val: loss: 168190.84375 acc: 0.9521040916442871\n",
      "step: 24985 , time : 0.001001119613647461\n",
      "train: loss: 335257.71875 acc: 0.9019857048988342  val: loss: 284637.78125 acc: 0.894954264163971\n",
      "step: 24990 , time : 0.0\n",
      "train: loss: 1193036.875 acc: 0.38655877113342285  val: loss: 764939.5 acc: 0.7786868810653687\n",
      "step: 24995 , time : 0.0010008811950683594\n",
      "train: loss: 510828.59375 acc: 0.6752486228942871  val: loss: 1458815.25 acc: 0.7234587669372559\n",
      "step: 25000 , time : 0.0\n",
      "train: loss: 757584.0625 acc: 0.6949176788330078  val: loss: 804505.75 acc: 0.7803786396980286\n",
      "step: 25005 , time : 0.0\n",
      "train: loss: 101720.6796875 acc: 0.9124824404716492  val: loss: 2789999.5 acc: 0.6562021374702454\n",
      "step: 25010 , time : 0.001001596450805664\n",
      "train: loss: 35925.46484375 acc: 0.9694026708602905  val: loss: 570057.75 acc: 0.7380142211914062\n",
      "step: 25015 , time : 0.0\n",
      "train: loss: 66186.296875 acc: 0.9535327553749084  val: loss: 2107338.0 acc: 0.5812530517578125\n",
      "step: 25020 , time : 0.0010008811950683594\n",
      "train: loss: 35279.91015625 acc: 0.9699208736419678  val: loss: 3025711.5 acc: 0.6517858505249023\n",
      "step: 25025 , time : 0.0010004043579101562\n",
      "train: loss: 26723.119140625 acc: 0.9796099066734314  val: loss: 606042.8125 acc: 0.6859458088874817\n",
      "step: 25030 , time : 0.0\n",
      "train: loss: 185951.71875 acc: 0.8950737118721008  val: loss: 836159.9375 acc: 0.7570746541023254\n",
      "step: 25035 , time : 0.0\n",
      "train: loss: 133128.46875 acc: 0.8874495029449463  val: loss: 162852.765625 acc: 0.8888309001922607\n",
      "step: 25040 , time : 0.0\n",
      "train: loss: 309191.875 acc: 0.7981848120689392  val: loss: 2965292.75 acc: 0.5973119735717773\n",
      "step: 25045 , time : 0.0\n",
      "train: loss: 103457.6484375 acc: 0.8935303092002869  val: loss: 339622.25 acc: 0.8476078510284424\n",
      "step: 25050 , time : 0.0\n",
      "train: loss: 195606.703125 acc: 0.8676294088363647  val: loss: 3481029.5 acc: 0.7013702392578125\n",
      "step: 25055 , time : 0.0\n",
      "train: loss: 641297.1875 acc: 0.701623797416687  val: loss: 650101.75 acc: 0.7382176518440247\n",
      "step: 25060 , time : 0.0\n",
      "train: loss: 315991.5 acc: 0.780426561832428  val: loss: 1310263.375 acc: 0.6865495443344116\n",
      "step: 25065 , time : 0.0\n",
      "train: loss: 144675.859375 acc: 0.8771341443061829  val: loss: 1093469.5 acc: 0.6769880056381226\n",
      "step: 25070 , time : 0.0\n",
      "train: loss: 602427.8125 acc: 0.7032185792922974  val: loss: 2704453.5 acc: 0.5952916145324707\n",
      "step: 25075 , time : 0.0\n",
      "train: loss: 535747.3125 acc: 0.7840724587440491  val: loss: 983796.5 acc: 0.7254948616027832\n",
      "step: 25080 , time : 0.0\n",
      "train: loss: 1566184.0 acc: 0.7854189872741699  val: loss: 1542489.75 acc: 0.7542937994003296\n",
      "step: 25085 , time : 0.0\n",
      "train: loss: 1239727.625 acc: 0.7788399457931519  val: loss: 642496.5 acc: 0.8437404632568359\n",
      "step: 25090 , time : 0.0\n",
      "train: loss: 813518.625 acc: 0.9414918422698975  val: loss: 1994773.125 acc: 0.25826317071914673\n",
      "step: 25095 , time : 0.0\n",
      "train: loss: 145174.765625 acc: 0.9869858622550964  val: loss: 911914.25 acc: 0.8150380849838257\n",
      "step: 25100 , time : 0.0\n",
      "train: loss: 366584.15625 acc: 0.9497756958007812  val: loss: 1882859.875 acc: 0.6490402817726135\n",
      "step: 25105 , time : 0.0010006427764892578\n",
      "train: loss: 69463.4375 acc: 0.9859375953674316  val: loss: 906325.625 acc: 0.6831047534942627\n",
      "step: 25110 , time : 0.0\n",
      "train: loss: 118560.484375 acc: 0.9895913600921631  val: loss: 567994.0 acc: 0.8657879829406738\n",
      "step: 25115 , time : 0.0010013580322265625\n",
      "train: loss: 99802.5625 acc: 0.9929285049438477  val: loss: 1513146.875 acc: 0.6277611255645752\n",
      "step: 25120 , time : 0.0\n",
      "train: loss: 51990.34375 acc: 0.9959705471992493  val: loss: 1535588.125 acc: 0.7610642910003662\n",
      "step: 25125 , time : 0.0\n",
      "train: loss: 74688.4921875 acc: 0.987372100353241  val: loss: 354622.46875 acc: 0.9557605981826782\n",
      "step: 25130 , time : 0.001001119613647461\n",
      "train: loss: 63715.80859375 acc: 0.97885662317276  val: loss: 2532100.5 acc: 0.6027570962905884\n",
      "step: 25135 , time : 0.001001119613647461\n",
      "train: loss: 45936.03515625 acc: 0.9882813692092896  val: loss: 374476.28125 acc: 0.8624981045722961\n",
      "step: 25140 , time : 0.0\n",
      "train: loss: 15039.01171875 acc: 0.9950704574584961  val: loss: 1241011.125 acc: 0.6803815960884094\n",
      "step: 25145 , time : 0.0\n",
      "train: loss: 18954.1640625 acc: 0.9862362742424011  val: loss: 466193.3125 acc: 0.8680950403213501\n",
      "step: 25150 , time : 0.0\n",
      "train: loss: 11115.533203125 acc: 0.9927488565444946  val: loss: 891976.5 acc: 0.8760943412780762\n",
      "step: 25155 , time : 0.0\n",
      "train: loss: 31690.888671875 acc: 0.9891881942749023  val: loss: 769446.625 acc: 0.6970562934875488\n",
      "step: 25160 , time : 0.0\n",
      "train: loss: 46661.75390625 acc: 0.9712535738945007  val: loss: 739841.75 acc: 0.09478336572647095\n",
      "step: 25165 , time : 0.0\n",
      "train: loss: 11344.7685546875 acc: 0.9658389091491699  val: loss: 1791472.5 acc: 0.7623162269592285\n",
      "step: 25170 , time : 0.0\n",
      "train: loss: 13270.4384765625 acc: 0.9740179181098938  val: loss: 695291.375 acc: 0.7005587816238403\n",
      "step: 25175 , time : 0.0\n",
      "train: loss: 11219.2529296875 acc: 0.9808764457702637  val: loss: 736671.25 acc: 0.8997864127159119\n",
      "step: 25180 , time : 0.0\n",
      "train: loss: 16616.37890625 acc: 0.9759795665740967  val: loss: 183985.40625 acc: 0.9716549515724182\n",
      "step: 25185 , time : 0.0\n",
      "train: loss: 16488.61328125 acc: 0.9734342098236084  val: loss: 226690.90625 acc: 0.871975302696228\n",
      "step: 25190 , time : 0.0\n",
      "train: loss: 30714.865234375 acc: 0.97585129737854  val: loss: 271135.5625 acc: 0.8500781059265137\n",
      "step: 25195 , time : 0.0\n",
      "train: loss: 29276.546875 acc: 0.9805371165275574  val: loss: 1810058.875 acc: 0.14452379941940308\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 25200 , time : 0.0\n",
      "train: loss: 20741.02734375 acc: 0.9875688552856445  val: loss: 621481.0 acc: 0.8728687763214111\n",
      "step: 25205 , time : 0.0\n",
      "train: loss: 9428.388671875 acc: 0.992745041847229  val: loss: 753382.25 acc: 0.9002220630645752\n",
      "step: 25210 , time : 0.0\n",
      "train: loss: 14753.40234375 acc: 0.9858348369598389  val: loss: 688353.5 acc: 0.877544105052948\n",
      "step: 25215 , time : 0.0010006427764892578\n",
      "train: loss: 91855.078125 acc: 0.9705776572227478  val: loss: 1449507.75 acc: 0.20954543352127075\n",
      "step: 25220 , time : 0.0010006427764892578\n",
      "train: loss: 30971.953125 acc: 0.9946997165679932  val: loss: 667592.25 acc: 0.8500906825065613\n",
      "step: 25225 , time : 0.0\n",
      "train: loss: 39848.8984375 acc: 0.9880250096321106  val: loss: 568171.625 acc: 0.9021965861320496\n",
      "step: 25230 , time : 0.0\n",
      "train: loss: 38553.859375 acc: 0.9868111610412598  val: loss: 1040963.875 acc: 0.873753011226654\n",
      "step: 25235 , time : 0.0\n",
      "train: loss: 51442.9296875 acc: 0.9830822944641113  val: loss: 1423993.125 acc: 0.8079255223274231\n",
      "step: 25240 , time : 0.0010004043579101562\n",
      "train: loss: 344715.6875 acc: 0.9067299365997314  val: loss: 1098456.75 acc: 0.9243645668029785\n",
      "step: 25245 , time : 0.0\n",
      "train: loss: 357650.65625 acc: 0.7306098937988281  val: loss: 559064.0 acc: 0.9528867602348328\n",
      "step: 25250 , time : 0.0\n",
      "train: loss: 83963.84375 acc: 0.9486717581748962  val: loss: 747903.5 acc: 0.6635392904281616\n",
      "step: 25255 , time : 0.0\n",
      "train: loss: 445458.875 acc: 0.9308046102523804  val: loss: 869800.3125 acc: 0.937086820602417\n",
      "step: 25260 , time : 0.0\n",
      "train: loss: 59514.98828125 acc: 0.994184136390686  val: loss: 662728.5 acc: 0.8199795484542847\n",
      "step: 25265 , time : 0.0\n",
      "train: loss: 706325.375 acc: 0.927727222442627  val: loss: 2013622.25 acc: 0.7600394487380981\n",
      "step: 25270 , time : 0.0\n",
      "train: loss: 203944.265625 acc: 0.9779385924339294  val: loss: 2043165.875 acc: 0.5446801781654358\n",
      "step: 25275 , time : 0.0\n",
      "train: loss: 194482.375 acc: 0.9539539217948914  val: loss: 1422130.375 acc: 0.8728999495506287\n",
      "step: 25280 , time : 0.0\n",
      "train: loss: 737387.375 acc: 0.951972246170044  val: loss: 820687.375 acc: 0.9292605519294739\n",
      "step: 25285 , time : 0.0\n",
      "train: loss: 507107.4375 acc: 0.9576283097267151  val: loss: 1801499.25 acc: 0.8170130252838135\n",
      "step: 25290 , time : 0.0\n",
      "train: loss: 271618.53125 acc: 0.9345004558563232  val: loss: 4266160.0 acc: 0.3673279285430908\n",
      "step: 25295 , time : 0.0\n",
      "train: loss: 1140838.125 acc: 0.9419428706169128  val: loss: 2189037.5 acc: 0.5505416393280029\n",
      "step: 25300 , time : 0.015625\n",
      "train: loss: 2323208.0 acc: 0.9315038919448853  val: loss: 805406.75 acc: 0.8975282311439514\n",
      "step: 25305 , time : 0.0\n",
      "train: loss: 1387651.625 acc: 0.9434888958930969  val: loss: 3285337.5 acc: -0.6257272958755493\n",
      "step: 25310 , time : 0.0\n",
      "train: loss: 512356.71875 acc: 0.958832859992981  val: loss: 205859.90625 acc: 0.9686545133590698\n",
      "step: 25315 , time : 0.0\n",
      "train: loss: 173308.90625 acc: 0.9894574284553528  val: loss: 1133194.0 acc: 0.8318628072738647\n",
      "step: 25320 , time : 0.0\n",
      "train: loss: 419118.0 acc: 0.9403902292251587  val: loss: 3700367.75 acc: 0.5639194250106812\n",
      "step: 25325 , time : 0.0\n",
      "train: loss: 358677.0 acc: 0.8171243667602539  val: loss: 832154.125 acc: 0.6333419680595398\n",
      "step: 25330 , time : 0.0\n",
      "train: loss: 299434.3125 acc: 0.9206148982048035  val: loss: 329084.9375 acc: 0.9482436180114746\n",
      "step: 25335 , time : 0.0010004043579101562\n",
      "train: loss: 1309317.125 acc: 0.7218711376190186  val: loss: 1101714.875 acc: 0.8639219403266907\n",
      "step: 25340 , time : 0.0\n",
      "train: loss: 773745.3125 acc: 0.8605567812919617  val: loss: 694706.125 acc: 0.7244101762771606\n",
      "step: 25345 , time : 0.0010006427764892578\n",
      "train: loss: 327781.53125 acc: 0.816521406173706  val: loss: 714218.75 acc: 0.8442389965057373\n",
      "step: 25350 , time : 0.0\n",
      "train: loss: 746359.6875 acc: 0.7902621030807495  val: loss: 1311882.125 acc: 0.8471436500549316\n",
      "step: 25355 , time : 0.0\n",
      "train: loss: 995126.375 acc: 0.5622310638427734  val: loss: 1095747.625 acc: 0.7336562871932983\n",
      "step: 25360 , time : 0.0\n",
      "train: loss: 628647.625 acc: 0.6913970112800598  val: loss: 1017806.4375 acc: 0.5678889751434326\n",
      "step: 25365 , time : 0.0\n",
      "train: loss: 280452.15625 acc: 0.8096993565559387  val: loss: 3031639.25 acc: 0.569496750831604\n",
      "step: 25370 , time : 0.0\n",
      "train: loss: 274251.625 acc: 0.7764930129051208  val: loss: 729467.125 acc: 0.701525092124939\n",
      "step: 25375 , time : 0.0\n",
      "train: loss: 222426.125 acc: 0.8657410144805908  val: loss: 789581.875 acc: 0.7963176369667053\n",
      "step: 25380 , time : 0.0\n",
      "train: loss: 104431.921875 acc: 0.9062818884849548  val: loss: 821549.125 acc: 0.7340548038482666\n",
      "step: 25385 , time : 0.0\n",
      "train: loss: 363930.59375 acc: 0.8166887760162354  val: loss: 3744464.75 acc: 0.6250630021095276\n",
      "step: 25390 , time : 0.0\n",
      "train: loss: 196057.453125 acc: 0.8878232836723328  val: loss: 1549259.875 acc: 0.6714229583740234\n",
      "step: 25395 , time : 0.0\n",
      "train: loss: 347453.59375 acc: 0.833345353603363  val: loss: 1384196.625 acc: 0.6407799124717712\n",
      "step: 25400 , time : 0.0\n",
      "train: loss: 56715.8984375 acc: 0.9498159289360046  val: loss: 3487303.5 acc: 0.6165245771408081\n",
      "step: 25405 , time : 0.0\n",
      "train: loss: 54722.5078125 acc: 0.9374014735221863  val: loss: 1396402.625 acc: 0.6509209871292114\n",
      "step: 25410 , time : 0.0\n",
      "train: loss: 194781.6875 acc: 0.7826467752456665  val: loss: 424292.78125 acc: 0.7546037435531616\n",
      "step: 25415 , time : 0.0\n",
      "train: loss: 67679.1484375 acc: 0.9319233894348145  val: loss: 1837331.25 acc: 0.6877988576889038\n",
      "step: 25420 , time : 0.0\n",
      "train: loss: 176306.703125 acc: 0.8407405018806458  val: loss: 1104922.875 acc: 0.6966668367385864\n",
      "step: 25425 , time : 0.0\n",
      "train: loss: 377917.40625 acc: 0.8080281019210815  val: loss: 678566.0 acc: 0.6883673667907715\n",
      "step: 25430 , time : 0.0010006427764892578\n",
      "train: loss: 586837.75 acc: 0.7629770040512085  val: loss: 615006.25 acc: 0.7229763865470886\n",
      "step: 25435 , time : 0.0\n",
      "train: loss: 235841.984375 acc: 0.8399563431739807  val: loss: 919968.75 acc: 0.7507826685905457\n",
      "step: 25440 , time : 0.0010008811950683594\n",
      "train: loss: 86961.8125 acc: 0.9267263412475586  val: loss: 1710081.125 acc: 0.6877446174621582\n",
      "step: 25445 , time : 0.0010006427764892578\n",
      "train: loss: 1144945.5 acc: 0.7112041711807251  val: loss: 845774.75 acc: 0.8100369572639465\n",
      "step: 25450 , time : 0.0\n",
      "train: loss: 1091559.5 acc: 0.7964911460876465  val: loss: 2199670.0 acc: 0.7448153495788574\n",
      "step: 25455 , time : 0.0010006427764892578\n",
      "train: loss: 738859.625 acc: 0.9396939873695374  val: loss: 1548473.5 acc: 0.684227466583252\n",
      "step: 25460 , time : 0.0\n",
      "train: loss: 201085.46875 acc: 0.9822625517845154  val: loss: 1532451.375 acc: 0.02956223487854004\n",
      "step: 25465 , time : 0.0\n",
      "train: loss: 298539.0625 acc: 0.9317193031311035  val: loss: 2453197.75 acc: 0.501998782157898\n",
      "step: 25470 , time : 0.0\n",
      "train: loss: 109882.8984375 acc: 0.979751706123352  val: loss: 299045.5625 acc: 0.9570250511169434\n",
      "step: 25475 , time : 0.015624523162841797\n",
      "train: loss: 106139.9296875 acc: 0.9913191199302673  val: loss: 1309879.75 acc: 0.7566200494766235\n",
      "step: 25480 , time : 0.0\n",
      "train: loss: 83543.6640625 acc: 0.9938662648200989  val: loss: 310878.03125 acc: 0.9129949808120728\n",
      "step: 25485 , time : 0.0\n",
      "train: loss: 101043.1875 acc: 0.9932259917259216  val: loss: 272256.4375 acc: 0.8035758137702942\n",
      "step: 25490 , time : 0.0\n",
      "train: loss: 52990.1015625 acc: 0.9927601218223572  val: loss: 598114.875 acc: 0.561454713344574\n",
      "step: 25495 , time : 0.0\n",
      "train: loss: 25315.91796875 acc: 0.9869509935379028  val: loss: 723431.5625 acc: 0.8735836744308472\n",
      "step: 25500 , time : 0.0\n",
      "train: loss: 32147.921875 acc: 0.9862886071205139  val: loss: 708945.0625 acc: 0.7600840330123901\n",
      "step: 25505 , time : 0.0\n",
      "train: loss: 98734.625 acc: 0.9777405858039856  val: loss: 821204.75 acc: 0.8357090950012207\n",
      "step: 25510 , time : 0.0\n",
      "train: loss: 10863.1923828125 acc: 0.9922846555709839  val: loss: 229869.109375 acc: 0.9020470380783081\n",
      "step: 25515 , time : 0.0\n",
      "train: loss: 28181.537109375 acc: 0.9815840721130371  val: loss: 2798772.5 acc: 0.6743422746658325\n",
      "step: 25520 , time : 0.0\n",
      "train: loss: 18667.990234375 acc: 0.9703762531280518  val: loss: 557152.0625 acc: 0.8452519774436951\n",
      "step: 25525 , time : 0.0\n",
      "train: loss: 6593.59521484375 acc: 0.9738818407058716  val: loss: 130378.390625 acc: 0.9707024097442627\n",
      "step: 25530 , time : 0.0\n",
      "train: loss: 18019.142578125 acc: 0.9496212601661682  val: loss: 1293817.125 acc: 0.8210782408714294\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 25535 , time : 0.0\n",
      "train: loss: 17129.142578125 acc: 0.9627317786216736  val: loss: 156404.703125 acc: 0.9721359610557556\n",
      "step: 25540 , time : 0.0010008811950683594\n",
      "train: loss: 11397.0791015625 acc: 0.9854401350021362  val: loss: 961068.875 acc: 0.8462086915969849\n",
      "step: 25545 , time : 0.0\n",
      "train: loss: 14828.8525390625 acc: 0.9633808135986328  val: loss: 515380.0 acc: 0.9463838934898376\n",
      "step: 25550 , time : 0.0\n",
      "train: loss: 46687.6328125 acc: 0.9279956817626953  val: loss: 488867.28125 acc: 0.9526242017745972\n",
      "step: 25555 , time : 0.0\n",
      "train: loss: 18258.748046875 acc: 0.9843555092811584  val: loss: 469346.59375 acc: 0.9435781240463257\n",
      "step: 25560 , time : 0.0\n",
      "train: loss: 24436.49609375 acc: 0.9909089803695679  val: loss: 491938.5 acc: 0.9331957697868347\n",
      "step: 25565 , time : 0.0\n",
      "train: loss: 19574.52734375 acc: 0.9895408749580383  val: loss: 1155463.875 acc: 0.9014238119125366\n",
      "step: 25570 , time : 0.0010006427764892578\n",
      "train: loss: 14467.7275390625 acc: 0.9796687960624695  val: loss: 1088771.375 acc: 0.664884090423584\n",
      "step: 25575 , time : 0.0\n",
      "train: loss: 5988.4921875 acc: 0.987210214138031  val: loss: 779995.25 acc: 0.6288110017776489\n",
      "step: 25580 , time : 0.0\n",
      "train: loss: 10017.8828125 acc: 0.9962670803070068  val: loss: 1215155.625 acc: 0.8008493185043335\n",
      "step: 25585 , time : 0.0\n",
      "train: loss: 101298.734375 acc: 0.9801990389823914  val: loss: 1801288.625 acc: 0.4072597026824951\n",
      "step: 25590 , time : 0.0\n",
      "train: loss: 44490.46875 acc: 0.9883338212966919  val: loss: 1759258.25 acc: 0.6801608204841614\n",
      "step: 25595 , time : 0.0010006427764892578\n",
      "train: loss: 51155.75390625 acc: 0.9487815499305725  val: loss: 2437873.25 acc: 0.6250089406967163\n",
      "step: 25600 , time : 0.0\n",
      "train: loss: 36302.23828125 acc: 0.9774019122123718  val: loss: 1519590.75 acc: 0.8873889446258545\n",
      "step: 25605 , time : 0.0010008811950683594\n",
      "train: loss: 55864.0234375 acc: 0.9838454723358154  val: loss: 3949970.5 acc: 0.01137453317642212\n",
      "step: 25610 , time : 0.0\n",
      "train: loss: 154733.859375 acc: 0.9394322633743286  val: loss: 2213111.0 acc: 0.5941687822341919\n",
      "step: 25615 , time : 0.015624761581420898\n",
      "train: loss: 152788.734375 acc: 0.8797130584716797  val: loss: 2622879.25 acc: 0.6719450950622559\n",
      "step: 25620 , time : 0.0\n",
      "train: loss: 254335.671875 acc: 0.9471625089645386  val: loss: 1102340.625 acc: 0.6799676418304443\n",
      "step: 25625 , time : 0.0\n",
      "train: loss: 168777.90625 acc: 0.9797234535217285  val: loss: 1521201.375 acc: 0.6738225817680359\n",
      "step: 25630 , time : 0.0\n",
      "train: loss: 65242.07421875 acc: 0.9940349459648132  val: loss: 623038.5625 acc: 0.8252978324890137\n",
      "step: 25635 , time : 0.0\n",
      "train: loss: 39600.25390625 acc: 0.9918776154518127  val: loss: 2449924.5 acc: 0.7176612615585327\n",
      "step: 25640 , time : 0.0\n",
      "train: loss: 172768.140625 acc: 0.961921751499176  val: loss: 440502.03125 acc: 0.9336113333702087\n",
      "step: 25645 , time : 0.0\n",
      "train: loss: 733548.1875 acc: 0.9644010066986084  val: loss: 1148940.75 acc: 0.780955970287323\n",
      "step: 25650 , time : 0.0\n",
      "train: loss: 434085.09375 acc: 0.9680840969085693  val: loss: 1921573.625 acc: 0.3579804301261902\n",
      "step: 25655 , time : 0.0\n",
      "train: loss: 324875.75 acc: 0.942825973033905  val: loss: 2107967.25 acc: 0.1435524821281433\n",
      "step: 25660 , time : 0.001001119613647461\n",
      "train: loss: 2706246.75 acc: 0.8817721605300903  val: loss: 662532.5625 acc: 0.874030590057373\n",
      "step: 25665 , time : 0.0\n",
      "train: loss: 406453.5 acc: 0.9823805093765259  val: loss: 937981.625 acc: 0.7941628098487854\n",
      "step: 25670 , time : 0.0010006427764892578\n",
      "train: loss: 764545.5 acc: 0.9714028239250183  val: loss: 1324555.875 acc: 0.6365818977355957\n",
      "step: 25675 , time : 0.0009999275207519531\n",
      "train: loss: 1547001.125 acc: 0.9165730476379395  val: loss: 405452.125 acc: 0.8553011417388916\n",
      "step: 25680 , time : 0.0\n",
      "train: loss: 415527.3125 acc: 0.966452956199646  val: loss: 303849.40625 acc: 0.9088957905769348\n",
      "step: 25685 , time : 0.0010008811950683594\n",
      "train: loss: 463974.8125 acc: 0.9351831674575806  val: loss: 432495.5625 acc: 0.9557088613510132\n",
      "step: 25690 , time : 0.0\n",
      "train: loss: 688100.75 acc: 0.9245527982711792  val: loss: 1025139.3125 acc: 0.8757989406585693\n",
      "step: 25695 , time : 0.0\n",
      "train: loss: 556940.75 acc: 0.8995051383972168  val: loss: 1246824.625 acc: 0.9153885841369629\n",
      "step: 25700 , time : 0.0\n",
      "train: loss: 1147989.25 acc: 0.2990819215774536  val: loss: 617704.125 acc: 0.8950003385543823\n",
      "step: 25705 , time : 0.0\n",
      "train: loss: 291224.65625 acc: 0.8219100832939148  val: loss: 1360714.0 acc: 0.8552424311637878\n",
      "step: 25710 , time : 0.015624761581420898\n",
      "train: loss: 340836.09375 acc: 0.8732473850250244  val: loss: 889666.875 acc: 0.5845940113067627\n",
      "step: 25715 , time : 0.0\n",
      "train: loss: 322431.3125 acc: 0.840631902217865  val: loss: 900032.25 acc: 0.8335685729980469\n",
      "step: 25720 , time : 0.0\n",
      "train: loss: 676505.375 acc: 0.6599413156509399  val: loss: 681064.4375 acc: 0.7810986042022705\n",
      "step: 25725 , time : 0.0\n",
      "train: loss: 959590.0625 acc: 0.6303619146347046  val: loss: 610624.0625 acc: 0.7921797633171082\n",
      "step: 25730 , time : 0.0\n",
      "train: loss: 449877.65625 acc: 0.7798020839691162  val: loss: 437618.21875 acc: 0.7463202476501465\n",
      "step: 25735 , time : 0.0\n",
      "train: loss: 166072.90625 acc: 0.8969243764877319  val: loss: 856604.875 acc: 0.6571622490882874\n",
      "step: 25740 , time : 0.0\n",
      "train: loss: 50578.33984375 acc: 0.9476993083953857  val: loss: 2710210.5 acc: 0.6246793866157532\n",
      "step: 25745 , time : 0.0\n",
      "train: loss: 371267.6875 acc: 0.7957031726837158  val: loss: 1872684.375 acc: 0.6342983841896057\n",
      "step: 25750 , time : 0.0\n",
      "train: loss: 149544.453125 acc: 0.9048210382461548  val: loss: 2702122.25 acc: 0.599936306476593\n",
      "step: 25755 , time : 0.0\n",
      "train: loss: 110309.921875 acc: 0.9337501525878906  val: loss: 632282.1875 acc: 0.7322189807891846\n",
      "step: 25760 , time : 0.0\n",
      "train: loss: 288004.71875 acc: 0.858684778213501  val: loss: 773218.3125 acc: 0.7347533702850342\n",
      "step: 25765 , time : 0.0\n",
      "train: loss: 52916.85546875 acc: 0.954751193523407  val: loss: 1102159.5 acc: 0.6239427328109741\n",
      "step: 25770 , time : 0.0\n",
      "train: loss: 545907.5 acc: 0.729584813117981  val: loss: 527328.0 acc: 0.747979998588562\n",
      "step: 25775 , time : 0.0\n",
      "train: loss: 100411.6015625 acc: 0.909300684928894  val: loss: 916142.0 acc: 0.7100322842597961\n",
      "step: 25780 , time : 0.0\n",
      "train: loss: 153305.125 acc: 0.865514874458313  val: loss: 1765142.125 acc: 0.6571037769317627\n",
      "step: 25785 , time : 0.0\n",
      "train: loss: 512877.9375 acc: 0.7614173889160156  val: loss: 944601.3125 acc: 0.6609863042831421\n",
      "step: 25790 , time : 0.0\n",
      "train: loss: 651359.125 acc: 0.6944923996925354  val: loss: 362598.96875 acc: 0.7987643480300903\n",
      "step: 25795 , time : 0.001001119613647461\n",
      "train: loss: 311220.59375 acc: 0.7855552434921265  val: loss: 2904615.5 acc: 0.5172017812728882\n",
      "step: 25800 , time : 0.0010004043579101562\n",
      "train: loss: 146179.359375 acc: 0.8513658046722412  val: loss: 573149.3125 acc: 0.7016686797142029\n",
      "step: 25805 , time : 0.001001119613647461\n",
      "train: loss: 895764.3125 acc: 0.6833395957946777  val: loss: 3659712.0 acc: 0.5492271184921265\n",
      "step: 25810 , time : 0.0\n",
      "train: loss: 2031331.875 acc: 0.6700593829154968  val: loss: 1902926.125 acc: 0.7451585531234741\n",
      "step: 25815 , time : 0.0\n",
      "train: loss: 600521.875 acc: 0.7573428153991699  val: loss: 679328.75 acc: 0.1696506142616272\n",
      "step: 25820 , time : 0.0\n",
      "train: loss: 838658.125 acc: 0.9437709450721741  val: loss: 634292.375 acc: 0.7678809762001038\n",
      "step: 25825 , time : 0.0\n",
      "train: loss: 775672.0 acc: 0.9191254377365112  val: loss: 133839.625 acc: 0.9632677435874939\n",
      "step: 25830 , time : 0.0\n",
      "train: loss: 66302.0078125 acc: 0.9802976250648499  val: loss: 124348.140625 acc: 0.9548719525337219\n",
      "step: 25835 , time : 0.0\n",
      "train: loss: 66575.9921875 acc: 0.9909286499023438  val: loss: 529489.75 acc: 0.8646221160888672\n",
      "step: 25840 , time : 0.0\n",
      "train: loss: 598283.5 acc: 0.9427746534347534  val: loss: 1215965.0 acc: 0.6465520262718201\n",
      "step: 25845 , time : 0.0\n",
      "train: loss: 348372.46875 acc: 0.9781991839408875  val: loss: 3421947.75 acc: 0.6503075361251831\n",
      "step: 25850 , time : 0.0\n",
      "train: loss: 79925.171875 acc: 0.9937885999679565  val: loss: 488033.0625 acc: 0.9230526685714722\n",
      "step: 25855 , time : 0.0\n",
      "train: loss: 53821.1015625 acc: 0.9946212768554688  val: loss: 1466110.5 acc: 0.8637759685516357\n",
      "step: 25860 , time : 0.0\n",
      "train: loss: 56727.1171875 acc: 0.9927768111228943  val: loss: 462855.75 acc: 0.9164681434631348\n",
      "step: 25865 , time : 0.0\n",
      "train: loss: 43461.47265625 acc: 0.9908847212791443  val: loss: 495964.15625 acc: 0.7239197492599487\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 25870 , time : 0.0\n",
      "train: loss: 9606.7060546875 acc: 0.9575369954109192  val: loss: 835443.75 acc: 0.9174408316612244\n",
      "step: 25875 , time : 0.0\n",
      "train: loss: 12317.5126953125 acc: 0.9710201025009155  val: loss: 605393.25 acc: 0.9077324271202087\n",
      "step: 25880 , time : 0.0\n",
      "train: loss: 8464.4658203125 acc: 0.9871652722358704  val: loss: 143419.0 acc: 0.9565804600715637\n",
      "step: 25885 , time : 0.0\n",
      "train: loss: 27087.978515625 acc: 0.9522984623908997  val: loss: 1287895.5 acc: 0.6970693469047546\n",
      "step: 25890 , time : 0.0\n",
      "train: loss: 7164.42041015625 acc: 0.9875364899635315  val: loss: 1526095.375 acc: 0.7777761220932007\n",
      "step: 25895 , time : 0.0\n",
      "train: loss: 7718.509765625 acc: 0.9342734813690186  val: loss: 563388.25 acc: 0.9360969066619873\n",
      "step: 25900 , time : 0.001001119613647461\n",
      "train: loss: 6799.6513671875 acc: 0.9869125485420227  val: loss: 1517577.5 acc: 0.7794674038887024\n",
      "step: 25905 , time : 0.0010004043579101562\n",
      "train: loss: 17573.646484375 acc: 0.9499872922897339  val: loss: 1052010.875 acc: 0.9044132232666016\n",
      "step: 25910 , time : 0.0\n",
      "train: loss: 8400.287109375 acc: 0.9794307947158813  val: loss: 1802388.5 acc: 0.5973092317581177\n",
      "step: 25915 , time : 0.0\n",
      "train: loss: 19878.19140625 acc: 0.9852895736694336  val: loss: 1811620.25 acc: 0.7896008491516113\n",
      "step: 25920 , time : 0.0\n",
      "train: loss: 26603.283203125 acc: 0.9816939830780029  val: loss: 1458953.375 acc: 0.8657962083816528\n",
      "step: 25925 , time : 0.0\n",
      "train: loss: 37879.234375 acc: 0.9650177359580994  val: loss: 1194298.625 acc: 0.42770087718963623\n",
      "step: 25930 , time : 0.0\n",
      "train: loss: 34477.49609375 acc: 0.9709079265594482  val: loss: 922928.6875 acc: 0.8445687294006348\n",
      "step: 25935 , time : 0.0\n",
      "train: loss: 13314.0263671875 acc: 0.9793753027915955  val: loss: 2057697.375 acc: 0.7386646866798401\n",
      "step: 25940 , time : 0.0\n",
      "train: loss: 13345.2333984375 acc: 0.9686707258224487  val: loss: 1721612.5 acc: 0.8300228714942932\n",
      "step: 25945 , time : 0.0\n",
      "train: loss: 14394.1279296875 acc: 0.9944475889205933  val: loss: 2696731.25 acc: 0.021419405937194824\n",
      "step: 25950 , time : 0.0\n",
      "train: loss: 79956.0703125 acc: 0.9850278496742249  val: loss: 1460872.625 acc: 0.5011134147644043\n",
      "step: 25955 , time : 0.0\n",
      "train: loss: 35127.1640625 acc: 0.9916117191314697  val: loss: 2099478.25 acc: 0.7664798498153687\n",
      "step: 25960 , time : 0.0\n",
      "train: loss: 29273.798828125 acc: 0.9905611872673035  val: loss: 740471.875 acc: 0.9051249623298645\n",
      "step: 25965 , time : 0.0\n",
      "train: loss: 30417.078125 acc: 0.9928162693977356  val: loss: 1333776.875 acc: -0.20843982696533203\n",
      "step: 25970 , time : 0.0\n",
      "train: loss: 81100.4921875 acc: 0.9674895405769348  val: loss: 908293.9375 acc: 0.6016378998756409\n",
      "step: 25975 , time : 0.015626192092895508\n",
      "train: loss: 63021.40234375 acc: 0.9836394786834717  val: loss: 2198604.5 acc: 0.4577288031578064\n",
      "step: 25980 , time : 0.0\n",
      "train: loss: 67359.5625 acc: 0.9596874713897705  val: loss: 802179.9375 acc: 0.927108645439148\n",
      "step: 25985 , time : 0.0\n",
      "train: loss: 983493.625 acc: 0.8144500255584717  val: loss: 1161239.875 acc: 0.8836777806282043\n",
      "step: 25990 , time : 0.0\n",
      "train: loss: 110112.484375 acc: 0.9891574382781982  val: loss: 1404998.625 acc: 0.843718409538269\n",
      "step: 25995 , time : 0.0010004043579101562\n",
      "train: loss: 61197.22265625 acc: 0.9933973550796509  val: loss: 1074532.75 acc: 0.8123153448104858\n",
      "step: 26000 , time : 0.0\n",
      "train: loss: 325886.1875 acc: 0.9626357555389404  val: loss: 1758611.75 acc: -0.03959083557128906\n",
      "step: 26005 , time : 0.001001119613647461\n",
      "train: loss: 519208.3125 acc: 0.9547030925750732  val: loss: 1627803.625 acc: 0.7254019975662231\n",
      "step: 26010 , time : 0.0010008811950683594\n",
      "train: loss: 385611.0625 acc: 0.9690797924995422  val: loss: 1451421.875 acc: 0.6912655830383301\n",
      "step: 26015 , time : 0.001001119613647461\n",
      "train: loss: 846404.9375 acc: 0.9024288654327393  val: loss: 1062933.625 acc: 0.8056915998458862\n",
      "step: 26020 , time : 0.0\n",
      "train: loss: 291807.5625 acc: 0.9627585411071777  val: loss: 1339635.5 acc: 0.8323715329170227\n",
      "step: 26025 , time : 0.0\n",
      "train: loss: 664250.5625 acc: 0.9429131150245667  val: loss: 2021540.625 acc: 0.4808911681175232\n",
      "step: 26030 , time : 0.0\n",
      "train: loss: 718162.3125 acc: 0.9788358807563782  val: loss: 842160.875 acc: 0.6679496765136719\n",
      "step: 26035 , time : 0.0\n",
      "train: loss: 2016174.25 acc: 0.9315218925476074  val: loss: 777198.1875 acc: 0.4900975227355957\n",
      "step: 26040 , time : 0.0\n",
      "train: loss: 650460.375 acc: 0.9772254824638367  val: loss: 1344157.0 acc: 0.6425973176956177\n",
      "step: 26045 , time : 0.0\n",
      "train: loss: 506276.8125 acc: 0.9589958190917969  val: loss: 692924.25 acc: 0.8358675241470337\n",
      "step: 26050 , time : 0.0\n",
      "train: loss: 158148.0625 acc: 0.9795078039169312  val: loss: 1055924.0 acc: 0.7986574172973633\n",
      "step: 26055 , time : 0.00099945068359375\n",
      "train: loss: 373028.34375 acc: 0.911483108997345  val: loss: 433490.6875 acc: 0.9370178580284119\n",
      "step: 26060 , time : 0.0\n",
      "train: loss: 385348.03125 acc: 0.8955231308937073  val: loss: 449169.875 acc: 0.8766027688980103\n",
      "step: 26065 , time : 0.0\n",
      "train: loss: 1162209.25 acc: 0.8135092258453369  val: loss: 1223680.75 acc: 0.7776291966438293\n",
      "step: 26070 , time : 0.0\n",
      "train: loss: 1390178.0 acc: 0.5912984609603882  val: loss: 991835.875 acc: 0.6423883438110352\n",
      "step: 26075 , time : 0.015625953674316406\n",
      "train: loss: 497672.5625 acc: 0.8528928756713867  val: loss: 3694807.0 acc: 0.4453837275505066\n",
      "step: 26080 , time : 0.0\n",
      "train: loss: 976982.5 acc: 0.8358253240585327  val: loss: 261220.65625 acc: 0.8960191607475281\n",
      "step: 26085 , time : 0.0\n",
      "train: loss: 1531891.125 acc: 0.4787648916244507  val: loss: 807733.1875 acc: 0.8342285752296448\n",
      "step: 26090 , time : 0.0\n",
      "train: loss: 1771658.625 acc: -0.19752740859985352  val: loss: 633753.75 acc: 0.7932751178741455\n",
      "step: 26095 , time : 0.0010008811950683594\n",
      "train: loss: 898617.25 acc: 0.63031005859375  val: loss: 866128.4375 acc: 0.620693027973175\n",
      "step: 26100 , time : 0.0\n",
      "train: loss: 386936.1875 acc: 0.7616627812385559  val: loss: 5652676.5 acc: 0.5744322538375854\n",
      "step: 26105 , time : 0.001001119613647461\n",
      "train: loss: 359325.0 acc: 0.8005251884460449  val: loss: 1430391.375 acc: 0.6573203802108765\n",
      "step: 26110 , time : 0.001001119613647461\n",
      "train: loss: 1252433.125 acc: 0.6919761300086975  val: loss: 791449.625 acc: 0.70384681224823\n",
      "step: 26115 , time : 0.0\n",
      "train: loss: 113002.2421875 acc: 0.9059659838676453  val: loss: 751656.75 acc: 0.7714391350746155\n",
      "step: 26120 , time : 0.0\n",
      "train: loss: 292699.71875 acc: 0.816025972366333  val: loss: 791330.75 acc: 0.6967203617095947\n",
      "step: 26125 , time : 0.0\n",
      "train: loss: 149009.4375 acc: 0.9037460684776306  val: loss: 728084.875 acc: 0.6933194398880005\n",
      "step: 26130 , time : 0.0\n",
      "train: loss: 325499.4375 acc: 0.8455806374549866  val: loss: 1328692.25 acc: 0.7122223377227783\n",
      "step: 26135 , time : 0.0\n",
      "train: loss: 114317.1640625 acc: 0.9201459884643555  val: loss: 549701.625 acc: 0.7345972061157227\n",
      "step: 26140 , time : 0.0\n",
      "train: loss: 126797.453125 acc: 0.8547418713569641  val: loss: 4594606.5 acc: 0.5183104872703552\n",
      "step: 26145 , time : 0.0\n",
      "train: loss: 222496.9375 acc: 0.8558206558227539  val: loss: 1409468.75 acc: 0.6550805568695068\n",
      "step: 26150 , time : 0.0\n",
      "train: loss: 474515.1875 acc: 0.7815577983856201  val: loss: 2921972.25 acc: 0.59458988904953\n",
      "step: 26155 , time : 0.0\n",
      "train: loss: 482212.8125 acc: 0.795067548751831  val: loss: 711926.875 acc: 0.6638622283935547\n",
      "step: 26160 , time : 0.01562666893005371\n",
      "train: loss: 1483528.875 acc: 0.6384618282318115  val: loss: 1615508.125 acc: 0.6474423408508301\n",
      "step: 26165 , time : 0.0\n",
      "train: loss: 450868.125 acc: 0.775579571723938  val: loss: 6020452.5 acc: 0.6085938215255737\n",
      "step: 26170 , time : 0.0\n",
      "train: loss: 529912.375 acc: 0.7589095830917358  val: loss: 1360014.375 acc: 0.7088063955307007\n",
      "step: 26175 , time : 0.0\n",
      "train: loss: 2483521.0 acc: 0.6882840394973755  val: loss: 2481762.25 acc: 0.7528611421585083\n",
      "step: 26180 , time : 0.00099945068359375\n",
      "train: loss: 972409.3125 acc: 0.7832837700843811  val: loss: 1794333.375 acc: 0.8282383680343628\n",
      "step: 26185 , time : 0.001001119613647461\n",
      "train: loss: 1147027.375 acc: 0.8966993689537048  val: loss: 2099913.75 acc: 0.8797525763511658\n",
      "step: 26190 , time : 0.0\n",
      "train: loss: 229026.96875 acc: 0.9754671454429626  val: loss: 681930.1875 acc: 0.8752788305282593\n",
      "step: 26195 , time : 0.0\n",
      "train: loss: 196560.015625 acc: 0.9684385061264038  val: loss: 925021.125 acc: 0.8967955708503723\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 26200 , time : 0.0\n",
      "train: loss: 250999.703125 acc: 0.95234215259552  val: loss: 339987.0625 acc: 0.9361841082572937\n",
      "step: 26205 , time : 0.0010008811950683594\n",
      "train: loss: 175017.609375 acc: 0.9821593761444092  val: loss: 1553594.125 acc: 0.37426960468292236\n",
      "step: 26210 , time : 0.0\n",
      "train: loss: 192790.921875 acc: 0.9863002300262451  val: loss: 1115819.875 acc: 0.5601776838302612\n",
      "step: 26215 , time : 0.0010004043579101562\n",
      "train: loss: 81368.296875 acc: 0.9946773052215576  val: loss: 847250.875 acc: 0.8899064064025879\n",
      "step: 26220 , time : 0.0010004043579101562\n",
      "train: loss: 140441.53125 acc: 0.9845089912414551  val: loss: 1162291.875 acc: 0.701176643371582\n",
      "step: 26225 , time : 0.0\n",
      "train: loss: 46954.0390625 acc: 0.9926820993423462  val: loss: 1730866.25 acc: 0.7170718908309937\n",
      "step: 26230 , time : 0.0\n",
      "train: loss: 64714.625 acc: 0.9907677173614502  val: loss: 1134831.625 acc: 0.8659240007400513\n",
      "step: 26235 , time : 0.001001119613647461\n",
      "train: loss: 14484.59375 acc: 0.990746021270752  val: loss: 1106582.875 acc: 0.9156302809715271\n",
      "step: 26240 , time : 0.0\n",
      "train: loss: 25021.796875 acc: 0.9943782091140747  val: loss: 1224640.625 acc: 0.8441328406333923\n",
      "step: 26245 , time : 0.0\n",
      "train: loss: 11081.8623046875 acc: 0.9947022199630737  val: loss: 2396446.0 acc: 0.3873162269592285\n",
      "step: 26250 , time : 0.0\n",
      "train: loss: 16613.365234375 acc: 0.9873491525650024  val: loss: 2724749.0 acc: 0.5278522968292236\n",
      "step: 26255 , time : 0.0\n",
      "train: loss: 18870.298828125 acc: 0.9869758486747742  val: loss: 3218195.25 acc: -0.8427995443344116\n",
      "step: 26260 , time : 0.0\n",
      "train: loss: 11455.279296875 acc: 0.980102002620697  val: loss: 1044942.75 acc: 0.7822264432907104\n",
      "step: 26265 , time : 0.0\n",
      "train: loss: 6077.27880859375 acc: 0.9833970665931702  val: loss: 3180169.75 acc: 0.47502583265304565\n",
      "step: 26270 , time : 0.015624761581420898\n",
      "train: loss: 12171.2685546875 acc: 0.9602181315422058  val: loss: 1882720.25 acc: 0.7422872185707092\n",
      "step: 26275 , time : 0.0\n",
      "train: loss: 8419.9169921875 acc: 0.9738179445266724  val: loss: 1120110.375 acc: 0.7514710426330566\n",
      "step: 26280 , time : 0.0\n",
      "train: loss: 33236.29296875 acc: 0.9726102948188782  val: loss: 566675.875 acc: 0.9294513463973999\n",
      "step: 26285 , time : 0.0\n",
      "train: loss: 37869.203125 acc: 0.9863630533218384  val: loss: 3723458.25 acc: -1.7036042213439941\n",
      "step: 26290 , time : 0.0\n",
      "train: loss: 23990.681640625 acc: 0.9851076602935791  val: loss: 1960038.125 acc: -0.05520355701446533\n",
      "step: 26295 , time : 0.0\n",
      "train: loss: 49994.4140625 acc: 0.9743541479110718  val: loss: 1493110.625 acc: 0.41479355096817017\n",
      "step: 26300 , time : 0.0010008811950683594\n",
      "train: loss: 16883.11328125 acc: 0.9813817739486694  val: loss: 1705838.375 acc: 0.6631516814231873\n",
      "step: 26305 , time : 0.00023484230041503906\n",
      "train: loss: 15864.3212890625 acc: 0.9864338636398315  val: loss: 1208429.75 acc: 0.5459181666374207\n",
      "step: 26310 , time : 0.0\n",
      "train: loss: 6385.5634765625 acc: 0.9939408898353577  val: loss: 862027.5625 acc: 0.6929241418838501\n",
      "step: 26315 , time : 0.0010004043579101562\n",
      "train: loss: 39529.57421875 acc: 0.9892987012863159  val: loss: 1614056.5 acc: 0.6186355352401733\n",
      "step: 26320 , time : 0.0\n",
      "train: loss: 44096.61328125 acc: 0.9878169894218445  val: loss: 1327495.625 acc: 0.5282338857650757\n",
      "step: 26325 , time : 0.0010006427764892578\n",
      "train: loss: 15223.1669921875 acc: 0.9956725239753723  val: loss: 1899448.5 acc: 0.17128169536590576\n",
      "step: 26330 , time : 0.0\n",
      "train: loss: 37874.375 acc: 0.9899511337280273  val: loss: 164229.953125 acc: 0.9810419678688049\n",
      "step: 26335 , time : 0.0\n",
      "train: loss: 80137.9921875 acc: 0.976308286190033  val: loss: 1408392.25 acc: 0.8445542454719543\n",
      "step: 26340 , time : 0.0\n",
      "train: loss: 161367.796875 acc: 0.9442378878593445  val: loss: 2698453.0 acc: 0.26792722940444946\n",
      "step: 26345 , time : 0.0012204647064208984\n",
      "train: loss: 60192.5625 acc: 0.9823001027107239  val: loss: 1581275.375 acc: 0.6013578176498413\n",
      "step: 26350 , time : 0.0\n",
      "train: loss: 109226.34375 acc: 0.972194254398346  val: loss: 731091.6875 acc: 0.8931790590286255\n",
      "step: 26355 , time : 0.0\n",
      "train: loss: 361334.59375 acc: 0.9421854019165039  val: loss: 2739800.75 acc: 0.5492426156997681\n",
      "step: 26360 , time : 0.0\n",
      "train: loss: 77983.7421875 acc: 0.9930938482284546  val: loss: 826823.0 acc: 0.8770017027854919\n",
      "step: 26365 , time : 0.0\n",
      "train: loss: 65711.765625 acc: 0.9919229745864868  val: loss: 345287.15625 acc: 0.9383211731910706\n",
      "step: 26370 , time : 0.0\n",
      "train: loss: 154594.546875 acc: 0.9719074964523315  val: loss: 1311297.75 acc: 0.14742231369018555\n",
      "step: 26375 , time : 0.0\n",
      "train: loss: 229573.5625 acc: 0.9814573526382446  val: loss: 3345109.25 acc: 0.13656604290008545\n",
      "step: 26380 , time : 0.0\n",
      "train: loss: 517560.71875 acc: 0.9748939275741577  val: loss: 821336.0 acc: 0.8101150989532471\n",
      "step: 26385 , time : 0.0\n",
      "train: loss: 235255.671875 acc: 0.9688878059387207  val: loss: 930444.8125 acc: 0.7502555251121521\n",
      "step: 26390 , time : 0.0\n",
      "train: loss: 128738.3203125 acc: 0.9790621995925903  val: loss: 469582.5 acc: 0.9288099408149719\n",
      "step: 26395 , time : 0.0\n",
      "train: loss: 1425197.125 acc: 0.9339348673820496  val: loss: 1307386.875 acc: 0.7901836633682251\n",
      "step: 26400 , time : 0.0\n",
      "train: loss: 1274875.375 acc: 0.9450886249542236  val: loss: 615851.8125 acc: 0.8011994957923889\n",
      "step: 26405 , time : 0.0\n",
      "train: loss: 636794.5625 acc: 0.9653741717338562  val: loss: 417110.09375 acc: 0.957840085029602\n",
      "step: 26410 , time : 0.0\n",
      "train: loss: 1222969.125 acc: 0.9577001333236694  val: loss: 916597.5 acc: 0.7539136409759521\n",
      "step: 26415 , time : 0.0\n",
      "train: loss: 175665.09375 acc: 0.9864251017570496  val: loss: 338934.53125 acc: 0.9529619812965393\n",
      "step: 26420 , time : 0.0\n",
      "train: loss: 549349.8125 acc: 0.9482648968696594  val: loss: 897443.75 acc: 0.6567164063453674\n",
      "step: 26425 , time : 0.0\n",
      "train: loss: 413041.59375 acc: 0.9589686393737793  val: loss: 449451.28125 acc: 0.9301764965057373\n",
      "step: 26430 , time : 0.0\n",
      "train: loss: 3249137.75 acc: 0.08763521909713745  val: loss: 834909.9375 acc: 0.9091289043426514\n",
      "step: 26435 , time : 0.0\n",
      "train: loss: 1416723.375 acc: 0.4021124243736267  val: loss: 836869.75 acc: 0.7571308612823486\n",
      "step: 26440 , time : 0.0\n",
      "train: loss: 838056.75 acc: 0.6548283100128174  val: loss: 888246.875 acc: 0.7363640069961548\n",
      "step: 26445 , time : 0.0\n",
      "train: loss: 527894.3125 acc: 0.8620322942733765  val: loss: 1220827.25 acc: 0.824499249458313\n",
      "step: 26450 , time : 0.0\n",
      "train: loss: 1117664.375 acc: 0.7237172722816467  val: loss: 846134.875 acc: 0.866921067237854\n",
      "step: 26455 , time : 0.0\n",
      "train: loss: 349809.3125 acc: 0.7950658798217773  val: loss: 1064611.25 acc: 0.6390678286552429\n",
      "step: 26460 , time : 0.0\n",
      "train: loss: 367983.3125 acc: 0.78849858045578  val: loss: 2985299.5 acc: 0.540416419506073\n",
      "step: 26465 , time : 0.0\n",
      "train: loss: 526139.5 acc: 0.7369015216827393  val: loss: 1182174.0 acc: 0.6326430439949036\n",
      "step: 26470 , time : 0.0\n",
      "train: loss: 136684.890625 acc: 0.8645310997962952  val: loss: 621737.25 acc: 0.7020847797393799\n",
      "step: 26475 , time : 0.0\n",
      "train: loss: 57825.71484375 acc: 0.9550197720527649  val: loss: 3009097.5 acc: 0.6590688228607178\n",
      "step: 26480 , time : 0.0\n",
      "train: loss: 109596.296875 acc: 0.9054025411605835  val: loss: 1066720.5 acc: 0.7451527118682861\n",
      "step: 26485 , time : 0.0\n",
      "train: loss: 282852.3125 acc: 0.8461506366729736  val: loss: 1142413.125 acc: 0.7009990811347961\n",
      "step: 26490 , time : 0.0\n",
      "train: loss: 60260.95703125 acc: 0.9473188519477844  val: loss: 5160172.0 acc: 0.4645169973373413\n",
      "step: 26495 , time : 0.0\n",
      "train: loss: 76274.03125 acc: 0.9373844265937805  val: loss: 360288.1875 acc: 0.7648612260818481\n",
      "step: 26500 , time : 0.0\n",
      "train: loss: 37663.19921875 acc: 0.9630998373031616  val: loss: 2838598.25 acc: 0.5887571573257446\n",
      "step: 26505 , time : 0.0\n",
      "train: loss: 257299.625 acc: 0.8326542973518372  val: loss: 1369778.625 acc: 0.6998788118362427\n",
      "step: 26510 , time : 0.0\n",
      "train: loss: 132671.546875 acc: 0.9059380888938904  val: loss: 628553.875 acc: 0.7209115028381348\n",
      "step: 26515 , time : 0.0\n",
      "train: loss: 243626.765625 acc: 0.811696469783783  val: loss: 1230559.75 acc: 0.6547293663024902\n",
      "step: 26520 , time : 0.0\n",
      "train: loss: 155424.828125 acc: 0.8869911432266235  val: loss: 960328.375 acc: 0.6525701284408569\n",
      "step: 26525 , time : 0.0\n",
      "train: loss: 283500.9375 acc: 0.8158494830131531  val: loss: 9777546.0 acc: 0.5025056600570679\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 26530 , time : 0.0\n",
      "train: loss: 142941.8125 acc: 0.8833310604095459  val: loss: 361178.28125 acc: 0.8357542753219604\n",
      "step: 26535 , time : 0.0\n",
      "train: loss: 384100.4375 acc: 0.8245270252227783  val: loss: 1226164.625 acc: 0.6512233018875122\n",
      "step: 26540 , time : 0.0010006427764892578\n",
      "train: loss: 912943.1875 acc: 0.7303974032402039  val: loss: 1348793.0 acc: 0.701024055480957\n",
      "step: 26545 , time : 0.0\n",
      "train: loss: 1291163.875 acc: 0.7564469575881958  val: loss: 2639651.75 acc: 0.7806645631790161\n",
      "step: 26550 , time : 0.0\n",
      "train: loss: 1367795.125 acc: 0.8957251906394958  val: loss: 827624.625 acc: 0.8384575843811035\n",
      "step: 26555 , time : 0.0010006427764892578\n",
      "train: loss: 178986.578125 acc: 0.9833948016166687  val: loss: 1697382.75 acc: 0.8194761276245117\n",
      "step: 26560 , time : 0.0\n",
      "train: loss: 208306.921875 acc: 0.9747647047042847  val: loss: 2219742.25 acc: 0.7386008501052856\n",
      "step: 26565 , time : 0.0010004043579101562\n",
      "train: loss: 385095.09375 acc: 0.9284789562225342  val: loss: 1602551.5 acc: 0.7699664831161499\n",
      "step: 26570 , time : 0.0\n",
      "train: loss: 114262.921875 acc: 0.9851078987121582  val: loss: 1634032.625 acc: 0.7072758674621582\n",
      "step: 26575 , time : 0.0\n",
      "train: loss: 74915.015625 acc: 0.9943997263908386  val: loss: 2982633.75 acc: -0.9546618461608887\n",
      "step: 26580 , time : 0.0\n",
      "train: loss: 280709.84375 acc: 0.9780921936035156  val: loss: 810325.625 acc: 0.5189242362976074\n",
      "step: 26585 , time : 0.0\n",
      "train: loss: 83683.703125 acc: 0.992168664932251  val: loss: 1621776.625 acc: 0.8261702060699463\n",
      "step: 26590 , time : 0.0\n",
      "train: loss: 121329.34375 acc: 0.9849655628204346  val: loss: 1206849.875 acc: 0.8796839118003845\n",
      "step: 26595 , time : 0.0\n",
      "train: loss: 35175.66796875 acc: 0.9908930659294128  val: loss: 489750.71875 acc: 0.926905632019043\n",
      "step: 26600 , time : 0.0\n",
      "train: loss: 71700.640625 acc: 0.9823353290557861  val: loss: 2787496.5 acc: 0.2789742946624756\n",
      "step: 26605 , time : 0.0\n",
      "train: loss: 15858.09765625 acc: 0.9724740982055664  val: loss: 3232056.5 acc: 0.45556312799453735\n",
      "step: 26610 , time : 0.0\n",
      "train: loss: 6556.78662109375 acc: 0.9960647821426392  val: loss: 1629304.625 acc: 0.4735197424888611\n",
      "step: 26615 , time : 0.0\n",
      "train: loss: 12894.689453125 acc: 0.9916316866874695  val: loss: 5018999.0 acc: 0.19755661487579346\n",
      "step: 26620 , time : 0.0\n",
      "train: loss: 9881.5244140625 acc: 0.9813010692596436  val: loss: 1090420.875 acc: 0.9339667558670044\n",
      "step: 26625 , time : 0.0\n",
      "train: loss: 3461.652587890625 acc: 0.9917452931404114  val: loss: 2039149.875 acc: -0.19414186477661133\n",
      "step: 26630 , time : 0.0\n",
      "train: loss: 6753.7880859375 acc: 0.9772496819496155  val: loss: 2062464.625 acc: 0.6046491861343384\n",
      "step: 26635 , time : 0.0\n",
      "train: loss: 10494.931640625 acc: 0.9716916680335999  val: loss: 738304.0625 acc: 0.898487389087677\n",
      "step: 26640 , time : 0.0\n",
      "train: loss: 11435.8359375 acc: 0.970992922782898  val: loss: 1075565.875 acc: 0.6202367544174194\n",
      "step: 26645 , time : 0.0\n",
      "train: loss: 69979.3515625 acc: 0.9685422778129578  val: loss: 1293862.125 acc: 0.21567338705062866\n",
      "step: 26650 , time : 0.0010008811950683594\n",
      "train: loss: 35989.53515625 acc: 0.9872475862503052  val: loss: 2348355.25 acc: -0.07099175453186035\n",
      "step: 26655 , time : 0.0\n",
      "train: loss: 48321.7890625 acc: 0.971332311630249  val: loss: 690968.375 acc: 0.855653703212738\n",
      "step: 26660 , time : 0.0\n",
      "train: loss: 18834.064453125 acc: 0.9890037178993225  val: loss: 1163574.125 acc: 0.872538685798645\n",
      "step: 26665 , time : 0.0010008811950683594\n",
      "train: loss: 13998.041015625 acc: 0.9807367920875549  val: loss: 1083384.875 acc: 0.7731902599334717\n",
      "step: 26670 , time : 0.0010008811950683594\n",
      "train: loss: 11234.6435546875 acc: 0.9840903282165527  val: loss: 1068092.875 acc: 0.5203952789306641\n",
      "step: 26675 , time : 0.0\n",
      "train: loss: 19598.841796875 acc: 0.9845751523971558  val: loss: 815964.125 acc: 0.36531710624694824\n",
      "step: 26680 , time : 0.0\n",
      "train: loss: 29331.16796875 acc: 0.9909572005271912  val: loss: 2154416.0 acc: 0.6459022760391235\n",
      "step: 26685 , time : 0.0\n",
      "train: loss: 37135.63671875 acc: 0.9914224147796631  val: loss: 460459.09375 acc: 0.9226436614990234\n",
      "step: 26690 , time : 0.0\n",
      "train: loss: 18659.52734375 acc: 0.995959997177124  val: loss: 1656905.875 acc: 0.0898776650428772\n",
      "step: 26695 , time : 0.0\n",
      "train: loss: 33606.6875 acc: 0.9903426170349121  val: loss: 1805904.125 acc: 0.4383002519607544\n",
      "step: 26700 , time : 0.0\n",
      "train: loss: 69189.984375 acc: 0.9847894906997681  val: loss: 575119.625 acc: 0.7576655149459839\n",
      "step: 26705 , time : 0.0\n",
      "train: loss: 121545.0625 acc: 0.9564363956451416  val: loss: 1340330.875 acc: 0.855984628200531\n",
      "step: 26710 , time : 0.0\n",
      "train: loss: 90052.453125 acc: 0.938367486000061  val: loss: 235891.734375 acc: 0.9372192025184631\n",
      "step: 26715 , time : 0.0\n",
      "train: loss: 257457.65625 acc: 0.9606167078018188  val: loss: 2217111.5 acc: 0.42764025926589966\n",
      "step: 26720 , time : 0.0\n",
      "train: loss: 141267.203125 acc: 0.9753854870796204  val: loss: 205544.890625 acc: 0.9551124572753906\n",
      "step: 26725 , time : 0.0\n",
      "train: loss: 119899.7109375 acc: 0.9885656237602234  val: loss: 2277913.75 acc: 0.45932430028915405\n",
      "step: 26730 , time : 0.0\n",
      "train: loss: 20686.84765625 acc: 0.9972354173660278  val: loss: 298792.875 acc: 0.9026657342910767\n",
      "step: 26735 , time : 0.0\n",
      "train: loss: 1002370.0 acc: 0.7770489454269409  val: loss: 468831.71875 acc: 0.8533252477645874\n",
      "step: 26740 , time : 0.0\n",
      "train: loss: 224530.34375 acc: 0.9858279228210449  val: loss: 501064.0625 acc: 0.7882711887359619\n",
      "step: 26745 , time : 0.0\n",
      "train: loss: 592172.5625 acc: 0.962191641330719  val: loss: 904679.1875 acc: 0.8065086007118225\n",
      "step: 26750 , time : 0.0\n",
      "train: loss: 294627.0625 acc: 0.953862726688385  val: loss: 1059662.375 acc: 0.8846909403800964\n",
      "step: 26755 , time : 0.0\n",
      "train: loss: 460590.15625 acc: 0.9646854400634766  val: loss: 1567331.625 acc: 0.5454461574554443\n",
      "step: 26760 , time : 0.0010004043579101562\n",
      "train: loss: 1621407.375 acc: 0.9476678967475891  val: loss: 1260314.0 acc: 0.5176256895065308\n",
      "step: 26765 , time : 0.0\n",
      "train: loss: 2434332.25 acc: 0.9324393272399902  val: loss: 437713.0625 acc: 0.9432159066200256\n",
      "step: 26770 , time : 0.0\n",
      "train: loss: 1384449.625 acc: 0.9133413434028625  val: loss: 509087.03125 acc: 0.8733318448066711\n",
      "step: 26775 , time : 0.0\n",
      "train: loss: 539784.0625 acc: 0.9485975503921509  val: loss: 786774.9375 acc: 0.6669387221336365\n",
      "step: 26780 , time : 0.0\n",
      "train: loss: 150388.03125 acc: 0.9600008130073547  val: loss: 312727.5625 acc: 0.959964394569397\n",
      "step: 26785 , time : 0.0\n",
      "train: loss: 612825.8125 acc: 0.9461706876754761  val: loss: 231322.171875 acc: 0.9588347673416138\n",
      "step: 26790 , time : 0.0\n",
      "train: loss: 401348.625 acc: 0.9402143359184265  val: loss: 742641.3125 acc: 0.8846368193626404\n",
      "step: 26795 , time : 0.0\n",
      "train: loss: 3294134.25 acc: 0.012009263038635254  val: loss: 935598.9375 acc: 0.8805524110794067\n",
      "step: 26800 , time : 0.0\n",
      "train: loss: 1450095.125 acc: 0.7223091721534729  val: loss: 900032.6875 acc: 0.726383626461029\n",
      "step: 26805 , time : 0.0\n",
      "train: loss: 773721.375 acc: 0.7313576936721802  val: loss: 2058811.125 acc: 0.8530569076538086\n",
      "step: 26810 , time : 0.0\n",
      "train: loss: 1106888.125 acc: 0.4754717946052551  val: loss: 826951.25 acc: 0.8258792757987976\n",
      "step: 26815 , time : 0.0\n",
      "train: loss: 442337.3125 acc: 0.8666684627532959  val: loss: 1002841.75 acc: 0.8680564761161804\n",
      "step: 26820 , time : 0.0\n",
      "train: loss: 1063013.25 acc: 0.7117379903793335  val: loss: 1405931.0 acc: 0.7393631339073181\n",
      "step: 26825 , time : 0.0\n",
      "train: loss: 743283.1875 acc: 0.7423548102378845  val: loss: 3765948.25 acc: 0.5504665374755859\n",
      "step: 26830 , time : 0.0\n",
      "train: loss: 121657.25 acc: 0.9046750068664551  val: loss: 6581640.0 acc: 0.6049333810806274\n",
      "step: 26835 , time : 0.0\n",
      "train: loss: 578276.0625 acc: 0.7004504203796387  val: loss: 2966443.5 acc: 0.6199371814727783\n",
      "step: 26840 , time : 0.0\n",
      "train: loss: 131401.484375 acc: 0.8799479603767395  val: loss: 2687304.0 acc: 0.6175395250320435\n",
      "step: 26845 , time : 0.0\n",
      "train: loss: 262984.65625 acc: 0.8628012537956238  val: loss: 3130213.25 acc: 0.6126877665519714\n",
      "step: 26850 , time : 0.0\n",
      "train: loss: 121587.6796875 acc: 0.9253671765327454  val: loss: 1502770.25 acc: 0.7187842130661011\n",
      "step: 26855 , time : 0.0\n",
      "train: loss: 59272.7734375 acc: 0.9557808637619019  val: loss: 3080273.75 acc: 0.5551239252090454\n",
      "step: 26860 , time : 0.0\n",
      "train: loss: 132584.703125 acc: 0.9065233469009399  val: loss: 2054223.875 acc: 0.6361737847328186\n",
      "step: 26865 , time : 0.0\n",
      "train: loss: 65557.1953125 acc: 0.9337447285652161  val: loss: 947929.625 acc: 0.7612980604171753\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 26870 , time : 0.0\n",
      "train: loss: 39582.56640625 acc: 0.9611700177192688  val: loss: 3182877.75 acc: 0.5820440053939819\n",
      "step: 26875 , time : 0.0\n",
      "train: loss: 54824.8984375 acc: 0.9187927842140198  val: loss: 1589794.375 acc: 0.6252188682556152\n",
      "step: 26880 , time : 0.0\n",
      "train: loss: 449965.53125 acc: 0.675069272518158  val: loss: 982729.375 acc: 0.669432520866394\n",
      "step: 26885 , time : 0.0\n",
      "train: loss: 331095.84375 acc: 0.793663501739502  val: loss: 3092256.25 acc: 0.6343947052955627\n",
      "step: 26890 , time : 0.0\n",
      "train: loss: 1110577.875 acc: 0.6147172451019287  val: loss: 3976835.25 acc: 0.5652538537979126\n",
      "step: 26895 , time : 0.0010004043579101562\n",
      "train: loss: 240476.453125 acc: 0.832599937915802  val: loss: 1119717.0 acc: 0.6920640468597412\n",
      "step: 26900 , time : 0.0\n",
      "train: loss: 575117.3125 acc: 0.7302987575531006  val: loss: 1600201.625 acc: 0.6319434642791748\n",
      "step: 26905 , time : 0.0\n",
      "train: loss: 731012.25 acc: 0.6922001838684082  val: loss: 974318.875 acc: 0.6760294437408447\n",
      "step: 26910 , time : 0.0\n",
      "train: loss: 1237507.625 acc: 0.8139274716377258  val: loss: 663125.0 acc: 0.6431502103805542\n",
      "step: 26915 , time : 0.0\n",
      "train: loss: 1438139.375 acc: 0.7848133444786072  val: loss: 749153.25 acc: 0.8751233220100403\n",
      "step: 26920 , time : 0.0\n",
      "train: loss: 588368.9375 acc: 0.9278796911239624  val: loss: 2124146.25 acc: 0.11643588542938232\n",
      "step: 26925 , time : 0.0\n",
      "train: loss: 152221.53125 acc: 0.9806358218193054  val: loss: 780627.625 acc: 0.889978289604187\n",
      "step: 26930 , time : 0.0\n",
      "train: loss: 169029.0625 acc: 0.9758679270744324  val: loss: 202587.515625 acc: 0.9494537115097046\n",
      "step: 26935 , time : 0.0\n",
      "train: loss: 167441.484375 acc: 0.9788194298744202  val: loss: 1765248.625 acc: 0.20814388990402222\n",
      "step: 26940 , time : 0.0\n",
      "train: loss: 55421.3046875 acc: 0.9963232278823853  val: loss: 1579022.125 acc: 0.7321078777313232\n",
      "step: 26945 , time : 0.0\n",
      "train: loss: 121976.296875 acc: 0.9911394715309143  val: loss: 1511301.5 acc: 0.6543713808059692\n",
      "step: 26950 , time : 0.0\n",
      "train: loss: 101412.21875 acc: 0.9917451739311218  val: loss: 872158.0 acc: 0.8193705081939697\n",
      "step: 26955 , time : 0.0\n",
      "train: loss: 88233.0703125 acc: 0.9756072759628296  val: loss: 1528937.625 acc: 0.7723763585090637\n",
      "step: 26960 , time : 0.0\n",
      "train: loss: 73786.0390625 acc: 0.9880421161651611  val: loss: 1371788.25 acc: 0.6384788751602173\n",
      "step: 26965 , time : 0.0\n",
      "train: loss: 18798.451171875 acc: 0.9910371899604797  val: loss: 1074936.375 acc: 0.7730107307434082\n",
      "step: 26970 , time : 0.0\n",
      "train: loss: 13172.119140625 acc: 0.9177118539810181  val: loss: 2061739.25 acc: 0.3026180863380432\n",
      "step: 26975 , time : 0.0\n",
      "train: loss: 9837.6259765625 acc: 0.9902406334877014  val: loss: 380059.25 acc: 0.9446150660514832\n",
      "step: 26980 , time : 0.0\n",
      "train: loss: 17152.556640625 acc: 0.9454770684242249  val: loss: 46150.65234375 acc: 0.9903824329376221\n",
      "step: 26985 , time : 0.0\n",
      "train: loss: 8561.9423828125 acc: 0.99462890625  val: loss: 1428576.875 acc: 0.4909132122993469\n",
      "step: 26990 , time : 0.0010008811950683594\n",
      "train: loss: 15952.0263671875 acc: 0.9778802990913391  val: loss: 804669.6875 acc: 0.9108231067657471\n",
      "step: 26995 , time : 0.0010006427764892578\n",
      "train: loss: 9678.263671875 acc: 0.9801772832870483  val: loss: 908947.0625 acc: 0.7339270114898682\n",
      "step: 27000 , time : 0.0\n",
      "train: loss: 9713.8671875 acc: 0.9854848980903625  val: loss: 493660.5 acc: 0.8580202460289001\n",
      "step: 27005 , time : 0.0\n",
      "train: loss: 33160.11328125 acc: 0.9754304885864258  val: loss: 124549.28125 acc: 0.9544201493263245\n",
      "step: 27010 , time : 0.0010004043579101562\n",
      "train: loss: 44797.17578125 acc: 0.9738039374351501  val: loss: 1038128.1875 acc: 0.48416221141815186\n",
      "step: 27015 , time : 0.0010006427764892578\n",
      "train: loss: 17045.005859375 acc: 0.9865258932113647  val: loss: 261110.828125 acc: 0.8571316003799438\n",
      "step: 27020 , time : 0.0\n",
      "train: loss: 37319.65625 acc: 0.9723535776138306  val: loss: 569273.75 acc: 0.8233124017715454\n",
      "step: 27025 , time : 0.0\n",
      "train: loss: 29235.35546875 acc: 0.981285035610199  val: loss: 580071.0625 acc: 0.9314305782318115\n",
      "step: 27030 , time : 0.0\n",
      "train: loss: 27226.748046875 acc: 0.9727779030799866  val: loss: 1251166.375 acc: 0.6168829798698425\n",
      "step: 27035 , time : 0.0\n",
      "train: loss: 18283.767578125 acc: 0.9855553507804871  val: loss: 523041.875 acc: 0.8879820108413696\n",
      "step: 27040 , time : 0.0\n",
      "train: loss: 28624.2421875 acc: 0.9417057037353516  val: loss: 958110.0625 acc: 0.8490830659866333\n",
      "step: 27045 , time : 0.0\n",
      "train: loss: 24757.5703125 acc: 0.9897381067276001  val: loss: 2242740.75 acc: 0.6754144430160522\n",
      "step: 27050 , time : 0.0\n",
      "train: loss: 26502.40625 acc: 0.9931856989860535  val: loss: 1026544.5625 acc: 0.5933785438537598\n",
      "step: 27055 , time : 0.0\n",
      "train: loss: 32639.970703125 acc: 0.9927048087120056  val: loss: 1634904.875 acc: 0.4230424165725708\n",
      "step: 27060 , time : 0.0\n",
      "train: loss: 40250.734375 acc: 0.9860239028930664  val: loss: 2189688.75 acc: 0.6784215569496155\n",
      "step: 27065 , time : 0.0\n",
      "train: loss: 81342.2265625 acc: 0.9798521995544434  val: loss: 1025426.1875 acc: 0.5817532539367676\n",
      "step: 27070 , time : 0.0\n",
      "train: loss: 150468.78125 acc: 0.9542123675346375  val: loss: 1233242.25 acc: 0.7891601920127869\n",
      "step: 27075 , time : 0.0\n",
      "train: loss: 158642.625 acc: 0.9663854241371155  val: loss: 237900.84375 acc: 0.9281014800071716\n",
      "step: 27080 , time : 0.0\n",
      "train: loss: 218018.21875 acc: 0.9155271053314209  val: loss: 262915.4375 acc: 0.9353747367858887\n",
      "step: 27085 , time : 0.0\n",
      "train: loss: 147069.71875 acc: 0.9847506284713745  val: loss: 188200.28125 acc: 0.9792464971542358\n",
      "step: 27090 , time : 0.0\n",
      "train: loss: 115573.578125 acc: 0.9897082448005676  val: loss: 420609.6875 acc: 0.8971346020698547\n",
      "step: 27095 , time : 0.0\n",
      "train: loss: 88721.609375 acc: 0.9843307137489319  val: loss: 999203.125 acc: 0.7326866984367371\n",
      "step: 27100 , time : 0.0\n",
      "train: loss: 608588.8125 acc: 0.8959962725639343  val: loss: 260838.84375 acc: 0.9176135063171387\n",
      "step: 27105 , time : 0.0\n",
      "train: loss: 456569.0 acc: 0.9702068567276001  val: loss: 2126925.0 acc: 0.7775556445121765\n",
      "step: 27110 , time : 0.0\n",
      "train: loss: 321521.25 acc: 0.9736301302909851  val: loss: 845319.125 acc: 0.8723369836807251\n",
      "step: 27115 , time : 0.0\n",
      "train: loss: 2056841.125 acc: 0.7995288372039795  val: loss: 1160807.25 acc: 0.8740233778953552\n",
      "step: 27120 , time : 0.001001119613647461\n",
      "train: loss: 394285.1875 acc: 0.9233125448226929  val: loss: 600108.5625 acc: 0.9325786828994751\n",
      "step: 27125 , time : 0.0\n",
      "train: loss: 769826.125 acc: 0.9716725945472717  val: loss: 699361.9375 acc: 0.8594902157783508\n",
      "step: 27130 , time : 0.0\n",
      "train: loss: 659205.75 acc: 0.9853406548500061  val: loss: 1045762.3125 acc: 0.9163864850997925\n",
      "step: 27135 , time : 0.0010006427764892578\n",
      "train: loss: 1026444.375 acc: 0.9714945554733276  val: loss: 1508425.5 acc: 0.7560876607894897\n",
      "step: 27140 , time : 0.001001119613647461\n",
      "train: loss: 312255.9375 acc: 0.9805794358253479  val: loss: 3054938.25 acc: 0.5953397154808044\n",
      "step: 27145 , time : 0.0\n",
      "train: loss: 435531.4375 acc: 0.9432411789894104  val: loss: 196442.59375 acc: 0.9770112037658691\n",
      "step: 27150 , time : 0.0\n",
      "train: loss: 465225.40625 acc: 0.9547250270843506  val: loss: 1978214.5 acc: 0.7371777296066284\n",
      "step: 27155 , time : 0.0\n",
      "train: loss: 342566.125 acc: 0.9574827551841736  val: loss: 768876.8125 acc: 0.8103317618370056\n",
      "step: 27160 , time : 0.0010008811950683594\n",
      "train: loss: 1458482.25 acc: 0.7965962290763855  val: loss: 789550.0625 acc: 0.8924508690834045\n",
      "step: 27165 , time : 0.0\n",
      "train: loss: 807564.9375 acc: 0.7868022918701172  val: loss: 1778634.375 acc: 0.8089773654937744\n",
      "step: 27170 , time : 0.0\n",
      "train: loss: 384007.28125 acc: 0.8509020209312439  val: loss: 769038.6875 acc: 0.5967772006988525\n",
      "step: 27175 , time : 0.0010004043579101562\n",
      "train: loss: 851978.0 acc: 0.8140852451324463  val: loss: 993248.1875 acc: 0.8477525115013123\n",
      "step: 27180 , time : 0.0010006427764892578\n",
      "train: loss: 482128.0 acc: 0.9047952890396118  val: loss: 1088218.375 acc: 0.7658408880233765\n",
      "step: 27185 , time : 0.0\n",
      "train: loss: 734764.75 acc: 0.6472489833831787  val: loss: 1676579.625 acc: 0.8015906810760498\n",
      "step: 27190 , time : 0.015624761581420898\n",
      "train: loss: 266790.71875 acc: 0.8112764954566956  val: loss: 2159593.25 acc: 0.6094970703125\n",
      "step: 27195 , time : 0.0\n",
      "train: loss: 220731.1875 acc: 0.8691821694374084  val: loss: 647897.4375 acc: 0.7528177499771118\n",
      "step: 27200 , time : 0.0\n",
      "train: loss: 175962.359375 acc: 0.8844950795173645  val: loss: 267098.0625 acc: 0.8349515199661255\n",
      "step: 27205 , time : 0.0\n",
      "train: loss: 152018.90625 acc: 0.9071758389472961  val: loss: 919462.25 acc: 0.7610687017440796\n",
      "step: 27210 , time : 0.0\n",
      "train: loss: 46006.7734375 acc: 0.9620206356048584  val: loss: 829435.25 acc: 0.7184076309204102\n",
      "step: 27215 , time : 0.0\n",
      "train: loss: 44351.19921875 acc: 0.9647476077079773  val: loss: 4595417.5 acc: 0.5508634448051453\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 27220 , time : 0.0\n",
      "train: loss: 348307.71875 acc: 0.8183761835098267  val: loss: 48671.9609375 acc: 0.9340993165969849\n",
      "step: 27225 , time : 0.0\n",
      "train: loss: 148381.9375 acc: 0.9092319011688232  val: loss: 671939.5 acc: 0.7276952862739563\n",
      "step: 27230 , time : 0.0\n",
      "train: loss: 185373.703125 acc: 0.8362821340560913  val: loss: 1280169.875 acc: 0.6585943102836609\n",
      "step: 27235 , time : 0.0\n",
      "train: loss: 299072.125 acc: 0.7976481914520264  val: loss: 1278099.5 acc: 0.6753255724906921\n",
      "step: 27240 , time : 0.0010004043579101562\n",
      "train: loss: 44774.40625 acc: 0.9364856481552124  val: loss: 920971.3125 acc: 0.742259681224823\n",
      "step: 27245 , time : 0.0\n",
      "train: loss: 141713.59375 acc: 0.8967638611793518  val: loss: 641268.6875 acc: 0.743977427482605\n",
      "step: 27250 , time : 0.0\n",
      "train: loss: 221126.40625 acc: 0.8682612776756287  val: loss: 6195144.5 acc: 0.5364494323730469\n",
      "step: 27255 , time : 0.0010006427764892578\n",
      "train: loss: 148289.65625 acc: 0.89042729139328  val: loss: 2219641.75 acc: 0.613947868347168\n",
      "step: 27260 , time : 0.0\n",
      "train: loss: 913368.375 acc: 0.7098945379257202  val: loss: 4179852.0 acc: 0.5814270377159119\n",
      "step: 27265 , time : 0.001001119613647461\n",
      "train: loss: 237248.890625 acc: 0.8475375175476074  val: loss: 2825534.0 acc: 0.5907476544380188\n",
      "step: 27270 , time : 0.0\n",
      "train: loss: 1032127.1875 acc: 0.7289400100708008  val: loss: 3567656.25 acc: 0.4624318480491638\n",
      "step: 27275 , time : 0.0\n",
      "train: loss: 2928562.75 acc: 0.7340116500854492  val: loss: 593028.625 acc: 0.7692118883132935\n",
      "step: 27280 , time : 0.0010001659393310547\n",
      "train: loss: 1596009.125 acc: 0.8289126753807068  val: loss: 534994.0625 acc: 0.8280646800994873\n",
      "step: 27285 , time : 0.0010004043579101562\n",
      "train: loss: 669176.5625 acc: 0.9498143792152405  val: loss: 1313532.25 acc: 0.8304120898246765\n",
      "step: 27290 , time : 0.0\n",
      "train: loss: 496725.75 acc: 0.9569886922836304  val: loss: 610869.875 acc: 0.7839310169219971\n",
      "step: 27295 , time : 0.0\n",
      "train: loss: 58340.97265625 acc: 0.9904668927192688  val: loss: 1857653.75 acc: -0.04211390018463135\n",
      "step: 27300 , time : 0.0010008811950683594\n",
      "train: loss: 149078.984375 acc: 0.9840421080589294  val: loss: 950969.4375 acc: 0.8814590573310852\n",
      "step: 27305 , time : 0.0\n",
      "train: loss: 58085.21484375 acc: 0.9946833252906799  val: loss: 1569211.625 acc: 0.8550227880477905\n",
      "step: 27310 , time : 0.001001119613647461\n",
      "train: loss: 106320.546875 acc: 0.993026614189148  val: loss: 1057852.25 acc: 0.4861629605293274\n",
      "step: 27315 , time : 0.0010008811950683594\n",
      "train: loss: 84636.5078125 acc: 0.9926903247833252  val: loss: 1119349.5 acc: 0.589713454246521\n",
      "step: 27320 , time : 0.0\n",
      "train: loss: 45088.3984375 acc: 0.9886332154273987  val: loss: 1343415.75 acc: 0.6793491840362549\n",
      "step: 27325 , time : 0.0010004043579101562\n",
      "train: loss: 35337.85546875 acc: 0.9930586218833923  val: loss: 1124502.25 acc: 0.5299128890037537\n",
      "step: 27330 , time : 0.0010006427764892578\n",
      "train: loss: 9343.6953125 acc: 0.9974415898323059  val: loss: 745868.75 acc: 0.8689976930618286\n",
      "step: 27335 , time : 0.0\n",
      "train: loss: 18731.154296875 acc: 0.9850375652313232  val: loss: 942143.8125 acc: 0.040234148502349854\n",
      "step: 27340 , time : 0.0010006427764892578\n",
      "train: loss: 26050.548828125 acc: 0.9929947853088379  val: loss: 1985767.75 acc: 0.8473716974258423\n",
      "step: 27345 , time : 0.0010006427764892578\n",
      "train: loss: 10153.607421875 acc: 0.9784112572669983  val: loss: 239778.5 acc: 0.9488771557807922\n",
      "step: 27350 , time : 0.0\n",
      "train: loss: 9994.3369140625 acc: 0.9925833344459534  val: loss: 649541.6875 acc: 0.8717528581619263\n",
      "step: 27355 , time : 0.0010006427764892578\n",
      "train: loss: 15721.556640625 acc: 0.9525646567344666  val: loss: 799017.5625 acc: 0.574571967124939\n",
      "step: 27360 , time : 0.0\n",
      "train: loss: 16405.640625 acc: 0.9337736964225769  val: loss: 377689.5625 acc: 0.9086648225784302\n",
      "step: 27365 , time : 0.0\n",
      "train: loss: 9194.0810546875 acc: 0.9864837527275085  val: loss: 496773.3125 acc: 0.842540979385376\n",
      "step: 27370 , time : 0.0010004043579101562\n",
      "train: loss: 11958.900390625 acc: 0.982851505279541  val: loss: 1461705.0 acc: 0.7615544199943542\n",
      "step: 27375 , time : 0.0\n",
      "train: loss: 24118.5078125 acc: 0.9816271066665649  val: loss: 1019112.0625 acc: 0.7563997507095337\n",
      "step: 27380 , time : 0.0\n",
      "train: loss: 32311.11328125 acc: 0.9810963869094849  val: loss: 68428.0078125 acc: 0.9726007580757141\n",
      "step: 27385 , time : 0.0010004043579101562\n",
      "train: loss: 32339.17578125 acc: 0.9606335163116455  val: loss: 1013212.375 acc: 0.608900785446167\n",
      "step: 27390 , time : 0.0\n",
      "train: loss: 97299.6640625 acc: 0.9318898320198059  val: loss: 1133474.75 acc: 0.866600513458252\n",
      "step: 27395 , time : 0.0\n",
      "train: loss: 125806.796875 acc: 0.9367174506187439  val: loss: 432297.96875 acc: 0.9271790981292725\n",
      "step: 27400 , time : 0.0\n",
      "train: loss: 11575.251953125 acc: 0.986491858959198  val: loss: 442926.34375 acc: 0.8721661567687988\n",
      "step: 27405 , time : 0.0010006427764892578\n",
      "train: loss: 13537.279296875 acc: 0.9852045774459839  val: loss: 605192.25 acc: 0.9642927646636963\n",
      "step: 27410 , time : 0.0\n",
      "train: loss: 18674.517578125 acc: 0.9934675693511963  val: loss: 325647.34375 acc: 0.9552109837532043\n",
      "step: 27415 , time : 0.0\n",
      "train: loss: 42600.91796875 acc: 0.9920738339424133  val: loss: 1683794.125 acc: 0.294700562953949\n",
      "step: 27420 , time : 0.0010006427764892578\n",
      "train: loss: 28305.013671875 acc: 0.9902094602584839  val: loss: 2423251.0 acc: 0.7512288689613342\n",
      "step: 27425 , time : 0.0010008811950683594\n",
      "train: loss: 62893.34765625 acc: 0.9828245639801025  val: loss: 242040.171875 acc: 0.9304150938987732\n",
      "step: 27430 , time : 0.0\n",
      "train: loss: 60680.7890625 acc: 0.9820555448532104  val: loss: 373635.75 acc: 0.9178517460823059\n",
      "step: 27435 , time : 0.0\n",
      "train: loss: 124057.046875 acc: 0.9779309034347534  val: loss: 211518.0625 acc: 0.9415343403816223\n",
      "step: 27440 , time : 0.0\n",
      "train: loss: 95603.0 acc: 0.9703649282455444  val: loss: 1400644.5 acc: 0.8784216642379761\n",
      "step: 27445 , time : 0.0\n",
      "train: loss: 86300.609375 acc: 0.9755480289459229  val: loss: 3694543.75 acc: 0.6934990882873535\n",
      "step: 27450 , time : 0.0010013580322265625\n",
      "train: loss: 265427.78125 acc: 0.9638087153434753  val: loss: 2676956.25 acc: 0.6048693656921387\n",
      "step: 27455 , time : 0.0\n",
      "train: loss: 238546.40625 acc: 0.9785543084144592  val: loss: 1718143.625 acc: 0.4223059415817261\n",
      "step: 27460 , time : 0.001001119613647461\n",
      "train: loss: 510001.4375 acc: 0.9298702478408813  val: loss: 862337.3125 acc: 0.8174949884414673\n",
      "step: 27465 , time : 0.0\n",
      "train: loss: 95831.125 acc: 0.9817861318588257  val: loss: 408368.65625 acc: 0.9016366600990295\n",
      "step: 27470 , time : 0.0\n",
      "train: loss: 220507.5 acc: 0.9487088918685913  val: loss: 834359.75 acc: 0.8802231550216675\n",
      "step: 27475 , time : 0.0\n",
      "train: loss: 1959977.25 acc: 0.8623150587081909  val: loss: 850628.9375 acc: 0.8060191869735718\n",
      "step: 27480 , time : 0.0\n",
      "train: loss: 758875.0 acc: 0.920848548412323  val: loss: 2457570.25 acc: 0.10181617736816406\n",
      "step: 27485 , time : 0.0\n",
      "train: loss: 364028.90625 acc: 0.9249281287193298  val: loss: 721480.8125 acc: 0.8015536069869995\n",
      "step: 27490 , time : 0.0\n",
      "train: loss: 2525078.25 acc: 0.8970071077346802  val: loss: 1194588.75 acc: 0.30618852376937866\n",
      "step: 27495 , time : 0.0\n",
      "train: loss: 1135690.25 acc: 0.9667121767997742  val: loss: 1526307.5 acc: 0.5308488607406616\n",
      "step: 27500 , time : 0.0\n",
      "train: loss: 720844.9375 acc: 0.9641803503036499  val: loss: 908127.1875 acc: 0.80472731590271\n",
      "step: 27505 , time : 0.0\n",
      "train: loss: 306605.21875 acc: 0.9792443513870239  val: loss: 424492.34375 acc: 0.9081629514694214\n",
      "step: 27510 , time : 0.0\n",
      "train: loss: 1299504.25 acc: 0.9236580729484558  val: loss: 1473789.625 acc: 0.4960763454437256\n",
      "step: 27515 , time : 0.0\n",
      "train: loss: 448884.0625 acc: 0.924635112285614  val: loss: 504845.5 acc: 0.9346837997436523\n",
      "step: 27520 , time : 0.0\n",
      "train: loss: 324800.78125 acc: 0.9399821758270264  val: loss: 1107389.25 acc: 0.3588073253631592\n",
      "step: 27525 , time : 0.0\n",
      "train: loss: 390529.0 acc: 0.9017492532730103  val: loss: 592220.1875 acc: 0.8073319792747498\n",
      "step: 27530 , time : 0.0\n",
      "train: loss: 2115634.75 acc: 0.8363866806030273  val: loss: 804763.6875 acc: 0.8921853303909302\n",
      "step: 27535 , time : 0.0\n",
      "train: loss: 796592.0625 acc: 0.6913319230079651  val: loss: 1722429.875 acc: 0.8474363088607788\n",
      "step: 27540 , time : 0.0\n",
      "train: loss: 967960.4375 acc: 0.5536690950393677  val: loss: 952886.4375 acc: 0.7913511991500854\n",
      "step: 27545 , time : 0.0\n",
      "train: loss: 720913.9375 acc: 0.7625517845153809  val: loss: 711590.9375 acc: 0.8121861219406128\n",
      "step: 27550 , time : 0.0\n",
      "train: loss: 930645.5 acc: 0.5773822069168091  val: loss: 1353633.75 acc: 0.8344156742095947\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 27555 , time : 0.0\n",
      "train: loss: 1227972.875 acc: 0.694438099861145  val: loss: 1090226.125 acc: 0.743140459060669\n",
      "step: 27560 , time : 0.0\n",
      "train: loss: 578688.1875 acc: 0.7825397849082947  val: loss: 3013480.75 acc: 0.6171442866325378\n",
      "step: 27565 , time : 0.0\n",
      "train: loss: 94116.28125 acc: 0.9308354258537292  val: loss: 2391637.25 acc: 0.5967268943786621\n",
      "step: 27570 , time : 0.0010004043579101562\n",
      "train: loss: 161491.78125 acc: 0.8841867446899414  val: loss: 1364002.125 acc: 0.6734129786491394\n",
      "step: 27575 , time : 0.001001119613647461\n",
      "train: loss: 95229.5625 acc: 0.9336566925048828  val: loss: 1987663.375 acc: 0.7105001211166382\n",
      "step: 27580 , time : 0.0\n",
      "train: loss: 246895.078125 acc: 0.8670237064361572  val: loss: 3693537.0 acc: 0.6263642907142639\n",
      "step: 27585 , time : 0.0\n",
      "train: loss: 51709.69921875 acc: 0.9571808576583862  val: loss: 2861364.5 acc: 0.5998548269271851\n",
      "step: 27590 , time : 0.0\n",
      "train: loss: 61118.49609375 acc: 0.9510830640792847  val: loss: 649927.375 acc: 0.7370780110359192\n",
      "step: 27595 , time : 0.0\n",
      "train: loss: 217757.578125 acc: 0.8673021197319031  val: loss: 2987670.5 acc: 0.646541178226471\n",
      "step: 27600 , time : 0.0\n",
      "train: loss: 149746.984375 acc: 0.8380070924758911  val: loss: 1655247.375 acc: 0.6521046161651611\n",
      "step: 27605 , time : 0.0\n",
      "train: loss: 224396.609375 acc: 0.8040810823440552  val: loss: 2522945.75 acc: 0.5905724763870239\n",
      "step: 27610 , time : 0.0\n",
      "train: loss: 152011.109375 acc: 0.8691880106925964  val: loss: 2519393.25 acc: 0.5702793598175049\n",
      "step: 27615 , time : 0.0\n",
      "train: loss: 702012.5 acc: 0.7098520994186401  val: loss: 5053112.5 acc: 0.5390095710754395\n",
      "step: 27620 , time : 0.0\n",
      "train: loss: 544324.875 acc: 0.7524970173835754  val: loss: 1081673.5 acc: 0.688508152961731\n",
      "step: 27625 , time : 0.0\n",
      "train: loss: 109557.7734375 acc: 0.8824602961540222  val: loss: 2571971.0 acc: 0.5468515157699585\n",
      "step: 27630 , time : 0.0\n",
      "train: loss: 503929.40625 acc: 0.7400164604187012  val: loss: 1172607.875 acc: 0.6388697624206543\n",
      "step: 27635 , time : 0.0\n",
      "train: loss: 306889.4375 acc: 0.832148551940918  val: loss: 2540004.0 acc: 0.6596709489822388\n",
      "step: 27640 , time : 0.0\n",
      "train: loss: 1372200.375 acc: 0.7388020753860474  val: loss: 655281.4375 acc: 0.732109010219574\n",
      "step: 27645 , time : 0.0010008811950683594\n",
      "train: loss: 875927.125 acc: 0.8464487195014954  val: loss: 741544.6875 acc: 0.8627924919128418\n",
      "step: 27650 , time : 0.0010008811950683594\n",
      "train: loss: 689950.375 acc: 0.9310125112533569  val: loss: 1618982.25 acc: 0.704677164554596\n",
      "step: 27655 , time : 0.0010004043579101562\n",
      "train: loss: 97263.5546875 acc: 0.9859177470207214  val: loss: 2122779.25 acc: -0.0030440092086791992\n",
      "step: 27660 , time : 0.0\n",
      "train: loss: 236312.90625 acc: 0.9718210101127625  val: loss: 737506.5 acc: 0.8685188889503479\n",
      "step: 27665 , time : 0.001001119613647461\n",
      "train: loss: 22743.048828125 acc: 0.991651713848114  val: loss: 986984.125 acc: 0.8892415761947632\n",
      "step: 27670 , time : 0.0\n",
      "train: loss: 606814.375 acc: 0.9492087364196777  val: loss: 1369430.625 acc: 0.8643070459365845\n",
      "step: 27675 , time : 0.0\n",
      "train: loss: 84006.2109375 acc: 0.9945768117904663  val: loss: 810920.75 acc: 0.5459567308425903\n",
      "step: 27680 , time : 0.0\n",
      "train: loss: 53870.15625 acc: 0.9946726560592651  val: loss: 256883.578125 acc: 0.9527133703231812\n",
      "step: 27685 , time : 0.0\n",
      "train: loss: 44623.296875 acc: 0.9944887757301331  val: loss: 1069626.125 acc: 0.5946794748306274\n",
      "step: 27690 , time : 0.0010004043579101562\n",
      "train: loss: 55133.2578125 acc: 0.9885162711143494  val: loss: 858789.875 acc: 0.5868628025054932\n",
      "step: 27695 , time : 0.0\n",
      "train: loss: 18694.86328125 acc: 0.9932717084884644  val: loss: 246468.9375 acc: 0.911973237991333\n",
      "step: 27700 , time : 0.0\n",
      "train: loss: 28328.16015625 acc: 0.9793943166732788  val: loss: 455918.6875 acc: 0.895793616771698\n",
      "step: 27705 , time : 0.0\n",
      "train: loss: 32003.583984375 acc: 0.9888797998428345  val: loss: 829140.5625 acc: 0.6177128553390503\n",
      "step: 27710 , time : 0.0\n",
      "train: loss: 8983.2548828125 acc: 0.9775435924530029  val: loss: 324223.3125 acc: 0.9319313764572144\n",
      "step: 27715 , time : 0.0\n",
      "train: loss: 20390.220703125 acc: 0.9664533138275146  val: loss: 388236.8125 acc: 0.8933877944946289\n",
      "step: 27720 , time : 0.0\n",
      "train: loss: 22442.814453125 acc: 0.9896523356437683  val: loss: 780761.3125 acc: 0.7815169095993042\n",
      "step: 27725 , time : 0.0\n",
      "train: loss: 9632.7197265625 acc: 0.9860582947731018  val: loss: 190166.125 acc: 0.974579393863678\n",
      "step: 27730 , time : 0.0\n",
      "train: loss: 18397.416015625 acc: 0.9713507294654846  val: loss: 481629.71875 acc: 0.9585440754890442\n",
      "step: 27735 , time : 0.0\n",
      "train: loss: 6643.0234375 acc: 0.9884207844734192  val: loss: 1144627.25 acc: 0.7889807224273682\n",
      "step: 27740 , time : 0.0010004043579101562\n",
      "train: loss: 33442.46484375 acc: 0.95366370677948  val: loss: 443449.8125 acc: 0.9131520390510559\n",
      "step: 27745 , time : 0.0\n",
      "train: loss: 28750.361328125 acc: 0.9773017168045044  val: loss: 1170885.5 acc: 0.9143924117088318\n",
      "step: 27750 , time : 0.0\n",
      "train: loss: 13959.0537109375 acc: 0.9945635199546814  val: loss: 1204722.0 acc: 0.7779525518417358\n",
      "step: 27755 , time : 0.0\n",
      "train: loss: 22764.484375 acc: 0.9807785153388977  val: loss: 875871.125 acc: 0.857241153717041\n",
      "step: 27760 , time : 0.0\n",
      "train: loss: 23122.4375 acc: 0.983747661113739  val: loss: 3925210.25 acc: 0.20877563953399658\n",
      "step: 27765 , time : 0.0010004043579101562\n",
      "train: loss: 5291.47265625 acc: 0.9905110001564026  val: loss: 251407.421875 acc: 0.9672921299934387\n",
      "step: 27770 , time : 0.0\n",
      "train: loss: 8841.0126953125 acc: 0.9724440574645996  val: loss: 2581582.0 acc: 0.21256566047668457\n",
      "step: 27775 , time : 0.0010004043579101562\n",
      "train: loss: 18475.1953125 acc: 0.9923014640808105  val: loss: 1822272.0 acc: -0.03149259090423584\n",
      "step: 27780 , time : 0.0\n",
      "train: loss: 34861.84765625 acc: 0.9910395741462708  val: loss: 732739.125 acc: 0.91725093126297\n",
      "step: 27785 , time : 0.0010013580322265625\n",
      "train: loss: 23732.193359375 acc: 0.9906502366065979  val: loss: 3023934.25 acc: 0.4362923502922058\n",
      "step: 27790 , time : 0.0\n",
      "train: loss: 22401.703125 acc: 0.9933801293373108  val: loss: 386158.125 acc: 0.9392993450164795\n",
      "step: 27795 , time : 0.0\n",
      "train: loss: 22986.875 acc: 0.9911024570465088  val: loss: 1361702.25 acc: 0.8239966630935669\n",
      "step: 27800 , time : 0.0\n",
      "train: loss: 148994.0625 acc: 0.9582445025444031  val: loss: 1136589.625 acc: -0.09213292598724365\n",
      "step: 27805 , time : 0.0010004043579101562\n",
      "train: loss: 156505.453125 acc: 0.9605637192726135  val: loss: 2204214.0 acc: 0.2951052784919739\n",
      "step: 27810 , time : 0.0010001659393310547\n",
      "train: loss: 581441.4375 acc: 0.6767281293869019  val: loss: 338993.09375 acc: 0.9299616813659668\n",
      "step: 27815 , time : 0.0\n",
      "train: loss: 150293.421875 acc: 0.9752218127250671  val: loss: 604559.625 acc: 0.9067894220352173\n",
      "step: 27820 , time : 0.0010004043579101562\n",
      "train: loss: 74592.015625 acc: 0.9885641932487488  val: loss: 1814934.5 acc: 0.8308892846107483\n",
      "step: 27825 , time : 0.0010004043579101562\n",
      "train: loss: 127439.359375 acc: 0.9834930300712585  val: loss: 1564308.875 acc: 0.8125231862068176\n",
      "step: 27830 , time : 0.0\n",
      "train: loss: 51002.2265625 acc: 0.9953474402427673  val: loss: 2872827.75 acc: 0.6849632263183594\n",
      "step: 27835 , time : 0.0\n",
      "train: loss: 217694.921875 acc: 0.963563084602356  val: loss: 1822730.25 acc: 0.649715781211853\n",
      "step: 27840 , time : 0.0010006427764892578\n",
      "train: loss: 361207.6875 acc: 0.9694206714630127  val: loss: 2883186.25 acc: 0.23303663730621338\n",
      "step: 27845 , time : 0.0\n",
      "train: loss: 411442.46875 acc: 0.9676617383956909  val: loss: 1906326.0 acc: 0.7640942335128784\n",
      "step: 27850 , time : 0.0\n",
      "train: loss: 239644.5 acc: 0.9773551821708679  val: loss: 750292.0625 acc: 0.720687747001648\n",
      "step: 27855 , time : 0.0010006427764892578\n",
      "train: loss: 910169.375 acc: 0.9383270144462585  val: loss: 954088.3125 acc: 0.6026273965835571\n",
      "step: 27860 , time : 0.0\n",
      "train: loss: 771276.125 acc: 0.9724521636962891  val: loss: 443250.8125 acc: 0.8764162659645081\n",
      "step: 27865 , time : 0.0\n",
      "train: loss: 2239741.25 acc: 0.9222712516784668  val: loss: 475299.6875 acc: 0.8418022394180298\n",
      "step: 27870 , time : 0.0010006427764892578\n",
      "train: loss: 1360689.5 acc: 0.9059066772460938  val: loss: 863223.625 acc: 0.09274822473526001\n",
      "step: 27875 , time : 0.001001119613647461\n",
      "train: loss: 249527.890625 acc: 0.9586952924728394  val: loss: 528612.75 acc: 0.7899191379547119\n",
      "step: 27880 , time : 0.001001119613647461\n",
      "train: loss: 537905.375 acc: 0.9684984683990479  val: loss: 251931.84375 acc: 0.9697259068489075\n",
      "step: 27885 , time : 0.0010006427764892578\n",
      "train: loss: 187457.84375 acc: 0.9673859477043152  val: loss: 939822.5625 acc: 0.8096205592155457\n",
      "step: 27890 , time : 0.0\n",
      "train: loss: 456617.9375 acc: 0.9348710179328918  val: loss: 1235757.5 acc: 0.7846661806106567\n",
      "step: 27895 , time : 0.0\n",
      "train: loss: 665577.6875 acc: 0.8722284436225891  val: loss: 831215.75 acc: 0.8541905879974365\n",
      "step: 27900 , time : 0.0\n",
      "train: loss: 626032.0 acc: 0.5485996007919312  val: loss: 997231.9375 acc: 0.752410888671875\n",
      "step: 27905 , time : 0.0\n",
      "train: loss: 610963.25 acc: 0.6544671058654785  val: loss: 959484.625 acc: 0.8296177387237549\n",
      "step: 27910 , time : 0.0\n",
      "train: loss: 1105523.375 acc: 0.7971770763397217  val: loss: 745866.9375 acc: 0.8449915647506714\n",
      "step: 27915 , time : 0.0\n",
      "train: loss: 772340.75 acc: 0.7056587934494019  val: loss: 1614271.375 acc: 0.7264589667320251\n",
      "step: 27920 , time : 0.0\n",
      "train: loss: 331116.5625 acc: 0.6959995031356812  val: loss: 712181.875 acc: 0.6634340286254883\n",
      "step: 27925 , time : 0.0\n",
      "train: loss: 228657.046875 acc: 0.8323332667350769  val: loss: 1983376.5 acc: 0.6124752163887024\n",
      "step: 27930 , time : 0.0\n",
      "train: loss: 582473.375 acc: 0.7465410232543945  val: loss: 612034.4375 acc: 0.707433819770813\n",
      "step: 27935 , time : 0.0\n",
      "train: loss: 267100.875 acc: 0.8278282880783081  val: loss: 3797408.5 acc: 0.5792849063873291\n",
      "step: 27940 , time : 0.0\n",
      "train: loss: 53530.77734375 acc: 0.9619593620300293  val: loss: 105802.0859375 acc: 0.9255178570747375\n",
      "step: 27945 , time : 0.0\n",
      "train: loss: 99031.4296875 acc: 0.9246948957443237  val: loss: 2028302.375 acc: 0.6052018404006958\n",
      "step: 27950 , time : 0.0010004043579101562\n",
      "train: loss: 46380.625 acc: 0.9624672532081604  val: loss: 3276650.5 acc: 0.5992406606674194\n",
      "step: 27955 , time : 0.0010006427764892578\n",
      "train: loss: 289628.84375 acc: 0.8289749622344971  val: loss: 3844215.0 acc: 0.5863214731216431\n",
      "step: 27960 , time : 0.0\n",
      "train: loss: 95339.6796875 acc: 0.9160100221633911  val: loss: 540736.5625 acc: 0.7185642719268799\n",
      "step: 27965 , time : 0.0\n",
      "train: loss: 164383.34375 acc: 0.878815770149231  val: loss: 592864.0 acc: 0.7214034795761108\n",
      "step: 27970 , time : 0.015625\n",
      "train: loss: 20496.71875 acc: 0.970965564250946  val: loss: 1166577.0 acc: 0.6628586053848267\n",
      "step: 27975 , time : 0.0\n",
      "train: loss: 164235.8125 acc: 0.879203200340271  val: loss: 1466663.375 acc: 0.7165281772613525\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 27980 , time : 0.0\n",
      "train: loss: 76979.1484375 acc: 0.9452021718025208  val: loss: 2636569.75 acc: 0.5763616561889648\n",
      "step: 27985 , time : 0.0010008811950683594\n",
      "train: loss: 156936.828125 acc: 0.8591412305831909  val: loss: 4005905.75 acc: 0.5222319960594177\n",
      "step: 27990 , time : 0.0010006427764892578\n",
      "train: loss: 164486.21875 acc: 0.8561415076255798  val: loss: 1501562.125 acc: 0.5551784038543701\n",
      "step: 27995 , time : 0.001001119613647461\n",
      "train: loss: 258295.484375 acc: 0.7741844058036804  val: loss: 542830.75 acc: 0.6943851709365845\n",
      "step: 28000 , time : 0.0010004043579101562\n",
      "train: loss: 399297.8125 acc: 0.8092363476753235  val: loss: 1563857.375 acc: 0.5850396156311035\n",
      "step: 28005 , time : 0.0\n",
      "train: loss: 1924461.125 acc: 0.7011605501174927  val: loss: 695869.875 acc: 0.7668887376785278\n",
      "step: 28010 , time : 0.0010006427764892578\n",
      "train: loss: 737648.0 acc: 0.7648836970329285  val: loss: 816509.4375 acc: 0.8586289882659912\n",
      "step: 28015 , time : 0.0\n",
      "train: loss: 493624.84375 acc: 0.9531266093254089  val: loss: 686037.75 acc: 0.7622453570365906\n",
      "step: 28020 , time : 0.0010006427764892578\n",
      "train: loss: 190550.296875 acc: 0.9794479012489319  val: loss: 435988.78125 acc: 0.9477920532226562\n",
      "step: 28025 , time : 0.0010006427764892578\n",
      "train: loss: 435589.28125 acc: 0.9132415652275085  val: loss: 348150.96875 acc: 0.9335042238235474\n",
      "step: 28030 , time : 0.0\n",
      "train: loss: 88222.34375 acc: 0.9809161424636841  val: loss: 307158.59375 acc: 0.9648133516311646\n",
      "step: 28035 , time : 0.0\n",
      "train: loss: 124750.2109375 acc: 0.9869949221611023  val: loss: 351768.09375 acc: 0.9706466794013977\n",
      "step: 28040 , time : 0.0\n",
      "train: loss: 83637.296875 acc: 0.9930925965309143  val: loss: 1465713.5 acc: 0.8088817596435547\n",
      "step: 28045 , time : 0.0\n",
      "train: loss: 108651.421875 acc: 0.9923733472824097  val: loss: 622342.4375 acc: 0.9120741486549377\n",
      "step: 28050 , time : 0.015625953674316406\n",
      "train: loss: 79825.8125 acc: 0.9873023629188538  val: loss: 800582.5 acc: 0.8558675050735474\n",
      "step: 28055 , time : 0.0\n",
      "train: loss: 76500.5390625 acc: 0.9863557815551758  val: loss: 2847716.0 acc: 0.6432074308395386\n",
      "step: 28060 , time : 0.0\n",
      "train: loss: 65030.44140625 acc: 0.9876015782356262  val: loss: 1021223.5625 acc: 0.8383951187133789\n",
      "step: 28065 , time : 0.0\n",
      "train: loss: 13757.6640625 acc: 0.9921375513076782  val: loss: 1670482.25 acc: 0.6101562976837158\n",
      "step: 28070 , time : 0.0\n",
      "train: loss: 12797.9833984375 acc: 0.9903364181518555  val: loss: 313007.5625 acc: 0.9571873545646667\n",
      "step: 28075 , time : 0.0\n",
      "train: loss: 11554.87109375 acc: 0.9773932099342346  val: loss: 565976.75 acc: 0.8035227060317993\n",
      "step: 28080 , time : 0.0\n",
      "train: loss: 11014.1826171875 acc: 0.9878059029579163  val: loss: 1765713.125 acc: 0.8857464790344238\n",
      "step: 28085 , time : 0.0\n",
      "train: loss: 16347.5458984375 acc: 0.9420753717422485  val: loss: 772067.3125 acc: 0.9467226266860962\n",
      "step: 28090 , time : 0.0\n",
      "train: loss: 10069.7861328125 acc: 0.9826445579528809  val: loss: 3573711.25 acc: -0.18350601196289062\n",
      "step: 28095 , time : 0.001001119613647461\n",
      "train: loss: 10451.1953125 acc: 0.9825010299682617  val: loss: 1445584.75 acc: 0.8579539060592651\n",
      "step: 28100 , time : 0.001001119613647461\n",
      "train: loss: 17995.970703125 acc: 0.9802398085594177  val: loss: 2012117.875 acc: -0.3500964641571045\n",
      "step: 28105 , time : 0.0\n",
      "train: loss: 15388.1435546875 acc: 0.9653145670890808  val: loss: 777775.9375 acc: 0.8343831896781921\n",
      "step: 28110 , time : 0.0010004043579101562\n",
      "train: loss: 9259.3671875 acc: 0.9925662279129028  val: loss: 2504301.0 acc: 0.4299526810646057\n",
      "step: 28115 , time : 0.0\n",
      "train: loss: 18655.09765625 acc: 0.9814303517341614  val: loss: 1802449.75 acc: 0.41166001558303833\n",
      "step: 28120 , time : 0.0\n",
      "train: loss: 15386.482421875 acc: 0.9910443425178528  val: loss: 5429339.5 acc: -1.6269268989562988\n",
      "step: 28125 , time : 0.0010004043579101562\n",
      "train: loss: 18647.2109375 acc: 0.9911991357803345  val: loss: 885285.375 acc: 0.6573616862297058\n",
      "step: 28130 , time : 0.0\n",
      "train: loss: 19435.435546875 acc: 0.9847726225852966  val: loss: 3517329.25 acc: -0.3852773904800415\n",
      "step: 28135 , time : 0.0010004043579101562\n",
      "train: loss: 11434.8349609375 acc: 0.9833270907402039  val: loss: 1293810.625 acc: 0.2934302091598511\n",
      "step: 28140 , time : 0.0\n",
      "train: loss: 11921.3076171875 acc: 0.9953907132148743  val: loss: 1456094.375 acc: 0.877659022808075\n",
      "step: 28145 , time : 0.0\n",
      "train: loss: 39990.53515625 acc: 0.9893152117729187  val: loss: 2382597.5 acc: 0.3812415599822998\n",
      "step: 28150 , time : 0.0\n",
      "train: loss: 46750.015625 acc: 0.9833856225013733  val: loss: 2801979.5 acc: -0.22205519676208496\n",
      "step: 28155 , time : 0.0\n",
      "train: loss: 11536.8681640625 acc: 0.9948247075080872  val: loss: 2203496.25 acc: 0.7648658156394958\n",
      "step: 28160 , time : 0.0\n",
      "train: loss: 30105.076171875 acc: 0.9912264347076416  val: loss: 749626.75 acc: 0.7459390163421631\n",
      "step: 28165 , time : 0.0\n",
      "train: loss: 178934.359375 acc: 0.9466763734817505  val: loss: 2926811.0 acc: 0.4551398754119873\n",
      "step: 28170 , time : 0.0\n",
      "train: loss: 77135.6171875 acc: 0.9710426926612854  val: loss: 590687.1875 acc: 0.8586580753326416\n",
      "step: 28175 , time : 0.01562643051147461\n",
      "train: loss: 46378.6171875 acc: 0.9857050776481628  val: loss: 2328577.0 acc: 0.5514615178108215\n",
      "step: 28180 , time : 0.0\n",
      "train: loss: 188287.40625 acc: 0.9653189182281494  val: loss: 1097107.0 acc: 0.8644090294837952\n",
      "step: 28185 , time : 0.0\n",
      "train: loss: 544442.8125 acc: 0.9020978808403015  val: loss: 2163950.5 acc: 0.06595790386199951\n",
      "step: 28190 , time : 0.0\n",
      "train: loss: 64552.12890625 acc: 0.9912758469581604  val: loss: 2951377.0 acc: 0.7769235372543335\n",
      "step: 28195 , time : 0.0\n",
      "train: loss: 29717.611328125 acc: 0.9948013424873352  val: loss: 1459728.875 acc: 0.8295160531997681\n",
      "step: 28200 , time : 0.0010004043579101562\n",
      "train: loss: 123063.9453125 acc: 0.9859158396720886  val: loss: 370766.71875 acc: 0.9259274005889893\n",
      "step: 28205 , time : 0.0\n",
      "train: loss: 663166.8125 acc: 0.9614092111587524  val: loss: 373314.125 acc: 0.9376857280731201\n",
      "step: 28210 , time : 0.0010004043579101562\n",
      "train: loss: 2309801.75 acc: 0.8891635537147522  val: loss: 848070.0 acc: 0.7916368842124939\n",
      "step: 28215 , time : 0.0010006427764892578\n",
      "train: loss: 297056.875 acc: 0.9219052791595459  val: loss: 1907151.5 acc: 0.7670234441757202\n",
      "step: 28220 , time : 0.0010006427764892578\n",
      "train: loss: 1055689.75 acc: 0.8740707039833069  val: loss: 545007.25 acc: 0.8654743432998657\n",
      "step: 28225 , time : 0.0010008811950683594\n",
      "train: loss: 702844.0 acc: 0.9692602157592773  val: loss: 304136.96875 acc: 0.847814679145813\n",
      "step: 28230 , time : 0.0\n",
      "train: loss: 837279.8125 acc: 0.9694371223449707  val: loss: 385265.25 acc: 0.9580172300338745\n",
      "step: 28235 , time : 0.0\n",
      "train: loss: 1891882.875 acc: 0.934104323387146  val: loss: 989639.25 acc: 0.6113073825836182\n",
      "step: 28240 , time : 0.0\n",
      "train: loss: 813156.375 acc: 0.9253928661346436  val: loss: 542223.625 acc: 0.6033141613006592\n",
      "step: 28245 , time : 0.015624284744262695\n",
      "train: loss: 174475.015625 acc: 0.9733631610870361  val: loss: 720481.0 acc: 0.8753262162208557\n",
      "step: 28250 , time : 0.0\n",
      "train: loss: 141946.953125 acc: 0.9713175892829895  val: loss: 423353.1875 acc: 0.9334461092948914\n",
      "step: 28255 , time : 0.0\n",
      "train: loss: 254691.046875 acc: 0.9602909088134766  val: loss: 103532.3125 acc: 0.9577701687812805\n",
      "step: 28260 , time : 0.0\n",
      "train: loss: 1826260.25 acc: 0.43624794483184814  val: loss: 1124005.75 acc: 0.7998828291893005\n",
      "step: 28265 , time : 0.0\n",
      "train: loss: 709318.25 acc: 0.6275616884231567  val: loss: 663433.6875 acc: 0.8662863969802856\n",
      "step: 28270 , time : 0.0\n",
      "train: loss: 1004369.8125 acc: 0.4990711808204651  val: loss: 532039.8125 acc: 0.7127223014831543\n",
      "step: 28275 , time : 0.0\n",
      "train: loss: 671685.0625 acc: 0.7870770692825317  val: loss: 836453.9375 acc: 0.7895792722702026\n",
      "step: 28280 , time : 0.0\n",
      "train: loss: 836925.5625 acc: 0.5826504230499268  val: loss: 476901.09375 acc: 0.8719089031219482\n",
      "step: 28285 , time : 0.0\n",
      "train: loss: 1023892.6875 acc: 0.7097359895706177  val: loss: 2574410.75 acc: 0.6134902834892273\n",
      "step: 28290 , time : 0.0\n",
      "train: loss: 118411.484375 acc: 0.9265317320823669  val: loss: 1596640.5 acc: 0.6269645690917969\n",
      "step: 28295 , time : 0.0\n",
      "train: loss: 32502.107421875 acc: 0.9671341180801392  val: loss: 1055418.125 acc: 0.6859253644943237\n",
      "step: 28300 , time : 0.0\n",
      "train: loss: 42457.9453125 acc: 0.9706160426139832  val: loss: 1169768.5 acc: 0.6885393857955933\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 28305 , time : 0.0\n",
      "train: loss: 34643.5390625 acc: 0.9685139656066895  val: loss: 1815151.125 acc: 0.6249958276748657\n",
      "step: 28310 , time : 0.0010004043579101562\n",
      "train: loss: 152155.625 acc: 0.9080103039741516  val: loss: 1205783.25 acc: 0.6812493801116943\n",
      "step: 28315 , time : 0.0\n",
      "train: loss: 270496.84375 acc: 0.8574634790420532  val: loss: 1413488.5 acc: 0.674411416053772\n",
      "step: 28320 , time : 0.0010004043579101562\n",
      "train: loss: 172705.0625 acc: 0.8957260847091675  val: loss: 2268781.0 acc: 0.5804257392883301\n",
      "step: 28325 , time : 0.0010004043579101562\n",
      "train: loss: 91087.015625 acc: 0.9174150228500366  val: loss: 1629585.5 acc: 0.5919134616851807\n",
      "step: 28330 , time : 0.0010004043579101562\n",
      "train: loss: 121239.890625 acc: 0.8840985298156738  val: loss: 1038900.3125 acc: 0.6518598794937134\n",
      "step: 28335 , time : 0.0010008811950683594\n",
      "train: loss: 209589.46875 acc: 0.8453137278556824  val: loss: 416416.25 acc: 0.7781881093978882\n",
      "step: 28340 , time : 0.0\n",
      "train: loss: 98980.921875 acc: 0.898350179195404  val: loss: 1298331.25 acc: 0.7140600681304932\n",
      "step: 28345 , time : 0.0\n",
      "train: loss: 99195.90625 acc: 0.9267500638961792  val: loss: 2947574.5 acc: 0.5335540771484375\n",
      "step: 28350 , time : 0.0\n",
      "train: loss: 288212.4375 acc: 0.8454770445823669  val: loss: 4953234.5 acc: 0.5077676773071289\n",
      "step: 28355 , time : 0.0\n",
      "train: loss: 391247.28125 acc: 0.7920295000076294  val: loss: 2229321.5 acc: 0.6032543778419495\n",
      "step: 28360 , time : 0.0\n",
      "train: loss: 286217.46875 acc: 0.7699819207191467  val: loss: 2906195.25 acc: 0.5456635355949402\n",
      "step: 28365 , time : 0.0\n",
      "train: loss: 180607.734375 acc: 0.8967252969741821  val: loss: 3973600.5 acc: 0.4909934401512146\n",
      "step: 28370 , time : 0.0\n",
      "train: loss: 1357342.5 acc: 0.6945536136627197  val: loss: 583628.625 acc: 0.850044310092926\n",
      "step: 28375 , time : 0.0\n",
      "train: loss: 1313260.0 acc: 0.7901260852813721  val: loss: 1887976.25 acc: 0.8310145735740662\n",
      "step: 28380 , time : 0.015625953674316406\n",
      "train: loss: 592487.8125 acc: 0.9182873964309692  val: loss: 628694.3125 acc: 0.8923973441123962\n",
      "step: 28385 , time : 0.0010004043579101562\n",
      "train: loss: 112164.734375 acc: 0.990039050579071  val: loss: 313987.03125 acc: 0.909241795539856\n",
      "step: 28390 , time : 0.0\n",
      "train: loss: 70033.0625 acc: 0.9933826327323914  val: loss: 1303020.5 acc: 0.7811641693115234\n",
      "step: 28395 , time : 0.0\n",
      "train: loss: 159935.296875 acc: 0.9766386151313782  val: loss: 961278.875 acc: 0.9133522510528564\n",
      "step: 28400 , time : 0.0\n",
      "train: loss: 100020.4453125 acc: 0.9932159185409546  val: loss: 583354.25 acc: 0.6682495474815369\n",
      "step: 28405 , time : 0.0\n",
      "train: loss: 55512.26953125 acc: 0.9957984089851379  val: loss: 1001080.625 acc: 0.7224013805389404\n",
      "step: 28410 , time : 0.0\n",
      "train: loss: 65833.3203125 acc: 0.9953603744506836  val: loss: 880553.0625 acc: 0.9037853479385376\n",
      "step: 28415 , time : 0.0\n",
      "train: loss: 67396.765625 acc: 0.9936915636062622  val: loss: 1248984.5 acc: 0.7639285326004028\n",
      "step: 28420 , time : 0.0\n",
      "train: loss: 33260.7578125 acc: 0.994008481502533  val: loss: 1710573.5 acc: -0.00370943546295166\n",
      "step: 28425 , time : 0.001001119613647461\n",
      "train: loss: 18839.177734375 acc: 0.9948013424873352  val: loss: 1757201.375 acc: 0.6942424774169922\n",
      "step: 28430 , time : 0.0\n",
      "train: loss: 40372.640625 acc: 0.9794808626174927  val: loss: 2176721.0 acc: 0.46672314405441284\n",
      "step: 28435 , time : 0.0010006427764892578\n",
      "train: loss: 10050.7958984375 acc: 0.993636965751648  val: loss: 2738167.25 acc: 0.7228604555130005\n",
      "step: 28440 , time : 0.0\n",
      "train: loss: 10673.0791015625 acc: 0.995880663394928  val: loss: 420161.8125 acc: 0.8548977375030518\n",
      "step: 28445 , time : 0.0010004043579101562\n",
      "train: loss: 17154.134765625 acc: 0.9745305776596069  val: loss: 3102896.0 acc: -0.11759698390960693\n",
      "step: 28450 , time : 0.0010006427764892578\n",
      "train: loss: 5559.205078125 acc: 0.9819279909133911  val: loss: 712120.75 acc: 0.8226791620254517\n",
      "step: 28455 , time : 0.001001119613647461\n",
      "train: loss: 11416.6513671875 acc: 0.9868486523628235  val: loss: 2745674.0 acc: -0.4379587173461914\n",
      "step: 28460 , time : 0.0\n",
      "train: loss: 7829.18798828125 acc: 0.9874050617218018  val: loss: 1299508.5 acc: 0.8430678844451904\n",
      "step: 28465 , time : 0.001001596450805664\n",
      "train: loss: 8074.7861328125 acc: 0.9791907668113708  val: loss: 2607245.75 acc: 0.5662503838539124\n",
      "step: 28470 , time : 0.0\n",
      "train: loss: 9556.0439453125 acc: 0.986640453338623  val: loss: 1417436.125 acc: 0.7661014795303345\n",
      "step: 28475 , time : 0.0\n",
      "train: loss: 26053.412109375 acc: 0.9697769284248352  val: loss: 1985236.875 acc: 0.6846225261688232\n",
      "step: 28480 , time : 0.0\n",
      "train: loss: 22716.49609375 acc: 0.9857282638549805  val: loss: 820263.4375 acc: 0.9240573644638062\n",
      "step: 28485 , time : 0.0\n",
      "train: loss: 21001.6171875 acc: 0.9883990287780762  val: loss: 3512110.25 acc: -0.21758925914764404\n",
      "step: 28490 , time : 0.0\n",
      "train: loss: 8347.6220703125 acc: 0.9865612387657166  val: loss: 1532459.5 acc: 0.6204386949539185\n",
      "step: 28495 , time : 0.0\n",
      "train: loss: 12451.6064453125 acc: 0.9919996857643127  val: loss: 974552.75 acc: 0.8129693865776062\n",
      "step: 28500 , time : 0.0\n",
      "train: loss: 11880.3837890625 acc: 0.9637920260429382  val: loss: 1526191.875 acc: 0.760766863822937\n",
      "step: 28505 , time : 0.0\n",
      "train: loss: 11929.337890625 acc: 0.994437575340271  val: loss: 2516598.75 acc: -0.5412791967391968\n",
      "step: 28510 , time : 0.0\n",
      "train: loss: 39107.78515625 acc: 0.9888849854469299  val: loss: 1315326.0 acc: 0.7650630474090576\n",
      "step: 28515 , time : 0.0\n",
      "train: loss: 45465.28515625 acc: 0.9846036434173584  val: loss: 2455834.25 acc: 0.5430649518966675\n",
      "step: 28520 , time : 0.0\n",
      "train: loss: 25849.40625 acc: 0.9887987971305847  val: loss: 2665053.25 acc: 0.646914005279541\n",
      "step: 28525 , time : 0.0\n",
      "train: loss: 32262.142578125 acc: 0.985974907875061  val: loss: 89447.7734375 acc: 0.9896106123924255\n",
      "step: 28530 , time : 0.0\n",
      "train: loss: 363631.71875 acc: 0.9031577110290527  val: loss: 2015683.0 acc: 0.5930228233337402\n",
      "step: 28535 , time : 0.0\n",
      "train: loss: 64181.87890625 acc: 0.974524736404419  val: loss: 1937907.0 acc: 0.0520932674407959\n",
      "step: 28540 , time : 0.0010006427764892578\n",
      "train: loss: 163208.1875 acc: 0.8486335873603821  val: loss: 479514.75 acc: 0.9507206082344055\n",
      "step: 28545 , time : 0.0\n",
      "train: loss: 949510.4375 acc: 0.841546356678009  val: loss: 1151646.625 acc: 0.5138098001480103\n",
      "step: 28550 , time : 0.001001119613647461\n",
      "train: loss: 60034.52734375 acc: 0.9921834468841553  val: loss: 2544244.0 acc: 0.43112653493881226\n",
      "step: 28555 , time : 0.0\n",
      "train: loss: 79856.765625 acc: 0.9920848608016968  val: loss: 1259579.125 acc: 0.41678231954574585\n",
      "step: 28560 , time : 0.001001119613647461\n",
      "train: loss: 102592.3671875 acc: 0.9824288487434387  val: loss: 1309308.25 acc: 0.5832844972610474\n",
      "step: 28565 , time : 0.0010008811950683594\n",
      "train: loss: 798208.6875 acc: 0.9065483212471008  val: loss: 644303.4375 acc: 0.883934497833252\n",
      "step: 28570 , time : 0.0\n",
      "train: loss: 344237.15625 acc: 0.9741466045379639  val: loss: 637751.75 acc: 0.7017717361450195\n",
      "step: 28575 , time : 0.0\n",
      "train: loss: 1018610.625 acc: 0.9224948883056641  val: loss: 1496348.0 acc: -0.15044498443603516\n",
      "step: 28580 , time : 0.0\n",
      "train: loss: 341629.53125 acc: 0.9538739919662476  val: loss: 1213753.75 acc: 0.6019324660301208\n",
      "step: 28585 , time : 0.015624284744262695\n",
      "train: loss: 322674.75 acc: 0.9727874994277954  val: loss: 944731.125 acc: 0.841494083404541\n",
      "step: 28590 , time : 0.0\n",
      "train: loss: 219662.15625 acc: 0.9851173162460327  val: loss: 1803473.0 acc: 0.8318614959716797\n",
      "step: 28595 , time : 0.0\n",
      "train: loss: 2735960.25 acc: 0.9085840582847595  val: loss: 190902.0625 acc: 0.8054947257041931\n",
      "step: 28600 , time : 0.0\n",
      "train: loss: 1742887.375 acc: 0.917186439037323  val: loss: 324830.1875 acc: 0.9522884488105774\n",
      "step: 28605 , time : 0.0\n",
      "train: loss: 256864.890625 acc: 0.9864181876182556  val: loss: 369118.40625 acc: 0.9445775151252747\n",
      "step: 28610 , time : 0.0010004043579101562\n",
      "train: loss: 408221.5625 acc: 0.9662420749664307  val: loss: 893714.0 acc: 0.2830483317375183\n",
      "step: 28615 , time : 0.0010008811950683594\n",
      "train: loss: 533924.0 acc: 0.9567752480506897  val: loss: 751443.75 acc: 0.845434844493866\n",
      "step: 28620 , time : 0.0010008811950683594\n",
      "train: loss: 399605.15625 acc: 0.861893892288208  val: loss: 755657.625 acc: 0.5474061369895935\n",
      "step: 28625 , time : 0.0010006427764892578\n",
      "train: loss: 2195130.0 acc: 0.631442666053772  val: loss: 1151075.75 acc: 0.8494621515274048\n",
      "step: 28630 , time : 0.0\n",
      "train: loss: 599269.6875 acc: 0.703350305557251  val: loss: 887542.0 acc: 0.6423225402832031\n",
      "step: 28635 , time : 0.0\n",
      "train: loss: 285539.15625 acc: 0.862628161907196  val: loss: 717847.0 acc: 0.834216833114624\n",
      "step: 28640 , time : 0.0\n",
      "train: loss: 415198.15625 acc: 0.8805851936340332  val: loss: 782317.125 acc: 0.8809633255004883\n",
      "step: 28645 , time : 0.0\n",
      "train: loss: 919196.3125 acc: 0.7079293131828308  val: loss: 2077474.875 acc: 0.7787051200866699\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 28650 , time : 0.0\n",
      "train: loss: 336043.53125 acc: 0.8285846710205078  val: loss: 906104.875 acc: 0.774480938911438\n",
      "step: 28655 , time : 0.0010001659393310547\n",
      "train: loss: 561202.3125 acc: 0.7718348503112793  val: loss: 2102503.75 acc: 0.5435777902603149\n",
      "step: 28660 , time : 0.0010008811950683594\n",
      "train: loss: 117233.453125 acc: 0.9091179966926575  val: loss: 922766.1875 acc: 0.6691814661026001\n",
      "step: 28665 , time : 0.0010006427764892578\n",
      "train: loss: 45055.53515625 acc: 0.9617055654525757  val: loss: 2388879.25 acc: 0.6133613586425781\n",
      "step: 28670 , time : 0.0010006427764892578\n",
      "train: loss: 453534.875 acc: 0.7735559940338135  val: loss: 4763447.5 acc: 0.43055957555770874\n",
      "step: 28675 , time : 0.0\n",
      "train: loss: 103674.5 acc: 0.9091182947158813  val: loss: 6092920.5 acc: 0.5004721879959106\n",
      "step: 28680 , time : 0.0\n",
      "train: loss: 393343.84375 acc: 0.8190706372261047  val: loss: 1281932.875 acc: 0.6641528606414795\n",
      "step: 28685 , time : 0.0\n",
      "train: loss: 132256.5 acc: 0.9111947417259216  val: loss: 2653693.25 acc: 0.622556209564209\n",
      "step: 28690 , time : 0.001001119613647461\n",
      "train: loss: 13753.412109375 acc: 0.9881647229194641  val: loss: 3125037.25 acc: 0.5648147463798523\n",
      "step: 28695 , time : 0.0\n",
      "train: loss: 218788.78125 acc: 0.7831710577011108  val: loss: 4238070.5 acc: 0.5394192934036255\n",
      "step: 28700 , time : 0.0\n",
      "train: loss: 86822.046875 acc: 0.9255855679512024  val: loss: 4906560.0 acc: 0.5618640184402466\n",
      "step: 28705 , time : 0.0\n",
      "train: loss: 320619.40625 acc: 0.6851541996002197  val: loss: 5544347.5 acc: 0.5469499826431274\n",
      "step: 28710 , time : 0.0\n",
      "train: loss: 306475.65625 acc: 0.818930983543396  val: loss: 1440999.125 acc: 0.6259740591049194\n",
      "step: 28715 , time : 0.0\n",
      "train: loss: 754891.6875 acc: 0.7007891535758972  val: loss: 844793.0 acc: 0.6987651586532593\n",
      "step: 28720 , time : 0.0\n",
      "train: loss: 320348.3125 acc: 0.7885525226593018  val: loss: 6305832.5 acc: 0.5359155535697937\n",
      "step: 28725 , time : 0.0\n",
      "train: loss: 128200.3828125 acc: 0.9034280776977539  val: loss: 3648417.75 acc: 0.5351920127868652\n",
      "step: 28730 , time : 0.0\n",
      "train: loss: 26409.29296875 acc: 0.9754014611244202  val: loss: 2259259.25 acc: 0.4919332265853882\n",
      "step: 28735 , time : 0.0\n",
      "train: loss: 1712028.25 acc: 0.7274712324142456  val: loss: 359201.4375 acc: 0.8426003456115723\n",
      "step: 28740 , time : 0.0\n",
      "train: loss: 1542188.0 acc: 0.7025225758552551  val: loss: 1399503.25 acc: -0.004297494888305664\n",
      "step: 28745 , time : 0.0\n",
      "train: loss: 777213.25 acc: 0.9312962889671326  val: loss: 582128.5625 acc: 0.8385300636291504\n",
      "step: 28750 , time : 0.0\n",
      "train: loss: 245626.59375 acc: 0.9737460017204285  val: loss: 748540.25 acc: 0.8769499063491821\n",
      "step: 28755 , time : 0.0\n",
      "train: loss: 73497.6015625 acc: 0.990551233291626  val: loss: 3793849.0 acc: 0.3424973487854004\n",
      "step: 28760 , time : 0.0010006427764892578\n",
      "train: loss: 76501.3515625 acc: 0.9916850924491882  val: loss: 641962.3125 acc: 0.899246335029602\n",
      "step: 28765 , time : 0.0\n",
      "train: loss: 77150.2265625 acc: 0.9877991080284119  val: loss: 2046903.5 acc: 0.6811975240707397\n",
      "step: 28770 , time : 0.0010006427764892578\n",
      "train: loss: 88068.9765625 acc: 0.9943679571151733  val: loss: 2183697.25 acc: 0.5526626110076904\n",
      "step: 28775 , time : 0.0\n",
      "train: loss: 76620.53125 acc: 0.99465012550354  val: loss: 1158306.75 acc: 0.8393310904502869\n",
      "step: 28780 , time : 0.0010004043579101562\n",
      "train: loss: 50157.99609375 acc: 0.995215892791748  val: loss: 3076703.75 acc: -1.0083131790161133\n",
      "step: 28785 , time : 0.0010004043579101562\n",
      "train: loss: 65513.890625 acc: 0.9905413389205933  val: loss: 1183797.0 acc: 0.7648569345474243\n",
      "step: 28790 , time : 0.0010006427764892578\n",
      "train: loss: 12889.1533203125 acc: 0.9966500997543335  val: loss: 758961.6875 acc: 0.8391383290290833\n",
      "step: 28795 , time : 0.0\n",
      "train: loss: 17093.810546875 acc: 0.9878603219985962  val: loss: 773186.6875 acc: 0.8912343978881836\n",
      "step: 28800 , time : 0.0\n",
      "train: loss: 13442.8046875 acc: 0.988477349281311  val: loss: 2266547.75 acc: 0.6331347227096558\n",
      "step: 28805 , time : 0.0\n",
      "train: loss: 4720.06396484375 acc: 0.9971954226493835  val: loss: 296687.90625 acc: 0.9641838073730469\n",
      "step: 28810 , time : 0.015625\n",
      "train: loss: 14601.6708984375 acc: 0.9788678884506226  val: loss: 1625704.25 acc: 0.7593545913696289\n",
      "step: 28815 , time : 0.0\n",
      "train: loss: 10158.4736328125 acc: 0.9643675684928894  val: loss: 3031655.75 acc: 0.07434284687042236\n",
      "step: 28820 , time : 0.0\n",
      "train: loss: 21425.763671875 acc: 0.9583250880241394  val: loss: 611638.3125 acc: 0.8935675024986267\n",
      "step: 28825 , time : 0.0\n",
      "train: loss: 12704.5693359375 acc: 0.9847726225852966  val: loss: 1221950.5 acc: 0.8371001482009888\n",
      "step: 28830 , time : 0.0\n",
      "train: loss: 7092.1162109375 acc: 0.9825094938278198  val: loss: 1714609.5 acc: 0.7701038122177124\n",
      "step: 28835 , time : 0.0\n",
      "train: loss: 5764.236328125 acc: 0.982006311416626  val: loss: 1124434.25 acc: 0.5451493263244629\n",
      "step: 28840 , time : 0.0\n",
      "train: loss: 40308.58203125 acc: 0.9742658734321594  val: loss: 548486.625 acc: 0.7901945114135742\n",
      "step: 28845 , time : 0.0\n",
      "train: loss: 34991.40625 acc: 0.9813797473907471  val: loss: 1145212.0 acc: 0.8159205913543701\n",
      "step: 28850 , time : 0.0\n",
      "train: loss: 12488.59375 acc: 0.9919730424880981  val: loss: 125488.2421875 acc: 0.9597795605659485\n",
      "step: 28855 , time : 0.0\n",
      "train: loss: 29020.9765625 acc: 0.9830935597419739  val: loss: 155969.015625 acc: 0.9728180766105652\n",
      "step: 28860 , time : 0.0\n",
      "train: loss: 7427.330078125 acc: 0.9944281578063965  val: loss: 463394.0 acc: 0.8892799019813538\n",
      "step: 28865 , time : 0.0\n",
      "train: loss: 20603.962890625 acc: 0.9794183373451233  val: loss: 1499101.25 acc: 0.7729340195655823\n",
      "step: 28870 , time : 0.0\n",
      "train: loss: 2544.134765625 acc: 0.9976359605789185  val: loss: 2975848.25 acc: 0.6690044403076172\n",
      "step: 28875 , time : 0.0\n",
      "train: loss: 24712.52734375 acc: 0.9942419528961182  val: loss: 1465857.625 acc: 0.6859136819839478\n",
      "step: 28880 , time : 0.001001119613647461\n",
      "train: loss: 23292.060546875 acc: 0.9943568706512451  val: loss: 1515690.0 acc: 0.4135935306549072\n",
      "step: 28885 , time : 0.0\n",
      "train: loss: 28303.974609375 acc: 0.9919100403785706  val: loss: 1617603.0 acc: 0.5326970815658569\n",
      "step: 28890 , time : 0.0\n",
      "train: loss: 22975.041015625 acc: 0.9940457940101624  val: loss: 404487.5 acc: 0.9587830901145935\n",
      "step: 28895 , time : 0.0010004043579101562\n",
      "train: loss: 218668.390625 acc: 0.9366772174835205  val: loss: 1693529.5 acc: 0.5018917322158813\n",
      "step: 28900 , time : 0.0\n",
      "train: loss: 185580.953125 acc: 0.9656393527984619  val: loss: 518734.90625 acc: 0.8865064978599548\n",
      "step: 28905 , time : 0.0\n",
      "train: loss: 81914.25 acc: 0.9528035521507263  val: loss: 2181767.0 acc: 0.5844049453735352\n",
      "step: 28910 , time : 0.0\n",
      "train: loss: 42879.2265625 acc: 0.9873969554901123  val: loss: 523121.59375 acc: 0.8838014602661133\n",
      "step: 28915 , time : 0.0\n",
      "train: loss: 485108.46875 acc: 0.9477936029434204  val: loss: 1287773.5 acc: 0.6910563707351685\n",
      "step: 28920 , time : 0.0\n",
      "train: loss: 138018.046875 acc: 0.9820430278778076  val: loss: 370228.1875 acc: 0.9398419260978699\n",
      "step: 28925 , time : 0.0\n",
      "train: loss: 221390.84375 acc: 0.9665125608444214  val: loss: 1113948.75 acc: 0.7784990072250366\n",
      "step: 28930 , time : 0.0\n",
      "train: loss: 351318.96875 acc: 0.9693453311920166  val: loss: 144771.203125 acc: 0.9524793028831482\n",
      "step: 28935 , time : 0.0\n",
      "train: loss: 273765.84375 acc: 0.978598415851593  val: loss: 756349.0625 acc: 0.7711243629455566\n",
      "step: 28940 , time : 0.0\n",
      "train: loss: 317116.71875 acc: 0.9754428267478943  val: loss: 2635564.75 acc: 0.6787469387054443\n",
      "step: 28945 , time : 0.0\n",
      "train: loss: 419091.84375 acc: 0.9673252105712891  val: loss: 800217.6875 acc: 0.35054194927215576\n",
      "step: 28950 , time : 0.0\n",
      "train: loss: 242403.875 acc: 0.9731661677360535  val: loss: 1401801.0 acc: 0.35355108976364136\n",
      "step: 28955 , time : 0.0\n",
      "train: loss: 503735.8125 acc: 0.9651818871498108  val: loss: 517259.03125 acc: 0.9131998419761658\n",
      "step: 28960 , time : 0.0\n",
      "train: loss: 1807504.25 acc: 0.9405332803726196  val: loss: 707090.3125 acc: 0.9406374096870422\n",
      "step: 28965 , time : 0.0\n",
      "train: loss: 1074331.5 acc: 0.9608863592147827  val: loss: 283663.21875 acc: 0.9397516846656799\n",
      "step: 28970 , time : 0.0\n",
      "train: loss: 962619.75 acc: 0.9374210238456726  val: loss: 407565.53125 acc: 0.9387981295585632\n",
      "step: 28975 , time : 0.0\n",
      "train: loss: 386416.71875 acc: 0.9508354663848877  val: loss: 352833.875 acc: 0.9408605098724365\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 28980 , time : 0.0\n",
      "train: loss: 371649.90625 acc: 0.9577599167823792  val: loss: 208967.015625 acc: 0.9694031476974487\n",
      "step: 28985 , time : 0.0\n",
      "train: loss: 509045.71875 acc: 0.9090686440467834  val: loss: 3870310.75 acc: 0.7535116076469421\n",
      "step: 28990 , time : 0.0\n",
      "train: loss: 2583469.25 acc: 0.318034827709198  val: loss: 1377681.0 acc: 0.9035052061080933\n",
      "step: 28995 , time : 0.0010006427764892578\n",
      "train: loss: 1769540.125 acc: 0.2763821482658386  val: loss: 1427070.875 acc: 0.799272894859314\n",
      "step: 29000 , time : 0.0\n",
      "train: loss: 495471.09375 acc: 0.7679753303527832  val: loss: 1033851.5625 acc: 0.8370405435562134\n",
      "step: 29005 , time : 0.0010006427764892578\n",
      "train: loss: 638471.0 acc: 0.7452280521392822  val: loss: 1713182.75 acc: 0.7870783805847168\n",
      "step: 29010 , time : 0.0\n",
      "train: loss: 644454.1875 acc: 0.7617928981781006  val: loss: 1521062.125 acc: 0.7609320282936096\n",
      "step: 29015 , time : 0.001001119613647461\n",
      "train: loss: 797381.9375 acc: 0.7455627918243408  val: loss: 1182639.25 acc: 0.6758884191513062\n",
      "step: 29020 , time : 0.001001119613647461\n",
      "train: loss: 180175.0625 acc: 0.8933672904968262  val: loss: 4717120.5 acc: 0.5203288197517395\n",
      "step: 29025 , time : 0.0\n",
      "train: loss: 127184.3515625 acc: 0.8876253962516785  val: loss: 2149546.75 acc: 0.5804976224899292\n",
      "step: 29030 , time : 0.0\n",
      "train: loss: 34837.5078125 acc: 0.9657065272331238  val: loss: 2971270.25 acc: 0.6341432332992554\n",
      "step: 29035 , time : 0.0\n",
      "train: loss: 78741.25 acc: 0.9280945658683777  val: loss: 2911035.75 acc: 0.5525275468826294\n",
      "step: 29040 , time : 0.0\n",
      "train: loss: 515921.9375 acc: 0.7756262421607971  val: loss: 1767179.5 acc: 0.571631669998169\n",
      "step: 29045 , time : 0.0\n",
      "train: loss: 237553.984375 acc: 0.8722436428070068  val: loss: 3649297.5 acc: 0.5014952421188354\n",
      "step: 29050 , time : 0.0\n",
      "train: loss: 43583.32421875 acc: 0.9608569741249084  val: loss: 1545539.75 acc: 0.6062917709350586\n",
      "step: 29055 , time : 0.0\n",
      "train: loss: 441684.6875 acc: 0.7790685296058655  val: loss: 1060095.125 acc: 0.6593596935272217\n",
      "step: 29060 , time : 0.0\n",
      "train: loss: 90868.875 acc: 0.9120650887489319  val: loss: 453302.03125 acc: 0.7964459657669067\n",
      "step: 29065 , time : 0.0\n",
      "train: loss: 136598.78125 acc: 0.8825011253356934  val: loss: 1063239.25 acc: 0.6979127526283264\n",
      "step: 29070 , time : 0.0\n",
      "train: loss: 153756.703125 acc: 0.8318116664886475  val: loss: 1771564.5 acc: 0.6070140600204468\n",
      "step: 29075 , time : 0.0\n",
      "train: loss: 158303.65625 acc: 0.8864692449569702  val: loss: 418833.125 acc: 0.7997825741767883\n",
      "step: 29080 , time : 0.0\n",
      "train: loss: 531400.0625 acc: 0.7788305282592773  val: loss: 2362172.0 acc: 0.6208677887916565\n",
      "step: 29085 , time : 0.0\n",
      "train: loss: 275700.4375 acc: 0.8354057669639587  val: loss: 1449291.25 acc: 0.6241997480392456\n",
      "step: 29090 , time : 0.0\n",
      "train: loss: 429676.0 acc: 0.7470836639404297  val: loss: 717224.75 acc: 0.6981788873672485\n",
      "step: 29095 , time : 0.015625476837158203\n",
      "train: loss: 156846.890625 acc: 0.8561779260635376  val: loss: 3150988.75 acc: 0.592388927936554\n",
      "step: 29100 , time : 0.0\n",
      "train: loss: 1049603.375 acc: 0.6748991012573242  val: loss: 2492590.0 acc: 0.6410290598869324\n",
      "step: 29105 , time : 0.0\n",
      "train: loss: 1125098.0 acc: 0.748235821723938  val: loss: 808859.375 acc: 0.5933943390846252\n",
      "step: 29110 , time : 0.0\n",
      "train: loss: 836430.1875 acc: 0.9161167144775391  val: loss: 472729.5625 acc: 0.8665146231651306\n",
      "step: 29115 , time : 0.0\n",
      "train: loss: 141846.21875 acc: 0.985236644744873  val: loss: 1173880.75 acc: 0.7824704647064209\n",
      "step: 29120 , time : 0.0010008811950683594\n",
      "train: loss: 89488.4296875 acc: 0.9915561079978943  val: loss: 1513680.75 acc: 0.5611224174499512\n",
      "step: 29125 , time : 0.0\n",
      "train: loss: 238888.453125 acc: 0.9429035186767578  val: loss: 465575.21875 acc: 0.947742223739624\n",
      "step: 29130 , time : 0.0\n",
      "train: loss: 477781.0 acc: 0.9521321058273315  val: loss: 1180799.375 acc: 0.8010281324386597\n",
      "step: 29135 , time : 0.0\n",
      "train: loss: 152905.375 acc: 0.9810560941696167  val: loss: 2003920.25 acc: 0.6786812543869019\n",
      "step: 29140 , time : 0.0010004043579101562\n",
      "train: loss: 63143.921875 acc: 0.9959128499031067  val: loss: 830288.0625 acc: 0.8376628160476685\n",
      "step: 29145 , time : 0.0\n",
      "train: loss: 42114.140625 acc: 0.9958422183990479  val: loss: 1148483.5 acc: 0.6538496017456055\n",
      "step: 29150 , time : 0.0\n",
      "train: loss: 53836.83203125 acc: 0.9924566149711609  val: loss: 1144662.75 acc: 0.8893404603004456\n",
      "step: 29155 , time : 0.015624761581420898\n",
      "train: loss: 11798.880859375 acc: 0.9923500418663025  val: loss: 619197.3125 acc: 0.8666770458221436\n",
      "step: 29160 , time : 0.0\n",
      "train: loss: 18493.45703125 acc: 0.9959169626235962  val: loss: 2505161.75 acc: 0.1312519907951355\n",
      "step: 29165 , time : 0.0\n",
      "train: loss: 5217.767578125 acc: 0.9823060035705566  val: loss: 1119753.25 acc: 0.8639161586761475\n",
      "step: 29170 , time : 0.0\n",
      "train: loss: 17627.48046875 acc: 0.9731578230857849  val: loss: 1497562.125 acc: 0.6519325971603394\n",
      "step: 29175 , time : 0.0\n",
      "train: loss: 22143.84375 acc: 0.987562358379364  val: loss: 177580.5625 acc: 0.8843558430671692\n",
      "step: 29180 , time : 0.0010006427764892578\n",
      "train: loss: 7900.89697265625 acc: 0.9897317290306091  val: loss: 605236.6875 acc: -0.44948744773864746\n",
      "step: 29185 , time : 0.0010004043579101562\n",
      "train: loss: 7315.4521484375 acc: 0.9875566363334656  val: loss: 1118031.5 acc: 0.8463414907455444\n",
      "step: 29190 , time : 0.0010006427764892578\n",
      "train: loss: 13897.9814453125 acc: 0.9773723483085632  val: loss: 1914082.25 acc: 0.3607900142669678\n",
      "step: 29195 , time : 0.0\n",
      "train: loss: 6556.43115234375 acc: 0.9880689978599548  val: loss: 694609.625 acc: 0.9269767999649048\n",
      "step: 29200 , time : 0.0\n",
      "train: loss: 12492.59765625 acc: 0.9674296975135803  val: loss: 980345.125 acc: 0.7761437892913818\n",
      "step: 29205 , time : 0.0\n",
      "train: loss: 41423.8203125 acc: 0.9833941459655762  val: loss: 1191603.0 acc: 0.6141984462738037\n",
      "step: 29210 , time : 0.0\n",
      "train: loss: 30109.708984375 acc: 0.983680784702301  val: loss: 870968.3125 acc: 0.6863481998443604\n",
      "step: 29215 , time : 0.0\n",
      "train: loss: 25382.74609375 acc: 0.9905458092689514  val: loss: 191412.640625 acc: 0.9015049338340759\n",
      "step: 29220 , time : 0.0\n",
      "train: loss: 25156.953125 acc: 0.9882543683052063  val: loss: 1354393.625 acc: 0.833870530128479\n",
      "step: 29225 , time : 0.0010004043579101562\n",
      "train: loss: 17365.826171875 acc: 0.985276460647583  val: loss: 2016967.625 acc: 0.6909664869308472\n",
      "step: 29230 , time : 0.0\n",
      "train: loss: 10338.404296875 acc: 0.9779831171035767  val: loss: 1037085.375 acc: 0.8583269119262695\n",
      "step: 29235 , time : 0.0\n",
      "train: loss: 11964.9775390625 acc: 0.9900503158569336  val: loss: 878178.6875 acc: 0.758426308631897\n",
      "step: 29240 , time : 0.0\n",
      "train: loss: 25812.990234375 acc: 0.9935771822929382  val: loss: 699195.625 acc: 0.7492349147796631\n",
      "step: 29245 , time : 0.0010004043579101562\n",
      "train: loss: 13582.5625 acc: 0.9968767166137695  val: loss: 583551.75 acc: 0.8570579290390015\n",
      "step: 29250 , time : 0.0010848045349121094\n",
      "train: loss: 36762.046875 acc: 0.9906361699104309  val: loss: 1783303.875 acc: 0.6334632635116577\n",
      "step: 29255 , time : 0.0\n",
      "train: loss: 48804.9609375 acc: 0.9781904816627502  val: loss: 113071.9375 acc: 0.9694229960441589\n",
      "step: 29260 , time : 0.015625\n",
      "train: loss: 28365.962890625 acc: 0.9906126260757446  val: loss: 680362.375 acc: 0.9020458459854126\n",
      "step: 29265 , time : 0.0\n",
      "train: loss: 64707.25 acc: 0.978934645652771  val: loss: 896755.3125 acc: 0.5996899008750916\n",
      "step: 29270 , time : 0.0\n",
      "train: loss: 62602.828125 acc: 0.9698331952095032  val: loss: 701774.3125 acc: 0.6058956980705261\n",
      "step: 29275 , time : 0.0\n",
      "train: loss: 164246.421875 acc: 0.9592024683952332  val: loss: 975213.875 acc: 0.8750712871551514\n",
      "step: 29280 , time : 0.0\n",
      "train: loss: 587847.125 acc: 0.9109864830970764  val: loss: 297837.15625 acc: 0.9698086380958557\n",
      "step: 29285 , time : 0.0\n",
      "train: loss: 56154.5390625 acc: 0.9954114556312561  val: loss: 132629.578125 acc: 0.9897840023040771\n",
      "step: 29290 , time : 0.0\n",
      "train: loss: 60782.75390625 acc: 0.9924855828285217  val: loss: 2045243.5 acc: 0.7183170914649963\n",
      "step: 29295 , time : 0.0\n",
      "train: loss: 218170.328125 acc: 0.9812800884246826  val: loss: 1947264.5 acc: 0.8470504879951477\n",
      "step: 29300 , time : 0.0\n",
      "train: loss: 249628.578125 acc: 0.9763897657394409  val: loss: 425867.75 acc: 0.9088655114173889\n",
      "step: 29305 , time : 0.0\n",
      "train: loss: 561234.0625 acc: 0.9613971710205078  val: loss: 1437363.25 acc: 0.8649688959121704\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 29310 , time : 0.0\n",
      "train: loss: 267229.1875 acc: 0.9757611751556396  val: loss: 793399.375 acc: 0.8749895691871643\n",
      "step: 29315 , time : 0.0\n",
      "train: loss: 250075.671875 acc: 0.9694067239761353  val: loss: 1465398.625 acc: 0.8996777534484863\n",
      "step: 29320 , time : 0.0\n",
      "train: loss: 935235.0 acc: 0.9592375159263611  val: loss: 1803603.75 acc: 0.8753954172134399\n",
      "step: 29325 , time : 0.0010006427764892578\n",
      "train: loss: 2025462.0 acc: 0.9379206895828247  val: loss: 813876.375 acc: 0.8732094168663025\n",
      "step: 29330 , time : 0.0010004043579101562\n",
      "train: loss: 1570363.875 acc: 0.9510247111320496  val: loss: 179945.859375 acc: 0.9666319489479065\n",
      "step: 29335 , time : 0.0010008811950683594\n",
      "train: loss: 551814.375 acc: 0.9655318260192871  val: loss: 908938.75 acc: 0.6737509965896606\n",
      "step: 29340 , time : 0.0010006427764892578\n",
      "train: loss: 321252.90625 acc: 0.9594335556030273  val: loss: 161002.9375 acc: 0.968696653842926\n",
      "step: 29345 , time : 0.0\n",
      "train: loss: 390432.6875 acc: 0.9751533269882202  val: loss: 515416.3125 acc: 0.9462783336639404\n",
      "step: 29350 , time : 0.0\n",
      "train: loss: 836865.375 acc: 0.8951488733291626  val: loss: 670291.875 acc: 0.9229941964149475\n",
      "step: 29355 , time : 0.001001119613647461\n",
      "train: loss: 1028375.625 acc: 0.7068995237350464  val: loss: 1916401.625 acc: 0.7391847372055054\n",
      "step: 29360 , time : 0.0\n",
      "train: loss: 1221508.625 acc: 0.6926075220108032  val: loss: 1925150.5 acc: 0.5533327460289001\n",
      "step: 29365 , time : 0.0\n",
      "train: loss: 664967.25 acc: 0.6137723922729492  val: loss: 1291412.75 acc: 0.7655577659606934\n",
      "step: 29370 , time : 0.0\n",
      "train: loss: 683730.875 acc: 0.60988450050354  val: loss: 1505542.875 acc: 0.7868976593017578\n",
      "step: 29375 , time : 0.0\n",
      "train: loss: 402022.40625 acc: 0.8289922475814819  val: loss: 796500.625 acc: 0.6067875623703003\n",
      "step: 29380 , time : 0.0\n",
      "train: loss: 1303424.25 acc: 0.14294534921646118  val: loss: 595923.4375 acc: 0.7181897163391113\n",
      "step: 29385 , time : 0.0\n",
      "train: loss: 887094.4375 acc: 0.6504988074302673  val: loss: 2334844.25 acc: 0.646521806716919\n",
      "step: 29390 , time : 0.0\n",
      "train: loss: 214337.40625 acc: 0.8660798668861389  val: loss: 1243708.5 acc: 0.6696501970291138\n",
      "step: 29395 , time : 0.0\n",
      "train: loss: 399371.71875 acc: 0.7514920830726624  val: loss: 1508850.875 acc: 0.6189233064651489\n",
      "step: 29400 , time : 0.0\n",
      "train: loss: 129908.5859375 acc: 0.8896467089653015  val: loss: 295664.96875 acc: 0.7218919396400452\n",
      "step: 29405 , time : 0.0\n",
      "train: loss: 81926.1171875 acc: 0.9300540089607239  val: loss: 996962.9375 acc: 0.7678380608558655\n",
      "step: 29410 , time : 0.0\n",
      "train: loss: 149631.640625 acc: 0.9096856713294983  val: loss: 664584.1875 acc: 0.7597495317459106\n",
      "step: 29415 , time : 0.0\n",
      "train: loss: 174328.296875 acc: 0.86665278673172  val: loss: 3813244.25 acc: 0.5854431986808777\n",
      "step: 29420 , time : 0.0\n",
      "train: loss: 148481.34375 acc: 0.9050983190536499  val: loss: 1422538.0 acc: 0.6641067266464233\n",
      "step: 29425 , time : 0.0\n",
      "train: loss: 13838.9248046875 acc: 0.9887982606887817  val: loss: 1754257.0 acc: 0.6980296969413757\n",
      "step: 29430 , time : 0.0\n",
      "train: loss: 251557.5625 acc: 0.8181477189064026  val: loss: 433023.34375 acc: 0.8001453876495361\n",
      "step: 29435 , time : 0.001001119613647461\n",
      "train: loss: 69200.1953125 acc: 0.9126800298690796  val: loss: 1017105.25 acc: 0.7100778818130493\n",
      "step: 29440 , time : 0.0010004043579101562\n",
      "train: loss: 64288.42578125 acc: 0.9272472858428955  val: loss: 614109.75 acc: 0.7455540895462036\n",
      "step: 29445 , time : 0.0\n",
      "train: loss: 584374.5 acc: 0.7900147438049316  val: loss: 2988916.25 acc: 0.617801308631897\n",
      "step: 29450 , time : 0.0010008811950683594\n",
      "train: loss: 233626.046875 acc: 0.8647753000259399  val: loss: 4045178.25 acc: 0.6182689666748047\n",
      "step: 29455 , time : 0.0\n",
      "train: loss: 40020.9921875 acc: 0.9596347808837891  val: loss: 2972270.5 acc: 0.5995311141014099\n",
      "step: 29460 , time : 0.0\n",
      "train: loss: 246121.640625 acc: 0.7900992035865784  val: loss: 1439706.25 acc: 0.6313676834106445\n",
      "step: 29465 , time : 0.0\n",
      "train: loss: 1284389.625 acc: 0.6894963383674622  val: loss: 1737605.75 acc: 0.683971643447876\n",
      "step: 29470 , time : 0.0\n",
      "train: loss: 1091782.625 acc: 0.815261960029602  val: loss: 1401957.25 acc: -0.37334465980529785\n",
      "step: 29475 , time : 0.0\n",
      "train: loss: 1209532.0 acc: 0.8640835285186768  val: loss: 936092.6875 acc: 0.6854656338691711\n",
      "step: 29480 , time : 0.0\n",
      "train: loss: 891592.1875 acc: 0.9200393557548523  val: loss: 759801.0 acc: 0.5726701021194458\n",
      "step: 29485 , time : 0.0\n",
      "train: loss: 212355.9375 acc: 0.9735979437828064  val: loss: 1100606.0 acc: 0.7693555951118469\n",
      "step: 29490 , time : 0.0\n",
      "train: loss: 188326.703125 acc: 0.9731405973434448  val: loss: 1362894.5 acc: 0.7040346264839172\n",
      "step: 29495 , time : 0.0\n",
      "train: loss: 113111.5546875 acc: 0.9863184094429016  val: loss: 1292266.125 acc: 0.8096396923065186\n",
      "step: 29500 , time : 0.0\n",
      "train: loss: 58083.90234375 acc: 0.9954907298088074  val: loss: 524734.8125 acc: 0.883557915687561\n",
      "step: 29505 , time : 0.0\n",
      "train: loss: 240841.625 acc: 0.9774516224861145  val: loss: 153237.09375 acc: 0.95442134141922\n",
      "step: 29510 , time : 0.0156252384185791\n",
      "train: loss: 1045677.0 acc: 0.9014325141906738  val: loss: 1393837.5 acc: 0.6319226622581482\n",
      "step: 29515 , time : 0.0\n",
      "train: loss: 46529.27734375 acc: 0.990391194820404  val: loss: 1507829.75 acc: 0.6589697599411011\n",
      "step: 29520 , time : 0.0\n",
      "train: loss: 62587.6953125 acc: 0.990602970123291  val: loss: 959264.0 acc: 0.5673732757568359\n",
      "step: 29525 , time : 0.0\n",
      "train: loss: 12686.5380859375 acc: 0.9956187009811401  val: loss: 642212.375 acc: 0.5762056708335876\n",
      "step: 29530 , time : 0.0\n",
      "train: loss: 8187.873046875 acc: 0.995415210723877  val: loss: 517859.71875 acc: 0.8626272678375244\n",
      "step: 29535 , time : 0.0\n",
      "train: loss: 14515.6474609375 acc: 0.9937745332717896  val: loss: 1142748.25 acc: 0.3495340347290039\n",
      "step: 29540 , time : 0.0\n",
      "train: loss: 15451.8427734375 acc: 0.975994884967804  val: loss: 150973.765625 acc: 0.955491304397583\n",
      "step: 29545 , time : 0.0010008811950683594\n",
      "train: loss: 13005.431640625 acc: 0.9830459356307983  val: loss: 240962.65625 acc: 0.9204255938529968\n",
      "step: 29550 , time : 0.0\n",
      "train: loss: 9483.27734375 acc: 0.9930688738822937  val: loss: 585609.625 acc: 0.9292700886726379\n",
      "step: 29555 , time : 0.0\n",
      "train: loss: 3403.349365234375 acc: 0.9867137670516968  val: loss: 246464.8125 acc: 0.762641191482544\n",
      "step: 29560 , time : 0.0\n",
      "train: loss: 8591.2734375 acc: 0.9825416803359985  val: loss: 1302937.125 acc: 0.6935888528823853\n",
      "step: 29565 , time : 0.0010006427764892578\n",
      "train: loss: 29822.0 acc: 0.9801816344261169  val: loss: 1139931.5 acc: 0.7245370745658875\n",
      "step: 29570 , time : 0.0\n",
      "train: loss: 22570.5546875 acc: 0.9826765656471252  val: loss: 185388.125 acc: 0.9710744619369507\n",
      "step: 29575 , time : 0.0\n",
      "train: loss: 27926.333984375 acc: 0.98361736536026  val: loss: 436033.5 acc: 0.8705193400382996\n",
      "step: 29580 , time : 0.0\n",
      "train: loss: 19022.908203125 acc: 0.9807819128036499  val: loss: 604327.3125 acc: 0.9011452794075012\n",
      "step: 29585 , time : 0.0\n",
      "train: loss: 8800.9560546875 acc: 0.9919444918632507  val: loss: 594401.625 acc: 0.8891127109527588\n",
      "step: 29590 , time : 0.0\n",
      "train: loss: 29741.361328125 acc: 0.975216805934906  val: loss: 266102.0 acc: 0.9439297914505005\n",
      "step: 29595 , time : 0.0\n",
      "train: loss: 11779.1513671875 acc: 0.9856089949607849  val: loss: 528982.9375 acc: 0.9516099095344543\n",
      "step: 29600 , time : 0.0\n",
      "train: loss: 3955.4619140625 acc: 0.9954652786254883  val: loss: 1355449.75 acc: 0.5299559831619263\n",
      "step: 29605 , time : 0.015625476837158203\n",
      "train: loss: 18708.8125 acc: 0.9924150109291077  val: loss: 1766989.0 acc: 0.8033990859985352\n",
      "step: 29610 , time : 0.0\n",
      "train: loss: 14727.833984375 acc: 0.9960981011390686  val: loss: 1039975.125 acc: 0.847815215587616\n",
      "step: 29615 , time : 0.0\n",
      "train: loss: 30356.75 acc: 0.9932652115821838  val: loss: 1196881.125 acc: 0.885265052318573\n",
      "step: 29620 , time : 0.0\n",
      "train: loss: 23893.357421875 acc: 0.9872868061065674  val: loss: 817867.25 acc: 0.8380013704299927\n",
      "step: 29625 , time : 0.0\n",
      "train: loss: 72146.390625 acc: 0.9824935793876648  val: loss: 2141537.25 acc: 0.8471577167510986\n",
      "step: 29630 , time : 0.0\n",
      "train: loss: 147990.5625 acc: 0.9294106364250183  val: loss: 767846.9375 acc: 0.9313670992851257\n",
      "step: 29635 , time : 0.0\n",
      "train: loss: 113572.8828125 acc: 0.9641973972320557  val: loss: 419345.5 acc: 0.8112466335296631\n",
      "step: 29640 , time : 0.0\n",
      "train: loss: 325280.25 acc: 0.8906075358390808  val: loss: 240357.625 acc: 0.8886697292327881\n",
      "step: 29645 , time : 0.0\n",
      "train: loss: 348407.9375 acc: 0.9259169101715088  val: loss: 483137.9375 acc: 0.9562327265739441\n",
      "step: 29650 , time : 0.0\n",
      "train: loss: 59537.87109375 acc: 0.994329035282135  val: loss: 2704985.75 acc: 0.5239633321762085\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 29655 , time : 0.0\n",
      "train: loss: 554103.3125 acc: 0.8546561598777771  val: loss: 1686172.25 acc: 0.8059460520744324\n",
      "step: 29660 , time : 0.0\n",
      "train: loss: 70313.59375 acc: 0.9864869117736816  val: loss: 1193521.875 acc: 0.6556339263916016\n",
      "step: 29665 , time : 0.0\n",
      "train: loss: 292715.1875 acc: 0.9716107845306396  val: loss: 1065842.875 acc: 0.8415716886520386\n",
      "step: 29670 , time : 0.0010004043579101562\n",
      "train: loss: 450436.21875 acc: 0.9511930346488953  val: loss: 1324196.25 acc: 0.7501543164253235\n",
      "step: 29675 , time : 0.0\n",
      "train: loss: 406674.5 acc: 0.9400483965873718  val: loss: 919991.25 acc: 0.8618651032447815\n",
      "step: 29680 , time : 0.0\n",
      "train: loss: 367153.25 acc: 0.928946852684021  val: loss: 768552.5625 acc: 0.8633781671524048\n",
      "step: 29685 , time : 0.0\n",
      "train: loss: 367155.4375 acc: 0.9579392075538635  val: loss: 3243231.25 acc: 0.19048786163330078\n",
      "step: 29690 , time : 0.0\n",
      "train: loss: 663045.4375 acc: 0.9767135381698608  val: loss: 346300.53125 acc: 0.932265043258667\n",
      "step: 29695 , time : 0.0\n",
      "train: loss: 895329.125 acc: 0.9671192765235901  val: loss: 219672.234375 acc: 0.9579849243164062\n",
      "step: 29700 , time : 0.0\n",
      "train: loss: 499538.375 acc: 0.9610658288002014  val: loss: 2574985.5 acc: 0.5831360816955566\n",
      "step: 29705 , time : 0.0\n",
      "train: loss: 435068.75 acc: 0.9565218091011047  val: loss: 1147273.125 acc: 0.36538487672805786\n",
      "step: 29710 , time : 0.0\n",
      "train: loss: 415562.6875 acc: 0.9620497226715088  val: loss: 1192145.125 acc: 0.8086708188056946\n",
      "step: 29715 , time : 0.0\n",
      "train: loss: 777681.25 acc: 0.8928544521331787  val: loss: 547729.9375 acc: 0.8543999195098877\n",
      "step: 29720 , time : 0.0\n",
      "train: loss: 1843836.25 acc: 0.562751829624176  val: loss: 902736.0 acc: 0.45261240005493164\n",
      "step: 29725 , time : 0.0\n",
      "train: loss: 1356760.0 acc: 0.5223439931869507  val: loss: 1301853.0 acc: 0.8675033450126648\n",
      "step: 29730 , time : 0.0\n",
      "train: loss: 536008.5 acc: 0.6930369138717651  val: loss: 1066780.375 acc: 0.6525607109069824\n",
      "step: 29735 , time : 0.0\n",
      "train: loss: 1087663.75 acc: 0.5734840631484985  val: loss: 696450.75 acc: 0.7988166809082031\n",
      "step: 29740 , time : 0.0\n",
      "train: loss: 938119.8125 acc: 0.8719339370727539  val: loss: 839544.75 acc: 0.7597862482070923\n",
      "step: 29745 , time : 0.0\n",
      "train: loss: 1532422.25 acc: 0.45114386081695557  val: loss: 1048354.875 acc: 0.8739773035049438\n",
      "step: 29750 , time : 0.0\n",
      "train: loss: 811631.0625 acc: 0.7184888124465942  val: loss: 1151680.875 acc: 0.7284033298492432\n",
      "step: 29755 , time : 0.0010001659393310547\n",
      "train: loss: 239947.6875 acc: 0.8308743834495544  val: loss: 907517.625 acc: 0.7256633639335632\n",
      "step: 29760 , time : 0.0\n",
      "train: loss: 371604.5625 acc: 0.7698016166687012  val: loss: 2209908.5 acc: 0.6824643015861511\n",
      "step: 29765 , time : 0.0\n",
      "train: loss: 74507.65625 acc: 0.9410266280174255  val: loss: 2158791.0 acc: 0.7259488105773926\n",
      "step: 29770 , time : 0.0010004043579101562\n",
      "train: loss: 22441.45703125 acc: 0.9825209379196167  val: loss: 905405.375 acc: 0.7442748546600342\n",
      "step: 29775 , time : 0.0010008811950683594\n",
      "train: loss: 208164.90625 acc: 0.8845969438552856  val: loss: 4696782.5 acc: 0.622200608253479\n",
      "step: 29780 , time : 0.0010004043579101562\n",
      "train: loss: 114039.015625 acc: 0.9080625176429749  val: loss: 952215.9375 acc: 0.6802455186843872\n",
      "step: 29785 , time : 0.001001596450805664\n",
      "train: loss: 29074.88671875 acc: 0.9778817892074585  val: loss: 458405.875 acc: 0.8603599071502686\n",
      "step: 29790 , time : 0.001001119613647461\n",
      "train: loss: 127881.515625 acc: 0.8583089709281921  val: loss: 1451940.875 acc: 0.8007776737213135\n",
      "step: 29795 , time : 0.0\n",
      "train: loss: 47082.390625 acc: 0.9435268044471741  val: loss: 3628342.75 acc: 0.5285698771476746\n",
      "step: 29800 , time : 0.0\n",
      "train: loss: 151299.296875 acc: 0.8556948900222778  val: loss: 415212.53125 acc: 0.7774625420570374\n",
      "step: 29805 , time : 0.0010013580322265625\n",
      "train: loss: 371463.125 acc: 0.7882802486419678  val: loss: 198864.921875 acc: 0.7794541120529175\n",
      "step: 29810 , time : 0.0010006427764892578\n",
      "train: loss: 131979.96875 acc: 0.9091969132423401  val: loss: 1540765.25 acc: 0.5624234676361084\n",
      "step: 29815 , time : 0.0\n",
      "train: loss: 383989.375 acc: 0.7591599822044373  val: loss: 787684.75 acc: 0.8244059681892395\n",
      "step: 29820 , time : 0.0\n",
      "train: loss: 1037491.875 acc: 0.7136670351028442  val: loss: 503380.8125 acc: 0.8074285984039307\n",
      "step: 29825 , time : 0.0\n",
      "train: loss: 271573.84375 acc: 0.7207022905349731  val: loss: 1805223.25 acc: 0.6256811022758484\n",
      "step: 29830 , time : 0.0\n",
      "train: loss: 249633.34375 acc: 0.8392563462257385  val: loss: 440879.15625 acc: 0.8167428970336914\n",
      "step: 29835 , time : 0.0\n",
      "train: loss: 2222754.75 acc: 0.7209467887878418  val: loss: 540678.25 acc: 0.8123773336410522\n",
      "step: 29840 , time : 0.0\n",
      "train: loss: 1630378.5 acc: 0.8103169202804565  val: loss: 348709.25 acc: 0.8626905679702759\n",
      "step: 29845 , time : 0.0\n",
      "train: loss: 239271.796875 acc: 0.9824725389480591  val: loss: 1096792.375 acc: 0.8188363313674927\n",
      "step: 29850 , time : 0.0\n",
      "train: loss: 212492.890625 acc: 0.9765220880508423  val: loss: 565244.1875 acc: 0.6303971409797668\n",
      "step: 29855 , time : 0.0010008811950683594\n",
      "train: loss: 111330.484375 acc: 0.9681366682052612  val: loss: 872660.0625 acc: 0.8594375848770142\n",
      "step: 29860 , time : 0.0\n",
      "train: loss: 464136.21875 acc: 0.901276707649231  val: loss: 426267.78125 acc: 0.7494142055511475\n",
      "step: 29865 , time : 0.0\n",
      "train: loss: 159288.359375 acc: 0.9875994920730591  val: loss: 963037.75 acc: 0.6042166948318481\n",
      "step: 29870 , time : 0.0\n",
      "train: loss: 161281.0 acc: 0.9902004599571228  val: loss: 420257.375 acc: 0.6766566038131714\n",
      "step: 29875 , time : 0.0010008811950683594\n",
      "train: loss: 55330.5 acc: 0.9965188503265381  val: loss: 176873.03125 acc: 0.9761488437652588\n",
      "step: 29880 , time : 0.0\n",
      "train: loss: 281669.75 acc: 0.9692572951316833  val: loss: 486558.25 acc: 0.9173446297645569\n",
      "step: 29885 , time : 0.0010008811950683594\n",
      "train: loss: 30499.876953125 acc: 0.9900138974189758  val: loss: 815072.25 acc: 0.8123579025268555\n",
      "step: 29890 , time : 0.0\n",
      "train: loss: 23686.015625 acc: 0.9911303520202637  val: loss: 473412.71875 acc: 0.848243236541748\n",
      "step: 29895 , time : 0.0010004043579101562\n",
      "train: loss: 8604.595703125 acc: 0.9667069315910339  val: loss: 307960.78125 acc: 0.9013838171958923\n",
      "step: 29900 , time : 0.0010006427764892578\n",
      "train: loss: 15789.615234375 acc: 0.9857439398765564  val: loss: 1077069.5 acc: 0.7617954015731812\n",
      "step: 29905 , time : 0.001001119613647461\n",
      "train: loss: 13307.0107421875 acc: 0.9913479685783386  val: loss: 796088.3125 acc: 0.8514074087142944\n",
      "step: 29910 , time : 0.0\n",
      "train: loss: 25278.470703125 acc: 0.979691743850708  val: loss: 370379.625 acc: 0.8437815308570862\n",
      "step: 29915 , time : 0.0\n",
      "train: loss: 4960.83740234375 acc: 0.9876210689544678  val: loss: 965974.125 acc: 0.9399427771568298\n",
      "step: 29920 , time : 0.0\n",
      "train: loss: 4703.02783203125 acc: 0.9885594844818115  val: loss: 246393.359375 acc: 0.9266220927238464\n",
      "step: 29925 , time : 0.0\n",
      "train: loss: 8361.9736328125 acc: 0.9833122491836548  val: loss: 1941528.375 acc: 0.7253046631813049\n",
      "step: 29930 , time : 0.0\n",
      "train: loss: 5391.732421875 acc: 0.9900561571121216  val: loss: 568875.125 acc: 0.9367905259132385\n",
      "step: 29935 , time : 0.0\n",
      "train: loss: 37817.22265625 acc: 0.9802837371826172  val: loss: 1531667.125 acc: 0.6169289350509644\n",
      "step: 29940 , time : 0.0010004043579101562\n",
      "train: loss: 10926.5009765625 acc: 0.9926535487174988  val: loss: 3428825.0 acc: 0.6159595251083374\n",
      "step: 29945 , time : 0.0010008811950683594\n",
      "train: loss: 24967.326171875 acc: 0.9833299517631531  val: loss: 766045.75 acc: 0.9214048385620117\n",
      "step: 29950 , time : 0.0\n",
      "train: loss: 21062.58984375 acc: 0.9875854253768921  val: loss: 606464.6875 acc: 0.901557207107544\n",
      "step: 29955 , time : 0.0\n",
      "train: loss: 49940.9140625 acc: 0.9512023329734802  val: loss: 3581832.5 acc: 0.6804393529891968\n",
      "step: 29960 , time : 0.0010004043579101562\n",
      "train: loss: 13253.6591796875 acc: 0.9912537336349487  val: loss: 1480245.5 acc: 0.6903365850448608\n",
      "step: 29965 , time : 0.0\n",
      "train: loss: 12804.546875 acc: 0.9832481741905212  val: loss: 2383894.75 acc: 0.7079803943634033\n",
      "step: 29970 , time : 0.0\n",
      "train: loss: 11185.6884765625 acc: 0.995778501033783  val: loss: 3496031.75 acc: -0.4353145360946655\n",
      "step: 29975 , time : 0.0\n",
      "train: loss: 31297.873046875 acc: 0.9891448616981506  val: loss: 2041859.875 acc: 0.8218446969985962\n",
      "step: 29980 , time : 0.0\n",
      "train: loss: 43801.796875 acc: 0.9863884449005127  val: loss: 3033796.25 acc: 0.555237352848053\n",
      "step: 29985 , time : 0.0\n",
      "train: loss: 16531.115234375 acc: 0.9933083653450012  val: loss: 2703485.75 acc: 0.20662623643875122\n",
      "step: 29990 , time : 0.0010004043579101562\n",
      "train: loss: 31065.56640625 acc: 0.9879525899887085  val: loss: 914647.375 acc: 0.6909717321395874\n",
      "step: 29995 , time : 0.0\n",
      "train: loss: 112456.484375 acc: 0.97502601146698  val: loss: 1020886.4375 acc: 0.8193309307098389\n",
      "step: 30000 , time : 0.0\n",
      "train: loss: 112226.3046875 acc: 0.9648997187614441  val: loss: 3448752.25 acc: -0.07248401641845703\n",
      "step: 30005 , time : 0.0010006427764892578\n",
      "train: loss: 54233.75 acc: 0.9797878265380859  val: loss: 1573210.25 acc: 0.8457667827606201\n",
      "step: 30010 , time : 0.0\n",
      "train: loss: 51693.92578125 acc: 0.9888255000114441  val: loss: 1558895.625 acc: 0.7914122939109802\n",
      "step: 30015 , time : 0.0\n",
      "train: loss: 69889.390625 acc: 0.9927189350128174  val: loss: 1320025.375 acc: -0.38567161560058594\n",
      "step: 30020 , time : 0.0\n",
      "train: loss: 90035.1015625 acc: 0.9870041608810425  val: loss: 2597979.0 acc: -0.09738564491271973\n",
      "step: 30025 , time : 0.0\n",
      "train: loss: 432052.46875 acc: 0.9496650695800781  val: loss: 480913.78125 acc: 0.8566743731498718\n",
      "step: 30030 , time : 0.0\n",
      "train: loss: 125732.4453125 acc: 0.9812885522842407  val: loss: 1054221.75 acc: 0.8034520149230957\n",
      "step: 30035 , time : 0.0\n",
      "train: loss: 1413339.75 acc: 0.913951575756073  val: loss: 1734554.875 acc: 0.7451159954071045\n",
      "step: 30040 , time : 0.0\n",
      "train: loss: 222606.453125 acc: 0.9669786691665649  val: loss: 1213190.875 acc: 0.7843009233474731\n",
      "step: 30045 , time : 0.0\n",
      "train: loss: 194956.53125 acc: 0.9588178396224976  val: loss: 1148173.875 acc: 0.7367528676986694\n",
      "step: 30050 , time : 0.0\n",
      "train: loss: 1063676.125 acc: 0.953799843788147  val: loss: 1148101.0 acc: 0.8277814388275146\n",
      "step: 30055 , time : 0.0\n",
      "train: loss: 817737.375 acc: 0.9735685586929321  val: loss: 895296.4375 acc: 0.7065469026565552\n",
      "step: 30060 , time : 0.0\n",
      "train: loss: 1309092.125 acc: 0.9633581638336182  val: loss: 446233.34375 acc: 0.906512975692749\n",
      "step: 30065 , time : 0.0\n",
      "train: loss: 1550073.875 acc: 0.9277018308639526  val: loss: 1318107.375 acc: 0.8697681427001953\n",
      "step: 30070 , time : 0.0\n",
      "train: loss: 327831.65625 acc: 0.9838745594024658  val: loss: 2072936.125 acc: 0.3782322406768799\n",
      "step: 30075 , time : 0.0\n",
      "train: loss: 770091.8125 acc: 0.9185629487037659  val: loss: 1154491.375 acc: 0.8589705228805542\n",
      "step: 30080 , time : 0.0\n",
      "train: loss: 689859.625 acc: 0.9428693056106567  val: loss: 254964.40625 acc: 0.9331282377243042\n",
      "step: 30085 , time : 0.0\n",
      "train: loss: 793909.9375 acc: 0.8697800636291504  val: loss: 843497.75 acc: 0.9054037928581238\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 30090 , time : 0.0010006427764892578\n",
      "train: loss: 1439056.75 acc: 0.25628262758255005  val: loss: 1312989.375 acc: 0.795785665512085\n",
      "step: 30095 , time : 0.0\n",
      "train: loss: 774653.1875 acc: 0.6300199031829834  val: loss: 1637565.25 acc: 0.6900569796562195\n",
      "step: 30100 , time : 0.001001119613647461\n",
      "train: loss: 322667.53125 acc: 0.8392376899719238  val: loss: 1388667.0 acc: 0.7020719647407532\n",
      "step: 30105 , time : 0.0010004043579101562\n",
      "train: loss: 599474.875 acc: 0.822323203086853  val: loss: 1119278.625 acc: 0.8294799327850342\n",
      "step: 30110 , time : 0.0010013580322265625\n",
      "train: loss: 2085456.125 acc: 0.4245759844779968  val: loss: 960975.5625 acc: 0.7845156788825989\n",
      "step: 30115 , time : 0.0010006427764892578\n",
      "train: loss: 633023.1875 acc: 0.5987808704376221  val: loss: 2861641.25 acc: 0.5511770248413086\n",
      "step: 30120 , time : 0.0\n",
      "train: loss: 346614.46875 acc: 0.74091637134552  val: loss: 1719350.375 acc: 0.5913213491439819\n",
      "step: 30125 , time : 0.0\n",
      "train: loss: 312170.40625 acc: 0.7780818343162537  val: loss: 1937697.875 acc: 0.6669851541519165\n",
      "step: 30130 , time : 0.0\n",
      "train: loss: 264450.34375 acc: 0.7490355968475342  val: loss: 2298823.5 acc: 0.6605771780014038\n",
      "step: 30135 , time : 0.0010008811950683594\n",
      "train: loss: 28730.55859375 acc: 0.9747664332389832  val: loss: 687142.6875 acc: 0.7308732271194458\n",
      "step: 30140 , time : 0.0\n",
      "train: loss: 96968.390625 acc: 0.9407307505607605  val: loss: 2739187.25 acc: 0.5949362516403198\n",
      "step: 30145 , time : 0.0\n",
      "train: loss: 324238.125 acc: 0.8280813694000244  val: loss: 2789151.75 acc: 0.5807493925094604\n",
      "step: 30150 , time : 0.0\n",
      "train: loss: 112619.234375 acc: 0.9294909834861755  val: loss: 1031945.3125 acc: 0.6438798904418945\n",
      "step: 30155 , time : 0.0\n",
      "train: loss: 429644.0625 acc: 0.7800829410552979  val: loss: 2494579.25 acc: 0.5985997915267944\n",
      "step: 30160 , time : 0.0\n",
      "train: loss: 29882.458984375 acc: 0.96910160779953  val: loss: 1966848.5 acc: 0.6060342788696289\n",
      "step: 30165 , time : 0.0\n",
      "train: loss: 136207.296875 acc: 0.8498997688293457  val: loss: 1385066.125 acc: 0.576593279838562\n",
      "step: 30170 , time : 0.0\n",
      "train: loss: 154455.09375 acc: 0.8548980951309204  val: loss: 733712.5 acc: 0.7173013091087341\n",
      "step: 30175 , time : 0.0\n",
      "train: loss: 446934.71875 acc: 0.7798892855644226  val: loss: 1311771.375 acc: 0.6942158937454224\n",
      "step: 30180 , time : 0.0\n",
      "train: loss: 207097.546875 acc: 0.8687536120414734  val: loss: 146859.59375 acc: 0.8761839866638184\n",
      "step: 30185 , time : 0.0\n",
      "train: loss: 259902.3125 acc: 0.8089319467544556  val: loss: 2372293.25 acc: 0.6207757592201233\n",
      "step: 30190 , time : 0.0\n",
      "train: loss: 662130.9375 acc: 0.6939841508865356  val: loss: 1520111.125 acc: 0.644955039024353\n",
      "step: 30195 , time : 0.0\n",
      "train: loss: 306452.625 acc: 0.8104292154312134  val: loss: 190053.34375 acc: 0.8542599678039551\n",
      "step: 30200 , time : 0.0\n",
      "train: loss: 1336886.625 acc: 0.6793593168258667  val: loss: 660300.875 acc: 0.7866793274879456\n",
      "step: 30205 , time : 0.0\n",
      "train: loss: 956545.5625 acc: 0.8401941061019897  val: loss: 628845.1875 acc: 0.7891051173210144\n",
      "step: 30210 , time : 0.0\n",
      "train: loss: 759077.9375 acc: 0.9360834360122681  val: loss: 646285.25 acc: 0.8867001533508301\n",
      "step: 30215 , time : 0.0010008811950683594\n",
      "train: loss: 279878.84375 acc: 0.9721183180809021  val: loss: 246552.59375 acc: 0.9417449235916138\n",
      "step: 30220 , time : 0.0010004043579101562\n",
      "train: loss: 139085.796875 acc: 0.9715803861618042  val: loss: 732331.5 acc: 0.7426496744155884\n",
      "step: 30225 , time : 0.0\n",
      "train: loss: 122056.2734375 acc: 0.9811118841171265  val: loss: 651398.0625 acc: 0.8202346563339233\n",
      "step: 30230 , time : 0.0010006427764892578\n",
      "train: loss: 59625.73828125 acc: 0.9958652853965759  val: loss: 221176.609375 acc: 0.956919252872467\n",
      "step: 30235 , time : 0.0010006427764892578\n",
      "train: loss: 99813.21875 acc: 0.9926191568374634  val: loss: 4716368.0 acc: 0.21425306797027588\n",
      "step: 30240 , time : 0.0\n",
      "train: loss: 119380.7578125 acc: 0.9900321960449219  val: loss: 975637.25 acc: 0.854414165019989\n",
      "step: 30245 , time : 0.0010008811950683594\n",
      "train: loss: 79261.8203125 acc: 0.9904991984367371  val: loss: 484954.9375 acc: 0.8552643060684204\n",
      "step: 30250 , time : 0.0010004043579101562\n",
      "train: loss: 53484.9609375 acc: 0.9936863780021667  val: loss: 543892.25 acc: 0.9711873531341553\n",
      "step: 30255 , time : 0.0013117790222167969\n",
      "train: loss: 29880.716796875 acc: 0.991097629070282  val: loss: 1209221.0 acc: 0.6510815024375916\n",
      "step: 30260 , time : 0.0\n",
      "train: loss: 3336.393798828125 acc: 0.9908156991004944  val: loss: 1890931.375 acc: 0.7934367060661316\n",
      "step: 30265 , time : 0.0\n",
      "train: loss: 9581.8916015625 acc: 0.9960106611251831  val: loss: 636863.125 acc: 0.9151958227157593\n",
      "step: 30270 , time : 0.0\n",
      "train: loss: 19077.078125 acc: 0.9874036312103271  val: loss: 436940.34375 acc: 0.9374751448631287\n",
      "step: 30275 , time : 0.0\n",
      "train: loss: 225247.171875 acc: 0.9215136766433716  val: loss: 1274855.125 acc: 0.8480539917945862\n",
      "step: 30280 , time : 0.0\n",
      "train: loss: 15801.23046875 acc: 0.9790486097335815  val: loss: 3021036.75 acc: 0.49780815839767456\n",
      "step: 30285 , time : 0.0\n",
      "train: loss: 7290.751953125 acc: 0.9796080589294434  val: loss: 2521608.75 acc: 0.24926871061325073\n",
      "step: 30290 , time : 0.0\n",
      "train: loss: 14541.90625 acc: 0.9867542386054993  val: loss: 1696784.125 acc: 0.7457984685897827\n",
      "step: 30295 , time : 0.0\n",
      "train: loss: 10019.53515625 acc: 0.9663746356964111  val: loss: 2143006.0 acc: 0.5923234224319458\n",
      "step: 30300 , time : 0.0\n",
      "train: loss: 20635.083984375 acc: 0.9707160592079163  val: loss: 3167272.25 acc: 0.40429407358169556\n",
      "step: 30305 , time : 0.0\n",
      "train: loss: 27425.763671875 acc: 0.9846327900886536  val: loss: 2360450.25 acc: 0.6423723697662354\n",
      "step: 30310 , time : 0.0\n",
      "train: loss: 39462.55859375 acc: 0.9590404033660889  val: loss: 1602353.125 acc: 0.8239786028862\n",
      "step: 30315 , time : 0.0\n",
      "train: loss: 38890.625 acc: 0.9696502089500427  val: loss: 2563509.75 acc: 0.4448466897010803\n",
      "step: 30320 , time : 0.0\n",
      "train: loss: 108735.234375 acc: 0.9176055192947388  val: loss: 1376314.875 acc: 0.8278399705886841\n",
      "step: 30325 , time : 0.001001119613647461\n",
      "train: loss: 19879.7890625 acc: 0.9764395356178284  val: loss: 2393614.0 acc: 0.8660058379173279\n",
      "step: 30330 , time : 0.0\n",
      "train: loss: 89195.1328125 acc: 0.8924205303192139  val: loss: 1503144.375 acc: 0.6451129913330078\n",
      "step: 30335 , time : 0.0\n",
      "train: loss: 11831.1845703125 acc: 0.9855383634567261  val: loss: 4510184.0 acc: 0.49843674898147583\n",
      "step: 30340 , time : 0.0010004043579101562\n",
      "train: loss: 19812.783203125 acc: 0.9962688088417053  val: loss: 2549489.75 acc: 0.006991863250732422\n",
      "step: 30345 , time : 0.0\n",
      "train: loss: 35531.37109375 acc: 0.9910696148872375  val: loss: 1220056.0 acc: 0.773070752620697\n",
      "step: 30350 , time : 0.0010006427764892578\n",
      "train: loss: 36273.34765625 acc: 0.9871876239776611  val: loss: 1227092.75 acc: 0.7890293002128601\n",
      "step: 30355 , time : 0.001001119613647461\n",
      "train: loss: 35137.58984375 acc: 0.9895076751708984  val: loss: 1396543.5 acc: 0.634902834892273\n",
      "step: 30360 , time : 0.0\n",
      "train: loss: 384629.9375 acc: 0.8486928343772888  val: loss: 2058131.25 acc: -0.15046799182891846\n",
      "step: 30365 , time : 0.0\n",
      "train: loss: 68752.96875 acc: 0.9715595841407776  val: loss: 393956.75 acc: 0.826685905456543\n",
      "step: 30370 , time : 0.0\n",
      "train: loss: 48680.0703125 acc: 0.9845408797264099  val: loss: 575829.4375 acc: 0.7439627647399902\n",
      "step: 30375 , time : 0.0010006427764892578\n",
      "train: loss: 434427.0625 acc: 0.9417340755462646  val: loss: 2045238.125 acc: 0.5157783031463623\n",
      "step: 30380 , time : 0.0010006427764892578\n",
      "train: loss: 72491.75 acc: 0.9877053499221802  val: loss: 2206838.0 acc: 0.48453301191329956\n",
      "step: 30385 , time : 0.0\n",
      "train: loss: 189700.625 acc: 0.977023720741272  val: loss: 1324341.625 acc: 0.8621629476547241\n",
      "step: 30390 , time : 0.0\n",
      "train: loss: 41860.3671875 acc: 0.9946661591529846  val: loss: 798900.1875 acc: 0.8919546604156494\n",
      "step: 30395 , time : 0.0\n",
      "train: loss: 266359.8125 acc: 0.9387818574905396  val: loss: 507473.6875 acc: 0.9457162022590637\n",
      "step: 30400 , time : 0.0\n",
      "train: loss: 295660.375 acc: 0.9829939007759094  val: loss: 523022.96875 acc: 0.8934823274612427\n",
      "step: 30405 , time : 0.0\n",
      "train: loss: 611334.875 acc: 0.9584078788757324  val: loss: 1999288.125 acc: 0.6911081671714783\n",
      "step: 30410 , time : 0.0\n",
      "train: loss: 491028.1875 acc: 0.9516096711158752  val: loss: 2247176.75 acc: 0.563646674156189\n",
      "step: 30415 , time : 0.0\n",
      "train: loss: 1392908.625 acc: 0.9341036081314087  val: loss: 1450888.75 acc: 0.7017779350280762\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 30420 , time : 0.015625715255737305\n",
      "train: loss: 377786.21875 acc: 0.986729621887207  val: loss: 412879.59375 acc: 0.8455091714859009\n",
      "step: 30425 , time : 0.0\n",
      "train: loss: 703048.75 acc: 0.9716855883598328  val: loss: 693920.25 acc: 0.8849925994873047\n",
      "step: 30430 , time : 0.0010008811950683594\n",
      "train: loss: 542196.9375 acc: 0.9582343697547913  val: loss: 1900656.375 acc: 0.5328646898269653\n",
      "step: 30435 , time : 0.0\n",
      "train: loss: 531852.9375 acc: 0.9705697298049927  val: loss: 1056864.5 acc: 0.6934041976928711\n",
      "step: 30440 , time : 0.0\n",
      "train: loss: 244418.9375 acc: 0.9769961833953857  val: loss: 786869.75 acc: 0.7212722301483154\n",
      "step: 30445 , time : 0.0\n",
      "train: loss: 325130.125 acc: 0.9064664840698242  val: loss: 414220.84375 acc: 0.7646321654319763\n",
      "step: 30450 , time : 0.0\n",
      "train: loss: 517237.4375 acc: 0.8577419519424438  val: loss: 366354.375 acc: 0.8762268424034119\n",
      "step: 30455 , time : 0.0010008811950683594\n",
      "train: loss: 1784455.375 acc: 0.3217291235923767  val: loss: 1512802.0 acc: 0.7712573409080505\n",
      "step: 30460 , time : 0.0\n",
      "train: loss: 747215.1875 acc: 0.36917662620544434  val: loss: 1264613.125 acc: 0.4258400797843933\n",
      "step: 30465 , time : 0.0\n",
      "train: loss: 609156.3125 acc: 0.8111190795898438  val: loss: 794171.1875 acc: 0.8186842799186707\n",
      "step: 30470 , time : 0.0\n",
      "train: loss: 217576.21875 acc: 0.9162031412124634  val: loss: 364573.5 acc: 0.8264040946960449\n",
      "step: 30475 , time : 0.0\n",
      "train: loss: 967650.875 acc: 0.6692281365394592  val: loss: 603184.25 acc: 0.836024820804596\n",
      "step: 30480 , time : 0.0\n",
      "train: loss: 388577.71875 acc: 0.739480197429657  val: loss: 3050836.75 acc: 0.4914586544036865\n",
      "step: 30485 , time : 0.0\n",
      "train: loss: 699520.125 acc: 0.6964247226715088  val: loss: 1013589.125 acc: 0.6058236360549927\n",
      "step: 30490 , time : 0.0\n",
      "train: loss: 128588.1640625 acc: 0.8915901184082031  val: loss: 4326906.0 acc: 0.5683046579360962\n",
      "step: 30495 , time : 0.0\n",
      "train: loss: 214345.046875 acc: 0.84726881980896  val: loss: 3280599.5 acc: 0.6095008850097656\n",
      "step: 30500 , time : 0.0\n",
      "train: loss: 126047.546875 acc: 0.8873814940452576  val: loss: 926392.5 acc: 0.7244481444358826\n",
      "step: 30505 , time : 0.0\n",
      "train: loss: 24406.5390625 acc: 0.9810853600502014  val: loss: 1412350.875 acc: 0.6667858362197876\n",
      "step: 30510 , time : 0.0\n",
      "train: loss: 38247.1640625 acc: 0.9681254029273987  val: loss: 1634679.125 acc: 0.6047379374504089\n",
      "step: 30515 , time : 0.0\n",
      "train: loss: 352405.03125 acc: 0.8471417427062988  val: loss: 631119.8125 acc: 0.7367193698883057\n",
      "step: 30520 , time : 0.0\n",
      "train: loss: 119683.1171875 acc: 0.9217581748962402  val: loss: 1175525.375 acc: 0.6958272457122803\n",
      "step: 30525 , time : 0.0\n",
      "train: loss: 103034.53125 acc: 0.8649512529373169  val: loss: 1087758.375 acc: 0.6247467398643494\n",
      "step: 30530 , time : 0.0\n",
      "train: loss: 118644.8984375 acc: 0.9110311269760132  val: loss: 2285432.25 acc: 0.6775234937667847\n",
      "step: 30535 , time : 0.01562643051147461\n",
      "train: loss: 130908.4921875 acc: 0.8958916068077087  val: loss: 2877587.25 acc: 0.6358987092971802\n",
      "step: 30540 , time : 0.0010004043579101562\n",
      "train: loss: 989078.4375 acc: 0.7101696729660034  val: loss: 3196726.25 acc: 0.6538498401641846\n",
      "step: 30545 , time : 0.0\n",
      "train: loss: 642717.125 acc: 0.6937913298606873  val: loss: 1700534.125 acc: 0.6594067215919495\n",
      "step: 30550 , time : 0.001001119613647461\n",
      "train: loss: 595029.75 acc: 0.7246767282485962  val: loss: 1911147.625 acc: 0.5888844728469849\n",
      "step: 30555 , time : 0.0\n",
      "train: loss: 184232.5625 acc: 0.8473801016807556  val: loss: 3239594.25 acc: 0.609959065914154\n",
      "step: 30560 , time : 0.001001119613647461\n",
      "train: loss: 73865.8125 acc: 0.9254437685012817  val: loss: 4521262.0 acc: 0.5584214329719543\n",
      "step: 30565 , time : 0.001001119613647461\n",
      "train: loss: 902921.875 acc: 0.6340068578720093  val: loss: 2095540.0 acc: 0.7314494848251343\n",
      "step: 30570 , time : 0.0\n",
      "train: loss: 1260389.75 acc: 0.8291880488395691  val: loss: 711780.875 acc: 0.8212765455245972\n",
      "step: 30575 , time : 0.0010004043579101562\n",
      "train: loss: 1015215.625 acc: 0.8844438195228577  val: loss: 1367102.75 acc: 0.8764165639877319\n",
      "step: 30580 , time : 0.0010008811950683594\n",
      "train: loss: 459303.125 acc: 0.9608872532844543  val: loss: 1073315.625 acc: 0.8235331177711487\n",
      "step: 30585 , time : 0.0\n",
      "train: loss: 248770.375 acc: 0.9598925113677979  val: loss: 1432506.375 acc: 0.7729571461677551\n",
      "step: 30590 , time : 0.0\n",
      "train: loss: 142704.90625 acc: 0.9787026047706604  val: loss: 787534.25 acc: 0.9123966097831726\n",
      "step: 30595 , time : 0.0\n",
      "train: loss: 87622.96875 acc: 0.9928677678108215  val: loss: 2354253.5 acc: 0.5023033618927002\n",
      "step: 30600 , time : 0.0\n",
      "train: loss: 71441.0 acc: 0.9942013025283813  val: loss: 1628440.5 acc: 0.6289401054382324\n",
      "step: 30605 , time : 0.0\n",
      "train: loss: 54797.67578125 acc: 0.9958997368812561  val: loss: 4639513.0 acc: 0.5257773399353027\n",
      "step: 30610 , time : 0.0\n",
      "train: loss: 85739.7890625 acc: 0.9901192784309387  val: loss: 1507481.0 acc: 0.8009034395217896\n",
      "step: 30615 , time : 0.0\n",
      "train: loss: 54486.3984375 acc: 0.9923009276390076  val: loss: 2143364.75 acc: 0.5697506666183472\n",
      "step: 30620 , time : 0.0\n",
      "train: loss: 12875.3369140625 acc: 0.9978231191635132  val: loss: 617993.9375 acc: 0.8059197664260864\n",
      "step: 30625 , time : 0.0\n",
      "train: loss: 26645.154296875 acc: 0.9822169542312622  val: loss: 846871.75 acc: 0.913910984992981\n",
      "step: 30630 , time : 0.0\n",
      "train: loss: 8307.1943359375 acc: 0.9608197212219238  val: loss: 1133173.625 acc: 0.7648122906684875\n",
      "step: 30635 , time : 0.0\n",
      "train: loss: 10587.11328125 acc: 0.9949902296066284  val: loss: 3457156.75 acc: -1.1902565956115723\n",
      "step: 30640 , time : 0.0\n",
      "train: loss: 25594.46875 acc: 0.9822245836257935  val: loss: 1558529.125 acc: 0.764953076839447\n",
      "step: 30645 , time : 0.0\n",
      "train: loss: 14120.3916015625 acc: 0.9907622933387756  val: loss: 1864946.125 acc: 0.7521030306816101\n",
      "step: 30650 , time : 0.0\n",
      "train: loss: 21929.59765625 acc: 0.9812231063842773  val: loss: 1375549.5 acc: 0.7562516927719116\n",
      "step: 30655 , time : 0.0\n",
      "train: loss: 6858.17431640625 acc: 0.9609716534614563  val: loss: 725944.5 acc: 0.7644847631454468\n",
      "step: 30660 , time : 0.0010013580322265625\n",
      "train: loss: 17444.935546875 acc: 0.9811321496963501  val: loss: 734941.75 acc: 0.5247633457183838\n",
      "step: 30665 , time : 0.0\n",
      "train: loss: 16430.572265625 acc: 0.9658924341201782  val: loss: 1372190.875 acc: 0.8287950158119202\n",
      "step: 30670 , time : 0.0010004043579101562\n",
      "train: loss: 15737.5869140625 acc: 0.9910277724266052  val: loss: 1244802.875 acc: 0.5560123324394226\n",
      "step: 30675 , time : 0.0010004043579101562\n",
      "train: loss: 15894.3349609375 acc: 0.9758096933364868  val: loss: 1598760.375 acc: 0.6478047370910645\n",
      "step: 30680 , time : 0.0\n",
      "train: loss: 35224.28515625 acc: 0.979054868221283  val: loss: 1494635.25 acc: 0.6596966981887817\n",
      "step: 30685 , time : 0.0\n",
      "train: loss: 7293.92431640625 acc: 0.9947493672370911  val: loss: 980587.375 acc: 0.8135090470314026\n",
      "step: 30690 , time : 0.0\n",
      "train: loss: 22429.103515625 acc: 0.9862832427024841  val: loss: 1701032.25 acc: 0.7849884033203125\n",
      "step: 30695 , time : 0.0\n",
      "train: loss: 24955.091796875 acc: 0.9592100977897644  val: loss: 2232846.75 acc: 0.4857824444770813\n",
      "step: 30700 , time : 0.0\n",
      "train: loss: 7546.14794921875 acc: 0.9955920577049255  val: loss: 1713888.5 acc: 0.10479748249053955\n",
      "step: 30705 , time : 0.0\n",
      "train: loss: 27028.87109375 acc: 0.9914592504501343  val: loss: 1697718.25 acc: 0.29401546716690063\n",
      "step: 30710 , time : 0.015625\n",
      "train: loss: 24442.46875 acc: 0.99314284324646  val: loss: 1816295.0 acc: 0.4350995421409607\n",
      "step: 30715 , time : 0.0\n",
      "train: loss: 26316.994140625 acc: 0.9931448101997375  val: loss: 665818.875 acc: 0.8495792150497437\n",
      "step: 30720 , time : 0.0\n",
      "train: loss: 64346.875 acc: 0.9694633483886719  val: loss: 520723.46875 acc: 0.9292502403259277\n",
      "step: 30725 , time : 0.0\n",
      "train: loss: 50121.578125 acc: 0.9894883632659912  val: loss: 1395630.875 acc: -0.058821678161621094\n",
      "step: 30730 , time : 0.0\n",
      "train: loss: 51055.78125 acc: 0.9884859919548035  val: loss: 935719.8125 acc: 0.9190709590911865\n",
      "step: 30735 , time : 0.0\n",
      "train: loss: 136491.796875 acc: 0.953628659248352  val: loss: 687600.125 acc: 0.5377583503723145\n",
      "step: 30740 , time : 0.0\n",
      "train: loss: 234706.609375 acc: 0.9513205885887146  val: loss: 964772.1875 acc: 0.8203247785568237\n",
      "step: 30745 , time : 0.0\n",
      "train: loss: 69332.484375 acc: 0.992958664894104  val: loss: 794793.6875 acc: 0.7557036876678467\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 30750 , time : 0.0\n",
      "train: loss: 116459.21875 acc: 0.9881372451782227  val: loss: 1135440.125 acc: 0.7623772025108337\n",
      "step: 30755 , time : 0.0\n",
      "train: loss: 30236.048828125 acc: 0.9962120652198792  val: loss: 1647138.5 acc: -0.10175418853759766\n",
      "step: 30760 , time : 0.0\n",
      "train: loss: 287919.53125 acc: 0.9555076360702515  val: loss: 1803056.875 acc: 0.5092505216598511\n",
      "step: 30765 , time : 0.0010006427764892578\n",
      "train: loss: 815112.875 acc: 0.9375664591789246  val: loss: 623408.5625 acc: 0.840426504611969\n",
      "step: 30770 , time : 0.0\n",
      "train: loss: 417860.34375 acc: 0.9606828689575195  val: loss: 1828922.625 acc: 0.562207818031311\n",
      "step: 30775 , time : 0.0\n",
      "train: loss: 366662.6875 acc: 0.9703214764595032  val: loss: 1229770.875 acc: 0.8026493787765503\n",
      "step: 30780 , time : 0.0010008811950683594\n",
      "train: loss: 562417.625 acc: 0.9549973011016846  val: loss: 640560.125 acc: 0.857561469078064\n",
      "step: 30785 , time : 0.0\n",
      "train: loss: 667679.625 acc: 0.9625208973884583  val: loss: 1396149.375 acc: 0.7743952870368958\n",
      "step: 30790 , time : 0.0\n",
      "train: loss: 465429.34375 acc: 0.9830790162086487  val: loss: 429029.40625 acc: 0.9144766926765442\n",
      "step: 30795 , time : 0.0\n",
      "train: loss: 838409.375 acc: 0.9507104158401489  val: loss: 511414.28125 acc: 0.9145861864089966\n",
      "step: 30800 , time : 0.0\n",
      "train: loss: 283068.0625 acc: 0.9680750370025635  val: loss: 747066.9375 acc: 0.8001736402511597\n",
      "step: 30805 , time : 0.0\n",
      "train: loss: 377673.53125 acc: 0.959326982498169  val: loss: 400661.90625 acc: 0.8834551572799683\n",
      "step: 30810 , time : 0.0\n",
      "train: loss: 411831.71875 acc: 0.9674527645111084  val: loss: 227146.15625 acc: 0.9002983570098877\n",
      "step: 30815 , time : 0.0\n",
      "train: loss: 322814.96875 acc: 0.9513107538223267  val: loss: 447117.65625 acc: 0.8940916657447815\n",
      "step: 30820 , time : 0.0\n",
      "train: loss: 1621081.625 acc: 0.45741087198257446  val: loss: 786121.0 acc: 0.8408464789390564\n",
      "step: 30825 , time : 0.0\n",
      "train: loss: 788070.8125 acc: 0.5992879867553711  val: loss: 899711.0625 acc: 0.5235989689826965\n",
      "step: 30830 , time : 0.0\n",
      "train: loss: 1424891.25 acc: 0.8208611011505127  val: loss: 1010347.875 acc: 0.8236996531486511\n",
      "step: 30835 , time : 0.0\n",
      "train: loss: 1165676.0 acc: 0.6432434320449829  val: loss: 672720.75 acc: 0.8292959332466125\n",
      "step: 30840 , time : 0.0\n",
      "train: loss: 665328.5 acc: 0.7478010654449463  val: loss: 523000.34375 acc: 0.8127124309539795\n",
      "step: 30845 , time : 0.0\n",
      "train: loss: 920855.4375 acc: 0.7239497303962708  val: loss: 3094998.0 acc: 0.5781722068786621\n",
      "step: 30850 , time : 0.0\n",
      "train: loss: 175941.3125 acc: 0.8599156737327576  val: loss: 2417268.75 acc: 0.6371088027954102\n",
      "step: 30855 , time : 0.0\n",
      "train: loss: 242946.5 acc: 0.8441121578216553  val: loss: 1866895.125 acc: 0.7060465812683105\n",
      "step: 30860 , time : 0.0\n",
      "train: loss: 122439.8671875 acc: 0.9095627069473267  val: loss: 460414.96875 acc: 0.8289716243743896\n",
      "step: 30865 , time : 0.0\n",
      "train: loss: 1484937.125 acc: 0.646953821182251  val: loss: 4426971.5 acc: 0.6664217710494995\n",
      "step: 30870 , time : 0.0\n",
      "train: loss: 6538.98828125 acc: 0.9944185018539429  val: loss: 3551120.25 acc: 0.5512508153915405\n",
      "step: 30875 , time : 0.0\n",
      "train: loss: 121431.734375 acc: 0.9245050549507141  val: loss: 3065528.5 acc: 0.6425368785858154\n",
      "step: 30880 , time : 0.0\n",
      "train: loss: 12179.4541015625 acc: 0.9898077845573425  val: loss: 6372811.5 acc: 0.5363759994506836\n",
      "step: 30885 , time : 0.001001119613647461\n",
      "train: loss: 638519.75 acc: 0.7330794334411621  val: loss: 3340548.75 acc: 0.6115381717681885\n",
      "step: 30890 , time : 0.0\n",
      "train: loss: 327667.21875 acc: 0.7666793465614319  val: loss: 1147765.125 acc: 0.6744328737258911\n",
      "step: 30895 , time : 0.0\n",
      "train: loss: 49580.921875 acc: 0.9553518891334534  val: loss: 1824516.625 acc: 0.6857489943504333\n",
      "step: 30900 , time : 0.0010008811950683594\n",
      "train: loss: 116932.8203125 acc: 0.9026333093643188  val: loss: 1008688.875 acc: 0.751497745513916\n",
      "step: 30905 , time : 0.0\n",
      "train: loss: 425268.34375 acc: 0.8027337789535522  val: loss: 802597.75 acc: 0.7303031086921692\n",
      "step: 30910 , time : 0.0\n",
      "train: loss: 38050.54296875 acc: 0.9627063870429993  val: loss: 1802524.375 acc: 0.5967476963996887\n",
      "step: 30915 , time : 0.0010008811950683594\n",
      "train: loss: 110368.046875 acc: 0.8890073299407959  val: loss: 4420047.0 acc: 0.5775036811828613\n",
      "step: 30920 , time : 0.0010008811950683594\n",
      "train: loss: 496864.21875 acc: 0.7091709971427917  val: loss: 653968.125 acc: 0.7187268137931824\n",
      "step: 30925 , time : 0.0\n",
      "train: loss: 242379.328125 acc: 0.8262585997581482  val: loss: 1556891.25 acc: 0.62340247631073\n",
      "step: 30930 , time : 0.0\n",
      "train: loss: 1510011.25 acc: 0.6834002137184143  val: loss: 2562829.0 acc: 0.7558480501174927\n",
      "step: 30935 , time : 0.0\n",
      "train: loss: 1125984.875 acc: 0.8281124234199524  val: loss: 884115.0625 acc: 0.821644127368927\n",
      "step: 30940 , time : 0.0\n",
      "train: loss: 286647.59375 acc: 0.9726935625076294  val: loss: 2777578.75 acc: 0.45575791597366333\n",
      "step: 30945 , time : 0.0\n",
      "train: loss: 520286.0625 acc: 0.9580119252204895  val: loss: 1013641.5 acc: 0.8251849412918091\n",
      "step: 30950 , time : 0.0\n",
      "train: loss: 271137.90625 acc: 0.9686729907989502  val: loss: 1208825.25 acc: 0.882561445236206\n",
      "step: 30955 , time : 0.0\n",
      "train: loss: 215996.3125 acc: 0.9609804749488831  val: loss: 860992.5 acc: 0.8759835362434387\n",
      "step: 30960 , time : 0.0010006427764892578\n",
      "train: loss: 154613.046875 acc: 0.9798650741577148  val: loss: 2363479.75 acc: 0.3524942398071289\n",
      "step: 30965 , time : 0.0\n",
      "train: loss: 237755.6875 acc: 0.9831017255783081  val: loss: 1903807.75 acc: 0.7665988206863403\n",
      "step: 30970 , time : 0.0\n",
      "train: loss: 54372.3359375 acc: 0.9961351156234741  val: loss: 1982070.375 acc: 0.2987911105155945\n",
      "step: 30975 , time : 0.0\n",
      "train: loss: 98834.5546875 acc: 0.988265872001648  val: loss: 1530145.875 acc: 0.8336855173110962\n",
      "step: 30980 , time : 0.0\n",
      "train: loss: 43046.671875 acc: 0.9923907518386841  val: loss: 2451132.75 acc: -0.6963193416595459\n",
      "step: 30985 , time : 0.0010006427764892578\n",
      "train: loss: 20220.923828125 acc: 0.9945665001869202  val: loss: 2282144.75 acc: 0.3301098346710205\n",
      "step: 30990 , time : 0.0\n",
      "train: loss: 19194.9609375 acc: 0.983862578868866  val: loss: 1284699.75 acc: 0.7022838592529297\n",
      "step: 30995 , time : 0.0\n",
      "train: loss: 10108.759765625 acc: 0.9931387305259705  val: loss: 1129844.625 acc: 0.6276102066040039\n",
      "step: 31000 , time : 0.0010006427764892578\n",
      "train: loss: 11634.83984375 acc: 0.9867190718650818  val: loss: 3696661.75 acc: 0.49537938833236694\n",
      "step: 31005 , time : 0.0010004043579101562\n",
      "train: loss: 5406.11962890625 acc: 0.9857888221740723  val: loss: 2283828.5 acc: 0.4866691827774048\n",
      "step: 31010 , time : 0.0010004043579101562\n",
      "train: loss: 4737.3125 acc: 0.9896809458732605  val: loss: 1616297.75 acc: 0.7806016206741333\n",
      "step: 31015 , time : 0.0\n",
      "train: loss: 19630.818359375 acc: 0.9700579047203064  val: loss: 1970350.5 acc: 0.8010741472244263\n",
      "step: 31020 , time : 0.0\n",
      "train: loss: 199425.15625 acc: 0.656356692314148  val: loss: 2105994.75 acc: 0.8286408185958862\n",
      "step: 31025 , time : 0.0\n",
      "train: loss: 4272.04248046875 acc: 0.9969574809074402  val: loss: 1529749.75 acc: 0.7870391011238098\n",
      "step: 31030 , time : 0.0\n",
      "train: loss: 29314.83984375 acc: 0.970734715461731  val: loss: 422029.1875 acc: 0.8972394466400146\n",
      "step: 31035 , time : 0.0\n",
      "train: loss: 14668.5947265625 acc: 0.9826338887214661  val: loss: 1005076.75 acc: 0.7632932662963867\n",
      "step: 31040 , time : 0.0\n",
      "train: loss: 31632.69140625 acc: 0.9534079432487488  val: loss: 850714.0 acc: 0.8677406311035156\n",
      "step: 31045 , time : 0.0\n",
      "train: loss: 17034.611328125 acc: 0.9886676073074341  val: loss: 1039811.3125 acc: 0.766363799571991\n",
      "step: 31050 , time : 0.0\n",
      "train: loss: 28086.888671875 acc: 0.9893324375152588  val: loss: 1421194.0 acc: 0.5451027154922485\n",
      "step: 31055 , time : 0.0\n",
      "train: loss: 93194.421875 acc: 0.92767333984375  val: loss: 1452953.375 acc: 0.8375882506370544\n",
      "step: 31060 , time : 0.0\n",
      "train: loss: 8947.1328125 acc: 0.9924566149711609  val: loss: 1039099.75 acc: 0.7673468589782715\n",
      "step: 31065 , time : 0.0\n",
      "train: loss: 8007.29345703125 acc: 0.9818354845046997  val: loss: 1203017.125 acc: 0.7428580522537231\n",
      "step: 31070 , time : 0.015625715255737305\n",
      "train: loss: 22178.607421875 acc: 0.9926487803459167  val: loss: 1403627.125 acc: 0.7893121838569641\n",
      "step: 31075 , time : 0.0\n",
      "train: loss: 42713.578125 acc: 0.9905073642730713  val: loss: 1319122.25 acc: 0.25018543004989624\n",
      "step: 31080 , time : 0.0\n",
      "train: loss: 34473.796875 acc: 0.9826849699020386  val: loss: 233925.09375 acc: 0.92010098695755\n",
      "step: 31085 , time : 0.0\n",
      "train: loss: 32070.65234375 acc: 0.9858371019363403  val: loss: 1702294.0 acc: 0.4060019254684448\n",
      "step: 31090 , time : 0.0010006427764892578\n",
      "train: loss: 149881.953125 acc: 0.8930827975273132  val: loss: 1310609.625 acc: 0.596327543258667\n",
      "step: 31095 , time : 0.0010008811950683594\n",
      "train: loss: 53024.1484375 acc: 0.9840630292892456  val: loss: 884964.4375 acc: 0.6213857531547546\n",
      "step: 31100 , time : 0.0\n",
      "train: loss: 45118.28125 acc: 0.9469670653343201  val: loss: 1108357.75 acc: 0.8587298393249512\n",
      "step: 31105 , time : 0.0010008811950683594\n",
      "train: loss: 80142.4609375 acc: 0.9835631251335144  val: loss: 46341.76171875 acc: 0.9361160397529602\n",
      "step: 31110 , time : 0.0\n",
      "train: loss: 382617.46875 acc: 0.9420471787452698  val: loss: 819354.0 acc: 0.7415503263473511\n",
      "step: 31115 , time : 0.0\n",
      "train: loss: 64489.73828125 acc: 0.994645893573761  val: loss: 961248.5625 acc: 0.8888558745384216\n",
      "step: 31120 , time : 0.0\n",
      "train: loss: 669523.5 acc: 0.9274031519889832  val: loss: 1651937.875 acc: 0.6611313819885254\n",
      "step: 31125 , time : 0.0\n",
      "train: loss: 359177.40625 acc: 0.9714393019676208  val: loss: 478121.90625 acc: 0.7618170976638794\n",
      "step: 31130 , time : 0.0\n",
      "train: loss: 177487.765625 acc: 0.9838168621063232  val: loss: 432558.40625 acc: 0.7323755025863647\n",
      "step: 31135 , time : 0.0\n",
      "train: loss: 507166.875 acc: 0.9416869878768921  val: loss: 919924.9375 acc: 0.5755937695503235\n",
      "step: 31140 , time : 0.0\n",
      "train: loss: 406498.40625 acc: 0.9583803415298462  val: loss: 907545.875 acc: 0.8159545660018921\n",
      "step: 31145 , time : 0.0\n",
      "train: loss: 474753.625 acc: 0.9563247561454773  val: loss: 632882.3125 acc: 0.850115954875946\n",
      "step: 31150 , time : 0.0\n",
      "train: loss: 1283213.375 acc: 0.9527249336242676  val: loss: 274754.59375 acc: 0.8271528482437134\n",
      "step: 31155 , time : 0.0\n",
      "train: loss: 781263.375 acc: 0.972883939743042  val: loss: 708061.375 acc: 0.7073713541030884\n",
      "step: 31160 , time : 0.0010004043579101562\n",
      "train: loss: 1247352.625 acc: 0.9561017751693726  val: loss: 353125.03125 acc: 0.9561645984649658\n",
      "step: 31165 , time : 0.0\n",
      "train: loss: 538099.9375 acc: 0.9207075834274292  val: loss: 364717.84375 acc: 0.9306156635284424\n",
      "step: 31170 , time : 0.0\n",
      "train: loss: 180788.765625 acc: 0.9825527667999268  val: loss: 540938.0 acc: 0.9187476634979248\n",
      "step: 31175 , time : 0.0\n",
      "train: loss: 403149.4375 acc: 0.9323258399963379  val: loss: 507570.21875 acc: 0.9517660737037659\n",
      "step: 31180 , time : 0.015626192092895508\n",
      "train: loss: 303770.9375 acc: 0.943804919719696  val: loss: 402032.71875 acc: 0.952450692653656\n",
      "step: 31185 , time : 0.0\n",
      "train: loss: 1177722.5 acc: 0.5414057970046997  val: loss: 659136.9375 acc: 0.8743913173675537\n",
      "step: 31190 , time : 0.0\n",
      "train: loss: 2010372.125 acc: 0.4589552879333496  val: loss: 2190093.5 acc: 0.7774688005447388\n",
      "step: 31195 , time : 0.0010004043579101562\n",
      "train: loss: 1221645.625 acc: 0.7521080374717712  val: loss: 1961247.625 acc: 0.7710105180740356\n",
      "step: 31200 , time : 0.0\n",
      "train: loss: 493677.96875 acc: 0.8795852661132812  val: loss: 866514.5625 acc: 0.797501802444458\n",
      "step: 31205 , time : 0.0\n",
      "train: loss: 702271.0625 acc: 0.7465915679931641  val: loss: 1555852.625 acc: 0.8896347284317017\n",
      "step: 31210 , time : 0.0\n",
      "train: loss: 1368426.875 acc: 0.3377224802970886  val: loss: 1442844.25 acc: 0.49811238050460815\n",
      "step: 31215 , time : 0.0\n",
      "train: loss: 2239427.5 acc: 0.6026308536529541  val: loss: 2344233.75 acc: 0.6456700563430786\n",
      "step: 31220 , time : 0.001001119613647461\n",
      "train: loss: 639966.8125 acc: 0.7197436690330505  val: loss: 580169.125 acc: 0.5853769779205322\n",
      "step: 31225 , time : 0.0010008811950683594\n",
      "train: loss: 298331.21875 acc: 0.7413619756698608  val: loss: 527564.5625 acc: 0.6763877868652344\n",
      "step: 31230 , time : 0.0010006427764892578\n",
      "train: loss: 221655.921875 acc: 0.8014705181121826  val: loss: 1244829.75 acc: 0.7201998233795166\n",
      "step: 31235 , time : 0.0\n",
      "train: loss: 70955.46875 acc: 0.9341073036193848  val: loss: 1468440.875 acc: 0.7024260759353638\n",
      "step: 31240 , time : 0.0\n",
      "train: loss: 120808.953125 acc: 0.9014358520507812  val: loss: 340191.34375 acc: 0.8193250298500061\n",
      "step: 31245 , time : 0.0\n",
      "train: loss: 69493.09375 acc: 0.9413429498672485  val: loss: 5246719.5 acc: 0.5141983032226562\n",
      "step: 31250 , time : 0.0\n",
      "train: loss: 60099.2265625 acc: 0.9542808532714844  val: loss: 1464101.625 acc: 0.6648591756820679\n",
      "step: 31255 , time : 0.0\n",
      "train: loss: 169577.703125 acc: 0.8484716415405273  val: loss: 3011444.75 acc: 0.5700387954711914\n",
      "step: 31260 , time : 0.0\n",
      "train: loss: 358751.3125 acc: 0.8057063817977905  val: loss: 2540971.5 acc: 0.6406739950180054\n",
      "step: 31265 , time : 0.0\n",
      "train: loss: 80912.75 acc: 0.9208102822303772  val: loss: 655407.3125 acc: 0.7798802852630615\n",
      "step: 31270 , time : 0.0\n",
      "train: loss: 469205.21875 acc: 0.7716544270515442  val: loss: 588648.6875 acc: 0.7305945754051208\n",
      "step: 31275 , time : 0.0\n",
      "train: loss: 467782.3125 acc: 0.7553362846374512  val: loss: 447204.3125 acc: 0.7794860601425171\n",
      "step: 31280 , time : 0.0\n",
      "train: loss: 123665.234375 acc: 0.8821889162063599  val: loss: 4092846.5 acc: 0.510694682598114\n",
      "step: 31285 , time : 0.0\n",
      "train: loss: 1153026.125 acc: 0.6681743860244751  val: loss: 1963992.875 acc: 0.6200128197669983\n",
      "step: 31290 , time : 0.0\n",
      "train: loss: 147640.46875 acc: 0.9017249345779419  val: loss: 5302113.5 acc: 0.43820953369140625\n",
      "step: 31295 , time : 0.0\n",
      "train: loss: 1579231.625 acc: 0.6128857135772705  val: loss: 4290573.0 acc: 0.5293219089508057\n",
      "step: 31300 , time : 0.0\n",
      "train: loss: 1416027.5 acc: 0.7734562158584595  val: loss: 1701795.875 acc: 0.8014727830886841\n",
      "step: 31305 , time : 0.0\n",
      "train: loss: 742360.125 acc: 0.9049925208091736  val: loss: 1196284.875 acc: 0.3586271405220032\n",
      "step: 31310 , time : 0.0010006427764892578\n",
      "train: loss: 269058.5 acc: 0.9748679995536804  val: loss: 1411665.625 acc: 0.859810471534729\n",
      "step: 31315 , time : 0.0\n",
      "train: loss: 627880.0625 acc: 0.9370673894882202  val: loss: 851487.0 acc: 0.930713951587677\n",
      "step: 31320 , time : 0.0010008811950683594\n",
      "train: loss: 138473.03125 acc: 0.9804091453552246  val: loss: 1297781.125 acc: 0.8153353333473206\n",
      "step: 31325 , time : 0.001001119613647461\n",
      "train: loss: 171765.578125 acc: 0.9785141348838806  val: loss: 509283.09375 acc: 0.8277875185012817\n",
      "step: 31330 , time : 0.0010006427764892578\n",
      "train: loss: 86990.4140625 acc: 0.9901745915412903  val: loss: 830807.25 acc: 0.8717416524887085\n",
      "step: 31335 , time : 0.0\n",
      "train: loss: 73628.796875 acc: 0.9945983290672302  val: loss: 948569.6875 acc: 0.9038035869598389\n",
      "step: 31340 , time : 0.0\n",
      "train: loss: 86077.328125 acc: 0.985634982585907  val: loss: 651523.8125 acc: 0.9085763096809387\n",
      "step: 31345 , time : 0.0\n",
      "train: loss: 75289.8125 acc: 0.9914928674697876  val: loss: 1370017.625 acc: 0.7676132917404175\n",
      "step: 31350 , time : 0.0\n",
      "train: loss: 17775.724609375 acc: 0.9910719990730286  val: loss: 258365.234375 acc: 0.961908757686615\n",
      "step: 31355 , time : 0.0\n",
      "train: loss: 28531.755859375 acc: 0.991919755935669  val: loss: 359139.1875 acc: 0.78886479139328\n",
      "step: 31360 , time : 0.0\n",
      "train: loss: 30706.267578125 acc: 0.9732166528701782  val: loss: 1029761.3125 acc: 0.7577361464500427\n",
      "step: 31365 , time : 0.0\n",
      "train: loss: 3899.14892578125 acc: 0.9912189841270447  val: loss: 392729.8125 acc: 0.7850856184959412\n",
      "step: 31370 , time : 0.0\n",
      "train: loss: 12514.3876953125 acc: 0.9794005751609802  val: loss: 383304.34375 acc: 0.862548828125\n",
      "step: 31375 , time : 0.0\n",
      "train: loss: 17840.255859375 acc: 0.988130509853363  val: loss: 2871612.75 acc: 0.5578712224960327\n",
      "step: 31380 , time : 0.0\n",
      "train: loss: 10602.59765625 acc: 0.9671308398246765  val: loss: 283293.6875 acc: 0.858329176902771\n",
      "step: 31385 , time : 0.0\n",
      "train: loss: 5793.05859375 acc: 0.9927647113800049  val: loss: 1070107.125 acc: 0.6386172771453857\n",
      "step: 31390 , time : 0.0\n",
      "train: loss: 5583.013671875 acc: 0.9844793677330017  val: loss: 1237812.875 acc: 0.7147905230522156\n",
      "step: 31395 , time : 0.0\n",
      "train: loss: 8502.6689453125 acc: 0.9851663708686829  val: loss: 639495.375 acc: 0.8829151391983032\n",
      "step: 31400 , time : 0.0\n",
      "train: loss: 13705.43359375 acc: 0.9878402352333069  val: loss: 92727.671875 acc: 0.9885354042053223\n",
      "step: 31405 , time : 0.0\n",
      "train: loss: 60522.86328125 acc: 0.9709376692771912  val: loss: 487474.6875 acc: 0.8872371912002563\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 31410 , time : 0.0\n",
      "train: loss: 15511.2880859375 acc: 0.9896512627601624  val: loss: 1018367.125 acc: 0.6246262788772583\n",
      "step: 31415 , time : 0.0\n",
      "train: loss: 29594.65234375 acc: 0.9880545139312744  val: loss: 2196539.75 acc: 0.5470314025878906\n",
      "step: 31420 , time : 0.0\n",
      "train: loss: 21159.822265625 acc: 0.9842776656150818  val: loss: 1427000.25 acc: 0.5319806933403015\n",
      "step: 31425 , time : 0.0010006427764892578\n",
      "train: loss: 15345.9013671875 acc: 0.9832668304443359  val: loss: 69415.0546875 acc: 0.9748080372810364\n",
      "step: 31430 , time : 0.0\n",
      "train: loss: 12657.029296875 acc: 0.9900457859039307  val: loss: 2538777.25 acc: -0.08760738372802734\n",
      "step: 31435 , time : 0.0\n",
      "train: loss: 31050.345703125 acc: 0.9914766550064087  val: loss: 2985592.75 acc: -0.027976632118225098\n",
      "step: 31440 , time : 0.0010004043579101562\n",
      "train: loss: 40791.21484375 acc: 0.9902045726776123  val: loss: 54283.4765625 acc: 0.9878858923912048\n",
      "step: 31445 , time : 0.0\n",
      "train: loss: 17146.75 acc: 0.9952221512794495  val: loss: 1874637.125 acc: 0.6296917796134949\n",
      "step: 31450 , time : 0.0010006427764892578\n",
      "train: loss: 40697.0546875 acc: 0.9837526082992554  val: loss: 1214757.25 acc: 0.7890323400497437\n",
      "step: 31455 , time : 0.0\n",
      "train: loss: 81281.703125 acc: 0.962881863117218  val: loss: 1032762.6875 acc: 0.881576657295227\n",
      "step: 31460 , time : 0.0\n",
      "train: loss: 173301.625 acc: 0.9590717554092407  val: loss: 369304.9375 acc: 0.8494748473167419\n",
      "step: 31465 , time : 0.0\n",
      "train: loss: 58311.4140625 acc: 0.9688219428062439  val: loss: 251187.921875 acc: 0.8791762590408325\n",
      "step: 31470 , time : 0.0\n",
      "train: loss: 576678.8125 acc: 0.8599057197570801  val: loss: 220461.109375 acc: 0.92972332239151\n",
      "step: 31475 , time : 0.0\n",
      "train: loss: 123780.78125 acc: 0.9857646822929382  val: loss: 461475.625 acc: 0.9544419646263123\n",
      "step: 31480 , time : 0.0\n",
      "train: loss: 100584.4453125 acc: 0.9891111254692078  val: loss: 751394.625 acc: 0.9055987596511841\n",
      "step: 31485 , time : 0.0\n",
      "train: loss: 116991.8828125 acc: 0.984201967716217  val: loss: 179054.546875 acc: 0.9267582893371582\n",
      "step: 31490 , time : 0.015625953674316406\n",
      "train: loss: 112706.6171875 acc: 0.9873507022857666  val: loss: 503009.5625 acc: 0.9233404994010925\n",
      "step: 31495 , time : 0.0\n",
      "train: loss: 343876.21875 acc: 0.9643661379814148  val: loss: 401005.625 acc: 0.9649873971939087\n",
      "step: 31500 , time : 0.0009999275207519531\n",
      "train: loss: 477886.375 acc: 0.968436598777771  val: loss: 361653.375 acc: 0.9553754329681396\n",
      "step: 31505 , time : 0.0010271072387695312\n",
      "train: loss: 427922.75 acc: 0.9766637682914734  val: loss: 1453286.125 acc: 0.8156567811965942\n",
      "step: 31510 , time : 0.0\n",
      "train: loss: 276161.09375 acc: 0.9742838144302368  val: loss: 897299.6875 acc: 0.7756502628326416\n",
      "step: 31515 , time : 0.0\n",
      "train: loss: 619271.25 acc: 0.9668276309967041  val: loss: 599354.75 acc: 0.9432618021965027\n",
      "step: 31520 , time : 0.0\n",
      "train: loss: 1591627.0 acc: 0.9451496005058289  val: loss: 359121.0625 acc: 0.8975966572761536\n",
      "step: 31525 , time : 0.0\n",
      "train: loss: 2081544.375 acc: 0.9292919635772705  val: loss: 576793.875 acc: 0.8701456785202026\n",
      "step: 31530 , time : 0.0\n",
      "train: loss: 351962.1875 acc: 0.9784101247787476  val: loss: 1127028.25 acc: 0.8262616395950317\n",
      "step: 31535 , time : 0.0010006427764892578\n",
      "train: loss: 1158259.75 acc: 0.9146305322647095  val: loss: 1030318.625 acc: 0.8898428082466125\n",
      "step: 31540 , time : 0.0\n",
      "train: loss: 817361.375 acc: 0.9379199147224426  val: loss: 179068.59375 acc: 0.9153803586959839\n",
      "step: 31545 , time : 0.0\n",
      "train: loss: 1488893.5 acc: 0.7075832486152649  val: loss: 1020992.125 acc: 0.8971507549285889\n",
      "step: 31550 , time : 0.0\n",
      "train: loss: 885540.75 acc: 0.8715789318084717  val: loss: 994013.625 acc: 0.6416275501251221\n",
      "step: 31555 , time : 0.0010008811950683594\n",
      "train: loss: 814671.6875 acc: 0.719230055809021  val: loss: 918492.4375 acc: 0.6278265714645386\n",
      "step: 31560 , time : 0.00019788742065429688\n",
      "train: loss: 745031.5 acc: 0.7568820118904114  val: loss: 1104313.125 acc: 0.8495000004768372\n",
      "step: 31565 , time : 0.0\n",
      "train: loss: 604482.125 acc: 0.6796901226043701  val: loss: 867612.5 acc: 0.8909096717834473\n",
      "step: 31570 , time : 0.0\n",
      "train: loss: 406184.0 acc: 0.9102740287780762  val: loss: 1869220.625 acc: 0.8531814217567444\n",
      "step: 31575 , time : 0.0\n",
      "train: loss: 1176113.125 acc: 0.7339991927146912  val: loss: 1349406.625 acc: 0.7473664283752441\n",
      "step: 31580 , time : 0.0\n",
      "train: loss: 421596.1875 acc: 0.7603041529655457  val: loss: 328595.46875 acc: 0.6987589001655579\n",
      "step: 31585 , time : 0.0\n",
      "train: loss: 573012.1875 acc: 0.757448673248291  val: loss: 628453.5 acc: 0.7733656167984009\n",
      "step: 31590 , time : 0.0\n",
      "train: loss: 1228413.125 acc: 0.6782214641571045  val: loss: 326635.84375 acc: 0.7531507611274719\n",
      "step: 31595 , time : 0.0\n",
      "train: loss: 107748.09375 acc: 0.9051041007041931  val: loss: 2285605.25 acc: 0.6289675235748291\n",
      "step: 31600 , time : 0.0\n",
      "train: loss: 35271.046875 acc: 0.9719612002372742  val: loss: 2795515.0 acc: 0.5397989749908447\n",
      "step: 31605 , time : 0.0\n",
      "train: loss: 81128.78125 acc: 0.9429894685745239  val: loss: 2955463.5 acc: 0.4301930069923401\n",
      "step: 31610 , time : 0.0\n",
      "train: loss: 83553.4140625 acc: 0.9368215799331665  val: loss: 844404.4375 acc: 0.737272322177887\n",
      "step: 31615 , time : 0.0\n",
      "train: loss: 52899.19140625 acc: 0.9559364914894104  val: loss: 3304574.0 acc: 0.577839732170105\n",
      "step: 31620 , time : 0.0\n",
      "train: loss: 225568.265625 acc: 0.8547937273979187  val: loss: 1148426.5 acc: 0.7167718410491943\n",
      "step: 31625 , time : 0.0\n",
      "train: loss: 87530.0625 acc: 0.9113799929618835  val: loss: 732262.125 acc: 0.7168926000595093\n",
      "step: 31630 , time : 0.0\n",
      "train: loss: 49910.59765625 acc: 0.9429751038551331  val: loss: 695923.5625 acc: 0.6986574530601501\n",
      "step: 31635 , time : 0.0\n",
      "train: loss: 772635.125 acc: 0.7104191780090332  val: loss: 1958787.5 acc: 0.6881753206253052\n",
      "step: 31640 , time : 0.0\n",
      "train: loss: 240613.296875 acc: 0.8365367650985718  val: loss: 1645895.875 acc: 0.6060677766799927\n",
      "step: 31645 , time : 0.0010008811950683594\n",
      "train: loss: 117534.0234375 acc: 0.8744785785675049  val: loss: 3026553.75 acc: 0.5457383394241333\n",
      "step: 31650 , time : 0.0\n",
      "train: loss: 369398.5 acc: 0.8387501239776611  val: loss: 735735.625 acc: 0.7584113478660583\n",
      "step: 31655 , time : 0.0\n",
      "train: loss: 53619.08984375 acc: 0.9359121322631836  val: loss: 4009843.0 acc: 0.41333717107772827\n",
      "step: 31660 , time : 0.0\n",
      "train: loss: 155723.078125 acc: 0.914191484451294  val: loss: 752732.75 acc: 0.7632861137390137\n",
      "step: 31665 , time : 0.0\n",
      "train: loss: 1241315.5 acc: 0.7830578684806824  val: loss: 1022180.375 acc: 0.86618572473526\n",
      "step: 31670 , time : 0.0\n",
      "train: loss: 1060503.375 acc: 0.8783798813819885  val: loss: 960225.375 acc: 0.7008475065231323\n",
      "step: 31675 , time : 0.0010006427764892578\n",
      "train: loss: 323584.1875 acc: 0.9748055338859558  val: loss: 1139653.125 acc: 0.3371731638908386\n",
      "step: 31680 , time : 0.0\n",
      "train: loss: 119126.5234375 acc: 0.9841536283493042  val: loss: 962030.375 acc: 0.7948430776596069\n",
      "step: 31685 , time : 0.0\n",
      "train: loss: 144277.78125 acc: 0.9857600331306458  val: loss: 470314.9375 acc: 0.9251013994216919\n",
      "step: 31690 , time : 0.0\n",
      "train: loss: 210269.0 acc: 0.9805276989936829  val: loss: 1487995.0 acc: 0.8645910024642944\n",
      "step: 31695 , time : 0.0\n",
      "train: loss: 109632.03125 acc: 0.9901516437530518  val: loss: 1448569.625 acc: 0.8380403518676758\n",
      "step: 31700 , time : 0.0\n",
      "train: loss: 94880.3203125 acc: 0.9940226078033447  val: loss: 920531.0 acc: 0.771940290927887\n",
      "step: 31705 , time : 0.0\n",
      "train: loss: 50625.30078125 acc: 0.9962223172187805  val: loss: 1004057.875 acc: 0.6511672139167786\n",
      "step: 31710 , time : 0.0\n",
      "train: loss: 41177.99609375 acc: 0.9942806363105774  val: loss: 1308578.625 acc: 0.6348361968994141\n",
      "step: 31715 , time : 0.0\n",
      "train: loss: 40345.63671875 acc: 0.9940624237060547  val: loss: 62785.30859375 acc: 0.9817183613777161\n",
      "step: 31720 , time : 0.0\n",
      "train: loss: 5755.91357421875 acc: 0.9968730211257935  val: loss: 363096.375 acc: 0.9247976541519165\n",
      "step: 31725 , time : 0.0\n",
      "train: loss: 3626.637451171875 acc: 0.9974764585494995  val: loss: 457678.46875 acc: 0.9394571781158447\n",
      "step: 31730 , time : 0.0\n",
      "train: loss: 8622.7734375 acc: 0.9874611496925354  val: loss: 242196.953125 acc: 0.9604519605636597\n",
      "step: 31735 , time : 0.0\n",
      "train: loss: 8991.7451171875 acc: 0.9937617182731628  val: loss: 680595.5 acc: 0.8480527997016907\n",
      "step: 31740 , time : 0.0\n",
      "train: loss: 26828.859375 acc: 0.9857710599899292  val: loss: 205413.625 acc: 0.9473158717155457\n",
      "step: 31745 , time : 0.0\n",
      "train: loss: 11331.068359375 acc: 0.9706917405128479  val: loss: 604046.875 acc: 0.7091710567474365\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 31750 , time : 0.0\n",
      "train: loss: 5961.21142578125 acc: 0.9846636056900024  val: loss: 176612.4375 acc: 0.934416651725769\n",
      "step: 31755 , time : 0.0010008811950683594\n",
      "train: loss: 8471.03125 acc: 0.9853859543800354  val: loss: 1198925.625 acc: 0.7101135849952698\n",
      "step: 31760 , time : 0.0010004043579101562\n",
      "train: loss: 16940.357421875 acc: 0.9732006192207336  val: loss: 1918127.75 acc: 0.7566146850585938\n",
      "step: 31765 , time : 0.0010006427764892578\n",
      "train: loss: 7417.80712890625 acc: 0.9930691719055176  val: loss: 223852.1875 acc: 0.9494849443435669\n",
      "step: 31770 , time : 0.0\n",
      "train: loss: 27962.1953125 acc: 0.9861494898796082  val: loss: 293613.46875 acc: 0.962577223777771\n",
      "step: 31775 , time : 0.0\n",
      "train: loss: 9645.0576171875 acc: 0.9932590126991272  val: loss: 1045355.625 acc: 0.8237217664718628\n",
      "step: 31780 , time : 0.0\n",
      "train: loss: 13804.01953125 acc: 0.9902501106262207  val: loss: 525706.5 acc: 0.9344944953918457\n",
      "step: 31785 , time : 0.0010008811950683594\n",
      "train: loss: 11783.400390625 acc: 0.9899526834487915  val: loss: 830018.0625 acc: 0.9215694069862366\n",
      "step: 31790 , time : 0.0\n",
      "train: loss: 7172.810546875 acc: 0.9900574684143066  val: loss: 1318543.0 acc: 0.843377411365509\n",
      "step: 31795 , time : 0.0\n",
      "train: loss: 6908.83203125 acc: 0.9959840178489685  val: loss: 2430678.0 acc: 0.6560398936271667\n",
      "step: 31800 , time : 0.0\n",
      "train: loss: 19881.65625 acc: 0.992794394493103  val: loss: 1125554.125 acc: 0.7742616534233093\n",
      "step: 31805 , time : 0.0\n",
      "train: loss: 75688.7109375 acc: 0.9674858450889587  val: loss: 1326421.375 acc: 0.446907103061676\n",
      "step: 31810 , time : 0.0\n",
      "train: loss: 40822.66796875 acc: 0.9904758930206299  val: loss: 409687.0625 acc: 0.9339410066604614\n",
      "step: 31815 , time : 0.0\n",
      "train: loss: 42003.94921875 acc: 0.9821659922599792  val: loss: 1783659.625 acc: 0.6822800636291504\n",
      "step: 31820 , time : 0.0\n",
      "train: loss: 35489.7578125 acc: 0.9887751936912537  val: loss: 1982899.625 acc: 0.4630468487739563\n",
      "step: 31825 , time : 0.0\n",
      "train: loss: 67066.8125 acc: 0.9799087047576904  val: loss: 2182734.25 acc: 0.6848727464675903\n",
      "step: 31830 , time : 0.0\n",
      "train: loss: 58234.9140625 acc: 0.9783244729042053  val: loss: 501249.0625 acc: 0.8850466012954712\n",
      "step: 31835 , time : 0.0\n",
      "train: loss: 426676.3125 acc: 0.8828104138374329  val: loss: 1895383.375 acc: 0.6573393940925598\n",
      "step: 31840 , time : 0.0\n",
      "train: loss: 153509.90625 acc: 0.9718647003173828  val: loss: 3350129.0 acc: -0.5669234991073608\n",
      "step: 31845 , time : 0.0\n",
      "train: loss: 83383.6171875 acc: 0.9927269220352173  val: loss: 2884513.75 acc: 0.6253939867019653\n",
      "step: 31850 , time : 0.0\n",
      "train: loss: 30145.0625 acc: 0.9957866668701172  val: loss: 2364655.25 acc: 0.5733143091201782\n",
      "step: 31855 , time : 0.0\n",
      "train: loss: 223880.875 acc: 0.9760541915893555  val: loss: 311773.03125 acc: 0.8282802104949951\n",
      "step: 31860 , time : 0.0\n",
      "train: loss: 142366.8125 acc: 0.9785066246986389  val: loss: 2298707.5 acc: 0.7853615283966064\n",
      "step: 31865 , time : 0.0\n",
      "train: loss: 394132.3125 acc: 0.9755548238754272  val: loss: 712248.375 acc: 0.7199934720993042\n",
      "step: 31870 , time : 0.0010006427764892578\n",
      "train: loss: 345831.28125 acc: 0.9709010720252991  val: loss: 949255.125 acc: 0.6979860663414001\n",
      "step: 31875 , time : 0.0\n",
      "train: loss: 253907.78125 acc: 0.9553813934326172  val: loss: 1064167.125 acc: 0.8333652019500732\n",
      "step: 31880 , time : 0.0010004043579101562\n",
      "train: loss: 595594.25 acc: 0.9685705900192261  val: loss: 2919842.75 acc: -0.7290410995483398\n",
      "step: 31885 , time : 0.0030024051666259766\n",
      "train: loss: 904209.75 acc: 0.9699342250823975  val: loss: 689878.75 acc: 0.8233617544174194\n",
      "step: 31890 , time : 0.0\n",
      "train: loss: 1799190.625 acc: 0.9228816032409668  val: loss: 377130.8125 acc: 0.951546311378479\n",
      "step: 31895 , time : 0.0\n",
      "train: loss: 453789.15625 acc: 0.9757044911384583  val: loss: 678695.625 acc: 0.8986657857894897\n",
      "step: 31900 , time : 0.0\n",
      "train: loss: 835436.5625 acc: 0.9349173903465271  val: loss: 467436.8125 acc: 0.948494017124176\n",
      "step: 31905 , time : 0.0\n",
      "train: loss: 263372.9375 acc: 0.9586472511291504  val: loss: 1593435.375 acc: 0.5330054759979248\n",
      "step: 31910 , time : 0.0\n",
      "train: loss: 217787.21875 acc: 0.9577264189720154  val: loss: 1591546.0 acc: 0.5828673243522644\n",
      "step: 31915 , time : 0.0\n",
      "train: loss: 1673274.125 acc: 0.37004613876342773  val: loss: 791537.5625 acc: 0.8555123209953308\n",
      "step: 31920 , time : 0.0\n",
      "train: loss: 679274.5 acc: 0.6977635622024536  val: loss: 1354634.625 acc: 0.7819673418998718\n",
      "step: 31925 , time : 0.0\n",
      "train: loss: 539905.75 acc: 0.8229071497917175  val: loss: 563928.375 acc: 0.837501585483551\n",
      "step: 31930 , time : 0.0\n",
      "train: loss: 374059.03125 acc: 0.8146858811378479  val: loss: 1662653.125 acc: 0.779937744140625\n",
      "step: 31935 , time : 0.0\n",
      "train: loss: 583395.75 acc: 0.8515792489051819  val: loss: 2286102.75 acc: 0.5676661729812622\n",
      "step: 31940 , time : 0.0\n",
      "train: loss: 790152.3125 acc: 0.6454399824142456  val: loss: 2556733.75 acc: 0.7557173371315002\n",
      "step: 31945 , time : 0.0\n",
      "train: loss: 419296.40625 acc: 0.8034394979476929  val: loss: 2324519.75 acc: 0.634429395198822\n",
      "step: 31950 , time : 0.0\n",
      "train: loss: 586421.875 acc: 0.741104006767273  val: loss: 452574.46875 acc: 0.7322758436203003\n",
      "step: 31955 , time : 0.0\n",
      "train: loss: 121393.5 acc: 0.9262399077415466  val: loss: 3179932.75 acc: 0.3370763659477234\n",
      "step: 31960 , time : 0.0\n",
      "train: loss: 61585.25390625 acc: 0.9476031064987183  val: loss: 1539687.25 acc: 0.6970119476318359\n",
      "step: 31965 , time : 0.0\n",
      "train: loss: 97422.140625 acc: 0.916596531867981  val: loss: 1443499.0 acc: 0.6246872544288635\n",
      "step: 31970 , time : 0.0\n",
      "train: loss: 272018.6875 acc: 0.8517603278160095  val: loss: 230122.71875 acc: 0.8468819856643677\n",
      "step: 31975 , time : 0.0\n",
      "train: loss: 386364.4375 acc: 0.831339418888092  val: loss: 1194115.25 acc: 0.68650221824646\n",
      "step: 31980 , time : 0.0\n",
      "train: loss: 245420.984375 acc: 0.8516883850097656  val: loss: 1117091.625 acc: 0.6522156000137329\n",
      "step: 31985 , time : 0.0\n",
      "train: loss: 141353.703125 acc: 0.9127061367034912  val: loss: 2829474.75 acc: 0.631213903427124\n",
      "step: 31990 , time : 0.0010004043579101562\n",
      "train: loss: 125558.375 acc: 0.9014953374862671  val: loss: 995236.75 acc: 0.6485758423805237\n",
      "step: 31995 , time : 0.0\n",
      "train: loss: 115930.171875 acc: 0.8814499378204346  val: loss: 4924740.0 acc: 0.5385918021202087\n",
      "step: 32000 , time : 0.0\n",
      "train: loss: 380146.8125 acc: 0.8119457960128784  val: loss: 949234.4375 acc: 0.6857787370681763\n",
      "step: 32005 , time : 0.0\n",
      "train: loss: 339497.96875 acc: 0.8215494751930237  val: loss: 898089.875 acc: 0.6881903409957886\n",
      "step: 32010 , time : 0.0010006427764892578\n",
      "train: loss: 592929.25 acc: 0.717366099357605  val: loss: 853162.0625 acc: 0.7051180601119995\n",
      "step: 32015 , time : 0.0\n",
      "train: loss: 140673.21875 acc: 0.7907999753952026  val: loss: 1018995.8125 acc: 0.6866357326507568\n",
      "step: 32020 , time : 0.0010006427764892578\n",
      "train: loss: 165375.078125 acc: 0.8766241669654846  val: loss: 1241702.0 acc: 0.6652348041534424\n",
      "step: 32025 , time : 0.0\n",
      "train: loss: 206438.328125 acc: 0.855400562286377  val: loss: 2917586.25 acc: 0.5450996160507202\n",
      "step: 32030 , time : 0.0\n",
      "train: loss: 1602328.125 acc: 0.8051272630691528  val: loss: 520918.65625 acc: 0.7980979681015015\n",
      "step: 32035 , time : 0.0\n",
      "train: loss: 894589.125 acc: 0.8643300533294678  val: loss: 507727.9375 acc: 0.7955416440963745\n",
      "step: 32040 , time : 0.0\n",
      "train: loss: 313539.96875 acc: 0.9695321917533875  val: loss: 274974.1875 acc: 0.9079990983009338\n",
      "step: 32045 , time : 0.0\n",
      "train: loss: 913387.0625 acc: 0.8904381394386292  val: loss: 375214.3125 acc: 0.8616910576820374\n",
      "step: 32050 , time : 0.0\n",
      "train: loss: 160560.609375 acc: 0.9804810285568237  val: loss: 1510885.875 acc: 0.8796046376228333\n",
      "step: 32055 , time : 0.0\n",
      "train: loss: 128468.75 acc: 0.9841549396514893  val: loss: 454065.875 acc: 0.9406481385231018\n",
      "step: 32060 , time : 0.0\n",
      "train: loss: 55414.8515625 acc: 0.9951456785202026  val: loss: 708041.3125 acc: 0.8907727599143982\n",
      "step: 32065 , time : 0.0\n",
      "train: loss: 98537.6484375 acc: 0.992962658405304  val: loss: 1012842.8125 acc: 0.6959840059280396\n",
      "step: 32070 , time : 0.0\n",
      "train: loss: 66157.765625 acc: 0.9942317008972168  val: loss: 840126.5625 acc: 0.6040380001068115\n",
      "step: 32075 , time : 0.0\n",
      "train: loss: 37994.16015625 acc: 0.9948743581771851  val: loss: 433944.9375 acc: 0.9582096338272095\n",
      "step: 32080 , time : 0.0\n",
      "train: loss: 52301.4609375 acc: 0.9691179990768433  val: loss: 75798.015625 acc: 0.9827905893325806\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 32085 , time : 0.0\n",
      "train: loss: 13421.4931640625 acc: 0.9928050637245178  val: loss: 480327.84375 acc: 0.9591830968856812\n",
      "step: 32090 , time : 0.0\n",
      "train: loss: 26831.5625 acc: 0.9879462122917175  val: loss: 875446.375 acc: 0.7777243852615356\n",
      "step: 32095 , time : 0.0\n",
      "train: loss: 7334.14111328125 acc: 0.9855279326438904  val: loss: 105242.6640625 acc: 0.9769382476806641\n",
      "step: 32100 , time : 0.0\n",
      "train: loss: 16602.080078125 acc: 0.9831835627555847  val: loss: 779107.25 acc: 0.9363779425621033\n",
      "step: 32105 , time : 0.0\n",
      "train: loss: 32420.5625 acc: 0.9692044258117676  val: loss: 1138957.75 acc: 0.8697010278701782\n",
      "step: 32110 , time : 0.0010013580322265625\n",
      "train: loss: 7990.57275390625 acc: 0.9794220328330994  val: loss: 532739.8125 acc: 0.936905026435852\n",
      "step: 32115 , time : 0.0\n",
      "train: loss: 5371.32177734375 acc: 0.9765992760658264  val: loss: 355416.25 acc: 0.9594793915748596\n",
      "step: 32120 , time : 0.0\n",
      "train: loss: 8702.888671875 acc: 0.971876859664917  val: loss: 359911.9375 acc: 0.8527010083198547\n",
      "step: 32125 , time : 0.0\n",
      "train: loss: 26217.58203125 acc: 0.9840030670166016  val: loss: 1399976.875 acc: 0.8911023139953613\n",
      "step: 32130 , time : 0.0010004043579101562\n",
      "train: loss: 23456.3359375 acc: 0.9857980608940125  val: loss: 2388313.0 acc: 0.7326042652130127\n",
      "step: 32135 , time : 0.0\n",
      "train: loss: 34068.68359375 acc: 0.9712572693824768  val: loss: 2133694.5 acc: 0.6317383050918579\n",
      "step: 32140 , time : 0.0\n",
      "train: loss: 15167.0205078125 acc: 0.9793128371238708  val: loss: 776769.3125 acc: 0.9388630986213684\n",
      "step: 32145 , time : 0.0\n",
      "train: loss: 22199.0859375 acc: 0.9880928993225098  val: loss: 579301.25 acc: 0.8924487829208374\n",
      "step: 32150 , time : 0.015625715255737305\n",
      "train: loss: 16674.990234375 acc: 0.990784227848053  val: loss: 4383851.0 acc: 0.40410250425338745\n",
      "step: 32155 , time : 0.0\n",
      "train: loss: 21969.611328125 acc: 0.9587803483009338  val: loss: 1202289.875 acc: 0.32385849952697754\n",
      "step: 32160 , time : 0.0\n",
      "train: loss: 9901.7265625 acc: 0.986683189868927  val: loss: 3412361.75 acc: 0.19250178337097168\n",
      "step: 32165 , time : 0.0\n",
      "train: loss: 13175.9091796875 acc: 0.9949791431427002  val: loss: 231871.53125 acc: 0.9557286500930786\n",
      "step: 32170 , time : 0.0\n",
      "train: loss: 44608.1953125 acc: 0.9876139163970947  val: loss: 2077189.875 acc: 0.12562459707260132\n",
      "step: 32175 , time : 0.0\n",
      "train: loss: 24798.048828125 acc: 0.9938145875930786  val: loss: 803989.125 acc: 0.9129374623298645\n",
      "step: 32180 , time : 0.0\n",
      "train: loss: 20913.689453125 acc: 0.9922204613685608  val: loss: 3230166.5 acc: -0.09193539619445801\n",
      "step: 32185 , time : 0.0\n",
      "train: loss: 37668.28515625 acc: 0.9918912053108215  val: loss: 1619697.375 acc: 0.7847260236740112\n",
      "step: 32190 , time : 0.0\n",
      "train: loss: 134803.9375 acc: 0.9782975912094116  val: loss: 1257999.375 acc: 0.862174391746521\n",
      "step: 32195 , time : 0.0\n",
      "train: loss: 58926.015625 acc: 0.9586999416351318  val: loss: 1536750.375 acc: 0.4553530216217041\n",
      "step: 32200 , time : 0.001001119613647461\n",
      "train: loss: 641961.1875 acc: 0.6699836254119873  val: loss: 1142304.0 acc: 0.7365520000457764\n",
      "step: 32205 , time : 0.0\n",
      "train: loss: 199545.1875 acc: 0.9506648778915405  val: loss: 2345029.25 acc: 0.1886686086654663\n",
      "step: 32210 , time : 0.0010008811950683594\n",
      "train: loss: 284481.0 acc: 0.9711671471595764  val: loss: 824088.625 acc: 0.680745542049408\n",
      "step: 32215 , time : 0.0010004043579101562\n",
      "train: loss: 151789.046875 acc: 0.9799929261207581  val: loss: 4265542.5 acc: -0.08452117443084717\n",
      "step: 32220 , time : 0.0010006427764892578\n",
      "train: loss: 813570.0 acc: 0.9041343331336975  val: loss: 378677.4375 acc: 0.9492611885070801\n",
      "step: 32225 , time : 0.0010006427764892578\n",
      "train: loss: 197529.65625 acc: 0.9859784245491028  val: loss: 960883.4375 acc: 0.8703029155731201\n",
      "step: 32230 , time : 0.0\n",
      "train: loss: 819422.125 acc: 0.9274986982345581  val: loss: 481430.75 acc: 0.9597377181053162\n",
      "step: 32235 , time : 0.0\n",
      "train: loss: 269628.5625 acc: 0.9447265267372131  val: loss: 1608701.875 acc: 0.3414744734764099\n",
      "step: 32240 , time : 0.0\n",
      "train: loss: 348923.9375 acc: 0.9171463251113892  val: loss: 1376382.875 acc: 0.7340183258056641\n",
      "step: 32245 , time : 0.0010006427764892578\n",
      "train: loss: 1006325.375 acc: 0.9552211165428162  val: loss: 838977.375 acc: 0.7399099469184875\n",
      "step: 32250 , time : 0.0\n",
      "train: loss: 1234298.375 acc: 0.9626983404159546  val: loss: 827703.3125 acc: 0.80315101146698\n",
      "step: 32255 , time : 0.0\n",
      "train: loss: 1591463.125 acc: 0.9457095265388489  val: loss: 749889.375 acc: 0.9379070997238159\n",
      "step: 32260 , time : 0.0\n",
      "train: loss: 499555.9375 acc: 0.9693337082862854  val: loss: 498410.03125 acc: 0.801405668258667\n",
      "step: 32265 , time : 0.015625\n",
      "train: loss: 1272569.25 acc: 0.931851327419281  val: loss: 719319.25 acc: 0.8993205428123474\n",
      "step: 32270 , time : 0.0\n",
      "train: loss: 485703.9375 acc: 0.9382973909378052  val: loss: 1204503.75 acc: 0.8719712495803833\n",
      "step: 32275 , time : 0.0\n",
      "train: loss: 414811.0 acc: 0.9140959978103638  val: loss: 123575.2265625 acc: 0.970005214214325\n",
      "step: 32280 , time : 0.0\n",
      "train: loss: 237742.4375 acc: 0.9398583769798279  val: loss: 833336.375 acc: 0.8379260897636414\n",
      "step: 32285 , time : 0.0009996891021728516\n",
      "train: loss: 500147.21875 acc: 0.7150780558586121  val: loss: 1568309.625 acc: 0.7513101100921631\n",
      "step: 32290 , time : 0.0\n",
      "train: loss: 910831.75 acc: 0.7200931310653687  val: loss: 1439141.875 acc: 0.8062849044799805\n",
      "step: 32295 , time : 0.0\n",
      "train: loss: 824113.0625 acc: 0.8047281503677368  val: loss: 2368123.5 acc: 0.8403496742248535\n",
      "step: 32300 , time : 0.0010006427764892578\n",
      "train: loss: 1518712.375 acc: 0.7974420785903931  val: loss: 488772.21875 acc: 0.8127739429473877\n",
      "step: 32305 , time : 0.0010004043579101562\n",
      "train: loss: 1165293.625 acc: 0.49889540672302246  val: loss: 1114628.625 acc: 0.8099993467330933\n",
      "step: 32310 , time : 0.0\n",
      "train: loss: 664995.75 acc: 0.7376759052276611  val: loss: 1432438.0 acc: 0.6403316259384155\n",
      "step: 32315 , time : 0.0010006427764892578\n",
      "train: loss: 217975.796875 acc: 0.8544158935546875  val: loss: 1525191.625 acc: 0.6496751308441162\n",
      "step: 32320 , time : 0.0\n",
      "train: loss: 136695.96875 acc: 0.8783275485038757  val: loss: 3211353.75 acc: 0.47738784551620483\n",
      "step: 32325 , time : 0.0\n",
      "train: loss: 53044.34375 acc: 0.9556220769882202  val: loss: 4030143.25 acc: 0.5212987661361694\n",
      "step: 32330 , time : 0.0\n",
      "train: loss: 96792.671875 acc: 0.9363961219787598  val: loss: 149690.25 acc: 0.8654088973999023\n",
      "step: 32335 , time : 0.0010006427764892578\n",
      "train: loss: 29241.916015625 acc: 0.9772554636001587  val: loss: 798969.75 acc: 0.6657609343528748\n",
      "step: 32340 , time : 0.0010006427764892578\n",
      "train: loss: 167532.78125 acc: 0.8874579668045044  val: loss: 642604.8125 acc: 0.7439205646514893\n",
      "step: 32345 , time : 0.0010006427764892578\n",
      "train: loss: 164854.375 acc: 0.8902256488800049  val: loss: 2348501.0 acc: 0.6035106182098389\n",
      "step: 32350 , time : 0.0010008811950683594\n",
      "train: loss: 417492.65625 acc: 0.7807348370552063  val: loss: 3413317.5 acc: 0.5355478525161743\n",
      "step: 32355 , time : 0.0\n",
      "train: loss: 204811.84375 acc: 0.8596029281616211  val: loss: 1428603.375 acc: 0.6767798066139221\n",
      "step: 32360 , time : 0.0\n",
      "train: loss: 61851.08984375 acc: 0.9375007748603821  val: loss: 594945.1875 acc: 0.7319827079772949\n",
      "step: 32365 , time : 0.0010006427764892578\n",
      "train: loss: 105995.71875 acc: 0.9141803979873657  val: loss: 141198.375 acc: 0.880446195602417\n",
      "step: 32370 , time : 0.0\n",
      "train: loss: 682935.875 acc: 0.7336605787277222  val: loss: 2106796.5 acc: 0.6543555855751038\n",
      "step: 32375 , time : 0.0\n",
      "train: loss: 281786.84375 acc: 0.844245433807373  val: loss: 1406713.875 acc: 0.5921652913093567\n",
      "step: 32380 , time : 0.0\n",
      "train: loss: 738893.875 acc: 0.7453913688659668  val: loss: 686384.8125 acc: 0.7486450672149658\n",
      "step: 32385 , time : 0.001001119613647461\n",
      "train: loss: 105205.6953125 acc: 0.8763971328735352  val: loss: 1620906.875 acc: 0.6015498638153076\n",
      "step: 32390 , time : 0.0\n",
      "train: loss: 779811.25 acc: 0.7480400204658508  val: loss: 750668.0 acc: 0.7854483723640442\n",
      "step: 32395 , time : 0.0010008811950683594\n",
      "train: loss: 1893290.25 acc: 0.8175057768821716  val: loss: 373229.6875 acc: 0.8703808784484863\n",
      "step: 32400 , time : 0.0010004043579101562\n",
      "train: loss: 915604.5625 acc: 0.6350569128990173  val: loss: 870315.3125 acc: 0.8828681111335754\n",
      "step: 32405 , time : 0.0010008811950683594\n",
      "train: loss: 654439.125 acc: 0.9435105323791504  val: loss: 1300222.375 acc: 0.6041673421859741\n",
      "step: 32410 , time : 0.0010008811950683594\n",
      "train: loss: 1295394.5 acc: 0.8679569959640503  val: loss: 216734.765625 acc: 0.9642766714096069\n",
      "step: 32415 , time : 0.0\n",
      "train: loss: 193746.484375 acc: 0.9786136150360107  val: loss: 1091607.125 acc: 0.8619868755340576\n",
      "step: 32420 , time : 0.0010008811950683594\n",
      "train: loss: 50999.21875 acc: 0.9784443974494934  val: loss: 1486261.625 acc: 0.777229368686676\n",
      "step: 32425 , time : 0.0010004043579101562\n",
      "train: loss: 81051.6484375 acc: 0.9922380447387695  val: loss: 341352.0 acc: 0.9691777229309082\n",
      "step: 32430 , time : 0.0\n",
      "train: loss: 678766.25 acc: 0.9473513960838318  val: loss: 2691408.75 acc: 0.49526649713516235\n",
      "step: 32435 , time : 0.0010004043579101562\n",
      "train: loss: 671716.3125 acc: 0.9430088400840759  val: loss: 451304.9375 acc: 0.936631441116333\n",
      "step: 32440 , time : 0.0010006427764892578\n",
      "train: loss: 105243.203125 acc: 0.9876774549484253  val: loss: 685917.25 acc: 0.8561098575592041\n",
      "step: 32445 , time : 0.0\n",
      "train: loss: 41564.1953125 acc: 0.989534318447113  val: loss: 427138.9375 acc: 0.9439283013343811\n",
      "step: 32450 , time : 0.0\n",
      "train: loss: 42715.9140625 acc: 0.9893873929977417  val: loss: 612639.375 acc: 0.9259896278381348\n",
      "step: 32455 , time : 0.0010004043579101562\n",
      "train: loss: 12622.224609375 acc: 0.9903851747512817  val: loss: 622354.875 acc: 0.8680631518363953\n",
      "step: 32460 , time : 0.0\n",
      "train: loss: 46414.9921875 acc: 0.9816755056381226  val: loss: 1331048.875 acc: 0.7104718685150146\n",
      "step: 32465 , time : 0.0\n",
      "train: loss: 7299.09033203125 acc: 0.9709883332252502  val: loss: 539626.1875 acc: 0.8369485139846802\n",
      "step: 32470 , time : 0.0010008811950683594\n",
      "train: loss: 24933.642578125 acc: 0.982094943523407  val: loss: 2025429.75 acc: 0.758331298828125\n",
      "step: 32475 , time : 0.0\n",
      "train: loss: 8551.873046875 acc: 0.9855750203132629  val: loss: 1333683.125 acc: 0.8324700593948364\n",
      "step: 32480 , time : 0.001001119613647461\n",
      "train: loss: 17615.513671875 acc: 0.9652964472770691  val: loss: 1861883.875 acc: 0.7110223770141602\n",
      "step: 32485 , time : 0.0010008811950683594\n",
      "train: loss: 10466.7421875 acc: 0.9642980694770813  val: loss: 2361272.0 acc: 0.5689422488212585\n",
      "step: 32490 , time : 0.0\n",
      "train: loss: 13542.2041015625 acc: 0.985617995262146  val: loss: 1865534.5 acc: 0.2997315526008606\n",
      "step: 32495 , time : 0.0010004043579101562\n",
      "train: loss: 13797.501953125 acc: 0.9754884243011475  val: loss: 1970870.875 acc: 0.7858549952507019\n",
      "step: 32500 , time : 0.0010006427764892578\n",
      "train: loss: 35114.10546875 acc: 0.9808228611946106  val: loss: 3127310.75 acc: -1.0175056457519531\n",
      "step: 32505 , time : 0.0\n",
      "train: loss: 26113.4765625 acc: 0.9820483922958374  val: loss: 753150.6875 acc: 0.6813555359840393\n",
      "step: 32510 , time : 0.0010008811950683594\n",
      "train: loss: 23593.1484375 acc: 0.9881489872932434  val: loss: 3366770.5 acc: 0.5591703653335571\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 32515 , time : 0.0\n",
      "train: loss: 111302.171875 acc: 0.9187209606170654  val: loss: 1897007.5 acc: 0.7096062898635864\n",
      "step: 32520 , time : 0.0010008811950683594\n",
      "train: loss: 16552.787109375 acc: 0.9880197048187256  val: loss: 2580504.0 acc: 0.4905012845993042\n",
      "step: 32525 , time : 0.0\n",
      "train: loss: 16734.904296875 acc: 0.9719180464744568  val: loss: 2417957.5 acc: 0.7840517163276672\n",
      "step: 32530 , time : 0.001001119613647461\n",
      "train: loss: 12945.5556640625 acc: 0.9930738806724548  val: loss: 4333695.5 acc: 0.11637789011001587\n",
      "step: 32535 , time : 0.0\n",
      "train: loss: 43119.671875 acc: 0.9891034364700317  val: loss: 2505504.5 acc: 0.5398225784301758\n",
      "step: 32540 , time : 0.0010004043579101562\n",
      "train: loss: 28576.66015625 acc: 0.9867221713066101  val: loss: 3472659.75 acc: 0.21463167667388916\n",
      "step: 32545 , time : 0.0\n",
      "train: loss: 51967.484375 acc: 0.9816334247589111  val: loss: 1693731.625 acc: 0.5401141047477722\n",
      "step: 32550 , time : 0.0010006427764892578\n",
      "train: loss: 32314.005859375 acc: 0.9879679083824158  val: loss: 2493427.25 acc: 0.6948069334030151\n",
      "step: 32555 , time : 0.0010006427764892578\n",
      "train: loss: 85717.3515625 acc: 0.9793853759765625  val: loss: 762374.8125 acc: 0.6929044723510742\n",
      "step: 32560 , time : 0.0\n",
      "train: loss: 266988.53125 acc: 0.8048900961875916  val: loss: 2312773.5 acc: 0.41553884744644165\n",
      "step: 32565 , time : 0.0\n",
      "train: loss: 486694.15625 acc: -0.13118517398834229  val: loss: 1675570.625 acc: 0.636975884437561\n",
      "step: 32570 , time : 0.0\n",
      "train: loss: 672965.3125 acc: 0.9025050401687622  val: loss: 447861.875 acc: 0.9374486207962036\n",
      "step: 32575 , time : 0.0\n",
      "train: loss: 301848.9375 acc: 0.954634964466095  val: loss: 270121.21875 acc: 0.9397486448287964\n",
      "step: 32580 , time : 0.0010004043579101562\n",
      "train: loss: 56263.88671875 acc: 0.9930271506309509  val: loss: 599000.9375 acc: 0.7952703237533569\n",
      "step: 32585 , time : 0.0010006427764892578\n",
      "train: loss: 226895.84375 acc: 0.9636947512626648  val: loss: 682690.6875 acc: 0.9069139361381531\n",
      "step: 32590 , time : 0.0\n",
      "train: loss: 292268.75 acc: 0.9586070775985718  val: loss: 3036176.75 acc: 0.2627463936805725\n",
      "step: 32595 , time : 0.0\n",
      "train: loss: 498636.5 acc: 0.9686336517333984  val: loss: 659354.625 acc: 0.836034893989563\n",
      "step: 32600 , time : 0.0010008811950683594\n",
      "train: loss: 329836.53125 acc: 0.9733014106750488  val: loss: 643527.875 acc: 0.9028277397155762\n",
      "step: 32605 , time : 0.0\n",
      "train: loss: 211675.984375 acc: 0.9723573327064514  val: loss: 2028017.125 acc: 0.7158252000808716\n",
      "step: 32610 , time : 0.0\n",
      "train: loss: 674346.3125 acc: 0.969565212726593  val: loss: 2926099.75 acc: 0.16401207447052002\n",
      "step: 32615 , time : 0.0010004043579101562\n",
      "train: loss: 1861045.625 acc: 0.9354874491691589  val: loss: 1451842.625 acc: 0.3988185524940491\n",
      "step: 32620 , time : 0.0\n",
      "train: loss: 1794387.125 acc: 0.9281162619590759  val: loss: 737223.75 acc: 0.9155420064926147\n",
      "step: 32625 , time : 0.0010008811950683594\n",
      "train: loss: 1446955.625 acc: 0.9388235211372375  val: loss: 884353.5625 acc: 0.4982364773750305\n",
      "step: 32630 , time : 0.0\n",
      "train: loss: 278550.0 acc: 0.9710226655006409  val: loss: 1080173.5 acc: 0.6465083360671997\n",
      "step: 32635 , time : 0.0\n",
      "train: loss: 345228.375 acc: 0.9562565684318542  val: loss: 584420.375 acc: 0.8409367799758911\n",
      "step: 32640 , time : 0.0010013580322265625\n",
      "train: loss: 263713.53125 acc: 0.9509589076042175  val: loss: 560217.75 acc: 0.8950486183166504\n",
      "step: 32645 , time : 0.0\n",
      "train: loss: 901795.75 acc: 0.8581397533416748  val: loss: 57401.26171875 acc: 0.9836861491203308\n",
      "step: 32650 , time : 0.0\n",
      "train: loss: 1269620.875 acc: 0.1431780457496643  val: loss: 537525.625 acc: 0.6535502672195435\n",
      "step: 32655 , time : 0.0\n",
      "train: loss: 956429.375 acc: 0.8302443027496338  val: loss: 719072.6875 acc: 0.6708096861839294\n",
      "step: 32660 , time : 0.0010006427764892578\n",
      "train: loss: 488898.71875 acc: 0.8214013576507568  val: loss: 2495986.75 acc: 0.6693817377090454\n",
      "step: 32665 , time : 0.0\n",
      "train: loss: 640117.0 acc: 0.8758098483085632  val: loss: 1362063.5 acc: 0.8177404403686523\n",
      "step: 32670 , time : 0.0\n",
      "train: loss: 1185490.375 acc: -0.05627596378326416  val: loss: 322753.09375 acc: 0.8076774477958679\n",
      "step: 32675 , time : 0.0\n",
      "train: loss: 851771.125 acc: 0.6765021085739136  val: loss: 1246375.375 acc: 0.6348336338996887\n",
      "step: 32680 , time : 0.0\n",
      "train: loss: 292264.4375 acc: 0.8350463509559631  val: loss: 832609.0 acc: 0.7298784852027893\n",
      "step: 32685 , time : 0.0\n",
      "train: loss: 590818.9375 acc: 0.7498500943183899  val: loss: 538674.875 acc: 0.7152292728424072\n",
      "step: 32690 , time : 0.0\n",
      "train: loss: 182051.15625 acc: 0.8549639582633972  val: loss: 930877.25 acc: 0.7321330308914185\n",
      "step: 32695 , time : 0.0010008811950683594\n",
      "train: loss: 41140.08984375 acc: 0.9661117792129517  val: loss: 1688703.25 acc: 0.5908668637275696\n",
      "step: 32700 , time : 0.0\n",
      "train: loss: 300166.3125 acc: 0.8492736220359802  val: loss: 1954424.375 acc: 0.6340343356132507\n",
      "step: 32705 , time : 0.0\n",
      "train: loss: 56897.12109375 acc: 0.9555830955505371  val: loss: 1215501.875 acc: 0.6323944330215454\n",
      "step: 32710 , time : 0.0010004043579101562\n",
      "train: loss: 71028.453125 acc: 0.9500093460083008  val: loss: 2045480.5 acc: 0.6059408187866211\n",
      "step: 32715 , time : 0.0\n",
      "train: loss: 42555.984375 acc: 0.9666045308113098  val: loss: 706763.1875 acc: 0.65059894323349\n",
      "step: 32720 , time : 0.0\n",
      "train: loss: 41543.328125 acc: 0.9564080238342285  val: loss: 3331676.5 acc: 0.5439964532852173\n",
      "step: 32725 , time : 0.0010004043579101562\n",
      "train: loss: 94112.546875 acc: 0.8617185354232788  val: loss: 3812383.0 acc: 0.4999053478240967\n",
      "step: 32730 , time : 0.0010006427764892578\n",
      "train: loss: 421726.0625 acc: 0.8036158084869385  val: loss: 3067781.25 acc: 0.6287841200828552\n",
      "step: 32735 , time : 0.0\n",
      "train: loss: 41491.015625 acc: 0.9569733142852783  val: loss: 1469366.875 acc: 0.5280965566635132\n",
      "step: 32740 , time : 0.0\n",
      "train: loss: 850911.5 acc: 0.7048339247703552  val: loss: 3233379.5 acc: 0.5800933837890625\n",
      "step: 32745 , time : 0.0\n",
      "train: loss: 136919.5 acc: 0.8928946256637573  val: loss: 2362442.5 acc: 0.6292040944099426\n",
      "step: 32750 , time : 0.0010006427764892578\n",
      "train: loss: 570619.5625 acc: 0.7539577484130859  val: loss: 1347849.75 acc: 0.7316164970397949\n",
      "step: 32755 , time : 0.0\n",
      "train: loss: 865054.25 acc: 0.6759876012802124  val: loss: 4236498.5 acc: 0.5232619047164917\n",
      "step: 32760 , time : 0.0010008811950683594\n",
      "train: loss: 1276053.125 acc: 0.8120594024658203  val: loss: 881043.75 acc: 0.8314467668533325\n",
      "step: 32765 , time : 0.0\n",
      "train: loss: 804957.0 acc: 0.8867626190185547  val: loss: 438352.0625 acc: 0.8867068886756897\n",
      "step: 32770 , time : 0.0010001659393310547\n",
      "train: loss: 551540.375 acc: 0.9516846537590027  val: loss: 219829.453125 acc: 0.9632472395896912\n",
      "step: 32775 , time : 0.0010004043579101562\n",
      "train: loss: 262552.09375 acc: 0.9754711389541626  val: loss: 980136.9375 acc: 0.9115944504737854\n",
      "step: 32780 , time : 0.0\n",
      "train: loss: 387403.8125 acc: 0.9556803703308105  val: loss: 1530344.0 acc: 0.5196413993835449\n",
      "step: 32785 , time : 0.001001119613647461\n",
      "train: loss: 40638.734375 acc: 0.989189863204956  val: loss: 240661.390625 acc: 0.9711432456970215\n",
      "step: 32790 , time : 0.0\n",
      "train: loss: 281442.65625 acc: 0.967326819896698  val: loss: 1962480.5 acc: 0.6762741804122925\n",
      "step: 32795 , time : 0.0\n",
      "train: loss: 60736.17578125 acc: 0.9964033365249634  val: loss: 1317247.375 acc: 0.7895259261131287\n",
      "step: 32800 , time : 0.0010008811950683594\n",
      "train: loss: 90046.7109375 acc: 0.9931480288505554  val: loss: 1436163.0 acc: 0.8597999811172485\n",
      "step: 32805 , time : 0.0\n",
      "train: loss: 133129.359375 acc: 0.9877393841743469  val: loss: 2376807.75 acc: 0.6932792663574219\n",
      "step: 32810 , time : 0.0\n",
      "train: loss: 51642.77734375 acc: 0.9859007596969604  val: loss: 843383.1875 acc: 0.7482086420059204\n",
      "step: 32815 , time : 0.0010006427764892578\n",
      "train: loss: 14393.5791015625 acc: 0.9881531000137329  val: loss: 1711295.875 acc: 0.67747962474823\n",
      "step: 32820 , time : 0.0010008811950683594\n",
      "train: loss: 13834.4658203125 acc: 0.9960471391677856  val: loss: 893760.4375 acc: 0.9180023670196533\n",
      "step: 32825 , time : 0.0\n",
      "train: loss: 20924.86328125 acc: 0.9845799207687378  val: loss: 2507259.5 acc: 0.5427920818328857\n",
      "step: 32830 , time : 0.0010006427764892578\n",
      "train: loss: 8126.52978515625 acc: 0.9903241395950317  val: loss: 1688967.125 acc: 0.5772660374641418\n",
      "step: 32835 , time : 0.0010006427764892578\n",
      "train: loss: 8343.1474609375 acc: 0.9819075465202332  val: loss: 3069062.75 acc: 0.5871552228927612\n",
      "step: 32840 , time : 0.0\n",
      "train: loss: 15241.005859375 acc: 0.9565691947937012  val: loss: 2726799.5 acc: 0.21568238735198975\n",
      "step: 32845 , time : 0.0\n",
      "train: loss: 32936.109375 acc: 0.9692872762680054  val: loss: 1419076.875 acc: 0.8519347906112671\n",
      "step: 32850 , time : 0.0\n",
      "train: loss: 7930.27099609375 acc: 0.978462278842926  val: loss: 1459812.375 acc: 0.8343762159347534\n",
      "step: 32855 , time : 0.0\n",
      "train: loss: 20882.013671875 acc: 0.9731225967407227  val: loss: 1368013.5 acc: 0.42377185821533203\n",
      "step: 32860 , time : 0.0\n",
      "train: loss: 14598.2119140625 acc: 0.9821882247924805  val: loss: 526887.125 acc: 0.8761710524559021\n",
      "step: 32865 , time : 0.0\n",
      "train: loss: 38831.25390625 acc: 0.9832155704498291  val: loss: 1766120.5 acc: 0.7298542261123657\n",
      "step: 32870 , time : 0.0\n",
      "train: loss: 29545.638671875 acc: 0.9712390899658203  val: loss: 2989405.25 acc: 0.5531489849090576\n",
      "step: 32875 , time : 0.0010006427764892578\n",
      "train: loss: 21194.91796875 acc: 0.9852848649024963  val: loss: 1381871.625 acc: -0.16117143630981445\n",
      "step: 32880 , time : 0.0\n",
      "train: loss: 22347.09765625 acc: 0.9890838265419006  val: loss: 1327393.0 acc: 0.7562706470489502\n",
      "step: 32885 , time : 0.0\n",
      "train: loss: 23344.70703125 acc: 0.9762300848960876  val: loss: 1133605.125 acc: 0.7821282148361206\n",
      "step: 32890 , time : 0.0\n",
      "train: loss: 9402.0009765625 acc: 0.9916461706161499  val: loss: 611257.4375 acc: 0.9159444570541382\n",
      "step: 32895 , time : 0.0010006427764892578\n",
      "train: loss: 16663.166015625 acc: 0.9940346479415894  val: loss: 603289.3125 acc: 0.4016151428222656\n",
      "step: 32900 , time : 0.0010008811950683594\n",
      "train: loss: 33556.484375 acc: 0.9931249022483826  val: loss: 1867167.0 acc: 0.7659721374511719\n",
      "step: 32905 , time : 0.0\n",
      "train: loss: 32790.43359375 acc: 0.9893197417259216  val: loss: 1046137.5625 acc: 0.805031418800354\n",
      "step: 32910 , time : 0.0\n",
      "train: loss: 33682.12890625 acc: 0.9867377877235413  val: loss: 740351.875 acc: 0.7435581684112549\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 32915 , time : 0.0010008811950683594\n",
      "train: loss: 38791.42578125 acc: 0.9548430442810059  val: loss: 4640355.5 acc: 0.32593077421188354\n",
      "step: 32920 , time : 0.0\n",
      "train: loss: 123534.2421875 acc: 0.9640440344810486  val: loss: 1893980.0 acc: 0.010332107543945312\n",
      "step: 32925 , time : 0.0\n",
      "train: loss: 234537.875 acc: 0.9289854168891907  val: loss: 579769.4375 acc: 0.8399901986122131\n",
      "step: 32930 , time : 0.0\n",
      "train: loss: 39785.78515625 acc: 0.9605780839920044  val: loss: 1187726.75 acc: 0.7231453657150269\n",
      "step: 32935 , time : 0.0010008811950683594\n",
      "train: loss: 100398.1484375 acc: 0.984604001045227  val: loss: 1269383.125 acc: 0.7990577816963196\n",
      "step: 32940 , time : 0.0010006427764892578\n",
      "train: loss: 399216.71875 acc: 0.9467135667800903  val: loss: 2017815.375 acc: 0.6079679727554321\n",
      "step: 32945 , time : 0.0\n",
      "train: loss: 81867.875 acc: 0.9923630356788635  val: loss: 945189.125 acc: 0.8854915499687195\n",
      "step: 32950 , time : 0.0\n",
      "train: loss: 43917.19140625 acc: 0.992340087890625  val: loss: 2377872.5 acc: 0.010709941387176514\n",
      "step: 32955 , time : 0.0\n",
      "train: loss: 163004.65625 acc: 0.9836758375167847  val: loss: 452127.59375 acc: 0.8638365268707275\n",
      "step: 32960 , time : 0.0\n",
      "train: loss: 781076.0625 acc: 0.9578753709793091  val: loss: 759085.9375 acc: 0.7972495555877686\n",
      "step: 32965 , time : 0.0\n",
      "train: loss: 379756.03125 acc: 0.9335216283798218  val: loss: 1366041.125 acc: 0.5221247673034668\n",
      "step: 32970 , time : 0.0\n",
      "train: loss: 399273.84375 acc: 0.9646039605140686  val: loss: 859141.9375 acc: 0.6179869771003723\n",
      "step: 32975 , time : 0.0\n",
      "train: loss: 1965000.875 acc: 0.9000769257545471  val: loss: 895370.5 acc: 0.7825447916984558\n",
      "step: 32980 , time : 0.0\n",
      "train: loss: 537292.5625 acc: 0.9764347672462463  val: loss: 800736.4375 acc: 0.7195878028869629\n",
      "step: 32985 , time : 0.0\n",
      "train: loss: 1092095.875 acc: 0.9638761878013611  val: loss: 575625.125 acc: 0.8431575298309326\n",
      "step: 32990 , time : 0.0\n",
      "train: loss: 867295.625 acc: 0.951691210269928  val: loss: 290595.375 acc: 0.9297910928726196\n",
      "step: 32995 , time : 0.0\n",
      "train: loss: 535015.625 acc: 0.9669208526611328  val: loss: 813904.6875 acc: 0.8596557378768921\n",
      "step: 33000 , time : 0.0\n",
      "train: loss: 380700.65625 acc: 0.9597393274307251  val: loss: 189122.65625 acc: 0.9550483822822571\n",
      "step: 33005 , time : 0.0\n",
      "train: loss: 196439.390625 acc: 0.9158858060836792  val: loss: 759794.75 acc: 0.7936863899230957\n",
      "step: 33010 , time : 0.0\n",
      "train: loss: 486941.625 acc: 0.9284860491752625  val: loss: 52475.7578125 acc: 0.939238965511322\n",
      "step: 33015 , time : 0.0\n",
      "train: loss: 1595532.0 acc: 0.4010620713233948  val: loss: 476307.75 acc: 0.8480944633483887\n",
      "step: 33020 , time : 0.0\n",
      "train: loss: 1772403.125 acc: 0.7552311420440674  val: loss: 2347761.0 acc: 0.7950631380081177\n",
      "step: 33025 , time : 0.0010004043579101562\n",
      "train: loss: 928623.25 acc: 0.8648043870925903  val: loss: 418318.1875 acc: 0.8747959136962891\n",
      "step: 33030 , time : 0.0\n",
      "train: loss: 199161.96875 acc: 0.9288239479064941  val: loss: 941204.1875 acc: 0.8671976327896118\n",
      "step: 33035 , time : 0.0010004043579101562\n",
      "train: loss: 931681.625 acc: 0.6163302659988403  val: loss: 650154.375 acc: 0.9123455882072449\n",
      "step: 33040 , time : 0.0010006427764892578\n",
      "train: loss: 926784.875 acc: 0.3002063035964966  val: loss: 2252587.0 acc: 0.7241222858428955\n",
      "step: 33045 , time : 0.0010004043579101562\n",
      "train: loss: 1017230.0 acc: 0.668396532535553  val: loss: 1548840.5 acc: 0.6730664968490601\n",
      "step: 33050 , time : 0.0010020732879638672\n",
      "train: loss: 68989.140625 acc: 0.9480276107788086  val: loss: 6987672.0 acc: 0.5371863842010498\n",
      "step: 33055 , time : 0.0\n",
      "train: loss: 66481.2109375 acc: 0.9504192471504211  val: loss: 3456269.5 acc: 0.5629488229751587\n",
      "step: 33060 , time : 0.0\n",
      "train: loss: 66686.5703125 acc: 0.9510658979415894  val: loss: 3257331.75 acc: 0.5637377500534058\n",
      "step: 33065 , time : 0.0\n",
      "train: loss: 18092.865234375 acc: 0.9864354729652405  val: loss: 3521664.75 acc: 0.5223482251167297\n",
      "step: 33070 , time : 0.0\n",
      "train: loss: 212446.140625 acc: 0.8711889386177063  val: loss: 2064437.0 acc: 0.633391797542572\n",
      "step: 33075 , time : 0.0\n",
      "train: loss: 375916.1875 acc: 0.8336309194564819  val: loss: 4669250.5 acc: 0.5284363031387329\n",
      "step: 33080 , time : 0.0010004043579101562\n",
      "train: loss: 305309.375 acc: 0.7976402640342712  val: loss: 3405835.25 acc: 0.5377857089042664\n",
      "step: 33085 , time : 0.0010004043579101562\n",
      "train: loss: 22410.044921875 acc: 0.9805285930633545  val: loss: 214739.46875 acc: 0.8757718801498413\n",
      "step: 33090 , time : 0.0\n",
      "train: loss: 161739.65625 acc: 0.8143486976623535  val: loss: 1323816.875 acc: 0.6837721467018127\n",
      "step: 33095 , time : 0.0\n",
      "train: loss: 69330.359375 acc: 0.9324247241020203  val: loss: 1998432.875 acc: 0.6470961570739746\n",
      "step: 33100 , time : 0.0010008811950683594\n",
      "train: loss: 341026.625 acc: 0.8146174550056458  val: loss: 1789196.125 acc: 0.6603507995605469\n",
      "step: 33105 , time : 0.0\n",
      "train: loss: 375437.9375 acc: 0.7835227251052856  val: loss: 3060505.0 acc: 0.5445858240127563\n",
      "step: 33110 , time : 0.0\n",
      "train: loss: 259034.875 acc: 0.8201212286949158  val: loss: 1704648.375 acc: 0.6069921255111694\n",
      "step: 33115 , time : 0.0010006427764892578\n",
      "train: loss: 95604.4296875 acc: 0.9024910926818848  val: loss: 3082375.75 acc: 0.5594046115875244\n",
      "step: 33120 , time : 0.0\n",
      "train: loss: 55393.72265625 acc: 0.936313271522522  val: loss: 1390625.75 acc: 0.6478144526481628\n",
      "step: 33125 , time : 0.0\n",
      "train: loss: 2822654.75 acc: 0.6683468222618103  val: loss: 2103750.5 acc: 0.7013943195343018\n",
      "step: 33130 , time : 0.0010004043579101562\n",
      "train: loss: 819730.0625 acc: 0.8803263902664185  val: loss: 1429689.875 acc: 0.8089577555656433\n",
      "step: 33135 , time : 0.0010006427764892578\n",
      "train: loss: 779181.125 acc: 0.8892222046852112  val: loss: 513448.78125 acc: 0.9140477180480957\n",
      "step: 33140 , time : 0.0010006427764892578\n",
      "train: loss: 245233.53125 acc: 0.9779332280158997  val: loss: 590317.375 acc: 0.8773958683013916\n",
      "step: 33145 , time : 0.0010008811950683594\n",
      "train: loss: 100986.234375 acc: 0.9784319996833801  val: loss: 2056032.625 acc: 0.7094224691390991\n",
      "step: 33150 , time : 0.0010006427764892578\n",
      "train: loss: 146395.3125 acc: 0.9754515886306763  val: loss: 2433563.75 acc: 0.5080276131629944\n",
      "step: 33155 , time : 0.0010008811950683594\n",
      "train: loss: 75321.203125 acc: 0.9911428689956665  val: loss: 776316.8125 acc: 0.7288187146186829\n",
      "step: 33160 , time : 0.0010006427764892578\n",
      "train: loss: 83471.4921875 acc: 0.9944174885749817  val: loss: 674207.8125 acc: 0.816598117351532\n",
      "step: 33165 , time : 0.0\n",
      "train: loss: 77262.015625 acc: 0.9953078031539917  val: loss: 1296155.375 acc: 0.8108938932418823\n",
      "step: 33170 , time : 0.0010006427764892578\n",
      "train: loss: 71912.234375 acc: 0.9871654510498047  val: loss: 1949459.25 acc: 0.23796719312667847\n",
      "step: 33175 , time : 0.0\n",
      "train: loss: 86234.1171875 acc: 0.9830282926559448  val: loss: 836854.9375 acc: 0.6380537152290344\n",
      "step: 33180 , time : 0.0010008811950683594\n",
      "train: loss: 18263.26171875 acc: 0.9896318912506104  val: loss: 597989.5 acc: 0.6590683460235596\n",
      "step: 33185 , time : 0.0010001659393310547\n",
      "train: loss: 17302.09375 acc: 0.9958728551864624  val: loss: 1490882.375 acc: 0.6453944444656372\n",
      "step: 33190 , time : 0.0\n",
      "train: loss: 6111.408203125 acc: 0.970252275466919  val: loss: 1141059.625 acc: 0.8769074082374573\n",
      "step: 33195 , time : 0.001001119613647461\n",
      "train: loss: 17048.59765625 acc: 0.9826731085777283  val: loss: 741893.3125 acc: 0.8953655362129211\n",
      "step: 33200 , time : 0.0\n",
      "train: loss: 17370.17578125 acc: 0.9864841103553772  val: loss: 1771053.875 acc: 0.6825374960899353\n",
      "step: 33205 , time : 0.0\n",
      "train: loss: 3732.941162109375 acc: 0.992393970489502  val: loss: 2217117.5 acc: 0.8257504105567932\n",
      "step: 33210 , time : 0.0\n",
      "train: loss: 14719.6943359375 acc: 0.9571067094802856  val: loss: 428621.0625 acc: 0.9492417573928833\n",
      "step: 33215 , time : 0.0\n",
      "train: loss: 30513.654296875 acc: 0.91241455078125  val: loss: 819062.1875 acc: 0.7918445467948914\n",
      "step: 33220 , time : 0.0\n",
      "train: loss: 9583.0224609375 acc: 0.9848374128341675  val: loss: 915062.1875 acc: 0.8085989952087402\n",
      "step: 33225 , time : 0.0\n",
      "train: loss: 9944.0341796875 acc: 0.9780746102333069  val: loss: 1476534.875 acc: 0.677295446395874\n",
      "step: 33230 , time : 0.0010006427764892578\n",
      "train: loss: 30370.373046875 acc: 0.9677067995071411  val: loss: 853162.125 acc: 0.4762306213378906\n",
      "step: 33235 , time : 0.0010004043579101562\n",
      "train: loss: 60108.2578125 acc: 0.9764816164970398  val: loss: 988744.5625 acc: 0.6323277950286865\n",
      "step: 33240 , time : 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: loss: 37209.37109375 acc: 0.9795490503311157  val: loss: 1221889.25 acc: 0.7745287418365479\n",
      "step: 33245 , time : 0.0\n",
      "train: loss: 107865.8671875 acc: 0.9124686121940613  val: loss: 781409.0625 acc: 0.8309102654457092\n",
      "step: 33250 , time : 0.0010008811950683594\n",
      "train: loss: 8045.40087890625 acc: 0.98737633228302  val: loss: 397899.15625 acc: 0.9579429030418396\n",
      "step: 33255 , time : 0.0\n",
      "train: loss: 6475.21044921875 acc: 0.9804769158363342  val: loss: 1702542.5 acc: 0.7385958433151245\n",
      "step: 33260 , time : 0.0\n",
      "train: loss: 19263.923828125 acc: 0.9917553663253784  val: loss: 2331240.5 acc: 0.47980332374572754\n",
      "step: 33265 , time : 0.0\n",
      "train: loss: 13939.8720703125 acc: 0.9966665506362915  val: loss: 1339180.5 acc: 0.17068743705749512\n",
      "step: 33270 , time : 0.0\n",
      "train: loss: 51795.0546875 acc: 0.985754132270813  val: loss: 1109520.375 acc: 0.4313315749168396\n",
      "step: 33275 , time : 0.0\n",
      "train: loss: 38665.75390625 acc: 0.9863401055335999  val: loss: 1668539.25 acc: 0.4158148169517517\n",
      "step: 33280 , time : 0.001001596450805664\n",
      "train: loss: 31180.296875 acc: 0.9869111776351929  val: loss: 385331.5 acc: 0.9167162179946899\n",
      "step: 33285 , time : 0.0\n",
      "train: loss: 126712.171875 acc: 0.9612919092178345  val: loss: 147127.046875 acc: 0.9589342474937439\n",
      "step: 33290 , time : 0.0010008811950683594\n",
      "train: loss: 73783.28125 acc: 0.9744160175323486  val: loss: 1302603.5 acc: 0.34141165018081665\n",
      "step: 33295 , time : 0.0\n",
      "train: loss: 376567.0625 acc: 0.8885709643363953  val: loss: 240032.046875 acc: 0.9186114072799683\n",
      "step: 33300 , time : 0.0\n",
      "train: loss: 217677.21875 acc: 0.9671732783317566  val: loss: 245304.90625 acc: 0.9508121013641357\n",
      "step: 33305 , time : 0.0\n",
      "train: loss: 147841.28125 acc: 0.9811705350875854  val: loss: 240749.265625 acc: 0.9585112929344177\n",
      "step: 33310 , time : 0.0\n",
      "train: loss: 138112.75 acc: 0.9852825999259949  val: loss: 269979.125 acc: 0.8596991300582886\n",
      "step: 33315 , time : 0.0\n",
      "train: loss: 40320.53125 acc: 0.9957470297813416  val: loss: 308658.8125 acc: 0.9600541591644287\n",
      "step: 33320 , time : 0.0\n",
      "train: loss: 348697.4375 acc: 0.9797324538230896  val: loss: 280125.375 acc: 0.9337893724441528\n",
      "step: 33325 , time : 0.0\n",
      "train: loss: 1001418.3125 acc: 0.9404538869857788  val: loss: 1219458.5 acc: 0.6776455044746399\n",
      "step: 33330 , time : 0.0\n",
      "train: loss: 840199.5 acc: 0.9487984776496887  val: loss: 463176.0625 acc: 0.8853639364242554\n",
      "step: 33335 , time : 0.0010008811950683594\n",
      "train: loss: 305982.8125 acc: 0.9528900384902954  val: loss: 2106248.0 acc: 0.3162367343902588\n",
      "step: 33340 , time : 0.0\n",
      "train: loss: 185774.234375 acc: 0.9667337536811829  val: loss: 331533.8125 acc: 0.8710155487060547\n",
      "step: 33345 , time : 0.0010008811950683594\n",
      "train: loss: 503870.96875 acc: 0.978855311870575  val: loss: 2100557.25 acc: 0.7514652013778687\n",
      "step: 33350 , time : 0.0\n",
      "train: loss: 743790.75 acc: 0.9793601036071777  val: loss: 287639.6875 acc: 0.9349163174629211\n",
      "step: 33355 , time : 0.0\n",
      "train: loss: 1542651.625 acc: 0.9326022863388062  val: loss: 848641.5 acc: 0.5492630004882812\n",
      "step: 33360 , time : 0.0\n",
      "train: loss: 437521.3125 acc: 0.9636978507041931  val: loss: 637200.8125 acc: 0.8746796250343323\n",
      "step: 33365 , time : 0.0\n",
      "train: loss: 426798.9375 acc: 0.9648727774620056  val: loss: 791612.4375 acc: 0.880308985710144\n",
      "step: 33370 , time : 0.0010006427764892578\n",
      "train: loss: 319270.0625 acc: 0.9034125208854675  val: loss: 396699.09375 acc: 0.9400995969772339\n",
      "step: 33375 , time : 0.0010004043579101562\n",
      "train: loss: 405870.40625 acc: 0.9413033127784729  val: loss: 311611.0625 acc: 0.9484609961509705\n",
      "step: 33380 , time : 0.001001119613647461\n",
      "train: loss: 2837145.5 acc: -0.31629157066345215  val: loss: 1038165.25 acc: 0.8695017099380493\n",
      "step: 33385 , time : 0.0010008811950683594\n",
      "train: loss: 1115399.125 acc: 0.3600425124168396  val: loss: 2446009.5 acc: 0.7181745767593384\n",
      "step: 33390 , time : 0.0\n",
      "train: loss: 654310.6875 acc: 0.7599802017211914  val: loss: 3878788.0 acc: 0.5795143246650696\n",
      "step: 33395 , time : 0.0010006427764892578\n",
      "train: loss: 295536.53125 acc: 0.8803555965423584  val: loss: 1152270.75 acc: 0.8799226880073547\n",
      "step: 33400 , time : 0.0010008811950683594\n",
      "train: loss: 1065017.875 acc: 0.7001969814300537  val: loss: 665078.375 acc: 0.8949370384216309\n",
      "step: 33405 , time : 0.0\n",
      "train: loss: 627910.0625 acc: 0.610768735408783  val: loss: 970438.1875 acc: 0.6003420352935791\n",
      "step: 33410 , time : 0.0\n",
      "train: loss: 340018.15625 acc: 0.7911165952682495  val: loss: 5804789.5 acc: 0.42341989278793335\n",
      "step: 33415 , time : 0.0010006427764892578\n",
      "train: loss: 502572.75 acc: 0.7530725002288818  val: loss: 4401476.5 acc: 0.5925816297531128\n",
      "step: 33420 , time : 0.0\n",
      "train: loss: 435721.15625 acc: 0.7814225554466248  val: loss: 2483420.75 acc: 0.6532497406005859\n",
      "step: 33425 , time : 0.0010008811950683594\n",
      "train: loss: 71047.96875 acc: 0.9415563344955444  val: loss: 3677002.5 acc: 0.5416649580001831\n",
      "step: 33430 , time : 0.0\n",
      "train: loss: 168048.703125 acc: 0.8771262168884277  val: loss: 4377953.0 acc: 0.5594492554664612\n",
      "step: 33435 , time : 0.0\n",
      "train: loss: 299443.65625 acc: 0.8280390501022339  val: loss: 3575814.25 acc: 0.565425455570221\n",
      "step: 33440 , time : 0.0010006427764892578\n",
      "train: loss: 141938.65625 acc: 0.8819547295570374  val: loss: 242520.09375 acc: 0.8203967809677124\n",
      "step: 33445 , time : 0.0010004043579101562\n",
      "train: loss: 312570.5 acc: 0.7947070002555847  val: loss: 3407722.0 acc: 0.49798130989074707\n",
      "step: 33450 , time : 0.0\n",
      "train: loss: 193969.40625 acc: 0.8848270773887634  val: loss: 993619.875 acc: 0.7563433051109314\n",
      "step: 33455 , time : 0.0010013580322265625\n",
      "train: loss: 298272.0625 acc: 0.8225634098052979  val: loss: 2581282.0 acc: 0.5707055926322937\n",
      "step: 33460 , time : 0.0\n",
      "train: loss: 168174.734375 acc: 0.8303191065788269  val: loss: 2380023.75 acc: 0.5656896829605103\n",
      "step: 33465 , time : 0.0010008811950683594\n",
      "train: loss: 29030.939453125 acc: 0.9750778079032898  val: loss: 2423617.75 acc: 0.5070416927337646\n",
      "step: 33470 , time : 0.0\n",
      "train: loss: 444111.78125 acc: 0.7860680818557739  val: loss: 2771050.25 acc: 0.5069918036460876\n",
      "step: 33475 , time : 0.0010008811950683594\n",
      "train: loss: 1350398.625 acc: 0.6091319918632507  val: loss: 685651.0 acc: 0.7508205771446228\n",
      "step: 33480 , time : 0.0020017623901367188\n",
      "train: loss: 595286.4375 acc: 0.7155479788780212  val: loss: 2772998.5 acc: 0.5019690990447998\n",
      "step: 33485 , time : 0.0010008811950683594\n",
      "train: loss: 58225.421875 acc: 0.9407560229301453  val: loss: 3517394.0 acc: 0.45121926069259644\n",
      "step: 33490 , time : 0.0\n",
      "train: loss: 1187372.875 acc: 0.729299783706665  val: loss: 436741.90625 acc: 0.7782360315322876\n",
      "step: 33495 , time : 0.0\n",
      "train: loss: 1200696.75 acc: 0.7426080703735352  val: loss: 1516956.375 acc: 0.8478468656539917\n",
      "step: 33500 , time : 0.0\n",
      "train: loss: 971519.75 acc: 0.8908116817474365  val: loss: 629498.125 acc: 0.8081291913986206\n",
      "step: 33505 , time : 0.0\n",
      "train: loss: 732864.4375 acc: 0.9200870394706726  val: loss: 1625136.0 acc: 0.6209096908569336\n",
      "step: 33510 , time : 0.0\n",
      "train: loss: 142356.859375 acc: 0.9484723210334778  val: loss: 1234671.25 acc: 0.669697642326355\n",
      "step: 33515 , time : 0.0\n",
      "train: loss: 98070.828125 acc: 0.9872269630432129  val: loss: 789635.1875 acc: 0.9018852114677429\n",
      "step: 33520 , time : 0.0\n",
      "train: loss: 118060.203125 acc: 0.9898274540901184  val: loss: 1594075.125 acc: 0.7549543976783752\n",
      "step: 33525 , time : 0.0\n",
      "train: loss: 71464.3671875 acc: 0.9952164888381958  val: loss: 3077660.0 acc: 0.6089549660682678\n",
      "step: 33530 , time : 0.0\n",
      "train: loss: 90681.5703125 acc: 0.9938310980796814  val: loss: 1055517.375 acc: 0.8188785314559937\n",
      "step: 33535 , time : 0.0\n",
      "train: loss: 63793.203125 acc: 0.9941611289978027  val: loss: 1478357.125 acc: 0.7930744290351868\n",
      "step: 33540 , time : 0.0\n",
      "train: loss: 40521.71875 acc: 0.992444634437561  val: loss: 457218.40625 acc: 0.9071662425994873\n",
      "step: 33545 , time : 0.0\n",
      "train: loss: 28278.8515625 acc: 0.9891499876976013  val: loss: 2298910.75 acc: -0.6617895364761353\n",
      "step: 33550 , time : 0.0\n",
      "train: loss: 22450.98828125 acc: 0.9921931624412537  val: loss: 630624.4375 acc: 0.6762471199035645\n",
      "step: 33555 , time : 0.0\n",
      "train: loss: 11465.8828125 acc: 0.9967692494392395  val: loss: 1400809.375 acc: 0.4669497609138489\n",
      "step: 33560 , time : 0.015625476837158203\n",
      "train: loss: 15212.845703125 acc: 0.9899118542671204  val: loss: 748885.5 acc: 0.7930753231048584\n",
      "step: 33565 , time : 0.0010006427764892578\n",
      "train: loss: 15632.955078125 acc: 0.9957596659660339  val: loss: 435988.1875 acc: 0.8649036884307861\n",
      "step: 33570 , time : 0.0\n",
      "train: loss: 27071.076171875 acc: 0.9507125616073608  val: loss: 1063214.125 acc: 0.49488312005996704\n",
      "step: 33575 , time : 0.0010008811950683594\n",
      "train: loss: 20317.8046875 acc: 0.9711525440216064  val: loss: 774339.0625 acc: 0.7809995412826538\n",
      "step: 33580 , time : 0.0\n",
      "train: loss: 4736.57373046875 acc: 0.9910632967948914  val: loss: 1907820.875 acc: 0.19782990217208862\n",
      "step: 33585 , time : 0.0010004043579101562\n",
      "train: loss: 10341.8994140625 acc: 0.9635446071624756  val: loss: 418186.125 acc: 0.9572662711143494\n",
      "step: 33590 , time : 0.0\n",
      "train: loss: 11148.0419921875 acc: 0.9761304259300232  val: loss: 501404.46875 acc: 0.9355233311653137\n",
      "step: 33595 , time : 0.0\n",
      "train: loss: 42298.390625 acc: 0.9648996591567993  val: loss: 754724.4375 acc: 0.509240984916687\n",
      "step: 33600 , time : 0.0\n",
      "train: loss: 14114.1923828125 acc: 0.9934128522872925  val: loss: 1112998.875 acc: 0.635584831237793\n",
      "step: 33605 , time : 0.0\n",
      "train: loss: 31249.34375 acc: 0.9788475632667542  val: loss: 1200844.375 acc: 0.6817585229873657\n",
      "step: 33610 , time : 0.0010006427764892578\n",
      "train: loss: 17605.951171875 acc: 0.9902468323707581  val: loss: 356649.53125 acc: 0.940119743347168\n",
      "step: 33615 , time : 0.0\n",
      "train: loss: 24634.359375 acc: 0.9728764891624451  val: loss: 451711.34375 acc: 0.6650840640068054\n",
      "step: 33620 , time : 0.0\n",
      "train: loss: 15621.216796875 acc: 0.9885773062705994  val: loss: 1224991.0 acc: 0.7783645391464233\n",
      "step: 33625 , time : 0.0\n",
      "train: loss: 5418.0595703125 acc: 0.9974939823150635  val: loss: 2280194.75 acc: 0.1614983081817627\n",
      "step: 33630 , time : 0.0\n",
      "train: loss: 37709.6171875 acc: 0.9916300773620605  val: loss: 702641.6875 acc: 0.8909065127372742\n",
      "step: 33635 , time : 0.0\n",
      "train: loss: 30429.337890625 acc: 0.9943699240684509  val: loss: 146378.6875 acc: 0.9821943044662476\n",
      "step: 33640 , time : 0.015624523162841797\n",
      "train: loss: 26955.83203125 acc: 0.99031001329422  val: loss: 510827.875 acc: 0.8967214822769165\n",
      "step: 33645 , time : 0.0\n",
      "train: loss: 34908.515625 acc: 0.9871666431427002  val: loss: 1641575.875 acc: 0.6243108510971069\n",
      "step: 33650 , time : 0.0\n",
      "train: loss: 115780.53125 acc: 0.9512847065925598  val: loss: 379229.875 acc: 0.8666151165962219\n",
      "step: 33655 , time : 0.0\n",
      "train: loss: 109102.8515625 acc: 0.9641298055648804  val: loss: 386171.03125 acc: 0.9296596646308899\n",
      "step: 33660 , time : 0.0\n",
      "train: loss: 57374.234375 acc: 0.9732790589332581  val: loss: 182977.890625 acc: 0.975380539894104\n",
      "step: 33665 , time : 0.0\n",
      "train: loss: 147426.265625 acc: 0.9704718589782715  val: loss: 3507285.25 acc: 0.4448275566101074\n",
      "step: 33670 , time : 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: loss: 70972.5546875 acc: 0.9935752749443054  val: loss: 373868.875 acc: 0.9334952235221863\n",
      "step: 33675 , time : 0.0010008811950683594\n",
      "train: loss: 136391.328125 acc: 0.9861894249916077  val: loss: 713728.625 acc: 0.9636140465736389\n",
      "step: 33680 , time : 0.0010006427764892578\n",
      "train: loss: 22500.013671875 acc: 0.9977875351905823  val: loss: 616652.0 acc: 0.9018376469612122\n",
      "step: 33685 , time : 0.0010006427764892578\n",
      "train: loss: 197379.25 acc: 0.9837164282798767  val: loss: 1576409.75 acc: 0.7690628170967102\n",
      "step: 33690 , time : 0.0010006427764892578\n",
      "train: loss: 315403.84375 acc: 0.9674877524375916  val: loss: 1683370.5 acc: 0.7756352424621582\n",
      "step: 33695 , time : 0.0010006427764892578\n",
      "train: loss: 363494.875 acc: 0.9595064520835876  val: loss: 426263.4375 acc: 0.934685468673706\n",
      "step: 33700 , time : 0.0\n",
      "train: loss: 460239.125 acc: 0.9645171165466309  val: loss: 650228.0625 acc: 0.9331812858581543\n",
      "step: 33705 , time : 0.001001119613647461\n",
      "train: loss: 142429.453125 acc: 0.9375143051147461  val: loss: 1281982.125 acc: 0.8580505847930908\n",
      "step: 33710 , time : 0.0\n",
      "train: loss: 666625.5 acc: 0.9551519155502319  val: loss: 1064435.625 acc: 0.9081093072891235\n",
      "step: 33715 , time : 0.0\n",
      "train: loss: 652285.5625 acc: 0.9775080680847168  val: loss: 2065744.25 acc: 0.22923314571380615\n",
      "step: 33720 , time : 0.0\n",
      "train: loss: 1866847.125 acc: 0.9047573804855347  val: loss: 2979247.25 acc: 0.26756465435028076\n",
      "step: 33725 , time : 0.015625\n",
      "train: loss: 593904.625 acc: 0.9525812268257141  val: loss: 1265733.375 acc: 0.8699744939804077\n",
      "step: 33730 , time : 0.0\n",
      "train: loss: 149832.40625 acc: 0.9653334617614746  val: loss: 2931061.25 acc: 0.6174049973487854\n",
      "step: 33735 , time : 0.0\n",
      "train: loss: 340313.59375 acc: 0.9637001156806946  val: loss: 196162.390625 acc: 0.9476679563522339\n",
      "step: 33740 , time : 0.0\n",
      "train: loss: 321944.1875 acc: 0.9323327541351318  val: loss: 1273795.375 acc: 0.8411712646484375\n",
      "step: 33745 , time : 0.0\n",
      "train: loss: 1457554.0 acc: 0.783447265625  val: loss: 909212.8125 acc: 0.9004424810409546\n",
      "step: 33750 , time : 0.0\n",
      "train: loss: 1347124.625 acc: 0.22585469484329224  val: loss: 1933669.875 acc: 0.7570816874504089\n",
      "step: 33755 , time : 0.0\n",
      "train: loss: 649015.8125 acc: 0.7038519382476807  val: loss: 1489457.25 acc: 0.6615135669708252\n",
      "step: 33760 , time : 0.0\n",
      "train: loss: 696579.4375 acc: 0.6439497470855713  val: loss: 1732018.75 acc: 0.7571940422058105\n",
      "step: 33765 , time : 0.0\n",
      "train: loss: 155385.296875 acc: 0.8895840644836426  val: loss: 179779.3125 acc: 0.9075016975402832\n",
      "step: 33770 , time : 0.0\n",
      "train: loss: 1016903.0625 acc: 0.6817857027053833  val: loss: 2863561.5 acc: 0.6081576347351074\n",
      "step: 33775 , time : 0.0\n",
      "train: loss: 124884.296875 acc: 0.9123414158821106  val: loss: 2450126.25 acc: 0.5365307331085205\n",
      "step: 33780 , time : 0.0\n",
      "train: loss: 283480.8125 acc: 0.8355982303619385  val: loss: 2360459.75 acc: 0.5713032484054565\n",
      "step: 33785 , time : 0.0\n",
      "train: loss: 27573.005859375 acc: 0.9735632538795471  val: loss: 1144990.5 acc: 0.6993046998977661\n",
      "step: 33790 , time : 0.0\n",
      "train: loss: 59608.3515625 acc: 0.9465291500091553  val: loss: 1222428.75 acc: 0.6644213199615479\n",
      "step: 33795 , time : 0.0\n",
      "train: loss: 89608.28125 acc: 0.9361635446548462  val: loss: 1139543.25 acc: 0.6397752165794373\n",
      "step: 33800 , time : 0.0\n",
      "train: loss: 108045.609375 acc: 0.9260594844818115  val: loss: 3778492.0 acc: 0.45695215463638306\n",
      "step: 33805 , time : 0.0\n",
      "train: loss: 330658.96875 acc: 0.8159486055374146  val: loss: 4995557.0 acc: 0.476448655128479\n",
      "step: 33810 , time : 0.0\n",
      "train: loss: 156755.203125 acc: 0.8899881839752197  val: loss: 351425.875 acc: 0.8244520425796509\n",
      "step: 33815 , time : 0.0\n",
      "train: loss: 28806.69140625 acc: 0.9619500041007996  val: loss: 1049078.0 acc: 0.677216112613678\n",
      "step: 33820 , time : 0.001001119613647461\n",
      "train: loss: 25945.689453125 acc: 0.9716209769248962  val: loss: 511857.25 acc: 0.6929696798324585\n",
      "step: 33825 , time : 0.0\n",
      "train: loss: 8485.982421875 acc: 0.99092036485672  val: loss: 1537939.5 acc: 0.6438422203063965\n",
      "step: 33830 , time : 0.0\n",
      "train: loss: 298961.40625 acc: 0.7475520372390747  val: loss: 795227.125 acc: 0.6747090816497803\n",
      "step: 33835 , time : 0.0\n",
      "train: loss: 59743.671875 acc: 0.9502707719802856  val: loss: 6376120.0 acc: 0.4718017578125\n",
      "step: 33840 , time : 0.015624523162841797\n",
      "train: loss: 64827.5546875 acc: 0.9388033747673035  val: loss: 1641724.375 acc: 0.6518232226371765\n",
      "step: 33845 , time : 0.0\n",
      "train: loss: 402399.375 acc: 0.7897546887397766  val: loss: 4206726.0 acc: 0.5249311923980713\n",
      "step: 33850 , time : 0.0\n",
      "train: loss: 69692.6484375 acc: 0.9208599925041199  val: loss: 1628635.75 acc: 0.6420590281486511\n",
      "step: 33855 , time : 0.0\n",
      "train: loss: 1039035.75 acc: 0.7143069505691528  val: loss: 172066.15625 acc: 0.8309333324432373\n",
      "step: 33860 , time : 0.0\n",
      "train: loss: 1219047.75 acc: 0.7476930618286133  val: loss: 471376.5 acc: 0.7756516933441162\n",
      "step: 33865 , time : 0.0\n",
      "train: loss: 1105029.0 acc: 0.8847131133079529  val: loss: 747410.75 acc: 0.85269695520401\n",
      "step: 33870 , time : 0.0\n",
      "train: loss: 334479.5625 acc: 0.9700607657432556  val: loss: 1142535.125 acc: 0.10474485158920288\n",
      "step: 33875 , time : 0.0\n",
      "train: loss: 125829.6328125 acc: 0.9850069880485535  val: loss: 196653.5 acc: 0.95366370677948\n",
      "step: 33880 , time : 0.0\n",
      "train: loss: 290414.9375 acc: 0.949825644493103  val: loss: 925208.3125 acc: 0.5388052463531494\n",
      "step: 33885 , time : 0.0010006427764892578\n",
      "train: loss: 102934.2890625 acc: 0.9897599220275879  val: loss: 375325.09375 acc: 0.8336960077285767\n",
      "step: 33890 , time : 0.0\n",
      "train: loss: 54759.97265625 acc: 0.9942490458488464  val: loss: 34973.19921875 acc: 0.9879589080810547\n",
      "step: 33895 , time : 0.0010004043579101562\n",
      "train: loss: 56613.25 acc: 0.9957126379013062  val: loss: 701025.5625 acc: 0.869720995426178\n",
      "step: 33900 , time : 0.0010008811950683594\n",
      "train: loss: 137417.515625 acc: 0.9856482744216919  val: loss: 1638339.0 acc: 0.46516841650009155\n",
      "step: 33905 , time : 0.0\n",
      "train: loss: 44170.21484375 acc: 0.992609977722168  val: loss: 1141822.5 acc: 0.7743402123451233\n",
      "step: 33910 , time : 0.0\n",
      "train: loss: 40707.1796875 acc: 0.9947418570518494  val: loss: 55336.6640625 acc: 0.9827978610992432\n",
      "step: 33915 , time : 0.0010006427764892578\n",
      "train: loss: 24042.755859375 acc: 0.9901849031448364  val: loss: 170596.25 acc: 0.9309283494949341\n",
      "step: 33920 , time : 0.0\n",
      "train: loss: 17671.365234375 acc: 0.9934715628623962  val: loss: 802076.5 acc: 0.7802205085754395\n",
      "step: 33925 , time : 0.0010006427764892578\n",
      "train: loss: 8223.275390625 acc: 0.9891599416732788  val: loss: 72791.375 acc: 0.9819250702857971\n",
      "step: 33930 , time : 0.0009999275207519531\n",
      "train: loss: 14173.3994140625 acc: 0.9921172261238098  val: loss: 520324.40625 acc: 0.24257361888885498\n",
      "step: 33935 , time : 0.0\n",
      "train: loss: 62348.421875 acc: 0.9641557335853577  val: loss: 1378152.875 acc: 0.8390902280807495\n",
      "step: 33940 , time : 0.0010004043579101562\n",
      "train: loss: 16402.4140625 acc: 0.9291474223136902  val: loss: 520472.125 acc: 0.857509970664978\n",
      "step: 33945 , time : 0.0010008811950683594\n",
      "train: loss: 8141.6943359375 acc: 0.9739745855331421  val: loss: 1698272.75 acc: 0.013219118118286133\n",
      "step: 33950 , time : 0.0\n",
      "train: loss: 5684.3408203125 acc: 0.9879618287086487  val: loss: 449377.03125 acc: 0.9251384735107422\n",
      "step: 33955 , time : 0.0\n",
      "train: loss: 13817.8134765625 acc: 0.9704552888870239  val: loss: 886897.1875 acc: 0.6713064312934875\n",
      "step: 33960 , time : 0.0\n",
      "train: loss: 28950.6875 acc: 0.9798940420150757  val: loss: 1590356.5 acc: 0.3899363875389099\n",
      "step: 33965 , time : 0.0\n",
      "train: loss: 35793.62109375 acc: 0.9681769609451294  val: loss: 2343343.0 acc: 0.558448851108551\n",
      "step: 33970 , time : 0.0\n",
      "train: loss: 29165.6875 acc: 0.9852949380874634  val: loss: 1380405.0 acc: 0.90301114320755\n",
      "step: 33975 , time : 0.0\n",
      "train: loss: 14000.279296875 acc: 0.9905922412872314  val: loss: 197925.671875 acc: 0.9721062779426575\n",
      "step: 33980 , time : 0.0\n",
      "train: loss: 12717.33203125 acc: 0.9858702421188354  val: loss: 744486.0625 acc: 0.9286994934082031\n",
      "step: 33985 , time : 0.0\n",
      "train: loss: 7474.41357421875 acc: 0.9934911727905273  val: loss: 2121465.25 acc: 0.6205840110778809\n",
      "step: 33990 , time : 0.0\n",
      "train: loss: 12344.962890625 acc: 0.987673282623291  val: loss: 1383663.75 acc: 0.8326717019081116\n",
      "step: 33995 , time : 0.0\n",
      "train: loss: 26914.208984375 acc: 0.9913173317909241  val: loss: 950555.0625 acc: 0.9065247774124146\n",
      "step: 34000 , time : 0.0\n",
      "train: loss: 34502.89453125 acc: 0.9888386130332947  val: loss: 232424.203125 acc: 0.9587479829788208\n",
      "step: 34005 , time : 0.0\n",
      "train: loss: 34107.59375 acc: 0.9908891916275024  val: loss: 628132.9375 acc: 0.8786817789077759\n",
      "step: 34010 , time : 0.0\n",
      "train: loss: 20196.4609375 acc: 0.9838686585426331  val: loss: 1842051.125 acc: 0.7781238555908203\n",
      "step: 34015 , time : 0.0\n",
      "train: loss: 39414.81640625 acc: 0.9791682362556458  val: loss: 2445142.5 acc: 0.8253389596939087\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 34020 , time : 0.0\n",
      "train: loss: 138607.1875 acc: 0.9565363526344299  val: loss: 1432788.5 acc: 0.8614685535430908\n",
      "step: 34025 , time : 0.0010008811950683594\n",
      "train: loss: 63875.4453125 acc: 0.9738511443138123  val: loss: 1391250.25 acc: 0.805945873260498\n",
      "step: 34030 , time : 0.0010004043579101562\n",
      "train: loss: 366560.8125 acc: 0.9488112330436707  val: loss: 4366004.5 acc: 0.6308109164237976\n",
      "step: 34035 , time : 0.0010008811950683594\n",
      "train: loss: 134621.046875 acc: 0.9865885376930237  val: loss: 2430057.5 acc: 0.3940895199775696\n",
      "step: 34040 , time : 0.0\n",
      "train: loss: 231430.359375 acc: 0.9763714671134949  val: loss: 774887.1875 acc: 0.7744175791740417\n",
      "step: 34045 , time : 0.0\n",
      "train: loss: 544400.4375 acc: 0.9265741109848022  val: loss: 2489986.0 acc: 0.5435118675231934\n",
      "step: 34050 , time : 0.0010006427764892578\n",
      "train: loss: 184347.609375 acc: 0.9790099859237671  val: loss: 368704.28125 acc: 0.9043002128601074\n",
      "step: 34055 , time : 0.0010006427764892578\n",
      "train: loss: 333023.03125 acc: 0.959403395652771  val: loss: 1066959.5 acc: 0.6819957494735718\n",
      "step: 34060 , time : 0.0\n",
      "train: loss: 340360.4375 acc: 0.9748079776763916  val: loss: 583917.3125 acc: 0.8293955326080322\n",
      "step: 34065 , time : 0.0\n",
      "train: loss: 323020.15625 acc: 0.9716591238975525  val: loss: 1013181.9375 acc: 0.7565123438835144\n",
      "step: 34070 , time : 0.0\n",
      "train: loss: 331209.875 acc: 0.9440770745277405  val: loss: 3082554.75 acc: 0.27416741847991943\n",
      "step: 34075 , time : 0.0010008811950683594\n",
      "train: loss: 750003.25 acc: 0.9577283263206482  val: loss: 981963.0 acc: 0.8025400042533875\n",
      "step: 34080 , time : 0.0\n",
      "train: loss: 1170806.0 acc: 0.9716152548789978  val: loss: 883887.0625 acc: 0.8399448990821838\n",
      "step: 34085 , time : 0.0\n",
      "train: loss: 779598.875 acc: 0.9677963852882385  val: loss: 1740178.5 acc: 0.5237026214599609\n",
      "step: 34090 , time : 0.0\n",
      "train: loss: 741845.625 acc: 0.9530693888664246  val: loss: 1343827.0 acc: 0.8381053805351257\n",
      "step: 34095 , time : 0.0010008811950683594\n",
      "train: loss: 377671.125 acc: 0.9482898712158203  val: loss: 1908099.0 acc: 0.8533758521080017\n",
      "step: 34100 , time : 0.0\n",
      "train: loss: 491763.0 acc: 0.8816853165626526  val: loss: 336482.4375 acc: 0.9337435960769653\n",
      "step: 34105 , time : 0.0\n",
      "train: loss: 359537.875 acc: 0.9626734852790833  val: loss: 898863.4375 acc: 0.8561742901802063\n",
      "step: 34110 , time : 0.0\n",
      "train: loss: 2456040.5 acc: 0.741672694683075  val: loss: 787870.0625 acc: 0.9227530360221863\n",
      "step: 34115 , time : 0.0\n",
      "train: loss: 1027132.375 acc: 0.3651233911514282  val: loss: 1844836.0 acc: 0.7446833848953247\n",
      "step: 34120 , time : 0.0\n",
      "train: loss: 729391.625 acc: 0.7419680953025818  val: loss: 1014674.375 acc: 0.7724153399467468\n",
      "step: 34125 , time : 0.0\n",
      "train: loss: 704229.0625 acc: 0.6294644474983215  val: loss: 1082728.5 acc: 0.8378127813339233\n",
      "step: 34130 , time : 0.0\n",
      "train: loss: 556315.0625 acc: 0.8509551286697388  val: loss: 597101.8125 acc: 0.8667009472846985\n",
      "step: 34135 , time : 0.0010008811950683594\n",
      "train: loss: 932607.6875 acc: 0.650009274482727  val: loss: 2159115.5 acc: 0.7427945733070374\n",
      "step: 34140 , time : 0.0010008811950683594\n",
      "train: loss: 451550.6875 acc: 0.7107517719268799  val: loss: 2000830.875 acc: 0.5401210784912109\n",
      "step: 34145 , time : 0.0\n",
      "train: loss: 151765.0 acc: 0.8788235187530518  val: loss: 2367138.75 acc: 0.6088088154792786\n",
      "step: 34150 , time : 0.0010006427764892578\n",
      "train: loss: 102475.9921875 acc: 0.9300640821456909  val: loss: 1475501.25 acc: 0.6532271504402161\n",
      "step: 34155 , time : 0.0\n",
      "train: loss: 473814.3125 acc: 0.7700167894363403  val: loss: 1270304.625 acc: 0.6569837331771851\n",
      "step: 34160 , time : 0.0010013580322265625\n",
      "train: loss: 102950.8359375 acc: 0.9166382551193237  val: loss: 721668.5 acc: 0.7212037444114685\n",
      "step: 34165 , time : 0.0\n",
      "train: loss: 68440.3671875 acc: 0.9527736902236938  val: loss: 3125062.25 acc: 0.5158280730247498\n",
      "step: 34170 , time : 0.0\n",
      "train: loss: 754130.5625 acc: 0.7159132957458496  val: loss: 1287391.875 acc: 0.6605018377304077\n",
      "step: 34175 , time : 0.0\n",
      "train: loss: 22573.6796875 acc: 0.9804103970527649  val: loss: 740000.0 acc: 0.6331003904342651\n",
      "step: 34180 , time : 0.0\n",
      "train: loss: 79098.0390625 acc: 0.9412208199501038  val: loss: 989075.1875 acc: 0.673850953578949\n",
      "step: 34185 , time : 0.0\n",
      "train: loss: 25401.423828125 acc: 0.9753863215446472  val: loss: 1938748.5 acc: 0.5547033548355103\n",
      "step: 34190 , time : 0.0\n",
      "train: loss: 57824.1328125 acc: 0.9346972703933716  val: loss: 321130.71875 acc: 0.80021733045578\n",
      "step: 34195 , time : 0.0\n",
      "train: loss: 176952.671875 acc: 0.8769469261169434  val: loss: 82146.171875 acc: 0.928067147731781\n",
      "step: 34200 , time : 0.0\n",
      "train: loss: 864111.5 acc: 0.6296703815460205  val: loss: 3052583.0 acc: 0.47052276134490967\n",
      "step: 34205 , time : 0.0\n",
      "train: loss: 957116.3125 acc: 0.7026805877685547  val: loss: 627968.4375 acc: 0.7656742334365845\n",
      "step: 34210 , time : 0.0\n",
      "train: loss: 597140.1875 acc: 0.7436859607696533  val: loss: 296424.0 acc: 0.853542685508728\n",
      "step: 34215 , time : 0.0\n",
      "train: loss: 38078.234375 acc: 0.9566834568977356  val: loss: 3746702.5 acc: 0.5611780881881714\n",
      "step: 34220 , time : 0.0\n",
      "train: loss: 977688.0 acc: 0.6966700553894043  val: loss: 106791.984375 acc: 0.9195899367332458\n",
      "step: 34225 , time : 0.0\n",
      "train: loss: 1947251.25 acc: 0.7598186135292053  val: loss: 1041052.8125 acc: 0.5405938029289246\n",
      "step: 34230 , time : 0.0\n",
      "train: loss: 800081.4375 acc: 0.9130550026893616  val: loss: 517588.9375 acc: 0.8185553550720215\n",
      "step: 34235 , time : 0.0\n",
      "train: loss: 430603.25 acc: 0.9654422402381897  val: loss: 855919.1875 acc: 0.2766135334968567\n",
      "step: 34240 , time : 0.0\n",
      "train: loss: 383659.4375 acc: 0.9509127140045166  val: loss: 370950.40625 acc: 0.8055775165557861\n",
      "step: 34245 , time : 0.001001119613647461\n",
      "train: loss: 278214.6875 acc: 0.9550512433052063  val: loss: 237308.375 acc: 0.6484794616699219\n",
      "step: 34250 , time : 0.0\n",
      "train: loss: 128022.59375 acc: 0.9866779446601868  val: loss: 352780.6875 acc: 0.9305843114852905\n",
      "step: 34255 , time : 0.0010006427764892578\n",
      "train: loss: 76372.9375 acc: 0.9889633655548096  val: loss: 1069608.75 acc: 0.7050269842147827\n",
      "step: 34260 , time : 0.0\n",
      "train: loss: 90932.625 acc: 0.9931809902191162  val: loss: 505449.84375 acc: 0.904231071472168\n",
      "step: 34265 , time : 0.0010008811950683594\n",
      "train: loss: 107858.8515625 acc: 0.9923302531242371  val: loss: 1908917.375 acc: 0.6866296529769897\n",
      "step: 34270 , time : 0.0010008811950683594\n",
      "train: loss: 55379.71875 acc: 0.9899643659591675  val: loss: 419208.9375 acc: 0.8525412082672119\n",
      "step: 34275 , time : 0.0\n",
      "train: loss: 42568.12109375 acc: 0.989348828792572  val: loss: 936942.5 acc: 0.7979820966720581\n",
      "step: 34280 , time : 0.0010008811950683594\n",
      "train: loss: 10131.73046875 acc: 0.9918630123138428  val: loss: 303408.3125 acc: 0.956398069858551\n",
      "step: 34285 , time : 0.0010008811950683594\n",
      "train: loss: 57510.609375 acc: 0.9744142293930054  val: loss: 525744.1875 acc: 0.9045366048812866\n",
      "step: 34290 , time : 0.0\n",
      "train: loss: 12481.5927734375 acc: 0.9866235852241516  val: loss: 425086.90625 acc: 0.9491358399391174\n",
      "step: 34295 , time : 0.0\n",
      "train: loss: 5617.62109375 acc: 0.9946245551109314  val: loss: 246584.921875 acc: 0.9634239673614502\n",
      "step: 34300 , time : 0.0\n",
      "train: loss: 173229.453125 acc: 0.7247406244277954  val: loss: 1320606.625 acc: 0.8281784057617188\n",
      "step: 34305 , time : 0.0\n",
      "train: loss: 9755.3447265625 acc: 0.9716365337371826  val: loss: 1225423.125 acc: 0.9135577082633972\n",
      "step: 34310 , time : 0.0\n",
      "train: loss: 7121.4892578125 acc: 0.980976402759552  val: loss: 1058747.75 acc: 0.9259933233261108\n",
      "step: 34315 , time : 0.0\n",
      "train: loss: 11974.3583984375 acc: 0.9922113418579102  val: loss: 312810.53125 acc: 0.9567079544067383\n",
      "step: 34320 , time : 0.0\n",
      "train: loss: 9062.2568359375 acc: 0.9885047674179077  val: loss: 1282530.25 acc: 0.898705005645752\n",
      "step: 34325 , time : 0.0\n",
      "train: loss: 31677.953125 acc: 0.9787967205047607  val: loss: 313639.375 acc: 0.8668625354766846\n",
      "step: 34330 , time : 0.0\n",
      "train: loss: 16213.2841796875 acc: 0.9850667715072632  val: loss: 496753.34375 acc: 0.9219492077827454\n",
      "step: 34335 , time : 0.0\n",
      "train: loss: 93359.53125 acc: 0.9386563301086426  val: loss: 2156273.0 acc: 0.7562611103057861\n",
      "step: 34340 , time : 0.0\n",
      "train: loss: 15411.6865234375 acc: 0.9859045147895813  val: loss: 1361323.125 acc: 0.3764599561691284\n",
      "step: 34345 , time : 0.0\n",
      "train: loss: 19122.568359375 acc: 0.9850672483444214  val: loss: 3424307.75 acc: 0.626825213432312\n",
      "step: 34350 , time : 0.0\n",
      "train: loss: 4897.71337890625 acc: 0.9896517992019653  val: loss: 2197142.0 acc: -0.7700726985931396\n",
      "step: 34355 , time : 0.0010006427764892578\n",
      "train: loss: 12861.947265625 acc: 0.9898040294647217  val: loss: 1190253.25 acc: 0.8582927584648132\n",
      "step: 34360 , time : 0.0010006427764892578\n",
      "train: loss: 24302.072265625 acc: 0.9942014217376709  val: loss: 3099364.25 acc: 0.4849679470062256\n",
      "step: 34365 , time : 0.0010008811950683594\n",
      "train: loss: 29778.4765625 acc: 0.9919482469558716  val: loss: 1097991.25 acc: 0.9121102690696716\n",
      "step: 34370 , time : 0.0\n",
      "train: loss: 33611.9453125 acc: 0.9932366609573364  val: loss: 1894344.375 acc: 0.8005795478820801\n",
      "step: 34375 , time : 0.0\n",
      "train: loss: 10363.0419921875 acc: 0.9889316558837891  val: loss: 1479010.625 acc: 0.6016043424606323\n",
      "step: 34380 , time : 0.0\n",
      "train: loss: 39028.59765625 acc: 0.9859914183616638  val: loss: 1840813.125 acc: 0.8090740442276001\n",
      "step: 34385 , time : 0.001001119613647461\n",
      "train: loss: 223750.34375 acc: 0.9608293771743774  val: loss: 1139106.375 acc: 0.7070196866989136\n",
      "step: 34390 , time : 0.0010008811950683594\n",
      "train: loss: 55557.42578125 acc: 0.9759019613265991  val: loss: 425756.09375 acc: 0.9362191557884216\n",
      "step: 34395 , time : 0.0\n",
      "train: loss: 858935.4375 acc: 0.7423778772354126  val: loss: 2025718.75 acc: 0.603400468826294\n",
      "step: 34400 , time : 0.0010006427764892578\n",
      "train: loss: 107315.4140625 acc: 0.9793910980224609  val: loss: 2561387.25 acc: 0.6362152099609375\n",
      "step: 34405 , time : 0.0010008811950683594\n",
      "train: loss: 42223.45703125 acc: 0.9963615536689758  val: loss: 2121477.75 acc: -0.3144479990005493\n",
      "step: 34410 , time : 0.0\n",
      "train: loss: 44253.296875 acc: 0.9946045875549316  val: loss: 1409160.5 acc: 0.7201588153839111\n",
      "step: 34415 , time : 0.0\n",
      "train: loss: 96044.4296875 acc: 0.9884273409843445  val: loss: 240926.875 acc: 0.9733470678329468\n",
      "step: 34420 , time : 0.0\n",
      "train: loss: 131162.640625 acc: 0.9814548492431641  val: loss: 1206326.5 acc: 0.7384614944458008\n",
      "step: 34425 , time : 0.0\n",
      "train: loss: 302677.125 acc: 0.9786434769630432  val: loss: 3803343.25 acc: 0.5642427802085876\n",
      "step: 34430 , time : 0.0\n",
      "train: loss: 523978.09375 acc: 0.944972574710846  val: loss: 1578740.5 acc: 0.6658952236175537\n",
      "step: 34435 , time : 0.0\n",
      "train: loss: 874567.25 acc: 0.8639190793037415  val: loss: 3479929.0 acc: 0.4193093180656433\n",
      "step: 34440 , time : 0.0\n",
      "train: loss: 441478.03125 acc: 0.9790432453155518  val: loss: 1193058.5 acc: 0.454099178314209\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 34445 , time : 0.0\n",
      "train: loss: 1356398.5 acc: 0.9572638273239136  val: loss: 183520.671875 acc: 0.9381051063537598\n",
      "step: 34450 , time : 0.0\n",
      "train: loss: 1265700.0 acc: 0.9385267496109009  val: loss: 1210703.25 acc: 0.8360736966133118\n",
      "step: 34455 , time : 0.0\n",
      "train: loss: 972067.6875 acc: 0.9483252167701721  val: loss: 1805013.0 acc: 0.7230839729309082\n",
      "step: 34460 , time : 0.0\n",
      "train: loss: 340891.78125 acc: 0.9790298938751221  val: loss: 922940.5 acc: 0.7921023368835449\n",
      "step: 34465 , time : 0.0\n",
      "train: loss: 444482.4375 acc: 0.9535505175590515  val: loss: 3426564.5 acc: 0.5199084281921387\n",
      "step: 34470 , time : 0.0010006427764892578\n",
      "train: loss: 767394.6875 acc: 0.8812544345855713  val: loss: 750194.1875 acc: 0.9273926615715027\n",
      "step: 34475 , time : 0.0\n",
      "train: loss: 3432085.0 acc: 0.5514461994171143  val: loss: 784757.5625 acc: 0.8265206813812256\n",
      "step: 34480 , time : 0.0\n",
      "train: loss: 2747079.5 acc: 0.7246353030204773  val: loss: 455284.4375 acc: 0.823998212814331\n",
      "step: 34485 , time : 0.0010004043579101562\n",
      "train: loss: 600194.9375 acc: 0.7121780514717102  val: loss: 1203104.5 acc: 0.7829358577728271\n",
      "step: 34490 , time : 0.0010006427764892578\n",
      "train: loss: 484964.84375 acc: 0.7519584894180298  val: loss: 734312.9375 acc: 0.8380307555198669\n",
      "step: 34495 , time : 0.0\n",
      "train: loss: 321732.5625 acc: 0.8654872179031372  val: loss: 285408.8125 acc: 0.7112433910369873\n",
      "step: 34500 , time : 0.0\n",
      "train: loss: 1095667.75 acc: 0.22204208374023438  val: loss: 581632.3125 acc: 0.6952454447746277\n",
      "step: 34505 , time : 0.0\n",
      "train: loss: 319779.4375 acc: 0.8126441240310669  val: loss: 2043035.875 acc: 0.5470876693725586\n",
      "step: 34510 , time : 0.0010006427764892578\n",
      "train: loss: 713973.75 acc: 0.7230330109596252  val: loss: 971836.0625 acc: 0.6122801899909973\n",
      "step: 34515 , time : 0.0\n",
      "train: loss: 199226.4375 acc: 0.8631417155265808  val: loss: 247053.1875 acc: 0.8499215841293335\n",
      "step: 34520 , time : 0.0\n",
      "train: loss: 35800.328125 acc: 0.9694429636001587  val: loss: 84519.6796875 acc: 0.915367066860199\n",
      "step: 34525 , time : 0.0\n",
      "train: loss: 52801.50390625 acc: 0.962731659412384  val: loss: 1026430.25 acc: 0.689720630645752\n",
      "step: 34530 , time : 0.0\n",
      "train: loss: 16954.125 acc: 0.9856953024864197  val: loss: 1058837.25 acc: 0.6168326735496521\n",
      "step: 34535 , time : 0.0\n",
      "train: loss: 28780.166015625 acc: 0.9783123135566711  val: loss: 1038330.4375 acc: 0.6688355207443237\n",
      "step: 34540 , time : 0.0\n",
      "train: loss: 109974.625 acc: 0.9106736183166504  val: loss: 3764988.0 acc: 0.47787559032440186\n",
      "step: 34545 , time : 0.0\n",
      "train: loss: 156692.890625 acc: 0.8855652809143066  val: loss: 133000.71875 acc: 0.8524521589279175\n",
      "step: 34550 , time : 0.0\n",
      "train: loss: 252067.390625 acc: 0.8307985067367554  val: loss: 1563975.0 acc: 0.6431661248207092\n",
      "step: 34555 , time : 0.0\n",
      "train: loss: 50557.203125 acc: 0.9469100832939148  val: loss: 2292659.0 acc: 0.5729366540908813\n",
      "step: 34560 , time : 0.0\n",
      "train: loss: 86805.375 acc: 0.9098348617553711  val: loss: 1976922.375 acc: 0.6536180377006531\n",
      "step: 34565 , time : 0.0010008811950683594\n",
      "train: loss: 301881.5 acc: 0.8137037754058838  val: loss: 1662174.75 acc: 0.6743243932723999\n",
      "step: 34570 , time : 0.0010008811950683594\n",
      "train: loss: 1323134.75 acc: 0.6401526927947998  val: loss: 538945.875 acc: 0.7548192739486694\n",
      "step: 34575 , time : 0.0010006427764892578\n",
      "train: loss: 281069.875 acc: 0.8003748655319214  val: loss: 1028437.5 acc: 0.7197979688644409\n",
      "step: 34580 , time : 0.0010004043579101562\n",
      "train: loss: 192987.21875 acc: 0.8493959903717041  val: loss: 1054114.25 acc: 0.628390908241272\n",
      "step: 34585 , time : 0.0\n",
      "train: loss: 651811.375 acc: 0.7759567499160767  val: loss: 2587664.75 acc: 0.576125979423523\n",
      "step: 34590 , time : 0.0010006427764892578\n",
      "train: loss: 1423311.375 acc: 0.6571350693702698  val: loss: 748859.5625 acc: 0.8610901832580566\n",
      "step: 34595 , time : 0.0\n",
      "train: loss: 1591799.75 acc: 0.8594319224357605  val: loss: 1339591.25 acc: 0.765807569026947\n",
      "step: 34600 , time : 0.0010006427764892578\n",
      "train: loss: 1103043.25 acc: 0.8864853382110596  val: loss: 2786911.25 acc: 0.6928471922874451\n",
      "step: 34605 , time : 0.0010008811950683594\n",
      "train: loss: 589529.9375 acc: 0.941026508808136  val: loss: 885347.4375 acc: 0.8918263912200928\n",
      "step: 34610 , time : 0.0\n",
      "train: loss: 242908.0 acc: 0.957694947719574  val: loss: 413109.5 acc: 0.9543620347976685\n",
      "step: 34615 , time : 0.0\n",
      "train: loss: 44690.11328125 acc: 0.9926847219467163  val: loss: 1213265.75 acc: 0.879605233669281\n",
      "step: 34620 , time : 0.0\n",
      "train: loss: 60578.7421875 acc: 0.9957149624824524  val: loss: 319893.0 acc: 0.8988094925880432\n",
      "step: 34625 , time : 0.0\n",
      "train: loss: 115554.671875 acc: 0.9912967681884766  val: loss: 739784.4375 acc: 0.8417964577674866\n",
      "step: 34630 , time : 0.005722761154174805\n",
      "train: loss: 110061.59375 acc: 0.9899100661277771  val: loss: 732139.375 acc: 0.9296216368675232\n",
      "step: 34635 , time : 0.0\n",
      "train: loss: 262800.0625 acc: 0.9631604552268982  val: loss: 565665.375 acc: 0.9059954285621643\n",
      "step: 34640 , time : 0.0\n",
      "train: loss: 72192.5859375 acc: 0.9867213368415833  val: loss: 1478672.25 acc: 0.3855839967727661\n",
      "step: 34645 , time : 0.0010008811950683594\n",
      "train: loss: 8523.900390625 acc: 0.9754388928413391  val: loss: 781071.625 acc: 0.8999888300895691\n",
      "step: 34650 , time : 0.0\n",
      "train: loss: 4396.6611328125 acc: 0.9808108806610107  val: loss: 1051062.5 acc: 0.7577722072601318\n",
      "step: 34655 , time : 0.0010004043579101562\n",
      "train: loss: 7396.80322265625 acc: 0.9813238978385925  val: loss: 144018.078125 acc: 0.9492160677909851\n",
      "step: 34660 , time : 0.0\n",
      "train: loss: 14624.701171875 acc: 0.992283046245575  val: loss: 1955641.0 acc: 0.7770472168922424\n",
      "step: 34665 , time : 0.0\n",
      "train: loss: 16076.1435546875 acc: 0.9534279108047485  val: loss: 804179.75 acc: 0.8741991519927979\n",
      "step: 34670 , time : 0.0\n",
      "train: loss: 18076.623046875 acc: 0.9827512502670288  val: loss: 1666250.75 acc: 0.7266477942466736\n",
      "step: 34675 , time : 0.0\n",
      "train: loss: 16930.150390625 acc: 0.9852901697158813  val: loss: 4373088.5 acc: 0.40455347299575806\n",
      "step: 34680 , time : 0.0010006427764892578\n",
      "train: loss: 3173.321044921875 acc: 0.9931092858314514  val: loss: 1427374.375 acc: 0.5948453545570374\n",
      "step: 34685 , time : 0.0\n",
      "train: loss: 12471.0244140625 acc: 0.9837731122970581  val: loss: 496059.96875 acc: 0.9376921057701111\n",
      "step: 34690 , time : 0.0010006427764892578\n",
      "train: loss: 30398.287109375 acc: 0.9732627868652344  val: loss: 2927345.5 acc: 0.5584553480148315\n",
      "step: 34695 , time : 0.0\n",
      "train: loss: 49654.21875 acc: 0.9692596197128296  val: loss: 1054059.875 acc: 0.7357277870178223\n",
      "step: 34700 , time : 0.0\n",
      "train: loss: 22660.181640625 acc: 0.9821915626525879  val: loss: 1040121.75 acc: 0.8051717877388\n",
      "step: 34705 , time : 0.0\n",
      "train: loss: 25459.984375 acc: 0.9873855710029602  val: loss: 680309.25 acc: 0.8691173791885376\n",
      "step: 34710 , time : 0.0\n",
      "train: loss: 40485.71875 acc: 0.9813129901885986  val: loss: 2808529.0 acc: 0.6460260152816772\n",
      "step: 34715 , time : 0.0\n",
      "train: loss: 14988.1943359375 acc: 0.9846392869949341  val: loss: 434727.125 acc: 0.8293013572692871\n",
      "step: 34720 , time : 0.0\n",
      "train: loss: 12674.072265625 acc: 0.9816411137580872  val: loss: 1761155.0 acc: 0.4987741708755493\n",
      "step: 34725 , time : 0.0\n",
      "train: loss: 28337.041015625 acc: 0.9882643222808838  val: loss: 1337327.125 acc: 0.6047933101654053\n",
      "step: 34730 , time : 0.0\n",
      "train: loss: 36550.57421875 acc: 0.9909380674362183  val: loss: 3179209.5 acc: 0.7444391250610352\n",
      "step: 34735 , time : 0.0010006427764892578\n",
      "train: loss: 22532.6015625 acc: 0.9904066324234009  val: loss: 3283540.5 acc: 0.45946717262268066\n",
      "step: 34740 , time : 0.0\n",
      "train: loss: 35251.5859375 acc: 0.9711999893188477  val: loss: 1148877.5 acc: 0.8179963827133179\n",
      "step: 34745 , time : 0.0010006427764892578\n",
      "train: loss: 55523.45703125 acc: 0.9819983839988708  val: loss: 925612.25 acc: 0.7763627767562866\n",
      "step: 34750 , time : 0.0\n",
      "train: loss: 159302.28125 acc: 0.9710331559181213  val: loss: 3539206.0 acc: 0.407683789730072\n",
      "step: 34755 , time : 0.0\n",
      "train: loss: 152559.0 acc: 0.9438442587852478  val: loss: 2383996.5 acc: 0.6278359889984131\n",
      "step: 34760 , time : 0.0\n",
      "train: loss: 345212.9375 acc: 0.8986381888389587  val: loss: 2815392.0 acc: 0.6496167778968811\n",
      "step: 34765 , time : 0.0010006427764892578\n",
      "train: loss: 50712.80078125 acc: 0.9919348955154419  val: loss: 1241356.875 acc: 0.8296011686325073\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 34770 , time : 0.001001119613647461\n",
      "train: loss: 317035.40625 acc: 0.9656921625137329  val: loss: 746172.875 acc: 0.7711580991744995\n",
      "step: 34775 , time : 0.0\n",
      "train: loss: 176721.1875 acc: 0.977344274520874  val: loss: 1391822.875 acc: 0.6144620180130005\n",
      "step: 34780 , time : 0.0\n",
      "train: loss: 160394.09375 acc: 0.966884434223175  val: loss: 2396099.25 acc: 0.6296027898788452\n",
      "step: 34785 , time : 0.0010008811950683594\n",
      "train: loss: 125208.4375 acc: 0.9809465408325195  val: loss: 569214.125 acc: 0.897818386554718\n",
      "step: 34790 , time : 0.0\n",
      "train: loss: 285979.46875 acc: 0.9727751016616821  val: loss: 423993.53125 acc: 0.9343087077140808\n",
      "step: 34795 , time : 0.0010006427764892578\n",
      "train: loss: 228768.078125 acc: 0.9755309224128723  val: loss: 643513.625 acc: 0.6929831504821777\n",
      "step: 34800 , time : 0.0\n",
      "train: loss: 307476.65625 acc: 0.9531465172767639  val: loss: 546772.875 acc: 0.8629259467124939\n",
      "step: 34805 , time : 0.0\n",
      "train: loss: 1152979.875 acc: 0.9586936235427856  val: loss: 453629.5625 acc: 0.9397275447845459\n",
      "step: 34810 , time : 0.0\n",
      "train: loss: 424962.34375 acc: 0.9858187437057495  val: loss: 495528.65625 acc: 0.7830523252487183\n",
      "step: 34815 , time : 0.0010004043579101562\n",
      "train: loss: 668068.25 acc: 0.9768877029418945  val: loss: 1043376.5 acc: 0.8623337149620056\n",
      "step: 34820 , time : 0.0\n",
      "train: loss: 844614.8125 acc: 0.9620832204818726  val: loss: 144816.9375 acc: 0.9322315454483032\n",
      "step: 34825 , time : 0.0\n",
      "train: loss: 767584.125 acc: 0.9340562224388123  val: loss: 282489.53125 acc: 0.8551713824272156\n",
      "step: 34830 , time : 0.0010008811950683594\n",
      "train: loss: 269624.3125 acc: 0.9626483917236328  val: loss: 199817.28125 acc: 0.94102543592453\n",
      "step: 34835 , time : 0.0\n",
      "train: loss: 346051.40625 acc: 0.9354972243309021  val: loss: 330917.375 acc: 0.9084993004798889\n",
      "step: 34840 , time : 0.0010006427764892578\n",
      "train: loss: 1349727.625 acc: 0.2924879193305969  val: loss: 340259.09375 acc: 0.9532658457756042\n",
      "step: 34845 , time : 0.0\n",
      "train: loss: 1153148.5 acc: 0.3246219754219055  val: loss: 1210224.875 acc: 0.7848288416862488\n",
      "step: 34850 , time : 0.0010006427764892578\n",
      "train: loss: 666392.75 acc: 0.74493408203125  val: loss: 875212.625 acc: 0.5960953831672668\n",
      "step: 34855 , time : 0.0\n",
      "train: loss: 261670.546875 acc: 0.8509764075279236  val: loss: 396071.65625 acc: 0.6678988933563232\n",
      "step: 34860 , time : 0.0\n",
      "train: loss: 522159.0 acc: 0.8722154498100281  val: loss: 1322073.0 acc: 0.7519532442092896\n",
      "step: 34865 , time : 0.0010008811950683594\n",
      "train: loss: 960476.8125 acc: 0.7399762868881226  val: loss: 749751.8125 acc: 0.7683314681053162\n",
      "step: 34870 , time : 0.001001119613647461\n",
      "train: loss: 213180.640625 acc: 0.843807578086853  val: loss: 1710311.25 acc: 0.6124085187911987\n",
      "step: 34875 , time : 0.0\n",
      "train: loss: 206455.6875 acc: 0.8644100427627563  val: loss: 1584137.5 acc: 0.6576588153839111\n",
      "step: 34880 , time : 0.0\n",
      "train: loss: 69627.5234375 acc: 0.9415669441223145  val: loss: 1365349.125 acc: 0.6587084531784058\n",
      "step: 34885 , time : 0.0010006427764892578\n",
      "train: loss: 216055.796875 acc: 0.8575630784034729  val: loss: 1285749.0 acc: 0.6826539039611816\n",
      "step: 34890 , time : 0.0010008811950683594\n",
      "train: loss: 22979.283203125 acc: 0.9788798093795776  val: loss: 876681.75 acc: 0.6248430013656616\n",
      "step: 34895 , time : 0.0\n",
      "train: loss: 38484.76953125 acc: 0.9725110530853271  val: loss: 730076.9375 acc: 0.71132493019104\n",
      "step: 34900 , time : 0.0\n",
      "train: loss: 800641.375 acc: 0.7047668099403381  val: loss: 250732.765625 acc: 0.8508206605911255\n",
      "step: 34905 , time : 0.0\n",
      "train: loss: 112930.6953125 acc: 0.9185025095939636  val: loss: 1127353.125 acc: 0.6662126779556274\n",
      "step: 34910 , time : 0.0\n",
      "train: loss: 139921.71875 acc: 0.9055614471435547  val: loss: 4405521.5 acc: 0.471449077129364\n",
      "step: 34915 , time : 0.0\n",
      "train: loss: 452121.8125 acc: 0.7460064888000488  val: loss: 3624796.25 acc: 0.5128566026687622\n",
      "step: 34920 , time : 0.0\n",
      "train: loss: 136172.015625 acc: 0.8550309538841248  val: loss: 4967442.0 acc: 0.45965564250946045\n",
      "step: 34925 , time : 0.0\n",
      "train: loss: 138334.90625 acc: 0.8828999400138855  val: loss: 788633.9375 acc: 0.7278474569320679\n",
      "step: 34930 , time : 0.0010006427764892578\n",
      "train: loss: 123082.8203125 acc: 0.9163057208061218  val: loss: 746556.125 acc: 0.7008381485939026\n",
      "step: 34935 , time : 0.0\n",
      "train: loss: 350921.0 acc: 0.8080000877380371  val: loss: 5066835.5 acc: 0.5003153085708618\n",
      "step: 34940 , time : 0.0\n",
      "train: loss: 794365.0625 acc: 0.7098122835159302  val: loss: 5614671.5 acc: 0.572376012802124\n",
      "step: 34945 , time : 0.0\n",
      "train: loss: 162608.515625 acc: 0.864922285079956  val: loss: 1229688.0 acc: 0.6448181867599487\n",
      "step: 34950 , time : 0.0\n",
      "train: loss: 444998.125 acc: 0.7491830587387085  val: loss: 5123313.0 acc: 0.5343903303146362\n",
      "step: 34955 , time : 0.0\n",
      "train: loss: 1179230.875 acc: 0.7731006145477295  val: loss: 1009003.0 acc: 0.7405288219451904\n",
      "step: 34960 , time : 0.0\n",
      "train: loss: 1062707.75 acc: 0.8715635538101196  val: loss: 244836.703125 acc: 0.9076153635978699\n",
      "step: 34965 , time : 0.0\n",
      "train: loss: 533393.6875 acc: 0.9597310423851013  val: loss: 1572501.75 acc: 0.8653790950775146\n",
      "step: 34970 , time : 0.0\n",
      "train: loss: 433205.375 acc: 0.9486919641494751  val: loss: 2097323.25 acc: 0.5493177175521851\n",
      "step: 34975 , time : 0.0\n",
      "train: loss: 70971.1796875 acc: 0.9830079078674316  val: loss: 179250.828125 acc: 0.9646397829055786\n",
      "step: 34980 , time : 0.0\n",
      "train: loss: 84987.78125 acc: 0.9873032569885254  val: loss: 4098893.0 acc: 0.5876049995422363\n",
      "step: 34985 , time : 0.0\n",
      "train: loss: 83094.25 acc: 0.9916862845420837  val: loss: 810426.3125 acc: 0.8466058373451233\n",
      "step: 34990 , time : 0.0\n",
      "train: loss: 88195.765625 acc: 0.9935340881347656  val: loss: 2198272.5 acc: 0.6224892139434814\n",
      "step: 34995 , time : 0.0\n",
      "train: loss: 90429.0625 acc: 0.9935222268104553  val: loss: 1532229.625 acc: -0.31536877155303955\n",
      "step: 35000 , time : 0.0010004043579101562\n",
      "train: loss: 75556.6640625 acc: 0.9891389608383179  val: loss: 1604609.625 acc: 0.7992768883705139\n",
      "step: 35005 , time : 0.0010004043579101562\n",
      "train: loss: 42081.1484375 acc: 0.9889520406723022  val: loss: 381410.0 acc: 0.869512677192688\n",
      "step: 35010 , time : 0.0\n",
      "train: loss: 12493.03125 acc: 0.9952057600021362  val: loss: 563012.1875 acc: 0.815160870552063\n",
      "step: 35015 , time : 0.0\n",
      "train: loss: 14638.890625 acc: 0.9963932037353516  val: loss: 784935.125 acc: 0.9031232595443726\n",
      "step: 35020 , time : 0.0\n",
      "train: loss: 6886.890625 acc: 0.9788932204246521  val: loss: 1886754.5 acc: 0.4417206048965454\n",
      "step: 35025 , time : 0.0\n",
      "train: loss: 53678.015625 acc: 0.9803327322006226  val: loss: 2201494.5 acc: 0.26069939136505127\n",
      "step: 35030 , time : 0.0\n",
      "train: loss: 13068.6572265625 acc: 0.9832859635353088  val: loss: 777725.25 acc: 0.7466548681259155\n",
      "step: 35035 , time : 0.0010006427764892578\n",
      "train: loss: 12203.7841796875 acc: 0.9709844589233398  val: loss: 2514073.0 acc: 0.5017607808113098\n",
      "step: 35040 , time : 0.0\n",
      "train: loss: 8040.07080078125 acc: 0.9830894470214844  val: loss: 1299132.5 acc: 0.7303747534751892\n",
      "step: 35045 , time : 0.0\n",
      "train: loss: 13772.37890625 acc: 0.9720109105110168  val: loss: 1879169.375 acc: 0.7284553050994873\n",
      "step: 35050 , time : 0.0010008811950683594\n",
      "train: loss: 20149.158203125 acc: 0.9851798415184021  val: loss: 1098155.125 acc: 0.8757491111755371\n",
      "step: 35055 , time : 0.0\n",
      "train: loss: 13871.109375 acc: 0.9670572280883789  val: loss: 2122510.5 acc: 0.4955753684043884\n",
      "step: 35060 , time : 0.0\n",
      "train: loss: 26656.146484375 acc: 0.9788625240325928  val: loss: 2018890.75 acc: 0.7282487750053406\n",
      "step: 35065 , time : 0.0010006427764892578\n",
      "train: loss: 22652.978515625 acc: 0.9776292443275452  val: loss: 1663283.125 acc: 0.39416420459747314\n",
      "step: 35070 , time : 0.0\n",
      "train: loss: 16089.32421875 acc: 0.9862075448036194  val: loss: 1133380.25 acc: 0.9043434262275696\n",
      "step: 35075 , time : 0.0\n",
      "train: loss: 6205.71044921875 acc: 0.9947013854980469  val: loss: 497572.8125 acc: 0.89939284324646\n",
      "step: 35080 , time : 0.0010008811950683594\n",
      "train: loss: 8088.99462890625 acc: 0.994123101234436  val: loss: 1888557.5 acc: 0.7568025588989258\n",
      "step: 35085 , time : 0.0\n",
      "train: loss: 4832.64794921875 acc: 0.9887275099754333  val: loss: 2523620.5 acc: 0.4248313307762146\n",
      "step: 35090 , time : 0.0\n",
      "train: loss: 24541.44921875 acc: 0.9876373410224915  val: loss: 1570372.25 acc: 0.7257786989212036\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 35095 , time : 0.0010004043579101562\n",
      "train: loss: 32012.427734375 acc: 0.99156653881073  val: loss: 1383473.5 acc: 0.8034148216247559\n",
      "step: 35100 , time : 0.0010008811950683594\n",
      "train: loss: 30587.005859375 acc: 0.9869121313095093  val: loss: 481581.21875 acc: 0.9559363126754761\n",
      "step: 35105 , time : 0.0010006427764892578\n",
      "train: loss: 69379.2734375 acc: 0.9750761985778809  val: loss: 1275499.25 acc: 0.6489793062210083\n",
      "step: 35110 , time : 0.0010008811950683594\n",
      "train: loss: 24156.779296875 acc: 0.9923806190490723  val: loss: 1639147.25 acc: 0.7355386018753052\n",
      "step: 35115 , time : 0.0\n",
      "train: loss: 261920.296875 acc: 0.9338844418525696  val: loss: 1530829.25 acc: 0.653610348701477\n",
      "step: 35120 , time : 0.0\n",
      "train: loss: 63243.33203125 acc: 0.9806717038154602  val: loss: 2431550.5 acc: -0.559013843536377\n",
      "step: 35125 , time : 0.0\n",
      "train: loss: 105954.2421875 acc: 0.9645791053771973  val: loss: 1292320.375 acc: 0.7705380916595459\n",
      "step: 35130 , time : 0.0\n",
      "train: loss: 312672.625 acc: 0.949655294418335  val: loss: 1079962.5 acc: 0.38058584928512573\n",
      "step: 35135 , time : 0.0\n",
      "train: loss: 107036.96875 acc: 0.9885657429695129  val: loss: 4141514.0 acc: -0.9935102462768555\n",
      "step: 35140 , time : 0.0\n",
      "train: loss: 51851.61328125 acc: 0.9942418932914734  val: loss: 723273.625 acc: 0.9010058045387268\n",
      "step: 35145 , time : 0.0010004043579101562\n",
      "train: loss: 159019.640625 acc: 0.9803640842437744  val: loss: 1060002.5 acc: 0.784608781337738\n",
      "step: 35150 , time : 0.0\n",
      "train: loss: 269822.09375 acc: 0.9660520553588867  val: loss: 1633815.125 acc: 0.34761738777160645\n",
      "step: 35155 , time : 0.0\n",
      "train: loss: 812753.875 acc: 0.9426701068878174  val: loss: 724662.75 acc: 0.8493175506591797\n",
      "step: 35160 , time : 0.0\n",
      "train: loss: 351612.75 acc: 0.9704939126968384  val: loss: 1039994.625 acc: 0.8805125951766968\n",
      "step: 35165 , time : 0.0\n",
      "train: loss: 238313.171875 acc: 0.9693498015403748  val: loss: 793709.625 acc: 0.749288022518158\n",
      "step: 35170 , time : 0.0010006427764892578\n",
      "train: loss: 1384901.875 acc: 0.9288220405578613  val: loss: 601510.375 acc: 0.7786778211593628\n",
      "step: 35175 , time : 0.0\n",
      "train: loss: 971465.125 acc: 0.9722439646720886  val: loss: 690551.6875 acc: 0.7959848642349243\n",
      "step: 35180 , time : 0.0010006427764892578\n",
      "train: loss: 757562.375 acc: 0.9656086564064026  val: loss: 949637.6875 acc: 0.6760525703430176\n",
      "step: 35185 , time : 0.0010006427764892578\n",
      "train: loss: 1357379.625 acc: 0.9276502132415771  val: loss: 406547.0 acc: 0.8850741386413574\n",
      "step: 35190 , time : 0.0\n",
      "train: loss: 770500.0625 acc: 0.9274865388870239  val: loss: 172375.296875 acc: 0.9538639187812805\n",
      "step: 35195 , time : 0.0010008811950683594\n",
      "train: loss: 491570.3125 acc: 0.9736348390579224  val: loss: 84804.1640625 acc: 0.9765692949295044\n",
      "step: 35200 , time : 0.0\n",
      "train: loss: 181819.625 acc: 0.9687147736549377  val: loss: 414666.53125 acc: 0.9477109313011169\n",
      "step: 35205 , time : 0.0\n",
      "train: loss: 522422.5625 acc: 0.9020352959632874  val: loss: 1176424.625 acc: 0.6340577602386475\n",
      "step: 35210 , time : 0.0\n",
      "train: loss: 874332.0 acc: 0.5678165555000305  val: loss: 985771.875 acc: 0.8736193180084229\n",
      "step: 35215 , time : 0.0\n",
      "train: loss: 843242.875 acc: 0.35339778661727905  val: loss: 680041.1875 acc: 0.8785945177078247\n",
      "step: 35220 , time : 0.0\n",
      "train: loss: 580700.375 acc: 0.7420886754989624  val: loss: 967508.0 acc: 0.5684054493904114\n",
      "step: 35225 , time : 0.0\n",
      "train: loss: 527136.125 acc: 0.8846692442893982  val: loss: 1941612.75 acc: 0.8546160459518433\n",
      "step: 35230 , time : 0.0010008811950683594\n",
      "train: loss: 945073.25 acc: 0.7255523204803467  val: loss: 1958387.5 acc: 0.8248084187507629\n",
      "step: 35235 , time : 0.0\n",
      "train: loss: 1502028.875 acc: -0.4049808979034424  val: loss: 5041076.0 acc: 0.7492152452468872\n",
      "step: 35240 , time : 0.0010006427764892578\n",
      "train: loss: 380159.0 acc: 0.6910659074783325  val: loss: 2343736.5 acc: 0.7330342531204224\n",
      "step: 35245 , time : 0.0\n",
      "train: loss: 55912.109375 acc: 0.9543301463127136  val: loss: 579007.625 acc: 0.8111176490783691\n",
      "step: 35250 , time : 0.0010006427764892578\n",
      "train: loss: 89082.1015625 acc: 0.9301987290382385  val: loss: 496851.53125 acc: 0.8008413314819336\n",
      "step: 35255 , time : 0.0\n",
      "train: loss: 15865.072265625 acc: 0.9867351651191711  val: loss: 1566713.375 acc: 0.6139895915985107\n",
      "step: 35260 , time : 0.0010004043579101562\n",
      "train: loss: 268739.84375 acc: 0.8654561042785645  val: loss: 1420154.75 acc: 0.6590681672096252\n",
      "step: 35265 , time : 0.0010006427764892578\n",
      "train: loss: 266910.0 acc: 0.7543228268623352  val: loss: 567869.3125 acc: 0.8416876792907715\n",
      "step: 35270 , time : 0.0010004043579101562\n",
      "train: loss: 222641.3125 acc: 0.8631905913352966  val: loss: 1842277.25 acc: 0.6730968952178955\n",
      "step: 35275 , time : 0.0\n",
      "train: loss: 268055.46875 acc: 0.8564666509628296  val: loss: 809819.875 acc: 0.8324288725852966\n",
      "step: 35280 , time : 0.0\n",
      "train: loss: 135849.859375 acc: 0.8654052019119263  val: loss: 1368316.0 acc: 0.7411062717437744\n",
      "step: 35285 , time : 0.0010004043579101562\n",
      "train: loss: 43512.10546875 acc: 0.9351840019226074  val: loss: 2869093.5 acc: 0.7100704312324524\n",
      "step: 35290 , time : 0.001001119613647461\n",
      "train: loss: 97994.296875 acc: 0.9162280559539795  val: loss: 815051.25 acc: 0.7584868669509888\n",
      "step: 35295 , time : 0.0\n",
      "train: loss: 171194.25 acc: 0.8435376882553101  val: loss: 1158476.75 acc: 0.6786600351333618\n",
      "step: 35300 , time : 0.0010004043579101562\n",
      "train: loss: 246086.796875 acc: 0.8430997133255005  val: loss: 1179427.0 acc: 0.8053428530693054\n",
      "step: 35305 , time : 0.0\n",
      "train: loss: 326414.03125 acc: 0.8285218477249146  val: loss: 299419.84375 acc: 0.8441966772079468\n",
      "step: 35310 , time : 0.0\n",
      "train: loss: 499703.875 acc: 0.6917423009872437  val: loss: 1522308.25 acc: 0.6952762007713318\n",
      "step: 35315 , time : 0.0\n",
      "train: loss: 1066922.0 acc: 0.7033601403236389  val: loss: 151135.0625 acc: 0.8671744465827942\n",
      "step: 35320 , time : 0.0010006427764892578\n",
      "train: loss: 3517799.75 acc: 0.6646507978439331  val: loss: 1957914.25 acc: 0.7333999872207642\n",
      "step: 35325 , time : 0.0\n",
      "train: loss: 907893.0 acc: 0.7810421586036682  val: loss: 560256.3125 acc: 0.7726759910583496\n",
      "step: 35330 , time : 0.0010006427764892578\n",
      "train: loss: 854210.625 acc: 0.9055622816085815  val: loss: 1307243.625 acc: 0.7898998260498047\n",
      "step: 35335 , time : 0.0\n",
      "train: loss: 175708.671875 acc: 0.9836170673370361  val: loss: 2899303.5 acc: 0.49833375215530396\n",
      "step: 35340 , time : 0.0010006427764892578\n",
      "train: loss: 222665.1875 acc: 0.9466366767883301  val: loss: 621521.25 acc: 0.7177088260650635\n",
      "step: 35345 , time : 0.0010004043579101562\n",
      "train: loss: 286445.1875 acc: 0.9351935982704163  val: loss: 897696.375 acc: 0.5908937454223633\n",
      "step: 35350 , time : 0.0010004043579101562\n",
      "train: loss: 120818.1875 acc: 0.9867739677429199  val: loss: 1610526.75 acc: 0.732948899269104\n",
      "step: 35355 , time : 0.0\n",
      "train: loss: 82955.421875 acc: 0.9935899972915649  val: loss: 1331000.25 acc: 0.6429719924926758\n",
      "step: 35360 , time : 0.0\n",
      "train: loss: 149692.9375 acc: 0.9889277815818787  val: loss: 744464.8125 acc: 0.8844485282897949\n",
      "step: 35365 , time : 0.0\n",
      "train: loss: 44626.140625 acc: 0.9954455494880676  val: loss: 2656679.5 acc: 0.7949767708778381\n",
      "step: 35370 , time : 0.0\n",
      "train: loss: 30717.11328125 acc: 0.9933372139930725  val: loss: 1357267.875 acc: 0.500196099281311\n",
      "step: 35375 , time : 0.0\n",
      "train: loss: 22864.275390625 acc: 0.9905354976654053  val: loss: 515549.46875 acc: 0.9161049127578735\n",
      "step: 35380 , time : 0.0010006427764892578\n",
      "train: loss: 6072.31494140625 acc: 0.9957435131072998  val: loss: 1744679.625 acc: 0.4669063687324524\n",
      "step: 35385 , time : 0.0010008811950683594\n",
      "train: loss: 9807.0986328125 acc: 0.9960730075836182  val: loss: 1727780.0 acc: 0.7866518497467041\n",
      "step: 35390 , time : 0.0010004043579101562\n",
      "train: loss: 5513.99658203125 acc: 0.9758280515670776  val: loss: 1384440.25 acc: 0.5645850300788879\n",
      "step: 35395 , time : 0.0\n",
      "train: loss: 14598.3984375 acc: 0.9908632636070251  val: loss: 3101475.0 acc: 0.16220098733901978\n",
      "step: 35400 , time : 0.0\n",
      "train: loss: 6105.59423828125 acc: 0.995628297328949  val: loss: 2538629.0 acc: 0.09993040561676025\n",
      "step: 35405 , time : 0.0\n",
      "train: loss: 8175.0244140625 acc: 0.9791722297668457  val: loss: 782297.5 acc: 0.842092752456665\n",
      "step: 35410 , time : 0.0\n",
      "train: loss: 9622.9580078125 acc: 0.9718908667564392  val: loss: 796823.5625 acc: 0.940609335899353\n",
      "step: 35415 , time : 0.0\n",
      "train: loss: 15688.0009765625 acc: 0.977087676525116  val: loss: 408478.65625 acc: 0.9614774584770203\n",
      "step: 35420 , time : 0.0\n",
      "train: loss: 20862.904296875 acc: 0.9708526730537415  val: loss: 376154.65625 acc: 0.9498130083084106\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 35425 , time : 0.0010004043579101562\n",
      "train: loss: 25569.35546875 acc: 0.9813395738601685  val: loss: 1936430.0 acc: 0.4570017457008362\n",
      "step: 35430 , time : 0.0010006427764892578\n",
      "train: loss: 22129.455078125 acc: 0.9808968901634216  val: loss: 491573.875 acc: 0.8273273706436157\n",
      "step: 35435 , time : 0.001001119613647461\n",
      "train: loss: 5966.587890625 acc: 0.9963494539260864  val: loss: 948466.9375 acc: 0.7655739784240723\n",
      "step: 35440 , time : 0.0010006427764892578\n",
      "train: loss: 8517.1923828125 acc: 0.9942931532859802  val: loss: 2458161.5 acc: 0.1841806173324585\n",
      "step: 35445 , time : 0.0\n",
      "train: loss: 12813.1826171875 acc: 0.9900978207588196  val: loss: 619809.625 acc: 0.7338438630104065\n",
      "step: 35450 , time : 0.002001047134399414\n",
      "train: loss: 5289.576171875 acc: 0.991241991519928  val: loss: 534598.6875 acc: 0.8961666822433472\n",
      "step: 35455 , time : 0.0\n",
      "train: loss: 7869.34033203125 acc: 0.9942985773086548  val: loss: 1512088.75 acc: 0.5759197473526001\n",
      "step: 35460 , time : 0.0010006427764892578\n",
      "train: loss: 35974.21875 acc: 0.9881316423416138  val: loss: 1380640.5 acc: 0.7734810709953308\n",
      "step: 35465 , time : 0.0\n",
      "train: loss: 28123.177734375 acc: 0.9933863282203674  val: loss: 525724.3125 acc: 0.8326616883277893\n",
      "step: 35470 , time : 0.001001119613647461\n",
      "train: loss: 17644.310546875 acc: 0.9831646680831909  val: loss: 200363.078125 acc: 0.9585672616958618\n",
      "step: 35475 , time : 0.0010004043579101562\n",
      "train: loss: 45043.7734375 acc: 0.9795043468475342  val: loss: 2047223.25 acc: 0.7320677042007446\n",
      "step: 35480 , time : 0.0\n",
      "train: loss: 414495.21875 acc: 0.8798285126686096  val: loss: 626049.375 acc: 0.925523042678833\n",
      "step: 35485 , time : 0.0\n",
      "train: loss: 122348.46875 acc: 0.9493218660354614  val: loss: 1304997.0 acc: 0.47781693935394287\n",
      "step: 35490 , time : 0.0009999275207519531\n",
      "train: loss: 52565.98828125 acc: 0.9676713943481445  val: loss: 2126259.5 acc: 0.565127968788147\n",
      "step: 35495 , time : 0.0030024051666259766\n",
      "train: loss: 258801.15625 acc: 0.9687244892120361  val: loss: 101116.1015625 acc: 0.9680436849594116\n",
      "step: 35500 , time : 0.0\n",
      "train: loss: 531756.625 acc: 0.9356806874275208  val: loss: 920817.9375 acc: 0.7043404579162598\n",
      "step: 35505 , time : 0.0\n",
      "train: loss: 127735.03125 acc: 0.9840291738510132  val: loss: 1328326.25 acc: 0.7731161117553711\n",
      "step: 35510 , time : 0.0\n",
      "train: loss: 89812.796875 acc: 0.9780473113059998  val: loss: 168753.640625 acc: 0.953984797000885\n",
      "step: 35515 , time : 0.0\n",
      "train: loss: 417357.75 acc: 0.9300867319107056  val: loss: 218977.90625 acc: 0.9449057579040527\n",
      "step: 35520 , time : 0.0\n",
      "train: loss: 611163.25 acc: 0.9595475196838379  val: loss: 1667702.25 acc: 0.21330809593200684\n",
      "step: 35525 , time : 0.0\n",
      "train: loss: 458299.71875 acc: 0.9460499882698059  val: loss: 630424.3125 acc: 0.759990394115448\n",
      "step: 35530 , time : 0.0010004043579101562\n",
      "train: loss: 172466.203125 acc: 0.9575974345207214  val: loss: 745271.0625 acc: 0.9299473166465759\n",
      "step: 35535 , time : 0.0\n",
      "train: loss: 678134.0625 acc: 0.9622083902359009  val: loss: 674757.4375 acc: 0.8961289525032043\n",
      "step: 35540 , time : 0.0010001659393310547\n",
      "train: loss: 353326.96875 acc: 0.9862567782402039  val: loss: 670550.25 acc: 0.8172080516815186\n",
      "step: 35545 , time : 0.0\n",
      "train: loss: 890451.5625 acc: 0.9753786325454712  val: loss: 324931.03125 acc: 0.9610676765441895\n",
      "step: 35550 , time : 0.001001119613647461\n",
      "train: loss: 1183153.75 acc: 0.9241257905960083  val: loss: 223149.109375 acc: 0.9602674841880798\n",
      "step: 35555 , time : 0.0010006427764892578\n",
      "train: loss: 624748.375 acc: 0.9596770405769348  val: loss: 2323448.5 acc: 0.7450124025344849\n",
      "step: 35560 , time : 0.0\n",
      "train: loss: 457110.96875 acc: 0.9430654048919678  val: loss: 342569.40625 acc: 0.9702756404876709\n",
      "step: 35565 , time : 0.0\n",
      "train: loss: 471869.1875 acc: 0.9409626126289368  val: loss: 476167.4375 acc: 0.966342031955719\n",
      "step: 35570 , time : 0.0\n",
      "train: loss: 430218.09375 acc: 0.8712738752365112  val: loss: 538702.9375 acc: 0.8886302709579468\n",
      "step: 35575 , time : 0.001001119613647461\n",
      "train: loss: 1528563.0 acc: 0.3885606527328491  val: loss: 1509494.5 acc: 0.8648762106895447\n",
      "step: 35580 , time : 0.0\n",
      "train: loss: 521244.25 acc: 0.7130601406097412  val: loss: 1782326.75 acc: 0.8076059818267822\n",
      "step: 35585 , time : 0.0\n",
      "train: loss: 546186.375 acc: 0.8066861629486084  val: loss: 1107655.875 acc: 0.8098664879798889\n",
      "step: 35590 , time : 0.0\n",
      "train: loss: 394340.09375 acc: 0.7593432664871216  val: loss: 826772.5 acc: 0.8233396410942078\n",
      "step: 35595 , time : 0.0\n",
      "train: loss: 929239.125 acc: 0.794337272644043  val: loss: 1154778.25 acc: 0.7388029098510742\n",
      "step: 35600 , time : 0.0\n",
      "train: loss: 800715.375 acc: 0.7655759453773499  val: loss: 1936962.375 acc: 0.7919213771820068\n",
      "step: 35605 , time : 0.0\n",
      "train: loss: 910572.4375 acc: 0.38076382875442505  val: loss: 2135862.5 acc: 0.7650483250617981\n",
      "step: 35610 , time : 0.0\n",
      "train: loss: 483310.25 acc: 0.7796447277069092  val: loss: 695469.375 acc: 0.7423586845397949\n",
      "step: 35615 , time : 0.0\n",
      "train: loss: 471399.59375 acc: 0.7704706788063049  val: loss: 1931112.75 acc: 0.693175196647644\n",
      "step: 35620 , time : 0.0\n",
      "train: loss: 169047.609375 acc: 0.8993253707885742  val: loss: 2797655.75 acc: 0.5898803472518921\n",
      "step: 35625 , time : 0.0\n",
      "train: loss: 5519.77587890625 acc: 0.9955922961235046  val: loss: 1047567.8125 acc: 0.694313645362854\n",
      "step: 35630 , time : 0.0009996891021728516\n",
      "train: loss: 17817.31640625 acc: 0.9872135519981384  val: loss: 772707.9375 acc: 0.7942404747009277\n",
      "step: 35635 , time : 0.0010008811950683594\n",
      "train: loss: 16100.0283203125 acc: 0.9869464039802551  val: loss: 1881904.875 acc: 0.6581704616546631\n",
      "step: 35640 , time : 0.0\n",
      "train: loss: 83673.1328125 acc: 0.9338463544845581  val: loss: 2096539.875 acc: 0.7132336497306824\n",
      "step: 35645 , time : 0.0\n",
      "train: loss: 17144.53125 acc: 0.9793567061424255  val: loss: 150790.03125 acc: 0.8605023622512817\n",
      "step: 35650 , time : 0.0010004043579101562\n",
      "train: loss: 371722.1875 acc: 0.8117442727088928  val: loss: 2209860.0 acc: 0.6902077198028564\n",
      "step: 35655 , time : 0.001001119613647461\n",
      "train: loss: 81077.7265625 acc: 0.9398025870323181  val: loss: 1787830.25 acc: 0.608661413192749\n",
      "step: 35660 , time : 0.0\n",
      "train: loss: 73499.2421875 acc: 0.9324844479560852  val: loss: 610206.625 acc: 0.7385916709899902\n",
      "step: 35665 , time : 0.0\n",
      "train: loss: 295578.03125 acc: 0.7874395847320557  val: loss: 2381274.25 acc: 0.6355226039886475\n",
      "step: 35670 , time : 0.0\n",
      "train: loss: 975205.9375 acc: 0.6539249420166016  val: loss: 3692373.0 acc: 0.6191313862800598\n",
      "step: 35675 , time : 0.0010008811950683594\n",
      "train: loss: 63235.66796875 acc: 0.9241913557052612  val: loss: 602240.9375 acc: 0.7303795218467712\n",
      "step: 35680 , time : 0.0\n",
      "train: loss: 879755.1875 acc: 0.7024723291397095  val: loss: 1737714.5 acc: 0.6933373212814331\n",
      "step: 35685 , time : 0.0\n",
      "train: loss: 1237688.25 acc: 0.6553125977516174  val: loss: 1628394.875 acc: 0.6896207332611084\n",
      "step: 35690 , time : 0.0\n",
      "train: loss: 1097953.125 acc: 0.8098055720329285  val: loss: 891432.625 acc: 0.7817890644073486\n",
      "step: 35695 , time : 0.0\n",
      "train: loss: 842033.375 acc: 0.918795645236969  val: loss: 497659.34375 acc: 0.8895918130874634\n",
      "step: 35700 , time : 0.0\n",
      "train: loss: 173217.34375 acc: 0.9851032495498657  val: loss: 1216551.875 acc: 0.8604632019996643\n",
      "step: 35705 , time : 0.015625953674316406\n",
      "train: loss: 199963.75 acc: 0.9536495804786682  val: loss: 848732.5625 acc: 0.7235640287399292\n",
      "step: 35710 , time : 0.0\n",
      "train: loss: 144211.1875 acc: 0.9747171998023987  val: loss: 465762.5625 acc: 0.9020217657089233\n",
      "step: 35715 , time : 0.0\n",
      "train: loss: 228032.734375 acc: 0.9790764451026917  val: loss: 3181451.5 acc: 0.6351298689842224\n",
      "step: 35720 , time : 0.0\n",
      "train: loss: 121166.59375 acc: 0.9908156394958496  val: loss: 1647607.625 acc: 0.6204684972763062\n",
      "step: 35725 , time : 0.0\n",
      "train: loss: 46786.39453125 acc: 0.9968036413192749  val: loss: 1868255.25 acc: 0.7414064407348633\n",
      "step: 35730 , time : 0.0\n",
      "train: loss: 121295.03125 acc: 0.987375795841217  val: loss: 1221724.25 acc: 0.8533262610435486\n",
      "step: 35735 , time : 0.0\n",
      "train: loss: 82974.921875 acc: 0.9843841195106506  val: loss: 396419.96875 acc: 0.9638422131538391\n",
      "step: 35740 , time : 0.0\n",
      "train: loss: 6429.29345703125 acc: 0.9817621111869812  val: loss: 1577680.625 acc: 0.7419217824935913\n",
      "step: 35745 , time : 0.001001119613647461\n",
      "train: loss: 8063.45849609375 acc: 0.9935810565948486  val: loss: 506698.59375 acc: 0.9017810821533203\n",
      "step: 35750 , time : 0.0\n",
      "train: loss: 10405.58984375 acc: 0.9931450486183167  val: loss: 1180459.875 acc: 0.5294739007949829\n",
      "step: 35755 , time : 0.0\n",
      "train: loss: 16665.109375 acc: 0.9769380688667297  val: loss: 547708.125 acc: 0.874864935874939\n",
      "step: 35760 , time : 0.0\n",
      "train: loss: 24481.451171875 acc: 0.985477089881897  val: loss: 484203.09375 acc: 0.8391733169555664\n",
      "step: 35765 , time : 0.0\n",
      "train: loss: 11935.0654296875 acc: 0.9608135223388672  val: loss: 818796.25 acc: 0.6722372770309448\n",
      "step: 35770 , time : 0.0\n",
      "train: loss: 4605.4638671875 acc: 0.9821224808692932  val: loss: 705758.3125 acc: 0.7668508291244507\n",
      "step: 35775 , time : 0.0\n",
      "train: loss: 8949.1865234375 acc: 0.9846550226211548  val: loss: 1166638.5 acc: 0.7920075058937073\n",
      "step: 35780 , time : 0.0\n",
      "train: loss: 31904.455078125 acc: 0.9717739820480347  val: loss: 767816.625 acc: 0.8090925812721252\n",
      "step: 35785 , time : 0.0\n",
      "train: loss: 35625.4453125 acc: 0.9395217299461365  val: loss: 602337.9375 acc: 0.8860685229301453\n",
      "step: 35790 , time : 0.0\n",
      "train: loss: 50821.03515625 acc: 0.9717258810997009  val: loss: 599152.4375 acc: 0.8107513189315796\n",
      "step: 35795 , time : 0.0\n",
      "train: loss: 9930.9736328125 acc: 0.9950178265571594  val: loss: 248684.25 acc: 0.9254474639892578\n",
      "step: 35800 , time : 0.0\n",
      "train: loss: 15249.9677734375 acc: 0.9916408061981201  val: loss: 211906.25 acc: 0.9347153902053833\n",
      "step: 35805 , time : 0.0\n",
      "train: loss: 8095.9619140625 acc: 0.9939302206039429  val: loss: 841319.0 acc: 0.7066769599914551\n",
      "step: 35810 , time : 0.0\n",
      "train: loss: 9808.8671875 acc: 0.9937846064567566  val: loss: 276334.90625 acc: 0.9626625180244446\n",
      "step: 35815 , time : 0.0\n",
      "train: loss: 8520.9716796875 acc: 0.9736659526824951  val: loss: 683664.6875 acc: 0.7815561294555664\n",
      "step: 35820 , time : 0.0\n",
      "train: loss: 17975.166015625 acc: 0.9868015646934509  val: loss: 877825.8125 acc: 0.6634362936019897\n",
      "step: 35825 , time : 0.0\n",
      "train: loss: 20923.14453125 acc: 0.9951596856117249  val: loss: 688283.875 acc: 0.85882568359375\n",
      "step: 35830 , time : 0.0\n",
      "train: loss: 34623.62890625 acc: 0.9886866211891174  val: loss: 345317.4375 acc: 0.9184512495994568\n",
      "step: 35835 , time : 0.0\n",
      "train: loss: 37786.44140625 acc: 0.99007248878479  val: loss: 993382.875 acc: 0.2630825638771057\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 35840 , time : 0.015625715255737305\n",
      "train: loss: 37293.0078125 acc: 0.9865177273750305  val: loss: 780536.0 acc: 0.8006755709648132\n",
      "step: 35845 , time : 0.0\n",
      "train: loss: 364584.9375 acc: 0.8752700090408325  val: loss: 965286.6875 acc: 0.7268505096435547\n",
      "step: 35850 , time : 0.0010006427764892578\n",
      "train: loss: 84367.25 acc: 0.9726126194000244  val: loss: 552242.375 acc: 0.9267433881759644\n",
      "step: 35855 , time : 0.0010008811950683594\n",
      "train: loss: 54484.44140625 acc: 0.974076509475708  val: loss: 1244821.25 acc: 0.8185323476791382\n",
      "step: 35860 , time : 0.0\n",
      "train: loss: 516378.53125 acc: 0.8519391417503357  val: loss: 977609.625 acc: 0.8445213437080383\n",
      "step: 35865 , time : 0.0\n",
      "train: loss: 365364.9375 acc: 0.9529989957809448  val: loss: 3143114.5 acc: 0.6067639589309692\n",
      "step: 35870 , time : 0.0010006427764892578\n",
      "train: loss: 64732.1953125 acc: 0.9900516271591187  val: loss: 805374.3125 acc: 0.9309542179107666\n",
      "step: 35875 , time : 0.0010008811950683594\n",
      "train: loss: 63737.98828125 acc: 0.9887737035751343  val: loss: 1798376.5 acc: 0.8908711075782776\n",
      "step: 35880 , time : 0.001001119613647461\n",
      "train: loss: 59514.96484375 acc: 0.990605890750885  val: loss: 2504873.0 acc: 0.5363758206367493\n",
      "step: 35885 , time : 0.0\n",
      "train: loss: 300991.40625 acc: 0.9800705313682556  val: loss: 478560.03125 acc: 0.9382099509239197\n",
      "step: 35890 , time : 0.0\n",
      "train: loss: 542172.0625 acc: 0.9568632245063782  val: loss: 824975.3125 acc: 0.8081853985786438\n",
      "step: 35895 , time : 0.0\n",
      "train: loss: 428060.59375 acc: 0.9747757911682129  val: loss: 745292.3125 acc: 0.7274848222732544\n",
      "step: 35900 , time : 0.0\n",
      "train: loss: 777567.625 acc: 0.9541625380516052  val: loss: 1980513.0 acc: 0.5072503089904785\n",
      "step: 35905 , time : 0.0\n",
      "train: loss: 1120591.0 acc: 0.9505488872528076  val: loss: 697609.6875 acc: 0.9239538311958313\n",
      "step: 35910 , time : 0.0\n",
      "train: loss: 1385908.25 acc: 0.9667731523513794  val: loss: 264593.65625 acc: 0.9061870574951172\n",
      "step: 35915 , time : 0.0\n",
      "train: loss: 838978.875 acc: 0.9628908038139343  val: loss: 1098030.25 acc: 0.8394061326980591\n",
      "step: 35920 , time : 0.0\n",
      "train: loss: 915517.4375 acc: 0.9339166879653931  val: loss: 2542772.75 acc: 0.5795685052871704\n",
      "step: 35925 , time : 0.0\n",
      "train: loss: 469810.4375 acc: 0.9538573026657104  val: loss: 1891227.25 acc: 0.7070609331130981\n",
      "step: 35930 , time : 0.0\n",
      "train: loss: 228679.734375 acc: 0.9361764788627625  val: loss: 1716107.75 acc: 0.795465350151062\n",
      "step: 35935 , time : 0.0\n",
      "train: loss: 434748.4375 acc: 0.9249923825263977  val: loss: 552989.3125 acc: 0.8941137194633484\n",
      "step: 35940 , time : 0.0\n",
      "train: loss: 1380784.875 acc: 0.5672785639762878  val: loss: 1382410.125 acc: 0.7221083045005798\n",
      "step: 35945 , time : 0.0\n",
      "train: loss: 1161367.75 acc: -0.29877495765686035  val: loss: 2405544.5 acc: 0.8252370953559875\n",
      "step: 35950 , time : 0.0\n",
      "train: loss: 557511.9375 acc: 0.820570707321167  val: loss: 1063060.75 acc: 0.8725115060806274\n",
      "step: 35955 , time : 0.0\n",
      "train: loss: 569732.75 acc: 0.6594755053520203  val: loss: 164581.453125 acc: 0.895287275314331\n",
      "step: 35960 , time : 0.0010008811950683594\n",
      "train: loss: 1304401.375 acc: 0.7147926092147827  val: loss: 1636517.125 acc: 0.4763050675392151\n",
      "step: 35965 , time : 0.0\n",
      "train: loss: 1652375.375 acc: 0.2197757363319397  val: loss: 711731.4375 acc: 0.7688144445419312\n",
      "step: 35970 , time : 0.0010008811950683594\n",
      "train: loss: 1441061.625 acc: 0.3905060291290283  val: loss: 1417442.375 acc: 0.744964599609375\n",
      "step: 35975 , time : 0.001001119613647461\n",
      "train: loss: 390370.59375 acc: 0.6942522525787354  val: loss: 1533588.875 acc: 0.7200421094894409\n",
      "step: 35980 , time : 0.0010008811950683594\n",
      "train: loss: 85000.8671875 acc: 0.9134692549705505  val: loss: 535971.625 acc: 0.7750716209411621\n",
      "step: 35985 , time : 0.0\n",
      "train: loss: 152551.578125 acc: 0.892683207988739  val: loss: 707273.6875 acc: 0.7424231767654419\n",
      "step: 35990 , time : 0.0\n",
      "train: loss: 59080.77734375 acc: 0.9547044634819031  val: loss: 331450.8125 acc: 0.819697380065918\n",
      "step: 35995 , time : 0.0\n",
      "train: loss: 467428.96875 acc: 0.8156139850616455  val: loss: 956521.3125 acc: 0.6835267543792725\n",
      "step: 36000 , time : 0.0\n",
      "train: loss: 79756.078125 acc: 0.9295119047164917  val: loss: 2311013.5 acc: 0.6443125605583191\n",
      "step: 36005 , time : 0.0\n",
      "train: loss: 194686.125 acc: 0.8588615655899048  val: loss: 2786680.0 acc: 0.6171907186508179\n",
      "step: 36010 , time : 0.0\n",
      "train: loss: 66843.484375 acc: 0.9322627186775208  val: loss: 1359478.0 acc: 0.6958767175674438\n",
      "step: 36015 , time : 0.0\n",
      "train: loss: 79304.0703125 acc: 0.9336711168289185  val: loss: 1656226.375 acc: 0.6577527523040771\n",
      "step: 36020 , time : 0.0\n",
      "train: loss: 386147.875 acc: 0.8058663606643677  val: loss: 2283835.5 acc: 0.5971783399581909\n",
      "step: 36025 , time : 0.0\n",
      "train: loss: 166346.015625 acc: 0.8870013952255249  val: loss: 1922573.75 acc: 0.6828658580780029\n",
      "step: 36030 , time : 0.0\n",
      "train: loss: 763129.5 acc: 0.727266252040863  val: loss: 1560109.125 acc: 0.6464824676513672\n",
      "step: 36035 , time : 0.0\n",
      "train: loss: 526557.4375 acc: 0.7456083297729492  val: loss: 1291860.375 acc: 0.7208093404769897\n",
      "step: 36040 , time : 0.0\n",
      "train: loss: 232242.78125 acc: 0.8190019130706787  val: loss: 692256.3125 acc: 0.7760733962059021\n",
      "step: 36045 , time : 0.0\n",
      "train: loss: 374211.625 acc: 0.7612334489822388  val: loss: 3406260.5 acc: 0.49651044607162476\n",
      "step: 36050 , time : 0.0010008811950683594\n",
      "train: loss: 1632537.625 acc: 0.6542341709136963  val: loss: 592542.6875 acc: 0.7629368305206299\n",
      "step: 36055 , time : 0.0010006427764892578\n",
      "train: loss: 1098390.125 acc: 0.796919584274292  val: loss: 889888.125 acc: 0.8026150465011597\n",
      "step: 36060 , time : 0.0\n",
      "train: loss: 574224.625 acc: 0.9183725118637085  val: loss: 1265545.75 acc: 0.762080729007721\n",
      "step: 36065 , time : 0.0\n",
      "train: loss: 531131.1875 acc: 0.9590005874633789  val: loss: 886742.4375 acc: 0.9078443050384521\n",
      "step: 36070 , time : 0.0\n",
      "train: loss: 277638.4375 acc: 0.9642181396484375  val: loss: 460908.40625 acc: 0.9063328504562378\n",
      "step: 36075 , time : 0.0\n",
      "train: loss: 1118395.5 acc: 0.8080121874809265  val: loss: 1337423.625 acc: 0.5061750411987305\n",
      "step: 36080 , time : 0.0010006427764892578\n",
      "train: loss: 140249.953125 acc: 0.9809443950653076  val: loss: 393317.9375 acc: 0.676376461982727\n",
      "step: 36085 , time : 0.0010004043579101562\n",
      "train: loss: 67265.203125 acc: 0.9955130219459534  val: loss: 792981.4375 acc: 0.6885207891464233\n",
      "step: 36090 , time : 0.0\n",
      "train: loss: 93951.8359375 acc: 0.9933227896690369  val: loss: 684564.125 acc: 0.8375808000564575\n",
      "step: 36095 , time : 0.001001119613647461\n",
      "train: loss: 820673.5625 acc: 0.9345914125442505  val: loss: 1153431.625 acc: 0.8626975417137146\n",
      "step: 36100 , time : 0.0010004043579101562\n",
      "train: loss: 65876.640625 acc: 0.9882325530052185  val: loss: 340788.78125 acc: 0.8019843697547913\n",
      "step: 36105 , time : 0.0\n",
      "train: loss: 86559.359375 acc: 0.9776064157485962  val: loss: 138997.703125 acc: 0.9452902674674988\n",
      "step: 36110 , time : 0.0\n",
      "train: loss: 33412.609375 acc: 0.9882053136825562  val: loss: 30866.17578125 acc: 0.9900756478309631\n",
      "step: 36115 , time : 0.0\n",
      "train: loss: 2301.241943359375 acc: 0.9924374222755432  val: loss: 917420.0625 acc: 0.8353095054626465\n",
      "step: 36120 , time : 0.0\n",
      "train: loss: 30485.755859375 acc: 0.986392080783844  val: loss: 1001053.375 acc: 0.704512894153595\n",
      "step: 36125 , time : 0.0\n",
      "train: loss: 12216.1435546875 acc: 0.9811282157897949  val: loss: 140195.140625 acc: 0.9809248447418213\n",
      "step: 36130 , time : 0.0\n",
      "train: loss: 2803.867431640625 acc: 0.991328775882721  val: loss: 875525.4375 acc: 0.9044548273086548\n",
      "step: 36135 , time : 0.0\n",
      "train: loss: 20484.138671875 acc: 0.8549209833145142  val: loss: 59553.125 acc: 0.9904847145080566\n",
      "step: 36140 , time : 0.0\n",
      "train: loss: 20053.220703125 acc: 0.9915894269943237  val: loss: 506643.0625 acc: 0.8602562546730042\n",
      "step: 36145 , time : 0.0\n",
      "train: loss: 4464.49365234375 acc: 0.9908344745635986  val: loss: 354528.75 acc: 0.9212565422058105\n",
      "step: 36150 , time : 0.0\n",
      "train: loss: 7608.5302734375 acc: 0.9801201224327087  val: loss: 342100.625 acc: 0.9461191296577454\n",
      "step: 36155 , time : 0.0\n",
      "train: loss: 32802.67578125 acc: 0.9838971495628357  val: loss: 1547659.0 acc: 0.5136770009994507\n",
      "step: 36160 , time : 0.0\n",
      "train: loss: 35746.2734375 acc: 0.9805717468261719  val: loss: 575330.5625 acc: 0.9186896681785583\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 36165 , time : 0.0\n",
      "train: loss: 24697.57421875 acc: 0.9874506592750549  val: loss: 724371.8125 acc: 0.8789945840835571\n",
      "step: 36170 , time : 0.0\n",
      "train: loss: 8763.560546875 acc: 0.991858184337616  val: loss: 1701653.625 acc: 0.780177891254425\n",
      "step: 36175 , time : 0.0\n",
      "train: loss: 13650.19921875 acc: 0.9886571168899536  val: loss: 872447.0625 acc: 0.9516772031784058\n",
      "step: 36180 , time : 0.0010004043579101562\n",
      "train: loss: 9083.1591796875 acc: 0.9833682775497437  val: loss: 2595513.0 acc: 0.8191774487495422\n",
      "step: 36185 , time : 0.0010004043579101562\n",
      "train: loss: 5909.47119140625 acc: 0.9945888519287109  val: loss: 3789140.0 acc: 0.5355873107910156\n",
      "step: 36190 , time : 0.0\n",
      "train: loss: 22903.6015625 acc: 0.9927811622619629  val: loss: 2107125.0 acc: 0.6736563444137573\n",
      "step: 36195 , time : 0.0\n",
      "train: loss: 34279.83203125 acc: 0.9911397099494934  val: loss: 1099027.375 acc: 0.7289208173751831\n",
      "step: 36200 , time : 0.0010006427764892578\n",
      "train: loss: 26174.572265625 acc: 0.9914169907569885  val: loss: 380777.0625 acc: 0.9624559283256531\n",
      "step: 36205 , time : 0.0\n",
      "train: loss: 32280.701171875 acc: 0.9915298819541931  val: loss: 1472625.875 acc: 0.4938601851463318\n",
      "step: 36210 , time : 0.0\n",
      "train: loss: 33006.640625 acc: 0.9851097464561462  val: loss: 861715.8125 acc: 0.8957876563072205\n",
      "step: 36215 , time : 0.0\n",
      "train: loss: 100638.046875 acc: 0.9607857465744019  val: loss: 1422928.375 acc: 0.7055791020393372\n",
      "step: 36220 , time : 0.015625\n",
      "train: loss: 115292.1640625 acc: 0.9647008180618286  val: loss: 398997.84375 acc: 0.8846307396888733\n",
      "step: 36225 , time : 0.0\n",
      "train: loss: 96562.5390625 acc: 0.9347079992294312  val: loss: 1311176.875 acc: 0.7778818607330322\n",
      "step: 36230 , time : 0.0\n",
      "train: loss: 113668.2734375 acc: 0.9872282147407532  val: loss: 1580327.75 acc: 0.8601664304733276\n",
      "step: 36235 , time : 0.0\n",
      "train: loss: 558093.875 acc: 0.9584224224090576  val: loss: 464907.375 acc: 0.8918629884719849\n",
      "step: 36240 , time : 0.0009999275207519531\n",
      "train: loss: 28322.029296875 acc: 0.9965518712997437  val: loss: 1353004.625 acc: 0.7830828428268433\n",
      "step: 36245 , time : 0.0010006427764892578\n",
      "train: loss: 803391.0625 acc: 0.8853590488433838  val: loss: 357035.71875 acc: 0.8648949265480042\n",
      "step: 36250 , time : 0.0\n",
      "train: loss: 343359.875 acc: 0.9695314764976501  val: loss: 349524.90625 acc: 0.9541232585906982\n",
      "step: 36255 , time : 0.0\n",
      "train: loss: 772679.125 acc: 0.9604040384292603  val: loss: 637414.875 acc: 0.46899157762527466\n",
      "step: 36260 , time : 0.0\n",
      "train: loss: 223077.84375 acc: 0.984472930431366  val: loss: 672913.125 acc: 0.7394171953201294\n",
      "step: 36265 , time : 0.0\n",
      "train: loss: 499762.3125 acc: 0.927617609500885  val: loss: 3675370.25 acc: 0.42511892318725586\n",
      "step: 36270 , time : 0.0\n",
      "train: loss: 887716.875 acc: 0.9638307094573975  val: loss: 2394648.0 acc: -0.07618522644042969\n",
      "step: 36275 , time : 0.0\n",
      "train: loss: 785526.8125 acc: 0.9783302545547485  val: loss: 1008963.0 acc: 0.8131587505340576\n",
      "step: 36280 , time : 0.0\n",
      "train: loss: 1463611.125 acc: 0.9212637543678284  val: loss: 429552.90625 acc: 0.9333335757255554\n",
      "step: 36285 , time : 0.0\n",
      "train: loss: 342992.375 acc: 0.9710890054702759  val: loss: 2433141.25 acc: 0.5999163389205933\n",
      "step: 36290 , time : 0.0010006427764892578\n",
      "train: loss: 100398.953125 acc: 0.9783881306648254  val: loss: 2652108.25 acc: 0.5390980243682861\n",
      "step: 36295 , time : 0.0\n",
      "train: loss: 1604344.125 acc: 0.659696638584137  val: loss: 1097318.625 acc: 0.8586512804031372\n",
      "step: 36300 , time : 0.0\n",
      "train: loss: 903019.5625 acc: 0.9176795482635498  val: loss: 1014399.6875 acc: 0.8689196109771729\n",
      "step: 36305 , time : 0.0010006427764892578\n",
      "train: loss: 2107022.75 acc: 0.4587936997413635  val: loss: 1048946.625 acc: 0.5291299819946289\n",
      "step: 36310 , time : 0.0010008811950683594\n",
      "train: loss: 1264765.75 acc: -0.15691065788269043  val: loss: 1087842.0 acc: 0.5962716341018677\n",
      "step: 36315 , time : 0.0010004043579101562\n",
      "train: loss: 676102.625 acc: 0.5461350679397583  val: loss: 675720.625 acc: 0.8393168449401855\n",
      "step: 36320 , time : 0.0\n",
      "train: loss: 377666.5 acc: 0.8320984244346619  val: loss: 666900.875 acc: 0.7938354015350342\n",
      "step: 36325 , time : 0.0\n",
      "train: loss: 578065.0625 acc: 0.797759473323822  val: loss: 710463.625 acc: 0.8212548494338989\n",
      "step: 36330 , time : 0.0\n",
      "train: loss: 842525.625 acc: 0.6029986143112183  val: loss: 2268583.75 acc: 0.776890218257904\n",
      "step: 36335 , time : 0.0\n",
      "train: loss: 580381.75 acc: 0.7641212940216064  val: loss: 1828610.125 acc: 0.7534582018852234\n",
      "step: 36340 , time : 0.0\n",
      "train: loss: 549162.6875 acc: 0.8010727167129517  val: loss: 1355826.75 acc: 0.7389857769012451\n",
      "step: 36345 , time : 0.0\n",
      "train: loss: 238628.59375 acc: 0.8507874011993408  val: loss: 2152268.5 acc: 0.6095291376113892\n",
      "step: 36350 , time : 0.0\n",
      "train: loss: 98799.46875 acc: 0.9076118469238281  val: loss: 2543826.25 acc: 0.6474490165710449\n",
      "step: 36355 , time : 0.0\n",
      "train: loss: 10219.0478515625 acc: 0.9897612929344177  val: loss: 2199997.0 acc: 0.6625587940216064\n",
      "step: 36360 , time : 0.0\n",
      "train: loss: 38943.61328125 acc: 0.9717327356338501  val: loss: 2131502.25 acc: 0.6618853807449341\n",
      "step: 36365 , time : 0.0\n",
      "train: loss: 273829.46875 acc: 0.844105064868927  val: loss: 1200315.25 acc: 0.6074118614196777\n",
      "step: 36370 , time : 0.0\n",
      "train: loss: 38268.65625 acc: 0.9729475378990173  val: loss: 1522524.5 acc: 0.6430131196975708\n",
      "step: 36375 , time : 0.015626192092895508\n",
      "train: loss: 161747.015625 acc: 0.8161238431930542  val: loss: 572928.5625 acc: 0.7172759771347046\n",
      "step: 36380 , time : 0.0\n",
      "train: loss: 35289.09765625 acc: 0.9681211709976196  val: loss: 159052.875 acc: 0.8618017435073853\n",
      "step: 36385 , time : 0.0\n",
      "train: loss: 507818.0 acc: 0.7682446241378784  val: loss: 1733453.25 acc: 0.6909936666488647\n",
      "step: 36390 , time : 0.0\n",
      "train: loss: 423074.84375 acc: 0.7512826323509216  val: loss: 1431929.75 acc: 0.6549142599105835\n",
      "step: 36395 , time : 0.001001596450805664\n",
      "train: loss: 108191.6328125 acc: 0.9169766902923584  val: loss: 659952.0625 acc: 0.6947741508483887\n",
      "step: 36400 , time : 0.0\n",
      "train: loss: 111732.234375 acc: 0.8922405242919922  val: loss: 1413480.25 acc: 0.7090071439743042\n",
      "step: 36405 , time : 0.0010006427764892578\n",
      "train: loss: 437594.40625 acc: 0.4152070879936218  val: loss: 889078.375 acc: 0.7343481183052063\n",
      "step: 36410 , time : 0.0010006427764892578\n",
      "train: loss: 245679.78125 acc: 0.6667823791503906  val: loss: 1855364.0 acc: 0.7134371399879456\n",
      "step: 36415 , time : 0.0\n",
      "train: loss: 143829.546875 acc: 0.888053834438324  val: loss: 378453.875 acc: 0.8174927830696106\n",
      "step: 36420 , time : 0.0010006427764892578\n",
      "train: loss: 976912.0625 acc: 0.6635140180587769  val: loss: 861217.1875 acc: 0.8201669454574585\n",
      "step: 36425 , time : 0.0\n",
      "train: loss: 1078558.375 acc: 0.7997052669525146  val: loss: 372392.96875 acc: 0.8744909167289734\n",
      "step: 36430 , time : 0.0\n",
      "train: loss: 583669.9375 acc: 0.9542121291160583  val: loss: 895732.0625 acc: 0.8559668064117432\n",
      "step: 36435 , time : 0.0\n",
      "train: loss: 288710.03125 acc: 0.9532044529914856  val: loss: 398212.34375 acc: 0.8737435936927795\n",
      "step: 36440 , time : 0.0\n",
      "train: loss: 93473.75 acc: 0.9816672205924988  val: loss: 872222.375 acc: 0.6232084035873413\n",
      "step: 36445 , time : 0.0\n",
      "train: loss: 576449.1875 acc: 0.9487866759300232  val: loss: 1358979.375 acc: 0.5373674631118774\n",
      "step: 36450 , time : 0.0\n",
      "train: loss: 89663.2265625 acc: 0.9927641749382019  val: loss: 449107.96875 acc: 0.9379240870475769\n",
      "step: 36455 , time : 0.0\n",
      "train: loss: 262093.46875 acc: 0.9725564122200012  val: loss: 678696.1875 acc: 0.7398923635482788\n",
      "step: 36460 , time : 0.0\n",
      "train: loss: 66292.4609375 acc: 0.9945881962776184  val: loss: 432790.78125 acc: 0.8026878833770752\n",
      "step: 36465 , time : 0.0\n",
      "train: loss: 54542.78515625 acc: 0.9927660822868347  val: loss: 315285.84375 acc: 0.9455768465995789\n",
      "step: 36470 , time : 0.0\n",
      "train: loss: 41436.65625 acc: 0.992054283618927  val: loss: 735920.1875 acc: 0.5611300468444824\n",
      "step: 36475 , time : 0.0\n",
      "train: loss: 8533.9287109375 acc: 0.9944469928741455  val: loss: 118893.8828125 acc: 0.9809243083000183\n",
      "step: 36480 , time : 0.0\n",
      "train: loss: 21506.091796875 acc: 0.9863370656967163  val: loss: 2914020.75 acc: 0.7642489075660706\n",
      "step: 36485 , time : 0.0\n",
      "train: loss: 19457.837890625 acc: 0.9866167902946472  val: loss: 952441.375 acc: 0.9026197195053101\n",
      "step: 36490 , time : 0.0\n",
      "train: loss: 9724.169921875 acc: 0.9900694489479065  val: loss: 394756.875 acc: 0.957812488079071\n",
      "step: 36495 , time : 0.0\n",
      "train: loss: 22906.677734375 acc: 0.987382173538208  val: loss: 254274.796875 acc: 0.9095096588134766\n",
      "step: 36500 , time : 0.0\n",
      "train: loss: 17932.033203125 acc: 0.9662628173828125  val: loss: 1337649.875 acc: 0.849088191986084\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 36505 , time : 0.0\n",
      "train: loss: 3163.03564453125 acc: 0.9899618029594421  val: loss: 917995.0625 acc: 0.8417744636535645\n",
      "step: 36510 , time : 0.0\n",
      "train: loss: 8017.86328125 acc: 0.9817140698432922  val: loss: 1928872.875 acc: 0.7314813137054443\n",
      "step: 36515 , time : 0.0\n",
      "train: loss: 10847.3896484375 acc: 0.9811664819717407  val: loss: 524066.34375 acc: 0.959400475025177\n",
      "step: 36520 , time : 0.0010004043579101562\n",
      "train: loss: 29110.875 acc: 0.9808163046836853  val: loss: 2213442.25 acc: 0.7912517786026001\n",
      "step: 36525 , time : 0.0\n",
      "train: loss: 15250.931640625 acc: 0.9860119223594666  val: loss: 757131.9375 acc: 0.9278085231781006\n",
      "step: 36530 , time : 0.0010001659393310547\n",
      "train: loss: 43526.08984375 acc: 0.9803022742271423  val: loss: 1976041.75 acc: 0.7221231460571289\n",
      "step: 36535 , time : 0.0\n",
      "train: loss: 7910.6884765625 acc: 0.9957190155982971  val: loss: 722927.1875 acc: 0.6346800327301025\n",
      "step: 36540 , time : 0.0\n",
      "train: loss: 10011.7900390625 acc: 0.9928723573684692  val: loss: 3137464.25 acc: 0.034154534339904785\n",
      "step: 36545 , time : 0.0010004043579101562\n",
      "train: loss: 13926.232421875 acc: 0.9905481338500977  val: loss: 1796020.0 acc: 0.5866032242774963\n",
      "step: 36550 , time : 0.001001119613647461\n",
      "train: loss: 26632.44140625 acc: 0.978570282459259  val: loss: 2835840.75 acc: 0.5526311993598938\n",
      "step: 36555 , time : 0.0\n",
      "train: loss: 40637.62109375 acc: 0.9827656745910645  val: loss: 3169401.75 acc: 0.7236955761909485\n",
      "step: 36560 , time : 0.0\n",
      "train: loss: 33306.90234375 acc: 0.9903062582015991  val: loss: 1777884.5 acc: 0.7771653532981873\n",
      "step: 36565 , time : 0.0\n",
      "train: loss: 26690.126953125 acc: 0.9917896389961243  val: loss: 2128119.75 acc: 0.7662873864173889\n",
      "step: 36570 , time : 0.0\n",
      "train: loss: 24792.80078125 acc: 0.9803987145423889  val: loss: 1598298.5 acc: 0.48368149995803833\n",
      "step: 36575 , time : 0.0\n",
      "train: loss: 35323.484375 acc: 0.9903227090835571  val: loss: 1246407.5 acc: 0.3245004415512085\n",
      "step: 36580 , time : 0.0\n",
      "train: loss: 62110.66796875 acc: 0.98832768201828  val: loss: 2676034.75 acc: 0.1082911491394043\n",
      "step: 36585 , time : 0.0\n",
      "train: loss: 48052.0390625 acc: 0.9791745543479919  val: loss: 1559448.5 acc: 0.7254253029823303\n",
      "step: 36590 , time : 0.0\n",
      "train: loss: 126543.390625 acc: 0.9614626169204712  val: loss: 176986.125 acc: 0.9550039172172546\n",
      "step: 36595 , time : 0.0\n",
      "train: loss: 171150.390625 acc: 0.9746574759483337  val: loss: 1214045.375 acc: 0.46130549907684326\n",
      "step: 36600 , time : 0.0\n",
      "train: loss: 109661.3359375 acc: 0.9913911819458008  val: loss: 2568843.75 acc: 0.5851479172706604\n",
      "step: 36605 , time : 0.0\n",
      "train: loss: 41252.1484375 acc: 0.9949893951416016  val: loss: 278651.21875 acc: 0.9303004741668701\n",
      "step: 36610 , time : 0.0\n",
      "train: loss: 236085.546875 acc: 0.9657450914382935  val: loss: 1142592.125 acc: 0.8443019390106201\n",
      "step: 36615 , time : 0.0\n",
      "train: loss: 176160.0 acc: 0.9805907607078552  val: loss: 2875894.0 acc: 0.5817049741744995\n",
      "step: 36620 , time : 0.0010006427764892578\n",
      "train: loss: 371920.21875 acc: 0.9742254018783569  val: loss: 947023.8125 acc: 0.8698900938034058\n",
      "step: 36625 , time : 0.0\n",
      "train: loss: 222314.5625 acc: 0.9526373147964478  val: loss: 781743.125 acc: 0.9290546178817749\n",
      "step: 36630 , time : 0.0010008811950683594\n",
      "train: loss: 158233.34375 acc: 0.964871346950531  val: loss: 947042.25 acc: 0.8710018992424011\n",
      "step: 36635 , time : 0.0\n",
      "train: loss: 624053.125 acc: 0.9640339612960815  val: loss: 1715419.25 acc: 0.6016839742660522\n",
      "step: 36640 , time : 0.0\n",
      "train: loss: 682989.875 acc: 0.9670855402946472  val: loss: 587800.25 acc: 0.8982479572296143\n",
      "step: 36645 , time : 0.0010008811950683594\n",
      "train: loss: 3832746.75 acc: 0.8539758920669556  val: loss: 1541908.375 acc: 0.6943231225013733\n",
      "step: 36650 , time : 0.0010006427764892578\n",
      "train: loss: 862563.625 acc: 0.9716582298278809  val: loss: 948423.375 acc: 0.7407423257827759\n",
      "step: 36655 , time : 0.0\n",
      "train: loss: 200220.328125 acc: 0.9754304885864258  val: loss: 939082.875 acc: 0.7941226959228516\n",
      "step: 36660 , time : 0.0\n",
      "train: loss: 615202.375 acc: 0.9164471626281738  val: loss: 1131916.25 acc: 0.7307966351509094\n",
      "step: 36665 , time : 0.0\n",
      "train: loss: 395470.875 acc: 0.959692656993866  val: loss: 267303.5 acc: 0.913440465927124\n",
      "step: 36670 , time : 0.0\n",
      "train: loss: 1806739.0 acc: 0.2061510682106018  val: loss: 1229943.0 acc: 0.6194119453430176\n",
      "step: 36675 , time : 0.0\n",
      "train: loss: 1813730.0 acc: 0.19226133823394775  val: loss: 2340687.25 acc: 0.673597514629364\n",
      "step: 36680 , time : 0.0\n",
      "train: loss: 1153592.125 acc: 0.7124643921852112  val: loss: 859406.9375 acc: 0.6878637671470642\n",
      "step: 36685 , time : 0.0\n",
      "train: loss: 334528.96875 acc: 0.8272143006324768  val: loss: 487986.03125 acc: 0.8427996635437012\n",
      "step: 36690 , time : 0.0\n",
      "train: loss: 405081.34375 acc: 0.8864126205444336  val: loss: 474601.46875 acc: 0.6432011127471924\n",
      "step: 36695 , time : 0.0\n",
      "train: loss: 1953052.0 acc: 0.013938069343566895  val: loss: 1195571.5 acc: 0.7539599537849426\n",
      "step: 36700 , time : 0.0\n",
      "train: loss: 871551.1875 acc: 0.4140058755874634  val: loss: 1546579.75 acc: 0.24963629245758057\n",
      "step: 36705 , time : 0.0\n",
      "train: loss: 879577.0625 acc: 0.4987735152244568  val: loss: 1512366.875 acc: 0.7511746883392334\n",
      "step: 36710 , time : 0.0\n",
      "train: loss: 729899.125 acc: 0.6618105173110962  val: loss: 1350105.875 acc: 0.7535815834999084\n",
      "step: 36715 , time : 0.0\n",
      "train: loss: 184924.84375 acc: 0.8699164986610413  val: loss: 997549.0 acc: 0.8233752846717834\n",
      "step: 36720 , time : 0.0\n",
      "train: loss: 151617.6875 acc: 0.8801220059394836  val: loss: 560781.9375 acc: 0.8988704681396484\n",
      "step: 36725 , time : 0.0\n",
      "train: loss: 345583.78125 acc: 0.8455175757408142  val: loss: 418094.84375 acc: 0.762821614742279\n",
      "step: 36730 , time : 0.0\n",
      "train: loss: 600578.6875 acc: 0.7188645005226135  val: loss: 1247622.25 acc: 0.8002892136573792\n",
      "step: 36735 , time : 0.0010006427764892578\n",
      "train: loss: 123649.3671875 acc: 0.9188202023506165  val: loss: 1509126.75 acc: 0.7415704727172852\n",
      "step: 36740 , time : 0.001001119613647461\n",
      "train: loss: 49192.94921875 acc: 0.9552326798439026  val: loss: 421726.84375 acc: 0.8702213764190674\n",
      "step: 36745 , time : 0.0\n",
      "train: loss: 159677.046875 acc: 0.800585150718689  val: loss: 319686.46875 acc: 0.9099819660186768\n",
      "step: 36750 , time : 0.0\n",
      "train: loss: 137768.890625 acc: 0.8596190214157104  val: loss: 1042463.125 acc: 0.6757643818855286\n",
      "step: 36755 , time : 0.0010006427764892578\n",
      "train: loss: 677223.3125 acc: 0.5279710292816162  val: loss: 1307645.875 acc: 0.7775930166244507\n",
      "step: 36760 , time : 0.0\n",
      "train: loss: 114957.46875 acc: 0.886841893196106  val: loss: 1133973.5 acc: 0.6726154088973999\n",
      "step: 36765 , time : 0.0\n",
      "train: loss: 576749.4375 acc: 0.5656509399414062  val: loss: 1733468.75 acc: 0.6637810468673706\n",
      "step: 36770 , time : 0.0\n",
      "train: loss: 548093.875 acc: 0.7233743071556091  val: loss: 770645.625 acc: 0.6892491579055786\n",
      "step: 36775 , time : 0.0\n",
      "train: loss: 276792.71875 acc: 0.8017846345901489  val: loss: 413952.1875 acc: 0.7705754041671753\n",
      "step: 36780 , time : 0.0\n",
      "train: loss: 200200.171875 acc: 0.7857649922370911  val: loss: 566443.875 acc: 0.7419980764389038\n",
      "step: 36785 , time : 0.0\n",
      "train: loss: 911098.9375 acc: 0.8065526485443115  val: loss: 1118360.5 acc: 0.7819042205810547\n",
      "step: 36790 , time : 0.0\n",
      "train: loss: 1784829.5 acc: 0.8466455340385437  val: loss: 1932858.375 acc: 0.7484047412872314\n",
      "step: 36795 , time : 0.0\n",
      "train: loss: 768462.6875 acc: 0.9231594800949097  val: loss: 1077029.5 acc: 0.875759482383728\n",
      "step: 36800 , time : 0.0\n",
      "train: loss: 539613.8125 acc: 0.9168976545333862  val: loss: 1261050.0 acc: 0.8986231088638306\n",
      "step: 36805 , time : 0.0\n",
      "train: loss: 416281.1875 acc: 0.8995858430862427  val: loss: 702430.0 acc: 0.8961371183395386\n",
      "step: 36810 , time : 0.0\n",
      "train: loss: 412068.96875 acc: 0.9116080403327942  val: loss: 1262828.875 acc: 0.9018972516059875\n",
      "step: 36815 , time : 0.015626192092895508\n",
      "train: loss: 481815.59375 acc: 0.9511576294898987  val: loss: 1088108.375 acc: 0.7072572708129883\n",
      "step: 36820 , time : 0.0\n",
      "train: loss: 341079.28125 acc: 0.9744491577148438  val: loss: 1516649.25 acc: 0.7549378871917725\n",
      "step: 36825 , time : 0.0\n",
      "train: loss: 393779.0 acc: 0.9563954472541809  val: loss: 1139957.375 acc: 0.7438048720359802\n",
      "step: 36830 , time : 0.0\n",
      "train: loss: 251533.0625 acc: 0.9745329022407532  val: loss: 1721924.5 acc: 0.8050686120986938\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 36835 , time : 0.0009999275207519531\n",
      "train: loss: 167104.34375 acc: 0.9498543739318848  val: loss: 474304.5625 acc: 0.957999587059021\n",
      "step: 36840 , time : 0.0\n",
      "train: loss: 131272.96875 acc: 0.9614211320877075  val: loss: 736027.75 acc: 0.9044573307037354\n",
      "step: 36845 , time : 0.0\n",
      "train: loss: 38875.4296875 acc: 0.9842066764831543  val: loss: 974549.3125 acc: 0.8084049224853516\n",
      "step: 36850 , time : 0.0\n",
      "train: loss: 17634.623046875 acc: 0.9718828797340393  val: loss: 2625127.25 acc: 0.35185420513153076\n",
      "step: 36855 , time : 0.0\n",
      "train: loss: 27219.337890625 acc: 0.9654289484024048  val: loss: 1525477.5 acc: 0.5939002633094788\n",
      "step: 36860 , time : 0.0010008811950683594\n",
      "train: loss: 18991.54296875 acc: 0.9709784388542175  val: loss: 2535501.75 acc: 0.6093049049377441\n",
      "step: 36865 , time : 0.0\n",
      "train: loss: 19293.021484375 acc: 0.9607529044151306  val: loss: 4255741.0 acc: 0.6182363629341125\n",
      "step: 36870 , time : 0.0\n",
      "train: loss: 9228.8798828125 acc: 0.9594371914863586  val: loss: 1956171.625 acc: -0.15667212009429932\n",
      "step: 36875 , time : 0.0\n",
      "train: loss: 15109.3056640625 acc: 0.9869322776794434  val: loss: 758591.875 acc: 0.936813235282898\n",
      "step: 36880 , time : 0.0\n",
      "train: loss: 4543.07275390625 acc: 0.9862233400344849  val: loss: 887732.25 acc: 0.6330453157424927\n",
      "step: 36885 , time : 0.0\n",
      "train: loss: 6017.68359375 acc: 0.9956716299057007  val: loss: 1367184.0 acc: 0.5183723568916321\n",
      "step: 36890 , time : 0.0\n",
      "train: loss: 23430.328125 acc: 0.9840530753135681  val: loss: 1523139.0 acc: 0.3137548565864563\n",
      "step: 36895 , time : 0.0\n",
      "train: loss: 103097.484375 acc: 0.9341834783554077  val: loss: 1520416.5 acc: 0.3314855694770813\n",
      "step: 36900 , time : 0.0\n",
      "train: loss: 21522.4765625 acc: 0.9839478135108948  val: loss: 670729.25 acc: 0.9435271620750427\n",
      "step: 36905 , time : 0.0\n",
      "train: loss: 17032.470703125 acc: 0.9887780547142029  val: loss: 2594939.75 acc: -0.13528430461883545\n",
      "step: 36910 , time : 0.0\n",
      "train: loss: 2729.156982421875 acc: 0.9947184324264526  val: loss: 3212742.25 acc: -1.305345058441162\n",
      "step: 36915 , time : 0.0\n",
      "train: loss: 13129.025390625 acc: 0.9897268414497375  val: loss: 1123959.875 acc: 0.7653889656066895\n",
      "step: 36920 , time : 0.0\n",
      "train: loss: 23505.79296875 acc: 0.9919583797454834  val: loss: 3419366.25 acc: -0.005712270736694336\n",
      "step: 36925 , time : 0.0\n",
      "train: loss: 13516.48046875 acc: 0.9931420683860779  val: loss: 2230784.75 acc: 0.6769919395446777\n",
      "step: 36930 , time : 0.0\n",
      "train: loss: 29285.841796875 acc: 0.9934238791465759  val: loss: 1101890.875 acc: 0.8247692584991455\n",
      "step: 36935 , time : 0.0\n",
      "train: loss: 30529.056640625 acc: 0.9753093719482422  val: loss: 2437754.25 acc: 0.35131216049194336\n",
      "step: 36940 , time : 0.0\n",
      "train: loss: 38255.390625 acc: 0.9794226884841919  val: loss: 821123.0 acc: 0.5491005182266235\n",
      "step: 36945 , time : 0.0\n",
      "train: loss: 108685.4453125 acc: 0.9697520136833191  val: loss: 2497226.5 acc: 0.5797936916351318\n",
      "step: 36950 , time : 0.0\n",
      "train: loss: 253499.578125 acc: 0.9152323007583618  val: loss: 633496.3125 acc: 0.7116245627403259\n",
      "step: 36955 , time : 0.0\n",
      "train: loss: 76631.1640625 acc: 0.9812229871749878  val: loss: 1605529.0 acc: 0.6135772466659546\n",
      "step: 36960 , time : 0.0010006427764892578\n",
      "train: loss: 145384.171875 acc: 0.9733312129974365  val: loss: 733086.875 acc: 0.8498319387435913\n",
      "step: 36965 , time : 0.0\n",
      "train: loss: 67739.3984375 acc: 0.9918990135192871  val: loss: 2550676.5 acc: 0.2152918577194214\n",
      "step: 36970 , time : 0.001001119613647461\n",
      "train: loss: 64690.640625 acc: 0.9937878847122192  val: loss: 2560817.75 acc: 0.443453311920166\n",
      "step: 36975 , time : 0.0\n",
      "train: loss: 165566.90625 acc: 0.9826067090034485  val: loss: 1387079.375 acc: 0.34409844875335693\n",
      "step: 36980 , time : 0.0\n",
      "train: loss: 136420.296875 acc: 0.9837155342102051  val: loss: 1009235.0 acc: 0.8318396210670471\n",
      "step: 36985 , time : 0.0\n",
      "train: loss: 325863.0 acc: 0.9665699005126953  val: loss: 565405.25 acc: 0.804959774017334\n",
      "step: 36990 , time : 0.0\n",
      "train: loss: 294576.875 acc: 0.9706238508224487  val: loss: 817251.125 acc: 0.8776382207870483\n",
      "step: 36995 , time : 0.0\n",
      "train: loss: 335714.75 acc: 0.9369622468948364  val: loss: 954501.75 acc: 0.8130033016204834\n",
      "step: 37000 , time : 0.0\n",
      "train: loss: 793851.125 acc: 0.9399936199188232  val: loss: 623413.4375 acc: 0.7499368786811829\n",
      "step: 37005 , time : 0.0\n",
      "train: loss: 1694045.25 acc: 0.9456461668014526  val: loss: 1384763.625 acc: 0.8399962186813354\n",
      "step: 37010 , time : 0.0\n",
      "train: loss: 858274.5 acc: 0.979396402835846  val: loss: 246806.421875 acc: 0.9429802298545837\n",
      "step: 37015 , time : 0.0\n",
      "train: loss: 1103014.375 acc: 0.9375130534172058  val: loss: 766485.25 acc: 0.8605216145515442\n",
      "step: 37020 , time : 0.0\n",
      "train: loss: 630632.1875 acc: 0.9453163146972656  val: loss: 271724.96875 acc: 0.8961378335952759\n",
      "step: 37025 , time : 0.0\n",
      "train: loss: 415367.875 acc: 0.9337978959083557  val: loss: 1766763.5 acc: 0.7549132108688354\n",
      "step: 37030 , time : 0.0\n",
      "train: loss: 431056.59375 acc: 0.9587481021881104  val: loss: 341003.71875 acc: 0.8199819326400757\n",
      "step: 37035 , time : 0.0\n",
      "train: loss: 1762098.0 acc: 0.6951756477355957  val: loss: 1033196.625 acc: 0.8857977390289307\n",
      "step: 37040 , time : 0.0\n",
      "train: loss: 1191459.5 acc: 0.6183721423149109  val: loss: 939456.6875 acc: 0.6819127798080444\n",
      "step: 37045 , time : 0.0\n",
      "train: loss: 877447.9375 acc: 0.7413580417633057  val: loss: 745685.5625 acc: 0.6378230452537537\n",
      "step: 37050 , time : 0.0\n",
      "train: loss: 539008.9375 acc: 0.757797122001648  val: loss: 474506.40625 acc: 0.7301311492919922\n",
      "step: 37055 , time : 0.0\n",
      "train: loss: 1680105.5 acc: 0.608641505241394  val: loss: 784817.5625 acc: 0.8584866523742676\n",
      "step: 37060 , time : 0.002001047134399414\n",
      "train: loss: 1689134.625 acc: 0.12342923879623413  val: loss: 829186.9375 acc: 0.8049606084823608\n",
      "step: 37065 , time : 0.0\n",
      "train: loss: 784614.6875 acc: 0.3366349935531616  val: loss: 2005700.75 acc: 0.5347638726234436\n",
      "step: 37070 , time : 0.0\n",
      "train: loss: 1236104.25 acc: 0.5182086229324341  val: loss: 2557155.0 acc: 0.634825587272644\n",
      "step: 37075 , time : 0.0\n",
      "train: loss: 683479.375 acc: 0.39397722482681274  val: loss: 648371.4375 acc: 0.7037513256072998\n",
      "step: 37080 , time : 0.0\n",
      "train: loss: 241498.0625 acc: 0.7834609746932983  val: loss: 1588862.25 acc: 0.7425097823143005\n",
      "step: 37085 , time : 0.0\n",
      "train: loss: 73682.71875 acc: 0.940556526184082  val: loss: 528559.5 acc: 0.843515157699585\n",
      "step: 37090 , time : 0.0\n",
      "train: loss: 243214.703125 acc: 0.8008014559745789  val: loss: 987101.375 acc: 0.6534178256988525\n",
      "step: 37095 , time : 0.0\n",
      "train: loss: 61349.421875 acc: 0.9473775625228882  val: loss: 1398828.25 acc: 0.7987604141235352\n",
      "step: 37100 , time : 0.0\n",
      "train: loss: 388651.96875 acc: 0.7288352251052856  val: loss: 716352.4375 acc: 0.8800344467163086\n",
      "step: 37105 , time : 0.0\n",
      "train: loss: 290678.125 acc: 0.7151578068733215  val: loss: 756706.25 acc: 0.8875402212142944\n",
      "step: 37110 , time : 0.0\n",
      "train: loss: 63296.37109375 acc: 0.9265238046646118  val: loss: 1667878.5 acc: 0.8692418932914734\n",
      "step: 37115 , time : 0.0\n",
      "train: loss: 120535.1015625 acc: 0.8993273973464966  val: loss: 2305930.75 acc: 0.8156150579452515\n",
      "step: 37120 , time : 0.0\n",
      "train: loss: 235760.8125 acc: 0.7568554282188416  val: loss: 1246135.25 acc: 0.8309586048126221\n",
      "step: 37125 , time : 0.0\n",
      "train: loss: 300995.5625 acc: 0.7441260814666748  val: loss: 756157.625 acc: 0.7735499143600464\n",
      "step: 37130 , time : 0.0\n",
      "train: loss: 993247.3125 acc: 0.26916295289993286  val: loss: 589633.6875 acc: 0.8433034420013428\n",
      "step: 37135 , time : 0.0\n",
      "train: loss: 336416.5625 acc: 0.7390540242195129  val: loss: 2413613.5 acc: 0.6838170289993286\n",
      "step: 37140 , time : 0.0\n",
      "train: loss: 373972.125 acc: 0.7130148410797119  val: loss: 2174688.75 acc: 0.7300140857696533\n",
      "step: 37145 , time : 0.0\n",
      "train: loss: 791474.4375 acc: 0.6771500706672668  val: loss: 2996185.75 acc: 0.7034695744514465\n",
      "step: 37150 , time : 0.015625715255737305\n",
      "train: loss: 874762.4375 acc: 0.7035239934921265  val: loss: 985155.5625 acc: 0.7221505641937256\n",
      "step: 37155 , time : 0.0\n",
      "train: loss: 1667338.875 acc: 0.8261339664459229  val: loss: 962909.4375 acc: 0.7315800189971924\n",
      "step: 37160 , time : 0.0\n",
      "train: loss: 801486.8125 acc: 0.9392796754837036  val: loss: 1281952.625 acc: 0.4507542848587036\n",
      "step: 37165 , time : 0.0\n",
      "train: loss: 1005678.3125 acc: 0.8655155897140503  val: loss: 1003979.6875 acc: 0.9087880849838257\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 37170 , time : 0.0\n",
      "train: loss: 570032.25 acc: 0.9161040782928467  val: loss: 2239580.25 acc: 0.5608299970626831\n",
      "step: 37175 , time : 0.0\n",
      "train: loss: 354261.375 acc: 0.948803186416626  val: loss: 1345316.125 acc: 0.867300271987915\n",
      "step: 37180 , time : 0.0010008811950683594\n",
      "train: loss: 221536.3125 acc: 0.9817633032798767  val: loss: 1614215.875 acc: 0.8791936039924622\n",
      "step: 37185 , time : 0.0010008811950683594\n",
      "train: loss: 222694.21875 acc: 0.9861651659011841  val: loss: 1741193.75 acc: 0.35782283544540405\n",
      "step: 37190 , time : 0.0\n",
      "train: loss: 223274.359375 acc: 0.9787060022354126  val: loss: 809892.625 acc: 0.6098532676696777\n",
      "step: 37195 , time : 0.0\n",
      "train: loss: 257909.109375 acc: 0.9650310277938843  val: loss: 1134927.5 acc: 0.6854239702224731\n",
      "step: 37200 , time : 0.0\n",
      "train: loss: 170950.78125 acc: 0.9296423196792603  val: loss: 1163411.5 acc: 0.7534210085868835\n",
      "step: 37205 , time : 0.0\n",
      "train: loss: 79263.78125 acc: 0.9813567399978638  val: loss: 617478.625 acc: 0.9452166557312012\n",
      "step: 37210 , time : 0.0\n",
      "train: loss: 3312.55322265625 acc: 0.9883350133895874  val: loss: 1324777.25 acc: 0.13820397853851318\n",
      "step: 37215 , time : 0.0\n",
      "train: loss: 59743.0625 acc: 0.9755616784095764  val: loss: 1060001.5 acc: 0.9033945798873901\n",
      "step: 37220 , time : 0.0\n",
      "train: loss: 11025.3076171875 acc: 0.9845017194747925  val: loss: 803811.5 acc: 0.8499380946159363\n",
      "step: 37225 , time : 0.0\n",
      "train: loss: 17110.423828125 acc: 0.9798840880393982  val: loss: 643335.5 acc: 0.7892372608184814\n",
      "step: 37230 , time : 0.0\n",
      "train: loss: 23620.220703125 acc: 0.9649237394332886  val: loss: 1358588.0 acc: 0.7075836658477783\n",
      "step: 37235 , time : 0.0\n",
      "train: loss: 10625.9794921875 acc: 0.9805963039398193  val: loss: 1796536.75 acc: 0.8471180200576782\n",
      "step: 37240 , time : 0.0\n",
      "train: loss: 8836.900390625 acc: 0.9785439968109131  val: loss: 877701.375 acc: 0.657318115234375\n",
      "step: 37245 , time : 0.0\n",
      "train: loss: 9283.650390625 acc: 0.9836671948432922  val: loss: 351940.1875 acc: 0.8962390422821045\n",
      "step: 37250 , time : 0.0\n",
      "train: loss: 27986.265625 acc: 0.9639769196510315  val: loss: 364518.78125 acc: 0.9035711884498596\n",
      "step: 37255 , time : 0.0\n",
      "train: loss: 52332.6953125 acc: 0.971270740032196  val: loss: 1929037.375 acc: 0.6908540725708008\n",
      "step: 37260 , time : 0.0\n",
      "train: loss: 55477.20703125 acc: 0.9751043915748596  val: loss: 1522019.125 acc: 0.7728831768035889\n",
      "step: 37265 , time : 0.0\n",
      "train: loss: 45717.50390625 acc: 0.9877384305000305  val: loss: 945269.5 acc: 0.7681265473365784\n",
      "step: 37270 , time : 0.0\n",
      "train: loss: 47569.94140625 acc: 0.9757848381996155  val: loss: 1964135.0 acc: 0.6325180530548096\n",
      "step: 37275 , time : 0.0\n",
      "train: loss: 30125.52734375 acc: 0.936011552810669  val: loss: 1452462.0 acc: 0.6025170087814331\n",
      "step: 37280 , time : 0.015625715255737305\n",
      "train: loss: 8268.47265625 acc: 0.9934009313583374  val: loss: 1877616.5 acc: 0.5907953977584839\n",
      "step: 37285 , time : 0.0\n",
      "train: loss: 15485.6796875 acc: 0.9901251196861267  val: loss: 2180690.75 acc: 0.5991107821464539\n",
      "step: 37290 , time : 0.0\n",
      "train: loss: 40092.1171875 acc: 0.9891970753669739  val: loss: 1552075.5 acc: 0.8158559799194336\n",
      "step: 37295 , time : 0.0\n",
      "train: loss: 24314.94140625 acc: 0.9912413358688354  val: loss: 2006367.75 acc: 0.6846154928207397\n",
      "step: 37300 , time : 0.0010008811950683594\n",
      "train: loss: 28991.59375 acc: 0.9930040240287781  val: loss: 1544382.5 acc: 0.7034101486206055\n",
      "step: 37305 , time : 0.0\n",
      "train: loss: 62205.50390625 acc: 0.9873740077018738  val: loss: 1101342.25 acc: 0.7114611268043518\n",
      "step: 37310 , time : 0.0\n",
      "train: loss: 155731.828125 acc: 0.9542416930198669  val: loss: 3463972.25 acc: 0.424396276473999\n",
      "step: 37315 , time : 0.0\n",
      "train: loss: 235053.46875 acc: 0.8977037668228149  val: loss: 828143.875 acc: 0.7958881258964539\n",
      "step: 37320 , time : 0.0\n",
      "train: loss: 48431.2734375 acc: 0.9795076251029968  val: loss: 577572.375 acc: 0.7754727602005005\n",
      "step: 37325 , time : 0.0\n",
      "train: loss: 205910.359375 acc: 0.950795590877533  val: loss: 353529.8125 acc: 0.9543841481208801\n",
      "step: 37330 , time : 0.0\n",
      "train: loss: 683668.8125 acc: 0.9088704586029053  val: loss: 1794183.75 acc: 0.19470363855361938\n",
      "step: 37335 , time : 0.0\n",
      "train: loss: 14422.505859375 acc: 0.998412013053894  val: loss: 230912.84375 acc: 0.9607887268066406\n",
      "step: 37340 , time : 0.0\n",
      "train: loss: 152353.515625 acc: 0.9723819494247437  val: loss: 789220.875 acc: 0.7236307859420776\n",
      "step: 37345 , time : 0.0\n",
      "train: loss: 621723.5 acc: 0.8959338665008545  val: loss: 1461080.0 acc: 0.5583008527755737\n",
      "step: 37350 , time : 0.0\n",
      "train: loss: 1041327.75 acc: 0.9183856844902039  val: loss: 601691.625 acc: 0.9183996915817261\n",
      "step: 37355 , time : 0.0\n",
      "train: loss: 268797.0 acc: 0.9454922676086426  val: loss: 1764128.625 acc: 0.20045346021652222\n",
      "step: 37360 , time : 0.0\n",
      "train: loss: 260230.265625 acc: 0.97382652759552  val: loss: 2193497.75 acc: 0.8046438694000244\n",
      "step: 37365 , time : 0.0\n",
      "train: loss: 282260.0 acc: 0.990056037902832  val: loss: 950057.9375 acc: 0.8000360131263733\n",
      "step: 37370 , time : 0.0\n",
      "train: loss: 1056656.25 acc: 0.9707105755805969  val: loss: 696705.8125 acc: 0.7581824064254761\n",
      "step: 37375 , time : 0.0\n",
      "train: loss: 2073428.75 acc: 0.938400149345398  val: loss: 1664101.125 acc: 0.7683212757110596\n",
      "step: 37380 , time : 0.0\n",
      "train: loss: 501194.15625 acc: 0.9775643348693848  val: loss: 844594.875 acc: 0.7784202098846436\n",
      "step: 37385 , time : 0.0\n",
      "train: loss: 543381.375 acc: 0.9467433094978333  val: loss: 274444.03125 acc: 0.9028804898262024\n",
      "step: 37390 , time : 0.0\n",
      "train: loss: 276508.03125 acc: 0.9705511331558228  val: loss: 161998.875 acc: 0.9814576506614685\n",
      "step: 37395 , time : 0.0\n",
      "train: loss: 273607.9375 acc: 0.965083122253418  val: loss: 1382537.5 acc: 0.5640628337860107\n",
      "step: 37400 , time : 0.0\n",
      "train: loss: 184634.9375 acc: 0.9387006163597107  val: loss: 254347.28125 acc: 0.9514926671981812\n",
      "step: 37405 , time : 0.0\n",
      "train: loss: 1261866.125 acc: 0.7959465980529785  val: loss: 1279850.25 acc: 0.7527638077735901\n",
      "step: 37410 , time : 0.0010008811950683594\n",
      "train: loss: 718131.9375 acc: 0.7181586027145386  val: loss: 1592931.5 acc: 0.8310250639915466\n",
      "step: 37415 , time : 0.0010004043579101562\n",
      "train: loss: 323033.90625 acc: 0.8367235660552979  val: loss: 767982.3125 acc: 0.9111420512199402\n",
      "step: 37420 , time : 0.0\n",
      "train: loss: 500875.15625 acc: 0.8935264945030212  val: loss: 327189.0 acc: 0.9423162937164307\n",
      "step: 37425 , time : 0.0010013580322265625\n",
      "train: loss: 1604427.375 acc: 0.3959522247314453  val: loss: 1177292.0 acc: 0.8205480575561523\n",
      "step: 37430 , time : 0.0\n",
      "train: loss: 736589.375 acc: 0.4903221130371094  val: loss: 1716876.75 acc: 0.766845703125\n",
      "step: 37435 , time : 0.0010006427764892578\n",
      "train: loss: 626318.8125 acc: 0.5827261209487915  val: loss: 4214018.5 acc: 0.6243436336517334\n",
      "step: 37440 , time : 0.001001119613647461\n",
      "train: loss: 758936.9375 acc: 0.3597649335861206  val: loss: 4063198.0 acc: 0.3934366703033447\n",
      "step: 37445 , time : 0.0010006427764892578\n",
      "train: loss: 261788.859375 acc: 0.802040159702301  val: loss: 2716622.0 acc: 0.765681266784668\n",
      "step: 37450 , time : 0.0010006427764892578\n",
      "train: loss: 135064.53125 acc: 0.8880809545516968  val: loss: 903001.6875 acc: 0.8794550895690918\n",
      "step: 37455 , time : 0.0\n",
      "train: loss: 75853.0546875 acc: 0.9370366930961609  val: loss: 817687.5 acc: 0.8110324144363403\n",
      "step: 37460 , time : 0.0\n",
      "train: loss: 102774.84375 acc: 0.9205721020698547  val: loss: 2427434.0 acc: 0.8023748993873596\n",
      "step: 37465 , time : 0.0\n",
      "train: loss: 187340.984375 acc: 0.8995016813278198  val: loss: 1288673.75 acc: 0.7547860145568848\n",
      "step: 37470 , time : 0.0\n",
      "train: loss: 265410.75 acc: 0.7916028499603271  val: loss: 1695036.5 acc: 0.8357711434364319\n",
      "step: 37475 , time : 0.0\n",
      "train: loss: 65317.9609375 acc: 0.9412522315979004  val: loss: 2508427.75 acc: 0.770058810710907\n",
      "step: 37480 , time : 0.0\n",
      "train: loss: 70555.1484375 acc: 0.930257260799408  val: loss: 930927.9375 acc: 0.8346511125564575\n",
      "step: 37485 , time : 0.0\n",
      "train: loss: 167114.046875 acc: 0.8171408772468567  val: loss: 1573477.375 acc: 0.8144916296005249\n",
      "step: 37490 , time : 0.0\n",
      "train: loss: 831002.8125 acc: 0.6911717057228088  val: loss: 884103.375 acc: 0.7055898904800415\n",
      "step: 37495 , time : 0.0\n",
      "train: loss: 651417.75 acc: 0.6462767124176025  val: loss: 2456325.5 acc: 0.7507815361022949\n",
      "step: 37500 , time : 0.0\n",
      "train: loss: 500660.09375 acc: 0.7201055288314819  val: loss: 2079772.125 acc: 0.7055834531784058\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 37505 , time : 0.0\n",
      "train: loss: 393125.375 acc: 0.7411339282989502  val: loss: 1030544.5625 acc: 0.787074625492096\n",
      "step: 37510 , time : 0.0\n",
      "train: loss: 239541.9375 acc: 0.861169159412384  val: loss: 1210901.875 acc: 0.6861621737480164\n",
      "step: 37515 , time : 0.0010004043579101562\n",
      "train: loss: 1912067.875 acc: 0.7079657316207886  val: loss: 739881.1875 acc: 0.736717700958252\n",
      "step: 37520 , time : 0.0\n",
      "train: loss: 1227905.125 acc: 0.7881255745887756  val: loss: 962748.3125 acc: 0.8190827965736389\n",
      "step: 37525 , time : 0.0\n",
      "train: loss: 1198747.5 acc: 0.8838970065116882  val: loss: 655829.8125 acc: 0.8568227291107178\n",
      "step: 37530 , time : 0.001001119613647461\n",
      "train: loss: 535004.625 acc: 0.9409720301628113  val: loss: 2004076.75 acc: 0.2525714039802551\n",
      "step: 37535 , time : 0.0\n",
      "train: loss: 983074.125 acc: 0.8806368112564087  val: loss: 552140.375 acc: 0.8314142227172852\n",
      "step: 37540 , time : 0.0\n",
      "train: loss: 450080.78125 acc: 0.9070963859558105  val: loss: 850736.375 acc: 0.7765114903450012\n",
      "step: 37545 , time : 0.0\n",
      "train: loss: 247648.375 acc: 0.9665662050247192  val: loss: 1260404.5 acc: 0.7924984097480774\n",
      "step: 37550 , time : 0.0\n",
      "train: loss: 481762.6875 acc: 0.9608830213546753  val: loss: 2705868.0 acc: 0.28177642822265625\n",
      "step: 37555 , time : 0.0\n",
      "train: loss: 170527.484375 acc: 0.9858607053756714  val: loss: 627191.5 acc: 0.9001769423484802\n",
      "step: 37560 , time : 0.0\n",
      "train: loss: 186551.484375 acc: 0.9812955856323242  val: loss: 849878.6875 acc: 0.6831440329551697\n",
      "step: 37565 , time : 0.0\n",
      "train: loss: 145679.671875 acc: 0.9516300559043884  val: loss: 1299685.75 acc: 0.5399428606033325\n",
      "step: 37570 , time : 0.0\n",
      "train: loss: 23361.48828125 acc: 0.9863172173500061  val: loss: 602587.3125 acc: 0.7719405889511108\n",
      "step: 37575 , time : 0.0\n",
      "train: loss: 138282.953125 acc: 0.9558629393577576  val: loss: 102748.9609375 acc: 0.9579105973243713\n",
      "step: 37580 , time : 0.0\n",
      "train: loss: 104114.7109375 acc: 0.9712659120559692  val: loss: 182360.90625 acc: 0.9580028057098389\n",
      "step: 37585 , time : 0.0\n",
      "train: loss: 9776.693359375 acc: 0.9795175790786743  val: loss: 720463.5625 acc: 0.8113150596618652\n",
      "step: 37590 , time : 0.0\n",
      "train: loss: 12065.9384765625 acc: 0.9926260113716125  val: loss: 1453485.25 acc: 0.6698703765869141\n",
      "step: 37595 , time : 0.0\n",
      "train: loss: 6844.68798828125 acc: 0.9852957725524902  val: loss: 1057923.25 acc: 0.8041669726371765\n",
      "step: 37600 , time : 0.0\n",
      "train: loss: 13798.8515625 acc: 0.9184389114379883  val: loss: 2489042.75 acc: -0.045449018478393555\n",
      "step: 37605 , time : 0.0\n",
      "train: loss: 14825.29296875 acc: 0.9652305245399475  val: loss: 573012.125 acc: 0.8487133383750916\n",
      "step: 37610 , time : 0.0\n",
      "train: loss: 8697.18359375 acc: 0.9896066784858704  val: loss: 31729.1015625 acc: 0.9930022358894348\n",
      "step: 37615 , time : 0.0\n",
      "train: loss: 9032.0625 acc: 0.9850972890853882  val: loss: 1754227.125 acc: 0.37375903129577637\n",
      "step: 37620 , time : 0.0\n",
      "train: loss: 23082.984375 acc: 0.9725632071495056  val: loss: 889900.375 acc: 0.48483890295028687\n",
      "step: 37625 , time : 0.0\n",
      "train: loss: 28653.62109375 acc: 0.9738617539405823  val: loss: 283312.75 acc: 0.932776927947998\n",
      "step: 37630 , time : 0.0\n",
      "train: loss: 63315.67578125 acc: 0.9178296327590942  val: loss: 669116.75 acc: 0.8511354327201843\n",
      "step: 37635 , time : 0.0\n",
      "train: loss: 11787.93359375 acc: 0.9911073446273804  val: loss: 315488.8125 acc: 0.9294664263725281\n",
      "step: 37640 , time : 0.0\n",
      "train: loss: 23447.55078125 acc: 0.975458025932312  val: loss: 711574.6875 acc: 0.861435055732727\n",
      "step: 37645 , time : 0.0010004043579101562\n",
      "train: loss: 6562.06689453125 acc: 0.9836554527282715  val: loss: 1308492.625 acc: 0.6702767014503479\n",
      "step: 37650 , time : 0.0010004043579101562\n",
      "train: loss: 16750.294921875 acc: 0.9901155233383179  val: loss: 857676.5 acc: 0.8664500117301941\n",
      "step: 37655 , time : 0.0\n",
      "train: loss: 20914.431640625 acc: 0.9942306280136108  val: loss: 1347531.25 acc: 0.7989456653594971\n",
      "step: 37660 , time : 0.0010008811950683594\n",
      "train: loss: 34251.71484375 acc: 0.9914026260375977  val: loss: 944836.1875 acc: 0.731817364692688\n",
      "step: 37665 , time : 0.0\n",
      "train: loss: 40462.60546875 acc: 0.9882049560546875  val: loss: 487192.8125 acc: 0.670089602470398\n",
      "step: 37670 , time : 0.0\n",
      "train: loss: 48736.49609375 acc: 0.982190728187561  val: loss: 1482563.5 acc: 0.7547311782836914\n",
      "step: 37675 , time : 0.0010008811950683594\n",
      "train: loss: 53396.8515625 acc: 0.9855101108551025  val: loss: 99488.34375 acc: 0.9843555688858032\n",
      "step: 37680 , time : 0.001001119613647461\n",
      "train: loss: 211043.359375 acc: 0.9396407604217529  val: loss: 543614.375 acc: 0.8515197038650513\n",
      "step: 37685 , time : 0.0\n",
      "train: loss: 55600.55859375 acc: 0.9481772184371948  val: loss: 184188.96875 acc: 0.9689385890960693\n",
      "step: 37690 , time : 0.0\n",
      "train: loss: 132794.484375 acc: 0.9745287299156189  val: loss: 127481.6796875 acc: 0.9571699500083923\n",
      "step: 37695 , time : 0.0\n",
      "train: loss: 358958.3125 acc: 0.9689114093780518  val: loss: 2251669.0 acc: 0.6048541069030762\n",
      "step: 37700 , time : 0.0\n",
      "train: loss: 305242.78125 acc: 0.947134256362915  val: loss: 1404673.0 acc: 0.8187318444252014\n",
      "step: 37705 , time : 0.0\n",
      "train: loss: 107937.4296875 acc: 0.9816696643829346  val: loss: 484223.46875 acc: 0.9433999061584473\n",
      "step: 37710 , time : 0.0\n",
      "train: loss: 249316.0625 acc: 0.9233400821685791  val: loss: 544390.0 acc: 0.9226670265197754\n",
      "step: 37715 , time : 0.0\n",
      "train: loss: 829161.0625 acc: 0.9495383501052856  val: loss: 523047.03125 acc: 0.9377761483192444\n",
      "step: 37720 , time : 0.0\n",
      "train: loss: 433241.6875 acc: 0.9614362716674805  val: loss: 1144767.5 acc: 0.8515805006027222\n",
      "step: 37725 , time : 0.0\n",
      "train: loss: 309674.25 acc: 0.9311599135398865  val: loss: 661378.625 acc: 0.8577262163162231\n",
      "step: 37730 , time : 0.0010004043579101562\n",
      "train: loss: 916923.9375 acc: 0.9597064852714539  val: loss: 2959047.75 acc: 0.6830521821975708\n",
      "step: 37735 , time : 0.0010013580322265625\n",
      "train: loss: 1915598.5 acc: 0.9391971826553345  val: loss: 515823.5625 acc: 0.9097934365272522\n",
      "step: 37740 , time : 0.0010008811950683594\n",
      "train: loss: 388873.125 acc: 0.9841917157173157  val: loss: 1213790.75 acc: 0.8776751160621643\n",
      "step: 37745 , time : 0.0010004043579101562\n",
      "train: loss: 1076732.375 acc: 0.9422927498817444  val: loss: 1060019.875 acc: 0.8652836084365845\n",
      "step: 37750 , time : 0.0\n",
      "train: loss: 232688.578125 acc: 0.983039915561676  val: loss: 291924.125 acc: 0.9675062894821167\n",
      "step: 37755 , time : 0.0010004043579101562\n",
      "train: loss: 733990.125 acc: 0.9406624436378479  val: loss: 351712.4375 acc: 0.9713229537010193\n",
      "step: 37760 , time : 0.0\n",
      "train: loss: 443723.65625 acc: 0.9371860027313232  val: loss: 395333.40625 acc: 0.9496251344680786\n",
      "step: 37765 , time : 0.0\n",
      "train: loss: 476436.40625 acc: 0.9357181191444397  val: loss: 130859.40625 acc: 0.9335348606109619\n",
      "step: 37770 , time : 0.0\n",
      "train: loss: 2547020.75 acc: 0.33930426836013794  val: loss: 1458029.0 acc: 0.7736126780509949\n",
      "step: 37775 , time : 0.0\n",
      "train: loss: 718159.5625 acc: 0.5248299837112427  val: loss: 701347.875 acc: 0.8719291687011719\n",
      "step: 37780 , time : 0.0\n",
      "train: loss: 460532.9375 acc: 0.8080594539642334  val: loss: 614119.625 acc: 0.6812828779220581\n",
      "step: 37785 , time : 0.0\n",
      "train: loss: 358579.84375 acc: 0.883084774017334  val: loss: 1324605.375 acc: 0.5513575077056885\n",
      "step: 37790 , time : 0.0\n",
      "train: loss: 1395705.25 acc: 0.761685311794281  val: loss: 811487.5625 acc: 0.6912066340446472\n",
      "step: 37795 , time : 0.0\n",
      "train: loss: 1859590.0 acc: 0.13485807180404663  val: loss: 1455236.375 acc: 0.6251299381256104\n",
      "step: 37800 , time : 0.0\n",
      "train: loss: 1173988.625 acc: 0.5226856470108032  val: loss: 2585240.75 acc: 0.6433597207069397\n",
      "step: 37805 , time : 0.0010006427764892578\n",
      "train: loss: 1021009.5625 acc: 0.5003160834312439  val: loss: 1455398.5 acc: 0.8125584125518799\n",
      "step: 37810 , time : 0.0\n",
      "train: loss: 253845.328125 acc: 0.8086466789245605  val: loss: 1602181.625 acc: 0.718924343585968\n",
      "step: 37815 , time : 0.0\n",
      "train: loss: 71960.4609375 acc: 0.9428613781929016  val: loss: 798144.8125 acc: 0.7843869924545288\n",
      "step: 37820 , time : 0.0\n",
      "train: loss: 74279.0546875 acc: 0.9393235445022583  val: loss: 525008.3125 acc: 0.61766117811203\n",
      "step: 37825 , time : 0.0\n",
      "train: loss: 88718.5546875 acc: 0.9375708103179932  val: loss: 821868.3125 acc: 0.6736322641372681\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 37830 , time : 0.0010004043579101562\n",
      "train: loss: 345751.25 acc: 0.7973883152008057  val: loss: 694215.9375 acc: 0.7897809743881226\n",
      "step: 37835 , time : 0.0\n",
      "train: loss: 184054.78125 acc: 0.8714597821235657  val: loss: 1672720.875 acc: 0.7188738584518433\n",
      "step: 37840 , time : 0.0010008811950683594\n",
      "train: loss: 208305.34375 acc: 0.8725733160972595  val: loss: 407179.34375 acc: 0.8127846717834473\n",
      "step: 37845 , time : 0.0010008811950683594\n",
      "train: loss: 103840.171875 acc: 0.8058937788009644  val: loss: 2966791.25 acc: 0.7101905941963196\n",
      "step: 37850 , time : 0.0\n",
      "train: loss: 167776.46875 acc: 0.8748317956924438  val: loss: 904219.6875 acc: 0.7469205260276794\n",
      "step: 37855 , time : 0.0\n",
      "train: loss: 293178.78125 acc: 0.8404908776283264  val: loss: 2865736.5 acc: 0.581519365310669\n",
      "step: 37860 , time : 0.0010008811950683594\n",
      "train: loss: 256929.484375 acc: 0.8060138821601868  val: loss: 1608591.375 acc: 0.6368210315704346\n",
      "step: 37865 , time : 0.0\n",
      "train: loss: 328769.8125 acc: 0.764030933380127  val: loss: 2760499.25 acc: 0.532263994216919\n",
      "step: 37870 , time : 0.0\n",
      "train: loss: 95592.9296875 acc: 0.9082950353622437  val: loss: 3507451.5 acc: 0.5558978319168091\n",
      "step: 37875 , time : 0.0\n",
      "train: loss: 537476.25 acc: 0.7862672805786133  val: loss: 1077477.875 acc: 0.6888263821601868\n",
      "step: 37880 , time : 0.0\n",
      "train: loss: 3048175.75 acc: 0.6068503856658936  val: loss: 4042709.25 acc: 0.6122901439666748\n",
      "step: 37885 , time : 0.0\n",
      "train: loss: 958948.625 acc: 0.7001886367797852  val: loss: 1055942.875 acc: 0.7978871464729309\n",
      "step: 37890 , time : 0.0\n",
      "train: loss: 1202695.375 acc: 0.882472038269043  val: loss: 834588.6875 acc: 0.5380316376686096\n",
      "step: 37895 , time : 0.0010004043579101562\n",
      "train: loss: 527868.8125 acc: 0.9569639563560486  val: loss: 1084547.5 acc: 0.8019766807556152\n",
      "step: 37900 , time : 0.0010004043579101562\n",
      "train: loss: 226159.796875 acc: 0.9704313278198242  val: loss: 1326536.75 acc: 0.8717656135559082\n",
      "step: 37905 , time : 0.0\n",
      "train: loss: 173097.421875 acc: 0.9732671976089478  val: loss: 601503.875 acc: 0.9248399138450623\n",
      "step: 37910 , time : 0.0\n",
      "train: loss: 376851.90625 acc: 0.9654008746147156  val: loss: 858785.875 acc: 0.5767194628715515\n",
      "step: 37915 , time : 0.0010001659393310547\n",
      "train: loss: 157050.96875 acc: 0.98824143409729  val: loss: 945096.875 acc: 0.6511855125427246\n",
      "step: 37920 , time : 0.0010008811950683594\n",
      "train: loss: 74768.1328125 acc: 0.9942604899406433  val: loss: 1281719.125 acc: 0.41586834192276\n",
      "step: 37925 , time : 0.0\n",
      "train: loss: 94230.109375 acc: 0.9901202917098999  val: loss: 1813281.25 acc: 0.7189037203788757\n",
      "step: 37930 , time : 0.0\n",
      "train: loss: 104185.6640625 acc: 0.9801785945892334  val: loss: 72808.28125 acc: 0.9733767509460449\n",
      "step: 37935 , time : 0.0010004043579101562\n",
      "train: loss: 48459.1328125 acc: 0.9802896976470947  val: loss: 439435.375 acc: 0.9080566167831421\n",
      "step: 37940 , time : 0.0010013580322265625\n",
      "train: loss: 80245.1953125 acc: 0.9690548777580261  val: loss: 1170140.375 acc: 0.779287576675415\n",
      "step: 37945 , time : 0.0010008811950683594\n",
      "train: loss: 6933.265625 acc: 0.9783515930175781  val: loss: 1146061.75 acc: 0.6897271871566772\n",
      "step: 37950 , time : 0.0010004043579101562\n",
      "train: loss: 19019.29296875 acc: 0.9711851477622986  val: loss: 1481674.25 acc: 0.744583249092102\n",
      "step: 37955 , time : 0.0010013580322265625\n",
      "train: loss: 25738.75 acc: 0.9639046788215637  val: loss: 773021.375 acc: 0.8632057905197144\n",
      "step: 37960 , time : 0.0010006427764892578\n",
      "train: loss: 5590.50146484375 acc: 0.9880090355873108  val: loss: 634867.875 acc: 0.8852266073226929\n",
      "step: 37965 , time : 0.0010004043579101562\n",
      "train: loss: 3748.197021484375 acc: 0.9797911643981934  val: loss: 744671.75 acc: 0.7184512615203857\n",
      "step: 37970 , time : 0.0\n",
      "train: loss: 3162.57958984375 acc: 0.9943702220916748  val: loss: 2707734.5 acc: 0.46224045753479004\n",
      "step: 37975 , time : 0.0010004043579101562\n",
      "train: loss: 11096.3876953125 acc: 0.979445219039917  val: loss: 267698.53125 acc: 0.9197927713394165\n",
      "step: 37980 , time : 0.0\n",
      "train: loss: 3541.306884765625 acc: 0.9958330392837524  val: loss: 817071.0 acc: 0.900847852230072\n",
      "step: 37985 , time : 0.0010008811950683594\n",
      "train: loss: 21453.33203125 acc: 0.9846110939979553  val: loss: 1346855.75 acc: 0.7498782873153687\n",
      "step: 37990 , time : 0.0010008811950683594\n",
      "train: loss: 26966.77734375 acc: 0.9795233607292175  val: loss: 1232033.625 acc: 0.5225681662559509\n",
      "step: 37995 , time : 0.0\n",
      "train: loss: 19911.76171875 acc: 0.9899115562438965  val: loss: 1043611.375 acc: 0.6835572123527527\n",
      "step: 38000 , time : 0.0\n",
      "train: loss: 104127.0234375 acc: 0.9162343740463257  val: loss: 605662.5625 acc: 0.8566886186599731\n",
      "step: 38005 , time : 0.0010004043579101562\n",
      "train: loss: 5852.96630859375 acc: 0.991462230682373  val: loss: 240488.453125 acc: 0.958136796951294\n",
      "step: 38010 , time : 0.0010006427764892578\n",
      "train: loss: 20302.751953125 acc: 0.9858784079551697  val: loss: 141998.703125 acc: 0.9106988906860352\n",
      "step: 38015 , time : 0.0\n",
      "train: loss: 17295.431640625 acc: 0.9926975965499878  val: loss: 947582.375 acc: 0.791662335395813\n",
      "step: 38020 , time : 0.0\n",
      "train: loss: 26376.220703125 acc: 0.9903998970985413  val: loss: 460332.1875 acc: 0.8572076559066772\n",
      "step: 38025 , time : 0.0010006427764892578\n",
      "train: loss: 27586.060546875 acc: 0.9891084432601929  val: loss: 350886.78125 acc: 0.9261689186096191\n",
      "step: 38030 , time : 0.0\n",
      "train: loss: 36636.9765625 acc: 0.9899736642837524  val: loss: 189659.9375 acc: 0.9560329914093018\n",
      "step: 38035 , time : 0.0010008811950683594\n",
      "train: loss: 39520.4921875 acc: 0.9901126623153687  val: loss: 785067.375 acc: 0.9361710548400879\n",
      "step: 38040 , time : 0.0010004043579101562\n",
      "train: loss: 55287.875 acc: 0.9833371043205261  val: loss: 572750.375 acc: 0.9110973477363586\n",
      "step: 38045 , time : 0.0\n",
      "train: loss: 167119.34375 acc: 0.9745227694511414  val: loss: 1024131.25 acc: 0.866294264793396\n",
      "step: 38050 , time : 0.0010004043579101562\n",
      "train: loss: 44038.9921875 acc: 0.9843395948410034  val: loss: 1584401.125 acc: 0.6512346267700195\n",
      "step: 38055 , time : 0.0\n",
      "train: loss: 293416.46875 acc: 0.9417787194252014  val: loss: 1023918.8125 acc: 0.7526746988296509\n",
      "step: 38060 , time : 0.0010006427764892578\n",
      "train: loss: 243864.8125 acc: 0.9770936965942383  val: loss: 1205038.0 acc: 0.8542088866233826\n",
      "step: 38065 , time : 0.0\n",
      "train: loss: 232500.875 acc: 0.9768286943435669  val: loss: 369556.96875 acc: 0.9685999155044556\n",
      "step: 38070 , time : 0.001001119613647461\n",
      "train: loss: 49570.82421875 acc: 0.9933897256851196  val: loss: 202270.46875 acc: 0.9792686104774475\n",
      "step: 38075 , time : 0.0\n",
      "train: loss: 243855.359375 acc: 0.9816096425056458  val: loss: 711961.375 acc: 0.8004925847053528\n",
      "step: 38080 , time : 0.0010004043579101562\n",
      "train: loss: 1675904.75 acc: 0.8079206943511963  val: loss: 739494.375 acc: 0.942892849445343\n",
      "step: 38085 , time : 0.0010006427764892578\n",
      "train: loss: 393798.4375 acc: 0.965661883354187  val: loss: 926098.8125 acc: 0.4886826276779175\n",
      "step: 38090 , time : 0.0010004043579101562\n",
      "train: loss: 322304.71875 acc: 0.9505777955055237  val: loss: 3903804.25 acc: 0.5157790184020996\n",
      "step: 38095 , time : 0.0\n",
      "train: loss: 1017785.9375 acc: 0.9640463590621948  val: loss: 528370.0 acc: 0.7739592790603638\n",
      "step: 38100 , time : 0.0\n",
      "train: loss: 295141.375 acc: 0.9867505431175232  val: loss: 866579.9375 acc: 0.8444703817367554\n",
      "step: 38105 , time : 0.0\n",
      "train: loss: 1477240.0 acc: 0.9311563372612  val: loss: 4264882.0 acc: 0.16275358200073242\n",
      "step: 38110 , time : 0.0\n",
      "train: loss: 866115.25 acc: 0.9513822197914124  val: loss: 835375.0 acc: 0.7727961540222168\n",
      "step: 38115 , time : 0.0\n",
      "train: loss: 1353299.375 acc: 0.8893992304801941  val: loss: 1176576.375 acc: 0.6493902206420898\n",
      "step: 38120 , time : 0.0\n",
      "train: loss: 169911.140625 acc: 0.9748702049255371  val: loss: 2358376.75 acc: 0.4525020122528076\n",
      "step: 38125 , time : 0.0\n",
      "train: loss: 376881.40625 acc: 0.9533889293670654  val: loss: 1623208.25 acc: 0.712084174156189\n",
      "step: 38130 , time : 0.0\n",
      "train: loss: 530043.3125 acc: 0.9057763814926147  val: loss: 1806044.25 acc: 0.6335200071334839\n",
      "step: 38135 , time : 0.0\n",
      "train: loss: 1234095.75 acc: 0.643486499786377  val: loss: 678750.0 acc: 0.8018741607666016\n",
      "step: 38140 , time : 0.0\n",
      "train: loss: 1331063.5 acc: 0.059536099433898926  val: loss: 1607517.625 acc: 0.7277857661247253\n",
      "step: 38145 , time : 0.0\n",
      "train: loss: 465506.03125 acc: 0.8001309037208557  val: loss: 1237249.5 acc: 0.6589093804359436\n",
      "step: 38150 , time : 0.0\n",
      "train: loss: 221109.84375 acc: 0.8635909557342529  val: loss: 1581302.125 acc: 0.5643441677093506\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 38155 , time : 0.0\n",
      "train: loss: 1254417.75 acc: 0.3504939675331116  val: loss: 393680.53125 acc: 0.9002623558044434\n",
      "step: 38160 , time : 0.0\n",
      "train: loss: 1988368.125 acc: 0.44582653045654297  val: loss: 2422099.75 acc: 0.6928622722625732\n",
      "step: 38165 , time : 0.001001119613647461\n",
      "train: loss: 1174401.0 acc: 0.5550021529197693  val: loss: 2156290.0 acc: 0.5383204817771912\n",
      "step: 38170 , time : 0.001001119613647461\n",
      "train: loss: 558809.125 acc: 0.7377954721450806  val: loss: 764422.75 acc: 0.7564966678619385\n",
      "step: 38175 , time : 0.0010008811950683594\n",
      "train: loss: 321365.65625 acc: 0.6895742416381836  val: loss: 900685.375 acc: 0.6871482133865356\n",
      "step: 38180 , time : 0.0\n",
      "train: loss: 449636.65625 acc: 0.5814024209976196  val: loss: 728013.0625 acc: 0.798640251159668\n",
      "step: 38185 , time : 0.0010006427764892578\n",
      "train: loss: 345407.28125 acc: 0.8431707620620728  val: loss: 1590162.25 acc: 0.8035896420478821\n",
      "step: 38190 , time : 0.0\n",
      "train: loss: 191653.296875 acc: 0.8934301137924194  val: loss: 1602828.75 acc: 0.7249456644058228\n",
      "step: 38195 , time : 0.0010004043579101562\n",
      "train: loss: 210788.375 acc: 0.8264182806015015  val: loss: 1335166.75 acc: 0.8504831790924072\n",
      "step: 38200 , time : 0.0010004043579101562\n",
      "train: loss: 270975.28125 acc: 0.8195564150810242  val: loss: 867523.5 acc: 0.7750975489616394\n",
      "step: 38205 , time : 0.0\n",
      "train: loss: 243034.21875 acc: 0.8588875532150269  val: loss: 1649192.0 acc: 0.8307517170906067\n",
      "step: 38210 , time : 0.0\n",
      "train: loss: 187496.09375 acc: 0.8199163675308228  val: loss: 2048756.0 acc: 0.7130715847015381\n",
      "step: 38215 , time : 0.0010006427764892578\n",
      "train: loss: 256373.8125 acc: 0.8450114727020264  val: loss: 1242858.75 acc: 0.8245548009872437\n",
      "step: 38220 , time : 0.001001119613647461\n",
      "train: loss: 550771.625 acc: 0.792317271232605  val: loss: 1037004.125 acc: 0.6896625757217407\n",
      "step: 38225 , time : 0.0010004043579101562\n",
      "train: loss: 486751.4375 acc: 0.7317429780960083  val: loss: 2291749.75 acc: 0.6399784088134766\n",
      "step: 38230 , time : 0.0\n",
      "train: loss: 418424.71875 acc: 0.6444457769393921  val: loss: 1385135.125 acc: 0.7669824361801147\n",
      "step: 38235 , time : 0.0\n",
      "train: loss: 577432.6875 acc: 0.6566147208213806  val: loss: 1324497.375 acc: 0.8000752329826355\n",
      "step: 38240 , time : 0.0\n",
      "train: loss: 500652.40625 acc: 0.6937209963798523  val: loss: 672978.5 acc: 0.777490496635437\n",
      "step: 38245 , time : 0.0\n",
      "train: loss: 1299933.5 acc: 0.6333495378494263  val: loss: 586279.3125 acc: 0.7460147142410278\n",
      "step: 38250 , time : 0.0\n",
      "train: loss: 1221760.0 acc: 0.7945363521575928  val: loss: 673580.75 acc: 0.6775025129318237\n",
      "step: 38255 , time : 0.0010004043579101562\n",
      "train: loss: 1224458.25 acc: 0.8789588212966919  val: loss: 1095340.125 acc: 0.6414050459861755\n",
      "step: 38260 , time : 0.0\n",
      "train: loss: 650537.3125 acc: 0.9394130110740662  val: loss: 1066012.625 acc: 0.8232223987579346\n",
      "step: 38265 , time : 0.0010008811950683594\n",
      "train: loss: 535369.6875 acc: 0.8155615925788879  val: loss: 900383.0 acc: 0.8976171016693115\n",
      "step: 38270 , time : 0.0\n",
      "train: loss: 585718.0 acc: 0.9027976989746094  val: loss: 573428.125 acc: 0.7396318912506104\n",
      "step: 38275 , time : 0.0\n",
      "train: loss: 607814.875 acc: 0.9419792294502258  val: loss: 720024.875 acc: 0.6901813745498657\n",
      "step: 38280 , time : 0.0\n",
      "train: loss: 147260.65625 acc: 0.9875444769859314  val: loss: 1637605.75 acc: 0.6088716983795166\n",
      "step: 38285 , time : 0.0\n",
      "train: loss: 220854.21875 acc: 0.9833062291145325  val: loss: 590651.0 acc: 0.863260805606842\n",
      "step: 38290 , time : 0.0\n",
      "train: loss: 319937.15625 acc: 0.9713790416717529  val: loss: 511894.8125 acc: 0.8466809988021851\n",
      "step: 38295 , time : 0.0\n",
      "train: loss: 230646.859375 acc: 0.9675650596618652  val: loss: 649651.1875 acc: 0.8597086071968079\n",
      "step: 38300 , time : 0.0010008811950683594\n",
      "train: loss: 91590.4921875 acc: 0.9313866496086121  val: loss: 387652.125 acc: 0.9261351823806763\n",
      "step: 38305 , time : 0.0010004043579101562\n",
      "train: loss: 321397.28125 acc: 0.9287904500961304  val: loss: 477095.4375 acc: 0.7367300987243652\n",
      "step: 38310 , time : 0.0010004043579101562\n",
      "train: loss: 69196.734375 acc: 0.9759457111358643  val: loss: 398659.96875 acc: 0.8947769403457642\n",
      "step: 38315 , time : 0.0010008811950683594\n",
      "train: loss: 56276.4296875 acc: 0.9604323506355286  val: loss: 148111.8125 acc: 0.9729677438735962\n",
      "step: 38320 , time : 0.0\n",
      "train: loss: 86637.4765625 acc: 0.9389309287071228  val: loss: 470356.46875 acc: 0.9250069856643677\n",
      "step: 38325 , time : 0.0010006427764892578\n",
      "train: loss: 9737.9736328125 acc: 0.9685577154159546  val: loss: 237631.109375 acc: 0.9030383825302124\n",
      "step: 38330 , time : 0.0\n",
      "train: loss: 7249.05029296875 acc: 0.9857958555221558  val: loss: 321838.71875 acc: 0.7259137630462646\n",
      "step: 38335 , time : 0.0\n",
      "train: loss: 6041.38671875 acc: 0.9731791019439697  val: loss: 1012409.0 acc: 0.9063503742218018\n",
      "step: 38340 , time : 0.0010006427764892578\n",
      "train: loss: 7156.1904296875 acc: 0.9813828468322754  val: loss: 193437.71875 acc: 0.9742246866226196\n",
      "step: 38345 , time : 0.0\n",
      "train: loss: 2895.61572265625 acc: 0.993216872215271  val: loss: 767104.125 acc: 0.9489380717277527\n",
      "step: 38350 , time : 0.0\n",
      "train: loss: 87432.515625 acc: 0.9610579609870911  val: loss: 108230.59375 acc: 0.9599108099937439\n",
      "step: 38355 , time : 0.0\n",
      "train: loss: 32836.2734375 acc: 0.9766132831573486  val: loss: 487792.71875 acc: 0.9369921088218689\n",
      "step: 38360 , time : 0.0010004043579101562\n",
      "train: loss: 201777.1875 acc: 0.758184015750885  val: loss: 675214.75 acc: 0.9301213622093201\n",
      "step: 38365 , time : 0.0\n",
      "train: loss: 39041.359375 acc: 0.9786763787269592  val: loss: 1324580.375 acc: 0.8134956955909729\n",
      "step: 38370 , time : 0.0\n",
      "train: loss: 14974.9697265625 acc: 0.9694093465805054  val: loss: 159585.3125 acc: 0.952727198600769\n",
      "step: 38375 , time : 0.0\n",
      "train: loss: 29766.203125 acc: 0.9796699285507202  val: loss: 282697.9375 acc: 0.9719152450561523\n",
      "step: 38380 , time : 0.0010004043579101562\n",
      "train: loss: 10102.1689453125 acc: 0.9941715002059937  val: loss: 886377.25 acc: 0.9123119711875916\n",
      "step: 38385 , time : 0.0\n",
      "train: loss: 40617.53515625 acc: 0.986327052116394  val: loss: 1886149.25 acc: 0.0028208494186401367\n",
      "step: 38390 , time : 0.0\n",
      "train: loss: 21662.7578125 acc: 0.994150698184967  val: loss: 498480.1875 acc: 0.7455915212631226\n",
      "step: 38395 , time : 0.0010006427764892578\n",
      "train: loss: 36654.85546875 acc: 0.9904830455780029  val: loss: 573068.6875 acc: 0.8772385120391846\n",
      "step: 38400 , time : 0.0\n",
      "train: loss: 52831.25 acc: 0.9370250701904297  val: loss: 2103674.0 acc: 0.7993464469909668\n",
      "step: 38405 , time : 0.0010004043579101562\n",
      "train: loss: 319426.46875 acc: 0.936228334903717  val: loss: 1074680.375 acc: 0.8035337924957275\n",
      "step: 38410 , time : 0.0010006427764892578\n",
      "train: loss: 253340.34375 acc: 0.9182495474815369  val: loss: 2195231.75 acc: 0.6156678199768066\n",
      "step: 38415 , time : 0.0010004043579101562\n",
      "train: loss: 65669.0234375 acc: 0.9692032337188721  val: loss: 1957433.375 acc: 0.8135384917259216\n",
      "step: 38420 , time : 0.0\n",
      "train: loss: 135379.09375 acc: 0.9723635911941528  val: loss: 1969288.25 acc: 0.6247520446777344\n",
      "step: 38425 , time : 0.0\n",
      "train: loss: 586940.375 acc: 0.9085733294487  val: loss: 2058660.75 acc: 0.6494846343994141\n",
      "step: 38430 , time : 0.0\n",
      "train: loss: 29249.009765625 acc: 0.997229814529419  val: loss: 2068157.625 acc: -0.15216350555419922\n",
      "step: 38435 , time : 0.0\n",
      "train: loss: 115406.3125 acc: 0.9706891179084778  val: loss: 1908330.375 acc: 0.8224742412567139\n",
      "step: 38440 , time : 0.0\n",
      "train: loss: 145429.546875 acc: 0.983348548412323  val: loss: 1720654.75 acc: 0.523343026638031\n",
      "step: 38445 , time : 0.0\n",
      "train: loss: 122397.34375 acc: 0.981051504611969  val: loss: 340016.875 acc: 0.9433455467224121\n",
      "step: 38450 , time : 0.0\n",
      "train: loss: 637417.125 acc: 0.9639978408813477  val: loss: 939248.125 acc: 0.5869972705841064\n",
      "step: 38455 , time : 0.0\n",
      "train: loss: 580037.125 acc: 0.9625868797302246  val: loss: 1579339.875 acc: 0.6221832633018494\n",
      "step: 38460 , time : 0.0\n",
      "train: loss: 1154704.5 acc: 0.8999267220497131  val: loss: 942090.375 acc: 0.3591136336326599\n",
      "step: 38465 , time : 0.0010004043579101562\n",
      "train: loss: 669267.125 acc: 0.9769290685653687  val: loss: 1569513.875 acc: 0.5325616598129272\n",
      "step: 38470 , time : 0.0\n",
      "train: loss: 726332.75 acc: 0.9725620746612549  val: loss: 1040136.5 acc: 0.6511356830596924\n",
      "step: 38475 , time : 0.0\n",
      "train: loss: 1418093.75 acc: 0.9393815398216248  val: loss: 1386608.0 acc: 0.78657466173172\n",
      "step: 38480 , time : 0.0\n",
      "train: loss: 740835.8125 acc: 0.9492795467376709  val: loss: 1498638.5 acc: 0.7204945087432861\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 38485 , time : 0.0\n",
      "train: loss: 568760.5625 acc: 0.934260904788971  val: loss: 1667165.375 acc: 0.5476951003074646\n",
      "step: 38490 , time : 0.0010004043579101562\n",
      "train: loss: 410962.625 acc: 0.9491443037986755  val: loss: 1369001.375 acc: 0.8628312945365906\n",
      "step: 38495 , time : 0.0\n",
      "train: loss: 827535.5 acc: 0.881491482257843  val: loss: 933426.5625 acc: 0.7812527418136597\n",
      "step: 38500 , time : 0.0\n",
      "train: loss: 2803705.25 acc: 0.0071683526039123535  val: loss: 1218418.5 acc: 0.7900817394256592\n",
      "step: 38505 , time : 0.0\n",
      "train: loss: 1603485.75 acc: 0.3107289671897888  val: loss: 1526936.25 acc: 0.7706276178359985\n",
      "step: 38510 , time : 0.0010008811950683594\n",
      "train: loss: 618106.25 acc: 0.6793577075004578  val: loss: 3895680.75 acc: 0.32177579402923584\n",
      "step: 38515 , time : 0.0010004043579101562\n",
      "train: loss: 490268.4375 acc: 0.8484978079795837  val: loss: 1127244.75 acc: 0.7144843339920044\n",
      "step: 38520 , time : 0.0\n",
      "train: loss: 1197378.5 acc: 0.739181399345398  val: loss: 1051686.125 acc: 0.7339425086975098\n",
      "step: 38525 , time : 0.0\n",
      "train: loss: 2374052.5 acc: 0.16110581159591675  val: loss: 2244649.25 acc: 0.7012133002281189\n",
      "step: 38530 , time : 0.0\n",
      "train: loss: 1418205.125 acc: 0.29156798124313354  val: loss: 1646442.875 acc: 0.4540162682533264\n",
      "step: 38535 , time : 0.0\n",
      "train: loss: 1083963.0 acc: 0.609670877456665  val: loss: 1331025.5 acc: 0.8413217067718506\n",
      "step: 38540 , time : 0.0010006427764892578\n",
      "train: loss: 679575.625 acc: 0.48120296001434326  val: loss: 533103.0 acc: 0.7699652910232544\n",
      "step: 38545 , time : 0.0010008811950683594\n",
      "train: loss: 750705.25 acc: 0.5303435921669006  val: loss: 828631.0625 acc: 0.8138954043388367\n",
      "step: 38550 , time : 0.0\n",
      "train: loss: 310086.96875 acc: 0.7409020662307739  val: loss: 555767.1875 acc: 0.6975002884864807\n",
      "step: 38555 , time : 0.0\n",
      "train: loss: 292682.28125 acc: 0.7472119331359863  val: loss: 997713.625 acc: 0.8097873330116272\n",
      "step: 38560 , time : 0.0\n",
      "train: loss: 411733.71875 acc: 0.7890035510063171  val: loss: 950449.4375 acc: 0.7602617144584656\n",
      "step: 38565 , time : 0.0\n",
      "train: loss: 135530.75 acc: 0.8701377511024475  val: loss: 1106951.5 acc: 0.8060061931610107\n",
      "step: 38570 , time : 0.0\n",
      "train: loss: 254439.9375 acc: 0.8578481674194336  val: loss: 249372.34375 acc: 0.8171460032463074\n",
      "step: 38575 , time : 0.001001119613647461\n",
      "train: loss: 72131.9375 acc: 0.9302468299865723  val: loss: 991068.75 acc: 0.7511546611785889\n",
      "step: 38580 , time : 0.0010006427764892578\n",
      "train: loss: 233064.375 acc: 0.8184121251106262  val: loss: 346754.78125 acc: 0.8517842292785645\n",
      "step: 38585 , time : 0.0\n",
      "train: loss: 573561.25 acc: 0.6670660972595215  val: loss: 967208.0625 acc: 0.6317803263664246\n",
      "step: 38590 , time : 0.0\n",
      "train: loss: 747016.6875 acc: 0.7380608916282654  val: loss: 1170066.75 acc: 0.7904344797134399\n",
      "step: 38595 , time : 0.0010008811950683594\n",
      "train: loss: 942705.8125 acc: 0.68574458360672  val: loss: 1381624.625 acc: 0.7289553880691528\n",
      "step: 38600 , time : 0.0\n",
      "train: loss: 235656.59375 acc: 0.8211581110954285  val: loss: 385623.875 acc: 0.7020413875579834\n",
      "step: 38605 , time : 0.0010008811950683594\n",
      "train: loss: 172474.65625 acc: 0.8113018274307251  val: loss: 933976.5625 acc: 0.6720190644264221\n",
      "step: 38610 , time : 0.0010006427764892578\n",
      "train: loss: 1335492.375 acc: 0.6906859278678894  val: loss: 798957.1875 acc: 0.8205081224441528\n",
      "step: 38615 , time : 0.001001119613647461\n",
      "train: loss: 1065730.75 acc: 0.6664499044418335  val: loss: 1001443.125 acc: 0.8067929744720459\n",
      "step: 38620 , time : 0.0010008811950683594\n",
      "train: loss: 1037161.6875 acc: 0.8922770023345947  val: loss: 1589346.125 acc: 0.2584022283554077\n",
      "step: 38625 , time : 0.001001119613647461\n",
      "train: loss: 650438.25 acc: 0.9395113587379456  val: loss: 499169.25 acc: 0.8911122679710388\n",
      "step: 38630 , time : 0.0\n",
      "train: loss: 478342.8125 acc: 0.9116633534431458  val: loss: 1626888.25 acc: 0.811525285243988\n",
      "step: 38635 , time : 0.0\n",
      "train: loss: 312241.25 acc: 0.9604281783103943  val: loss: 1095196.875 acc: 0.8690316677093506\n",
      "step: 38640 , time : 0.0010008811950683594\n",
      "train: loss: 283910.625 acc: 0.9509609341621399  val: loss: 1156234.0 acc: 0.7993333339691162\n",
      "step: 38645 , time : 0.0010006427764892578\n",
      "train: loss: 245407.53125 acc: 0.9751845598220825  val: loss: 1017386.125 acc: 0.8063740134239197\n",
      "step: 38650 , time : 0.0\n",
      "train: loss: 147379.5 acc: 0.988765299320221  val: loss: 837958.125 acc: 0.789717972278595\n",
      "step: 38655 , time : 0.0\n",
      "train: loss: 189072.796875 acc: 0.9768136739730835  val: loss: 251270.640625 acc: 0.9583138227462769\n",
      "step: 38660 , time : 0.0\n",
      "train: loss: 228897.96875 acc: 0.9738913774490356  val: loss: 1007431.375 acc: 0.7054136395454407\n",
      "step: 38665 , time : 0.0\n",
      "train: loss: 59587.421875 acc: 0.9680209755897522  val: loss: 628545.1875 acc: 0.9054049253463745\n",
      "step: 38670 , time : 0.0\n",
      "train: loss: 53042.87890625 acc: 0.9514791965484619  val: loss: 797720.1875 acc: 0.9180419445037842\n",
      "step: 38675 , time : 0.0\n",
      "train: loss: 7263.57421875 acc: 0.9974564909934998  val: loss: 557575.375 acc: 0.928659975528717\n",
      "step: 38680 , time : 0.0\n",
      "train: loss: 9732.259765625 acc: 0.9877233505249023  val: loss: 258254.09375 acc: 0.9669307470321655\n",
      "step: 38685 , time : 0.0\n",
      "train: loss: 102020.421875 acc: 0.8397312760353088  val: loss: 402932.53125 acc: 0.9522358179092407\n",
      "step: 38690 , time : 0.0\n",
      "train: loss: 9509.0068359375 acc: 0.9880927205085754  val: loss: 1194422.375 acc: 0.7093302011489868\n",
      "step: 38695 , time : 0.0\n",
      "train: loss: 7781.88525390625 acc: 0.9719212651252747  val: loss: 1000103.8125 acc: 0.8926078677177429\n",
      "step: 38700 , time : 0.0\n",
      "train: loss: 4043.32080078125 acc: 0.9896290302276611  val: loss: 361013.5 acc: 0.9378389716148376\n",
      "step: 38705 , time : 0.0\n",
      "train: loss: 47253.6484375 acc: 0.9521562457084656  val: loss: 2400181.5 acc: 0.774950385093689\n",
      "step: 38710 , time : 0.0\n",
      "train: loss: 29258.193359375 acc: 0.9478708505630493  val: loss: 722063.3125 acc: 0.902565598487854\n",
      "step: 38715 , time : 0.0\n",
      "train: loss: 78440.796875 acc: 0.951490044593811  val: loss: 996637.75 acc: 0.8052645325660706\n",
      "step: 38720 , time : 0.001001119613647461\n",
      "train: loss: 24984.48828125 acc: 0.9821821451187134  val: loss: 3940364.0 acc: 0.5385918021202087\n",
      "step: 38725 , time : 0.0010008811950683594\n",
      "train: loss: 60732.81640625 acc: 0.9637091159820557  val: loss: 616820.6875 acc: 0.8544082641601562\n",
      "step: 38730 , time : 0.0010006427764892578\n",
      "train: loss: 25307.080078125 acc: 0.9872720241546631  val: loss: 933697.0625 acc: 0.83640456199646\n",
      "step: 38735 , time : 0.0\n",
      "train: loss: 10546.646484375 acc: 0.9850938320159912  val: loss: 1078982.25 acc: 0.9010761976242065\n",
      "step: 38740 , time : 0.0010006427764892578\n",
      "train: loss: 13016.76171875 acc: 0.9877542853355408  val: loss: 1889294.75 acc: 0.6736788749694824\n",
      "step: 38745 , time : 0.0010006427764892578\n",
      "train: loss: 9440.73828125 acc: 0.9929522275924683  val: loss: 617835.1875 acc: 0.8696967959403992\n",
      "step: 38750 , time : 0.0\n",
      "train: loss: 44061.0703125 acc: 0.9863705635070801  val: loss: 2393122.75 acc: -0.4241909980773926\n",
      "step: 38755 , time : 0.0\n",
      "train: loss: 36730.52734375 acc: 0.9914079904556274  val: loss: 1499010.625 acc: 0.3215709328651428\n",
      "step: 38760 , time : 0.0010004043579101562\n",
      "train: loss: 48304.9453125 acc: 0.9846595525741577  val: loss: 1955800.625 acc: 0.1632259488105774\n",
      "step: 38765 , time : 0.0\n",
      "train: loss: 23737.984375 acc: 0.9916375875473022  val: loss: 2150128.75 acc: 0.6237597465515137\n",
      "step: 38770 , time : 0.0\n",
      "train: loss: 86102.1015625 acc: 0.9713909029960632  val: loss: 821991.0625 acc: 0.7407388687133789\n",
      "step: 38775 , time : 0.0\n",
      "train: loss: 434942.46875 acc: 0.8658062219619751  val: loss: 2190660.25 acc: 0.24227029085159302\n",
      "step: 38780 , time : 0.001001119613647461\n",
      "train: loss: 121545.6484375 acc: 0.9538785815238953  val: loss: 2197974.75 acc: 0.7404103875160217\n",
      "step: 38785 , time : 0.0\n",
      "train: loss: 610812.9375 acc: 0.8584127426147461  val: loss: 2126122.5 acc: 0.5277904272079468\n",
      "step: 38790 , time : 0.0\n",
      "train: loss: 86631.0234375 acc: 0.9802747368812561  val: loss: 812346.25 acc: 0.8512073159217834\n",
      "step: 38795 , time : 0.0010006427764892578\n",
      "train: loss: 125077.640625 acc: 0.9857752323150635  val: loss: 1248527.75 acc: 0.8361601829528809\n",
      "step: 38800 , time : 0.0010013580322265625\n",
      "train: loss: 58839.94921875 acc: 0.9950165152549744  val: loss: 1725768.375 acc: 0.7407662868499756\n",
      "step: 38805 , time : 0.0\n",
      "train: loss: 234064.703125 acc: 0.9675092697143555  val: loss: 1214308.75 acc: 0.502691388130188\n",
      "step: 38810 , time : 0.0\n",
      "train: loss: 156085.953125 acc: 0.9713022112846375  val: loss: 896070.3125 acc: 0.9014331102371216\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 38815 , time : 0.0010006427764892578\n",
      "train: loss: 670809.125 acc: 0.9589982032775879  val: loss: 1576428.625 acc: 0.7306362986564636\n",
      "step: 38820 , time : 0.0\n",
      "train: loss: 390711.0625 acc: 0.9608707427978516  val: loss: 1885417.75 acc: 0.8542655110359192\n",
      "step: 38825 , time : 0.0\n",
      "train: loss: 266293.96875 acc: 0.7797938585281372  val: loss: 1367584.875 acc: 0.8553557991981506\n",
      "step: 38830 , time : 0.0010004043579101562\n",
      "train: loss: 932849.8125 acc: 0.9649290442466736  val: loss: 963266.0 acc: 0.866862416267395\n",
      "step: 38835 , time : 0.0010006427764892578\n",
      "train: loss: 1133819.375 acc: 0.9478517770767212  val: loss: 1132466.875 acc: 0.8060826063156128\n",
      "step: 38840 , time : 0.0010008811950683594\n",
      "train: loss: 953234.8125 acc: 0.9683950543403625  val: loss: 1194969.25 acc: 0.7936674952507019\n",
      "step: 38845 , time : 0.0\n",
      "train: loss: 1128164.875 acc: 0.9206929802894592  val: loss: 1575873.625 acc: 0.5825207233428955\n",
      "step: 38850 , time : 0.0\n",
      "train: loss: 427811.40625 acc: 0.9709204435348511  val: loss: 113094.5078125 acc: 0.9381626844406128\n",
      "step: 38855 , time : 0.0010006427764892578\n",
      "train: loss: 428870.4375 acc: 0.9561945796012878  val: loss: 832183.5625 acc: 0.7884379625320435\n",
      "step: 38860 , time : 0.0\n",
      "train: loss: 239882.140625 acc: 0.9524932503700256  val: loss: 1546934.5 acc: 0.7787032127380371\n",
      "step: 38865 , time : 0.0\n",
      "train: loss: 2612298.0 acc: 0.46129077672958374  val: loss: 232012.5 acc: 0.9241248965263367\n",
      "step: 38870 , time : 0.0\n",
      "train: loss: 624933.75 acc: 0.6436704993247986  val: loss: 757877.8125 acc: 0.841893196105957\n",
      "step: 38875 , time : 0.0\n",
      "train: loss: 1205809.0 acc: 0.617242693901062  val: loss: 661004.8125 acc: 0.7397470474243164\n",
      "step: 38880 , time : 0.0010008811950683594\n",
      "train: loss: 1019712.0 acc: 0.5550952553749084  val: loss: 1565553.0 acc: 0.7468610405921936\n",
      "step: 38885 , time : 0.0\n",
      "train: loss: 702322.75 acc: 0.7996727824211121  val: loss: 2088807.375 acc: 0.7889764308929443\n",
      "step: 38890 , time : 0.0\n",
      "train: loss: 1759836.5 acc: -0.2895972728729248  val: loss: 785880.6875 acc: 0.412966251373291\n",
      "step: 38895 , time : 0.0\n",
      "train: loss: 1007371.125 acc: 0.46141356229782104  val: loss: 1541640.375 acc: 0.2676053047180176\n",
      "step: 38900 , time : 0.0\n",
      "train: loss: 660432.3125 acc: 0.539345920085907  val: loss: 742280.5 acc: 0.27328240871429443\n",
      "step: 38905 , time : 0.0\n",
      "train: loss: 300804.625 acc: 0.7415057420730591  val: loss: 927860.3125 acc: 0.7339625358581543\n",
      "step: 38910 , time : 0.0\n",
      "train: loss: 152785.203125 acc: 0.8755617141723633  val: loss: 1998874.875 acc: 0.6790406703948975\n",
      "step: 38915 , time : 0.0\n",
      "train: loss: 217438.046875 acc: 0.81687331199646  val: loss: 616330.125 acc: 0.8225475549697876\n",
      "step: 38920 , time : 0.0\n",
      "train: loss: 136582.828125 acc: 0.8799803256988525  val: loss: 668450.9375 acc: 0.79694664478302\n",
      "step: 38925 , time : 0.0\n",
      "train: loss: 160028.5 acc: 0.9089898467063904  val: loss: 617469.0 acc: 0.7902165651321411\n",
      "step: 38930 , time : 0.0\n",
      "train: loss: 124601.390625 acc: 0.9036926627159119  val: loss: 666810.625 acc: 0.7016004323959351\n",
      "step: 38935 , time : 0.0010008811950683594\n",
      "train: loss: 165923.234375 acc: 0.8740861415863037  val: loss: 21802.646484375 acc: 0.9594023823738098\n",
      "step: 38940 , time : 0.0010004043579101562\n",
      "train: loss: 87252.671875 acc: 0.9337370991706848  val: loss: 1209995.0 acc: 0.7499176263809204\n",
      "step: 38945 , time : 0.0010006427764892578\n",
      "train: loss: 137104.34375 acc: 0.840428352355957  val: loss: 2096846.875 acc: 0.6616180539131165\n",
      "step: 38950 , time : 0.0010006427764892578\n",
      "train: loss: 586404.0625 acc: 0.6762926578521729  val: loss: 617441.3125 acc: 0.7342014312744141\n",
      "step: 38955 , time : 0.0010008811950683594\n",
      "train: loss: 65650.0078125 acc: 0.9379968643188477  val: loss: 1048604.875 acc: 0.6457099318504333\n",
      "step: 38960 , time : 0.0\n",
      "train: loss: 241591.59375 acc: 0.8623722791671753  val: loss: 1915414.625 acc: 0.6169767379760742\n",
      "step: 38965 , time : 0.0\n",
      "train: loss: 447963.0 acc: 0.787814199924469  val: loss: 308105.71875 acc: 0.793402373790741\n",
      "step: 38970 , time : 0.0\n",
      "train: loss: 762684.875 acc: 0.5958578586578369  val: loss: 2479296.0 acc: 0.5864193439483643\n",
      "step: 38975 , time : 0.0010004043579101562\n",
      "train: loss: 1360972.0 acc: 0.6719409823417664  val: loss: 763444.375 acc: 0.723615825176239\n",
      "step: 38980 , time : 0.0010004043579101562\n",
      "train: loss: 2867448.75 acc: 0.7224709391593933  val: loss: 2400396.25 acc: 0.7728420495986938\n",
      "step: 38985 , time : 0.0\n",
      "train: loss: 1685659.375 acc: 0.841763973236084  val: loss: 1025420.25 acc: 0.8295018672943115\n",
      "step: 38990 , time : 0.001001119613647461\n",
      "train: loss: 936801.0 acc: 0.9106221795082092  val: loss: 1306435.0 acc: 0.8823252320289612\n",
      "step: 38995 , time : 0.0\n",
      "train: loss: 281807.28125 acc: 0.9643784761428833  val: loss: 402164.8125 acc: 0.9600001573562622\n",
      "step: 39000 , time : 0.0010008811950683594\n",
      "train: loss: 890245.1875 acc: 0.8497580885887146  val: loss: 594845.125 acc: 0.9479379653930664\n",
      "step: 39005 , time : 0.0\n",
      "train: loss: 329634.9375 acc: 0.9616354703903198  val: loss: 3776111.0 acc: 0.7195855379104614\n",
      "step: 39010 , time : 0.0\n",
      "train: loss: 147259.265625 acc: 0.9871893525123596  val: loss: 1224602.0 acc: 0.8621859550476074\n",
      "step: 39015 , time : 0.0010006427764892578\n",
      "train: loss: 173376.75 acc: 0.9876142740249634  val: loss: 1203843.25 acc: 0.8361980319023132\n",
      "step: 39020 , time : 0.0\n",
      "train: loss: 144571.515625 acc: 0.984320878982544  val: loss: 407790.15625 acc: 0.8465273380279541\n",
      "step: 39025 , time : 0.0\n",
      "train: loss: 127224.8203125 acc: 0.9827108383178711  val: loss: 1413073.0 acc: 0.5819354057312012\n",
      "step: 39030 , time : 0.0010004043579101562\n",
      "train: loss: 90189.2578125 acc: 0.9810266494750977  val: loss: 1435020.0 acc: 0.752971887588501\n",
      "step: 39035 , time : 0.0020024776458740234\n",
      "train: loss: 62390.10546875 acc: 0.9824011325836182  val: loss: 1149065.0 acc: 0.6956151723861694\n",
      "step: 39040 , time : 0.0\n",
      "train: loss: 31963.328125 acc: 0.9890141487121582  val: loss: 1814701.125 acc: 0.6470067501068115\n",
      "step: 39045 , time : 0.0\n",
      "train: loss: 80308.5859375 acc: 0.9764854907989502  val: loss: 1734227.875 acc: 0.7647677659988403\n",
      "step: 39050 , time : 0.0010013580322265625\n",
      "train: loss: 24961.248046875 acc: 0.9587703347206116  val: loss: 687384.625 acc: 0.7333528995513916\n",
      "step: 39055 , time : 0.0\n",
      "train: loss: 11482.955078125 acc: 0.9742805361747742  val: loss: 2021164.375 acc: 0.41971355676651\n",
      "step: 39060 , time : 0.0010004043579101562\n",
      "train: loss: 19606.74609375 acc: 0.9685774445533752  val: loss: 1042175.375 acc: 0.908316969871521\n",
      "step: 39065 , time : 0.0\n",
      "train: loss: 2808.477294921875 acc: 0.9913152456283569  val: loss: 895479.5 acc: 0.8873957991600037\n",
      "step: 39070 , time : 0.0010006427764892578\n",
      "train: loss: 4555.57421875 acc: 0.9862385392189026  val: loss: 1455956.75 acc: 0.4153623580932617\n",
      "step: 39075 , time : 0.0010006427764892578\n",
      "train: loss: 18103.142578125 acc: 0.9531406164169312  val: loss: 1638618.75 acc: 0.35917890071868896\n",
      "step: 39080 , time : 0.0010004043579101562\n",
      "train: loss: 29911.11328125 acc: 0.9734656810760498  val: loss: 2363498.75 acc: 0.6155635118484497\n",
      "step: 39085 , time : 0.0\n",
      "train: loss: 34617.36328125 acc: 0.9846558570861816  val: loss: 1530248.125 acc: 0.7540549039840698\n",
      "step: 39090 , time : 0.0010008811950683594\n",
      "train: loss: 105834.609375 acc: 0.9342104196548462  val: loss: 4363122.0 acc: 0.24592095613479614\n",
      "step: 39095 , time : 0.0\n",
      "train: loss: 41416.42578125 acc: 0.9805176258087158  val: loss: 897367.1875 acc: 0.7693266272544861\n",
      "step: 39100 , time : 0.0\n",
      "train: loss: 19087.91015625 acc: 0.981016993522644  val: loss: 869553.625 acc: 0.8697746396064758\n",
      "step: 39105 , time : 0.0010004043579101562\n",
      "train: loss: 4720.37890625 acc: 0.9918016195297241  val: loss: 2779856.5 acc: 0.4518324136734009\n",
      "step: 39110 , time : 0.0\n",
      "train: loss: 9878.0830078125 acc: 0.9926415085792542  val: loss: 1486952.875 acc: 0.4023021459579468\n",
      "step: 39115 , time : 0.0\n",
      "train: loss: 5992.60693359375 acc: 0.9975246787071228  val: loss: 1736261.5 acc: 0.5591493844985962\n",
      "step: 39120 , time : 0.0010008811950683594\n",
      "train: loss: 27912.3515625 acc: 0.9931350350379944  val: loss: 1485954.25 acc: 0.7102803587913513\n",
      "step: 39125 , time : 0.0\n",
      "train: loss: 33376.25 acc: 0.9904904961585999  val: loss: 2626474.75 acc: 0.049182116985321045\n",
      "step: 39130 , time : 0.0\n",
      "train: loss: 27901.7578125 acc: 0.9914796948432922  val: loss: 684797.6875 acc: 0.9012885093688965\n",
      "step: 39135 , time : 0.0010008811950683594\n",
      "train: loss: 65387.03515625 acc: 0.9745372533798218  val: loss: 1754905.0 acc: 0.7875903844833374\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 39140 , time : 0.0\n",
      "train: loss: 286081.5 acc: 0.9443655610084534  val: loss: 2435218.5 acc: 0.33098000288009644\n",
      "step: 39145 , time : 0.0\n",
      "train: loss: 316046.375 acc: 0.7662204504013062  val: loss: 1278468.375 acc: 0.7000842690467834\n",
      "step: 39150 , time : 0.0\n",
      "train: loss: 179465.96875 acc: 0.967103123664856  val: loss: 301126.28125 acc: 0.9388217329978943\n",
      "step: 39155 , time : 0.0\n",
      "train: loss: 104058.2578125 acc: 0.9885138273239136  val: loss: 350942.84375 acc: 0.9184103012084961\n",
      "step: 39160 , time : 0.0010004043579101562\n",
      "train: loss: 134822.296875 acc: 0.9871137738227844  val: loss: 501412.65625 acc: 0.9005966782569885\n",
      "step: 39165 , time : 0.0\n",
      "train: loss: 73012.125 acc: 0.9914501309394836  val: loss: 484281.34375 acc: 0.9205047488212585\n",
      "step: 39170 , time : 0.0010006427764892578\n",
      "train: loss: 478073.90625 acc: 0.8989415168762207  val: loss: 1968879.5 acc: 0.8377785086631775\n",
      "step: 39175 , time : 0.0010006427764892578\n",
      "train: loss: 241711.625 acc: 0.9815124273300171  val: loss: 647904.0625 acc: 0.8843356370925903\n",
      "step: 39180 , time : 0.0010004043579101562\n",
      "train: loss: 213992.5 acc: 0.9844506978988647  val: loss: 852692.8125 acc: 0.2503708004951477\n",
      "step: 39185 , time : 0.0\n",
      "train: loss: 307118.90625 acc: 0.9695436358451843  val: loss: 1508617.0 acc: 0.8429266214370728\n",
      "step: 39190 , time : 0.0\n",
      "train: loss: 1904985.5 acc: 0.7339398860931396  val: loss: 419770.625 acc: 0.9420774579048157\n",
      "step: 39195 , time : 0.0\n",
      "train: loss: 646665.0625 acc: 0.9765326380729675  val: loss: 515270.625 acc: 0.9036843180656433\n",
      "step: 39200 , time : 0.0\n",
      "train: loss: 726918.375 acc: 0.9747785925865173  val: loss: 1200394.5 acc: 0.7269328236579895\n",
      "step: 39205 , time : 0.0\n",
      "train: loss: 723226.0 acc: 0.9743149876594543  val: loss: 352493.28125 acc: 0.9424473643302917\n",
      "step: 39210 , time : 0.0\n",
      "train: loss: 1009957.625 acc: 0.9475516676902771  val: loss: 463403.8125 acc: 0.8429068326950073\n",
      "step: 39215 , time : 0.0\n",
      "train: loss: 350620.90625 acc: 0.9020326137542725  val: loss: 256703.1875 acc: 0.9274433255195618\n",
      "step: 39220 , time : 0.0\n",
      "train: loss: 152201.46875 acc: 0.9831029772758484  val: loss: 307249.25 acc: 0.7474384307861328\n",
      "step: 39225 , time : 0.0\n",
      "train: loss: 654463.875 acc: 0.9020836353302002  val: loss: 84120.3828125 acc: 0.9896071553230286\n",
      "step: 39230 , time : 0.0\n",
      "train: loss: 2652141.75 acc: 0.5617664456367493  val: loss: 486691.9375 acc: 0.9481967091560364\n",
      "step: 39235 , time : 0.0\n",
      "train: loss: 558099.5 acc: 0.6728354692459106  val: loss: 924598.5 acc: 0.5696030855178833\n",
      "step: 39240 , time : 0.0\n",
      "train: loss: 699123.9375 acc: 0.7518815994262695  val: loss: 1155414.25 acc: 0.6318809986114502\n",
      "step: 39245 , time : 0.0\n",
      "train: loss: 1500907.25 acc: 0.5506200790405273  val: loss: 787935.875 acc: 0.766322135925293\n",
      "step: 39250 , time : 0.0\n",
      "train: loss: 335387.59375 acc: 0.8929077982902527  val: loss: 575985.75 acc: 0.8893897533416748\n",
      "step: 39255 , time : 0.0\n",
      "train: loss: 2209405.5 acc: -0.4538177251815796  val: loss: 167748.4375 acc: 0.825204610824585\n",
      "step: 39260 , time : 0.0010008811950683594\n",
      "train: loss: 1433063.5 acc: 0.4306355118751526  val: loss: 1844885.875 acc: 0.3146178722381592\n",
      "step: 39265 , time : 0.0\n",
      "train: loss: 1396687.875 acc: 0.5676393508911133  val: loss: 3295184.75 acc: 0.6215652823448181\n",
      "step: 39270 , time : 0.0010004043579101562\n",
      "train: loss: 399734.3125 acc: 0.641276478767395  val: loss: 765797.375 acc: 0.8192919492721558\n",
      "step: 39275 , time : 0.0\n",
      "train: loss: 652205.6875 acc: 0.3782588243484497  val: loss: 1003692.0 acc: 0.7334049940109253\n",
      "step: 39280 , time : 0.0\n",
      "train: loss: 233138.09375 acc: 0.796145498752594  val: loss: 503493.125 acc: 0.85477614402771\n",
      "step: 39285 , time : 0.0\n",
      "train: loss: 142547.015625 acc: 0.9023364186286926  val: loss: 442851.5625 acc: 0.909986138343811\n",
      "step: 39290 , time : 0.0\n",
      "train: loss: 119942.8125 acc: 0.904393196105957  val: loss: 1187709.5 acc: 0.7534148693084717\n",
      "step: 39295 , time : 0.0005545616149902344\n",
      "train: loss: 393334.40625 acc: 0.7820195555686951  val: loss: 1168434.25 acc: 0.8279814124107361\n",
      "step: 39300 , time : 0.0010004043579101562\n",
      "train: loss: 216542.25 acc: 0.8748399019241333  val: loss: 1038608.5625 acc: 0.6188186407089233\n",
      "step: 39305 , time : 0.0\n",
      "train: loss: 107548.5859375 acc: 0.8892744183540344  val: loss: 1077103.5 acc: 0.7950174808502197\n",
      "step: 39310 , time : 0.0010004043579101562\n",
      "train: loss: 56822.51171875 acc: 0.9298838376998901  val: loss: 1370299.5 acc: 0.8105192184448242\n",
      "step: 39315 , time : 0.0\n",
      "train: loss: 350815.78125 acc: 0.8407353162765503  val: loss: 3503521.5 acc: 0.6766157746315002\n",
      "step: 39320 , time : 0.0\n",
      "train: loss: 553029.5625 acc: 0.7578464150428772  val: loss: 1927637.5 acc: 0.6701995730400085\n",
      "step: 39325 , time : 0.0010006427764892578\n",
      "train: loss: 787249.25 acc: 0.5759227275848389  val: loss: 3381876.25 acc: 0.69759202003479\n",
      "step: 39330 , time : 0.0\n",
      "train: loss: 684495.625 acc: 0.47958576679229736  val: loss: 1004746.875 acc: 0.747691810131073\n",
      "step: 39335 , time : 0.0\n",
      "train: loss: 230109.734375 acc: 0.7610436677932739  val: loss: 1922658.125 acc: 0.7131586074829102\n",
      "step: 39340 , time : 0.0\n",
      "train: loss: 707671.625 acc: 0.6659708023071289  val: loss: 1442241.375 acc: 0.8192124366760254\n",
      "step: 39345 , time : 0.0010006427764892578\n",
      "train: loss: 1427122.0 acc: 0.7783772349357605  val: loss: 1886404.125 acc: 0.837059497833252\n",
      "step: 39350 , time : 0.0\n",
      "train: loss: 1219217.25 acc: 0.8262605667114258  val: loss: 1299081.25 acc: 0.5905542373657227\n",
      "step: 39355 , time : 0.0010008811950683594\n",
      "train: loss: 937371.625 acc: 0.9359977841377258  val: loss: 1020885.375 acc: 0.7988486289978027\n",
      "step: 39360 , time : 0.0\n",
      "train: loss: 596223.5625 acc: 0.9013080596923828  val: loss: 1759426.875 acc: 0.5647698640823364\n",
      "step: 39365 , time : 0.0\n",
      "train: loss: 244645.6875 acc: 0.9518218636512756  val: loss: 989210.125 acc: 0.8961752653121948\n",
      "step: 39370 , time : 0.0\n",
      "train: loss: 385623.0625 acc: 0.9238237738609314  val: loss: 841610.9375 acc: 0.9059298038482666\n",
      "step: 39375 , time : 0.0010004043579101562\n",
      "train: loss: 236335.9375 acc: 0.9804326891899109  val: loss: 1108509.875 acc: 0.8852229714393616\n",
      "step: 39380 , time : 0.0\n",
      "train: loss: 302380.84375 acc: 0.9740095138549805  val: loss: 1526559.75 acc: 0.7568215131759644\n",
      "step: 39385 , time : 0.0010004043579101562\n",
      "train: loss: 217992.59375 acc: 0.9818704128265381  val: loss: 2095550.625 acc: 0.7913119792938232\n",
      "step: 39390 , time : 0.0010006427764892578\n",
      "train: loss: 324231.40625 acc: 0.9425432682037354  val: loss: 1414613.25 acc: 0.3890039920806885\n",
      "step: 39395 , time : 0.0010008811950683594\n",
      "train: loss: 400975.9375 acc: 0.8950971961021423  val: loss: 731483.3125 acc: 0.8968418836593628\n",
      "step: 39400 , time : 0.0\n",
      "train: loss: 210646.5625 acc: 0.9694680571556091  val: loss: 995437.75 acc: 0.7756087779998779\n",
      "step: 39405 , time : 0.0\n",
      "train: loss: 29335.765625 acc: 0.9909067749977112  val: loss: 163790.609375 acc: 0.9391947388648987\n",
      "step: 39410 , time : 0.0\n",
      "train: loss: 13771.515625 acc: 0.9777393341064453  val: loss: 2213877.0 acc: 0.7987684011459351\n",
      "step: 39415 , time : 0.0\n",
      "train: loss: 12312.7353515625 acc: 0.9795506000518799  val: loss: 2404334.25 acc: 0.7406431436538696\n",
      "step: 39420 , time : 0.0010006427764892578\n",
      "train: loss: 28006.30859375 acc: 0.8573803305625916  val: loss: 714881.375 acc: 0.8302375078201294\n",
      "step: 39425 , time : 0.0010008811950683594\n",
      "train: loss: 6809.67919921875 acc: 0.9770255088806152  val: loss: 746257.625 acc: 0.9180041551589966\n",
      "step: 39430 , time : 0.0\n",
      "train: loss: 26673.0546875 acc: 0.967156708240509  val: loss: 2528186.75 acc: 0.5666007399559021\n",
      "step: 39435 , time : 0.0\n",
      "train: loss: 3462.37060546875 acc: 0.9866583943367004  val: loss: 454760.96875 acc: 0.8865209221839905\n",
      "step: 39440 , time : 0.0\n",
      "train: loss: 3112.835205078125 acc: 0.9887334704399109  val: loss: 1507001.0 acc: 0.6633802652359009\n",
      "step: 39445 , time : 0.0\n",
      "train: loss: 26515.291015625 acc: 0.9790129065513611  val: loss: 1348281.875 acc: 0.7688910961151123\n",
      "step: 39450 , time : 0.0\n",
      "train: loss: 51318.0 acc: 0.9709717035293579  val: loss: 569657.875 acc: 0.9389089345932007\n",
      "step: 39455 , time : 0.0\n",
      "train: loss: 16520.986328125 acc: 0.9851073026657104  val: loss: 1807392.625 acc: 0.5621470212936401\n",
      "step: 39460 , time : 0.0\n",
      "train: loss: 106261.2578125 acc: 0.9577909111976624  val: loss: 404638.40625 acc: 0.9239020943641663\n",
      "step: 39465 , time : 0.0\n",
      "train: loss: 15026.640625 acc: 0.9877794981002808  val: loss: 2360543.75 acc: 0.411007285118103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 39470 , time : 0.0\n",
      "train: loss: 23603.396484375 acc: 0.9835777282714844  val: loss: 2205262.0 acc: 0.41501253843307495\n",
      "step: 39475 , time : 0.0\n",
      "train: loss: 10993.458984375 acc: 0.9885319471359253  val: loss: 1485413.5 acc: -0.41784775257110596\n",
      "step: 39480 , time : 0.0\n",
      "train: loss: 30602.59375 acc: 0.991985023021698  val: loss: 1395513.625 acc: 0.26138317584991455\n",
      "step: 39485 , time : 0.0\n",
      "train: loss: 20509.798828125 acc: 0.9924492835998535  val: loss: 571835.5 acc: 0.8565020561218262\n",
      "step: 39490 , time : 0.0\n",
      "train: loss: 27388.55078125 acc: 0.9909895658493042  val: loss: 2710790.5 acc: -0.38818633556365967\n",
      "step: 39495 , time : 0.0010004043579101562\n",
      "train: loss: 25537.48828125 acc: 0.9868795871734619  val: loss: 803494.375 acc: 0.7254638075828552\n",
      "step: 39500 , time : 0.0\n",
      "train: loss: 51805.265625 acc: 0.9851909875869751  val: loss: 154568.75 acc: 0.9775296449661255\n",
      "step: 39505 , time : 0.0\n",
      "train: loss: 135597.65625 acc: 0.9631357789039612  val: loss: 2259494.75 acc: 0.011884093284606934\n",
      "step: 39510 , time : 0.0010008811950683594\n",
      "train: loss: 230362.671875 acc: 0.9306514263153076  val: loss: 365975.78125 acc: 0.9345912933349609\n",
      "step: 39515 , time : 0.0010008811950683594\n",
      "train: loss: 94764.859375 acc: 0.9786736965179443  val: loss: 678126.75 acc: 0.7550169229507446\n",
      "step: 39520 , time : 0.0\n",
      "train: loss: 520994.90625 acc: 0.8937208652496338  val: loss: 151389.375 acc: 0.9771009683609009\n",
      "step: 39525 , time : 0.0\n",
      "train: loss: 784417.875 acc: 0.9281007051467896  val: loss: 1606125.625 acc: 0.7393792867660522\n",
      "step: 39530 , time : 0.001001119613647461\n",
      "train: loss: 48275.8671875 acc: 0.9908962249755859  val: loss: 660825.3125 acc: 0.8871957659721375\n",
      "step: 39535 , time : 0.0\n",
      "train: loss: 320852.34375 acc: 0.9661058187484741  val: loss: 214815.65625 acc: 0.8327621817588806\n",
      "step: 39540 , time : 0.0010006427764892578\n",
      "train: loss: 618495.0 acc: 0.9659759998321533  val: loss: 585350.9375 acc: 0.8225017786026001\n",
      "step: 39545 , time : 0.0\n",
      "train: loss: 952666.3125 acc: 0.9289929270744324  val: loss: 651437.0625 acc: 0.8353537917137146\n",
      "step: 39550 , time : 0.0010006427764892578\n",
      "train: loss: 396556.5 acc: 0.9277334213256836  val: loss: 1390837.625 acc: 0.6559757590293884\n",
      "step: 39555 , time : 0.0\n",
      "train: loss: 392687.6875 acc: 0.9726893901824951  val: loss: 937898.9375 acc: 0.7943950891494751\n",
      "step: 39560 , time : 0.0\n",
      "train: loss: 460787.875 acc: 0.9682542681694031  val: loss: 783287.25 acc: 0.7849593758583069\n",
      "step: 39565 , time : 0.0010006427764892578\n",
      "train: loss: 2139708.25 acc: 0.9348966479301453  val: loss: 969553.375 acc: 0.31004178524017334\n",
      "step: 39570 , time : 0.0010006427764892578\n",
      "train: loss: 1428897.125 acc: 0.9505823850631714  val: loss: 274115.84375 acc: 0.9045628905296326\n",
      "step: 39575 , time : 0.0010008811950683594\n",
      "train: loss: 1187159.0 acc: 0.9434946179389954  val: loss: 542255.125 acc: 0.9219622611999512\n",
      "step: 39580 , time : 0.0\n",
      "train: loss: 586651.375 acc: 0.9588176608085632  val: loss: 840288.375 acc: 0.9085603356361389\n",
      "step: 39585 , time : 0.0010006427764892578\n",
      "train: loss: 938374.5625 acc: 0.93028324842453  val: loss: 580581.5625 acc: 0.8322957754135132\n",
      "step: 39590 , time : 0.0010004043579101562\n",
      "train: loss: 248280.890625 acc: 0.9722728133201599  val: loss: 725235.5 acc: 0.7580957412719727\n",
      "step: 39595 , time : 0.0\n",
      "train: loss: 1125648.5 acc: 0.7009535431861877  val: loss: 244767.359375 acc: 0.8682373762130737\n",
      "step: 39600 , time : 0.0010004043579101562\n",
      "train: loss: 897162.1875 acc: 0.6114386320114136  val: loss: 953210.75 acc: 0.7721039056777954\n",
      "step: 39605 , time : 0.0010008811950683594\n",
      "train: loss: 596855.1875 acc: 0.8852299451828003  val: loss: 1078807.75 acc: 0.8909496665000916\n",
      "step: 39610 , time : 0.0\n",
      "train: loss: 257090.0 acc: 0.8904261589050293  val: loss: 319480.78125 acc: 0.9083385467529297\n",
      "step: 39615 , time : 0.0010008811950683594\n",
      "train: loss: 1348106.75 acc: 0.3520183563232422  val: loss: 842887.375 acc: 0.7526103258132935\n",
      "step: 39620 , time : 0.0\n",
      "train: loss: 1852293.0 acc: 0.4261851906776428  val: loss: 551705.0625 acc: 0.849365770816803\n",
      "step: 39625 , time : 0.0\n",
      "train: loss: 1480944.75 acc: 0.5174465179443359  val: loss: 3081184.75 acc: 0.7202103137969971\n",
      "step: 39630 , time : 0.0010006427764892578\n",
      "train: loss: 714779.3125 acc: 0.47122323513031006  val: loss: 2887300.25 acc: 0.6483352184295654\n",
      "step: 39635 , time : 0.0\n",
      "train: loss: 282761.96875 acc: 0.7802751064300537  val: loss: 6121830.0 acc: 0.6625301837921143\n",
      "step: 39640 , time : 0.0\n",
      "train: loss: 63350.42578125 acc: 0.9484286308288574  val: loss: 2897681.25 acc: 0.6330372095108032\n",
      "step: 39645 , time : 0.0010004043579101562\n",
      "train: loss: 128842.4296875 acc: 0.9162275791168213  val: loss: 2029819.75 acc: 0.6645182371139526\n",
      "step: 39650 , time : 0.0\n",
      "train: loss: 82870.859375 acc: 0.9404983520507812  val: loss: 2119271.0 acc: 0.6243488788604736\n",
      "step: 39655 , time : 0.0\n",
      "train: loss: 110007.15625 acc: 0.8836883306503296  val: loss: 1420985.0 acc: 0.6439117789268494\n",
      "step: 39660 , time : 0.0010008811950683594\n",
      "train: loss: 448870.9375 acc: 0.7797816395759583  val: loss: 564561.375 acc: 0.7909397482872009\n",
      "step: 39665 , time : 0.0\n",
      "train: loss: 155928.640625 acc: 0.9032412171363831  val: loss: 1613480.0 acc: 0.7102628946304321\n",
      "step: 39670 , time : 0.0\n",
      "train: loss: 136708.375 acc: 0.8667078018188477  val: loss: 978567.9375 acc: 0.7155482769012451\n",
      "step: 39675 , time : 0.0\n",
      "train: loss: 241601.84375 acc: 0.7984887361526489  val: loss: 2807148.0 acc: 0.5758363008499146\n",
      "step: 39680 , time : 0.0\n",
      "train: loss: 69194.9765625 acc: 0.9298956990242004  val: loss: 5222807.5 acc: 0.6348164081573486\n",
      "step: 39685 , time : 0.0\n",
      "train: loss: 97431.0859375 acc: 0.918635904788971  val: loss: 2081981.75 acc: 0.6434553861618042\n",
      "step: 39690 , time : 0.0\n",
      "train: loss: 808965.9375 acc: 0.6566798686981201  val: loss: 964163.1875 acc: 0.6979116201400757\n",
      "step: 39695 , time : 0.0010008811950683594\n",
      "train: loss: 566651.5 acc: 0.7424768209457397  val: loss: 2540052.0 acc: 0.6140085458755493\n",
      "step: 39700 , time : 0.0\n",
      "train: loss: 87716.78125 acc: 0.9265186786651611  val: loss: 4007214.75 acc: 0.5997048616409302\n",
      "step: 39705 , time : 0.0\n",
      "train: loss: 342994.28125 acc: 0.822246789932251  val: loss: 1862883.5 acc: 0.6451340913772583\n",
      "step: 39710 , time : 0.0010004043579101562\n",
      "train: loss: 1219871.75 acc: 0.7566921710968018  val: loss: 3899394.25 acc: 0.6262214183807373\n",
      "step: 39715 , time : 0.0010006427764892578\n",
      "train: loss: 1103217.25 acc: 0.7555501461029053  val: loss: 1835651.25 acc: 0.772997260093689\n",
      "step: 39720 , time : 0.0010004043579101562\n",
      "train: loss: 592035.5 acc: 0.9351669549942017  val: loss: 426530.96875 acc: 0.9514961838722229\n",
      "step: 39725 , time : 0.0\n",
      "train: loss: 1229029.5 acc: 0.8481476306915283  val: loss: 1619308.0 acc: 0.5810063481330872\n",
      "step: 39730 , time : 0.0\n",
      "train: loss: 269350.78125 acc: 0.9569621086120605  val: loss: 1738308.75 acc: 0.03828948736190796\n",
      "step: 39735 , time : 0.0010006427764892578\n",
      "train: loss: 255718.125 acc: 0.9427477717399597  val: loss: 1288088.875 acc: 0.608243465423584\n",
      "step: 39740 , time : 0.001001119613647461\n",
      "train: loss: 190941.71875 acc: 0.9777538776397705  val: loss: 314765.3125 acc: 0.8119505643844604\n",
      "step: 39745 , time : 0.0\n",
      "train: loss: 114186.234375 acc: 0.991724967956543  val: loss: 1406514.75 acc: 0.7046747803688049\n",
      "step: 39750 , time : 0.0\n",
      "train: loss: 118174.2109375 acc: 0.9914394617080688  val: loss: 2154064.5 acc: 0.5402772426605225\n",
      "step: 39755 , time : 0.0010008811950683594\n",
      "train: loss: 324561.46875 acc: 0.9533314108848572  val: loss: 273068.1875 acc: 0.9471847414970398\n",
      "step: 39760 , time : 0.0\n",
      "train: loss: 11798.568359375 acc: 0.9955786466598511  val: loss: 1139360.75 acc: 0.825210690498352\n",
      "step: 39765 , time : 0.0\n",
      "train: loss: 58967.9921875 acc: 0.9871938824653625  val: loss: 1758008.5 acc: 0.6545605063438416\n",
      "step: 39770 , time : 0.0010006427764892578\n",
      "train: loss: 6301.61279296875 acc: 0.9853823184967041  val: loss: 1344826.5 acc: 0.7100334167480469\n",
      "step: 39775 , time : 0.0\n",
      "train: loss: 7060.64697265625 acc: 0.9969008564949036  val: loss: 399357.875 acc: 0.9315975904464722\n",
      "step: 39780 , time : 0.0010006427764892578\n",
      "train: loss: 21700.1328125 acc: 0.9483652114868164  val: loss: 1691165.25 acc: 0.11708420515060425\n",
      "step: 39785 , time : 0.0010006427764892578\n",
      "train: loss: 16657.416015625 acc: 0.9883479475975037  val: loss: 1021946.625 acc: 0.7414756417274475\n",
      "step: 39790 , time : 0.0\n",
      "train: loss: 4399.8828125 acc: 0.9934466481208801  val: loss: 2398744.25 acc: 0.24909991025924683\n",
      "step: 39795 , time : 0.0010006427764892578\n",
      "train: loss: 4754.71044921875 acc: 0.9917948842048645  val: loss: 950999.875 acc: 0.7517544627189636\n",
      "step: 39800 , time : 0.0\n",
      "train: loss: 3115.0751953125 acc: 0.9921606183052063  val: loss: 1250830.25 acc: 0.876140296459198\n",
      "step: 39805 , time : 0.0010006427764892578\n",
      "train: loss: 7339.712890625 acc: 0.9896209836006165  val: loss: 895726.9375 acc: 0.7264368534088135\n",
      "step: 39810 , time : 0.0\n",
      "train: loss: 21805.357421875 acc: 0.9759986996650696  val: loss: 1572026.0 acc: 0.8386368751525879\n",
      "step: 39815 , time : 0.001001119613647461\n",
      "train: loss: 38123.14453125 acc: 0.9745801091194153  val: loss: 2078870.0 acc: 0.5083359479904175\n",
      "step: 39820 , time : 0.0\n",
      "train: loss: 27428.453125 acc: 0.9869943261146545  val: loss: 1187662.0 acc: 0.6351735591888428\n",
      "step: 39825 , time : 0.0010006427764892578\n",
      "train: loss: 99215.25 acc: 0.9103118777275085  val: loss: 479622.6875 acc: 0.8739181756973267\n",
      "step: 39830 , time : 0.0010006427764892578\n",
      "train: loss: 26147.732421875 acc: 0.9795963168144226  val: loss: 584352.625 acc: 0.8788903951644897\n",
      "step: 39835 , time : 0.001001119613647461\n",
      "train: loss: 97725.5078125 acc: 0.9121845960617065  val: loss: 1394854.75 acc: 0.40065592527389526\n",
      "step: 39840 , time : 0.001165628433227539\n",
      "train: loss: 10835.3056640625 acc: 0.9794999361038208  val: loss: 1380546.5 acc: 0.2698189616203308\n",
      "step: 39845 , time : 0.0\n",
      "train: loss: 14437.103515625 acc: 0.9930337071418762  val: loss: 918581.4375 acc: 0.8651568293571472\n",
      "step: 39850 , time : 0.0\n",
      "train: loss: 26628.732421875 acc: 0.9915593862533569  val: loss: 1047223.1875 acc: 0.8593864440917969\n",
      "step: 39855 , time : 0.0\n",
      "train: loss: 29472.224609375 acc: 0.9908317923545837  val: loss: 2040964.75 acc: -0.3702806234359741\n",
      "step: 39860 , time : 0.0\n",
      "train: loss: 44641.8203125 acc: 0.9785944223403931  val: loss: 1475044.25 acc: 0.5648154020309448\n",
      "step: 39865 , time : 0.0\n",
      "train: loss: 30954.619140625 acc: 0.9885811805725098  val: loss: 1010423.6875 acc: 0.4099334478378296\n",
      "step: 39870 , time : 0.0\n",
      "train: loss: 33029.33984375 acc: 0.9914525151252747  val: loss: 1084168.875 acc: 0.6525982618331909\n",
      "step: 39875 , time : 0.0\n",
      "train: loss: 95747.1796875 acc: 0.9689396023750305  val: loss: 79401.25 acc: 0.9773188233375549\n",
      "step: 39880 , time : 0.0\n",
      "train: loss: 344736.40625 acc: 0.7956818342208862  val: loss: 1328478.875 acc: 0.5443181991577148\n",
      "step: 39885 , time : 0.0010008811950683594\n",
      "train: loss: 152583.09375 acc: 0.9714695811271667  val: loss: 165394.8125 acc: 0.9767318964004517\n",
      "step: 39890 , time : 0.0010006427764892578\n",
      "train: loss: 99902.765625 acc: 0.9916017651557922  val: loss: 850469.6875 acc: 0.7615931034088135\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 39895 , time : 0.0010008811950683594\n",
      "train: loss: 44996.8359375 acc: 0.9951779246330261  val: loss: 525837.9375 acc: 0.9102429747581482\n",
      "step: 39900 , time : 0.0010004043579101562\n",
      "train: loss: 53851.05078125 acc: 0.9931505918502808  val: loss: 1237859.625 acc: 0.7807801961898804\n",
      "step: 39905 , time : 0.001001119613647461\n",
      "train: loss: 157674.3125 acc: 0.9741597771644592  val: loss: 542499.75 acc: 0.7842197418212891\n",
      "step: 39910 , time : 0.0010006427764892578\n",
      "train: loss: 502962.03125 acc: 0.9659236073493958  val: loss: 1994878.375 acc: 0.7585176825523376\n",
      "step: 39915 , time : 0.0\n",
      "train: loss: 645372.0625 acc: 0.9677656292915344  val: loss: 1801785.75 acc: 0.8252153992652893\n",
      "step: 39920 , time : 0.0010006427764892578\n",
      "train: loss: 323761.0 acc: 0.965606153011322  val: loss: 1002134.6875 acc: 0.8858177065849304\n",
      "step: 39925 , time : 0.0\n",
      "train: loss: 1305779.25 acc: 0.9551563262939453  val: loss: 660279.375 acc: 0.9090217351913452\n",
      "step: 39930 , time : 0.0010006427764892578\n",
      "train: loss: 527773.875 acc: 0.9704026579856873  val: loss: 505195.8125 acc: 0.9641476273536682\n",
      "step: 39935 , time : 0.0\n",
      "train: loss: 1072224.5 acc: 0.9521563053131104  val: loss: 904227.6875 acc: 0.8148743510246277\n",
      "step: 39940 , time : 0.0\n",
      "train: loss: 568018.8125 acc: 0.9731597304344177  val: loss: 976607.4375 acc: 0.8727290034294128\n",
      "step: 39945 , time : 0.0010004043579101562\n",
      "train: loss: 736360.6875 acc: 0.9595984816551208  val: loss: 693505.875 acc: 0.9084052443504333\n",
      "step: 39950 , time : 0.0010006427764892578\n",
      "train: loss: 596713.1875 acc: 0.9433905482292175  val: loss: 500146.3125 acc: 0.9194528460502625\n",
      "step: 39955 , time : 0.0\n",
      "train: loss: 323314.90625 acc: 0.9035466909408569  val: loss: 911792.375 acc: 0.9337496757507324\n",
      "step: 39960 , time : 0.0\n",
      "train: loss: 421841.625 acc: 0.9332882165908813  val: loss: 1182316.75 acc: 0.9118502736091614\n",
      "step: 39965 , time : 0.0\n",
      "train: loss: 863368.625 acc: 0.6676038503646851  val: loss: 1386922.125 acc: 0.8823473453521729\n",
      "step: 39970 , time : 0.0010025501251220703\n",
      "train: loss: 624338.375 acc: 0.8145891427993774  val: loss: 1639585.5 acc: 0.8052967190742493\n",
      "step: 39975 , time : 0.0010006427764892578\n",
      "train: loss: 376852.84375 acc: 0.7715442180633545  val: loss: 1413300.375 acc: 0.846336841583252\n",
      "step: 39980 , time : 0.0\n",
      "train: loss: 469788.8125 acc: 0.8856462836265564  val: loss: 2055195.25 acc: 0.6322439908981323\n",
      "step: 39985 , time : 0.0\n",
      "train: loss: 1057188.5 acc: 0.6324764490127563  val: loss: 1651833.5 acc: 0.8022199869155884\n",
      "step: 39990 , time : 0.0010008811950683594\n",
      "train: loss: 1698166.5 acc: 0.3898646831512451  val: loss: 2194483.25 acc: 0.6358942985534668\n",
      "step: 39995 , time : 0.001001119613647461\n",
      "train: loss: 799673.25 acc: 0.40103644132614136  val: loss: 853771.375 acc: 0.1130189299583435\n",
      "step: 40000 , time : 0.0\n",
      "train: loss: 1097757.625 acc: 0.6037181615829468  val: loss: 1984458.75 acc: 0.704932689666748\n",
      "step: 40005 , time : 0.0\n",
      "train: loss: 879146.625 acc: 0.676791250705719  val: loss: 728563.0 acc: 0.7203872799873352\n",
      "step: 40010 , time : 0.0\n",
      "train: loss: 321534.71875 acc: 0.722899317741394  val: loss: 799465.375 acc: 0.8298850059509277\n",
      "step: 40015 , time : 0.0010006427764892578\n",
      "train: loss: 87475.171875 acc: 0.938151478767395  val: loss: 1022262.75 acc: 0.6247910261154175\n",
      "step: 40020 , time : 0.001001119613647461\n",
      "train: loss: 329517.375 acc: 0.8521023988723755  val: loss: 514723.78125 acc: 0.7168136239051819\n",
      "step: 40025 , time : 0.001001119613647461\n",
      "train: loss: 47892.71875 acc: 0.9600735902786255  val: loss: 862149.0 acc: 0.8111562728881836\n",
      "step: 40030 , time : 0.0\n",
      "train: loss: 342450.84375 acc: 0.7599344253540039  val: loss: 705872.3125 acc: 0.8475834131240845\n",
      "step: 40035 , time : 0.001001596450805664\n",
      "train: loss: 65406.17578125 acc: 0.9433035254478455  val: loss: 1222586.125 acc: 0.8745324611663818\n",
      "step: 40040 , time : 0.0\n",
      "train: loss: 53476.01171875 acc: 0.9363593459129333  val: loss: 999666.625 acc: 0.6084622144699097\n",
      "step: 40045 , time : 0.0010008811950683594\n",
      "train: loss: 416260.1875 acc: 0.7480506896972656  val: loss: 681237.9375 acc: 0.850393533706665\n",
      "step: 40050 , time : 0.0\n",
      "train: loss: 468844.78125 acc: 0.7292264699935913  val: loss: 771801.6875 acc: 0.8610000014305115\n",
      "step: 40055 , time : 0.0\n",
      "train: loss: 604235.8125 acc: 0.6363356113433838  val: loss: 519026.75 acc: 0.6808253526687622\n",
      "step: 40060 , time : 0.001001119613647461\n",
      "train: loss: 131807.265625 acc: 0.8626248836517334  val: loss: 515522.0625 acc: 0.7573726773262024\n",
      "step: 40065 , time : 0.0\n",
      "train: loss: 436882.09375 acc: 0.6132795214653015  val: loss: 1561692.125 acc: 0.7246832847595215\n",
      "step: 40070 , time : 0.0\n",
      "train: loss: 481433.84375 acc: 0.7470463514328003  val: loss: 1052233.75 acc: 0.674262285232544\n",
      "step: 40075 , time : 0.0\n",
      "train: loss: 1812694.0 acc: 0.717847466468811  val: loss: 1501272.125 acc: 0.6140506863594055\n",
      "step: 40080 , time : 0.0010004043579101562\n",
      "train: loss: 1064466.25 acc: 0.8366223573684692  val: loss: 1022298.375 acc: 0.7985538840293884\n",
      "step: 40085 , time : 0.0010006427764892578\n",
      "train: loss: 1018574.75 acc: 0.9109224081039429  val: loss: 1245176.75 acc: 0.8704439401626587\n",
      "step: 40090 , time : 0.0\n",
      "train: loss: 525001.3125 acc: 0.9309097528457642  val: loss: 858752.6875 acc: 0.8567019104957581\n",
      "step: 40095 , time : 0.0010006427764892578\n",
      "train: loss: 485985.15625 acc: 0.9225714206695557  val: loss: 743239.9375 acc: 0.831678569316864\n",
      "step: 40100 , time : 0.0\n",
      "train: loss: 544164.0625 acc: 0.9109274744987488  val: loss: 452631.1875 acc: 0.8936572074890137\n",
      "step: 40105 , time : 0.0\n",
      "train: loss: 417852.46875 acc: 0.9669711589813232  val: loss: 1073932.0 acc: 0.7461114525794983\n",
      "step: 40110 , time : 0.0\n",
      "train: loss: 256642.765625 acc: 0.9798892736434937  val: loss: 1511241.125 acc: 0.7465014457702637\n",
      "step: 40115 , time : 0.0010008811950683594\n",
      "train: loss: 258223.796875 acc: 0.980448305606842  val: loss: 887282.8125 acc: 0.7440228462219238\n",
      "step: 40120 , time : 0.0\n",
      "train: loss: 302819.78125 acc: 0.9596359133720398  val: loss: 1794816.25 acc: 0.42423510551452637\n",
      "step: 40125 , time : 0.0\n",
      "train: loss: 146623.515625 acc: 0.9599999189376831  val: loss: 844603.6875 acc: 0.7048072814941406\n",
      "step: 40130 , time : 0.0011739730834960938\n",
      "train: loss: 127281.9296875 acc: 0.9381548762321472  val: loss: 967775.5625 acc: 0.4583202600479126\n",
      "step: 40135 , time : 0.0\n",
      "train: loss: 165849.359375 acc: 0.9642798900604248  val: loss: 275555.125 acc: 0.9348337054252625\n",
      "step: 40140 , time : 0.0\n",
      "train: loss: 1579.4483642578125 acc: 0.9951704740524292  val: loss: 532764.8125 acc: 0.7976589798927307\n",
      "step: 40145 , time : 0.0\n",
      "train: loss: 12511.0009765625 acc: 0.9805359244346619  val: loss: 474566.3125 acc: 0.752043604850769\n",
      "step: 40150 , time : 0.0\n",
      "train: loss: 43520.53125 acc: 0.9712929725646973  val: loss: 755474.125 acc: 0.7844420671463013\n",
      "step: 40155 , time : 0.0\n",
      "train: loss: 11280.5283203125 acc: 0.934111475944519  val: loss: 157604.203125 acc: 0.9612200260162354\n",
      "step: 40160 , time : 0.0009999275207519531\n",
      "train: loss: 6206.169921875 acc: 0.9865186214447021  val: loss: 88328.3984375 acc: 0.9679028391838074\n",
      "step: 40165 , time : 0.0\n",
      "train: loss: 5512.40576171875 acc: 0.983013391494751  val: loss: 908034.4375 acc: 0.7331465482711792\n",
      "step: 40170 , time : 0.0\n",
      "train: loss: 8269.314453125 acc: 0.970220685005188  val: loss: 1298199.25 acc: 0.8487389087677002\n",
      "step: 40175 , time : 0.0\n",
      "train: loss: 9346.1669921875 acc: 0.973767876625061  val: loss: 675875.1875 acc: 0.9080750942230225\n",
      "step: 40180 , time : 0.0010008811950683594\n",
      "train: loss: 28007.857421875 acc: 0.9744161367416382  val: loss: 917219.875 acc: 0.8279128074645996\n",
      "step: 40185 , time : 0.0010004043579101562\n",
      "train: loss: 20392.912109375 acc: 0.9844904541969299  val: loss: 595970.125 acc: 0.7093026638031006\n",
      "step: 40190 , time : 0.0\n",
      "train: loss: 9222.7548828125 acc: 0.9923513531684875  val: loss: 66855.203125 acc: 0.9452772736549377\n",
      "step: 40195 , time : 0.0010006427764892578\n",
      "train: loss: 113430.25 acc: 0.9384172558784485  val: loss: 509131.53125 acc: 0.8641560077667236\n",
      "step: 40200 , time : 0.0\n",
      "train: loss: 13771.8564453125 acc: 0.9844987392425537  val: loss: 223865.3125 acc: 0.8861055374145508\n",
      "step: 40205 , time : 0.0\n",
      "train: loss: 4592.8525390625 acc: 0.9889867305755615  val: loss: 1085503.0 acc: 0.8271329402923584\n",
      "step: 40210 , time : 0.0\n",
      "train: loss: 12863.73046875 acc: 0.9935048222541809  val: loss: 2131188.75 acc: 0.6697583198547363\n",
      "step: 40215 , time : 0.001001119613647461\n",
      "train: loss: 46315.87890625 acc: 0.9888834953308105  val: loss: 529432.6875 acc: 0.8630311489105225\n",
      "step: 40220 , time : 0.0\n",
      "train: loss: 46204.52734375 acc: 0.9869145154953003  val: loss: 1300099.0 acc: 0.8325279951095581\n",
      "step: 40225 , time : 0.0020020008087158203\n",
      "train: loss: 31235.6796875 acc: 0.9898797273635864  val: loss: 1011356.9375 acc: 0.8096243739128113\n",
      "step: 40230 , time : 0.0010006427764892578\n",
      "train: loss: 46914.71875 acc: 0.9721130728721619  val: loss: 1844115.0 acc: 0.7860985398292542\n",
      "step: 40235 , time : 0.0\n",
      "train: loss: 175450.90625 acc: 0.9528802037239075  val: loss: 471465.0 acc: 0.8316308856010437\n",
      "step: 40240 , time : 0.0010006427764892578\n",
      "train: loss: 177518.046875 acc: 0.8914874792098999  val: loss: 1805362.375 acc: 0.7354363799095154\n",
      "step: 40245 , time : 0.0\n",
      "train: loss: 68516.1484375 acc: 0.958820641040802  val: loss: 711420.9375 acc: 0.8979469537734985\n",
      "step: 40250 , time : 0.001001119613647461\n",
      "train: loss: 92142.1015625 acc: 0.9798130393028259  val: loss: 453265.3125 acc: 0.9409770369529724\n",
      "step: 40255 , time : 0.0010006427764892578\n",
      "train: loss: 108830.4140625 acc: 0.9907978773117065  val: loss: 3051230.25 acc: 0.6443051099777222\n",
      "step: 40260 , time : 0.0010006427764892578\n",
      "train: loss: 99469.5703125 acc: 0.9866728186607361  val: loss: 200516.015625 acc: 0.9375894069671631\n",
      "step: 40265 , time : 0.0009996891021728516\n",
      "train: loss: 121201.7265625 acc: 0.9832278490066528  val: loss: 352809.78125 acc: 0.9296331405639648\n",
      "step: 40270 , time : 0.0\n",
      "train: loss: 158250.109375 acc: 0.971049427986145  val: loss: 668759.9375 acc: 0.8908672332763672\n",
      "step: 40275 , time : 0.0\n",
      "train: loss: 302066.625 acc: 0.9761207103729248  val: loss: 2384844.25 acc: 0.6873546838760376\n",
      "step: 40280 , time : 0.0\n",
      "train: loss: 532666.4375 acc: 0.917570173740387  val: loss: 1082803.75 acc: 0.8989782333374023\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 40285 , time : 0.0010006427764892578\n",
      "train: loss: 271654.09375 acc: 0.954940676689148  val: loss: 1013657.75 acc: 0.7068548202514648\n",
      "step: 40290 , time : 0.001001119613647461\n",
      "train: loss: 1058467.375 acc: 0.9420114159584045  val: loss: 3348136.25 acc: 0.49990057945251465\n",
      "step: 40295 , time : 0.0010004043579101562\n",
      "train: loss: 1200750.375 acc: 0.965600311756134  val: loss: 1950622.5 acc: 0.13753795623779297\n",
      "step: 40300 , time : 0.0010008811950683594\n",
      "train: loss: 1237627.125 acc: 0.9629238843917847  val: loss: 4967477.5 acc: 0.44789987802505493\n",
      "step: 40305 , time : 0.0\n",
      "train: loss: 469550.6875 acc: 0.9728108644485474  val: loss: 1074453.5 acc: 0.8662029504776001\n",
      "step: 40310 , time : 0.001001119613647461\n",
      "train: loss: 1080636.375 acc: 0.9169418811798096  val: loss: 227732.75 acc: 0.9536219835281372\n",
      "step: 40315 , time : 0.0010006427764892578\n",
      "train: loss: 589750.0 acc: 0.9461824297904968  val: loss: 1830173.75 acc: 0.35559868812561035\n",
      "step: 40320 , time : 0.0\n",
      "train: loss: 293777.03125 acc: 0.8872259259223938  val: loss: 1275387.375 acc: 0.8726339936256409\n",
      "step: 40325 , time : 0.0\n",
      "train: loss: 397909.4375 acc: 0.950318455696106  val: loss: 384678.0 acc: 0.9417468309402466\n",
      "step: 40330 , time : 0.0\n",
      "train: loss: 1517528.75 acc: 0.3657688498497009  val: loss: 1046035.9375 acc: 0.7868191003799438\n",
      "step: 40335 , time : 0.001001119613647461\n",
      "train: loss: 1580285.0 acc: 0.619684100151062  val: loss: 1423811.5 acc: 0.6844228506088257\n",
      "step: 40340 , time : 0.0\n",
      "train: loss: 827122.5625 acc: 0.7354531288146973  val: loss: 921427.5625 acc: 0.8258367776870728\n",
      "step: 40345 , time : 0.0\n",
      "train: loss: 602435.125 acc: 0.7762767672538757  val: loss: 1453066.625 acc: 0.8654991984367371\n",
      "step: 40350 , time : 0.0\n",
      "train: loss: 2104727.25 acc: 0.004313468933105469  val: loss: 1707389.25 acc: 0.7536458373069763\n",
      "step: 40355 , time : 0.001001119613647461\n",
      "train: loss: 1324669.0 acc: 0.49436312913894653  val: loss: 2325351.5 acc: 0.7189896106719971\n",
      "step: 40360 , time : 0.0010008811950683594\n",
      "train: loss: 622614.6875 acc: 0.5897037982940674  val: loss: 1935767.625 acc: 0.47713351249694824\n",
      "step: 40365 , time : 0.0010013580322265625\n",
      "train: loss: 631198.875 acc: 0.383874773979187  val: loss: 1090711.75 acc: 0.58955979347229\n",
      "step: 40370 , time : 0.0\n",
      "train: loss: 552723.6875 acc: 0.5635101199150085  val: loss: 917582.5 acc: 0.7985259890556335\n",
      "step: 40375 , time : 0.0\n",
      "train: loss: 220677.578125 acc: 0.8448818325996399  val: loss: 1477825.0 acc: 0.8037723302841187\n",
      "step: 40380 , time : 0.0\n",
      "train: loss: 362197.375 acc: 0.7257286906242371  val: loss: 855501.375 acc: 0.8757708668708801\n",
      "step: 40385 , time : 0.0\n",
      "train: loss: 117002.1953125 acc: 0.9093594551086426  val: loss: 1198171.75 acc: 0.7894922494888306\n",
      "step: 40390 , time : 0.0\n",
      "train: loss: 83987.0 acc: 0.9307026267051697  val: loss: 1873336.0 acc: 0.8300914764404297\n",
      "step: 40395 , time : 0.0\n",
      "train: loss: 248152.65625 acc: 0.8338163495063782  val: loss: 413350.65625 acc: 0.7067453861236572\n",
      "step: 40400 , time : 0.0010004043579101562\n",
      "train: loss: 110188.8828125 acc: 0.9138911962509155  val: loss: 830411.25 acc: 0.8107036352157593\n",
      "step: 40405 , time : 0.0\n",
      "train: loss: 158013.109375 acc: 0.8522058129310608  val: loss: 793172.1875 acc: 0.8806891441345215\n",
      "step: 40410 , time : 0.0\n",
      "train: loss: 86670.5546875 acc: 0.917855978012085  val: loss: 1503399.5 acc: 0.7591813206672668\n",
      "step: 40415 , time : 0.0010008811950683594\n",
      "train: loss: 205055.625 acc: 0.7835941314697266  val: loss: 1751999.0 acc: 0.817980170249939\n",
      "step: 40420 , time : 0.0\n",
      "train: loss: 510538.09375 acc: 0.7900028228759766  val: loss: 721009.625 acc: 0.684570848941803\n",
      "step: 40425 , time : 0.0010006427764892578\n",
      "train: loss: 998788.375 acc: 0.5882970094680786  val: loss: 717542.25 acc: 0.7686328291893005\n",
      "step: 40430 , time : 0.0\n",
      "train: loss: 469565.90625 acc: 0.7919750213623047  val: loss: 1114005.125 acc: 0.7441275119781494\n",
      "step: 40435 , time : 0.0\n",
      "train: loss: 962852.25 acc: 0.6627581715583801  val: loss: 679189.5 acc: 0.6439985036849976\n",
      "step: 40440 , time : 0.0\n",
      "train: loss: 1144673.25 acc: 0.730334997177124  val: loss: 629464.5 acc: 0.7092570066452026\n",
      "step: 40445 , time : 0.0\n",
      "train: loss: 1213975.0 acc: 0.7465021014213562  val: loss: 818845.0625 acc: 0.8053489923477173\n",
      "step: 40450 , time : 0.0010004043579101562\n",
      "train: loss: 600430.3125 acc: 0.9245912432670593  val: loss: 978484.5 acc: 0.8283615112304688\n",
      "step: 40455 , time : 0.0\n",
      "train: loss: 607358.8125 acc: 0.9283590316772461  val: loss: 1053537.625 acc: 0.8491121530532837\n",
      "step: 40460 , time : 0.0010004043579101562\n",
      "train: loss: 478625.1875 acc: 0.8444608449935913  val: loss: 605250.1875 acc: 0.7418901920318604\n",
      "step: 40465 , time : 0.0010008811950683594\n",
      "train: loss: 363244.34375 acc: 0.9312271475791931  val: loss: 443488.78125 acc: 0.7010670900344849\n",
      "step: 40470 , time : 0.0010006427764892578\n",
      "train: loss: 424805.4375 acc: 0.9584993720054626  val: loss: 1497329.5 acc: 0.6875084638595581\n",
      "step: 40475 , time : 0.0\n",
      "train: loss: 181641.03125 acc: 0.9878764748573303  val: loss: 618374.25 acc: 0.8668059706687927\n",
      "step: 40480 , time : 0.0\n",
      "train: loss: 228613.75 acc: 0.982141375541687  val: loss: 1405135.25 acc: 0.8342915177345276\n",
      "step: 40485 , time : 0.0010001659393310547\n",
      "train: loss: 317886.375 acc: 0.937030553817749  val: loss: 789699.9375 acc: 0.8733577132225037\n",
      "step: 40490 , time : 0.0\n",
      "train: loss: 414362.6875 acc: 0.9338896870613098  val: loss: 727625.0625 acc: 0.5181880593299866\n",
      "step: 40495 , time : 0.0010006427764892578\n",
      "train: loss: 84901.9609375 acc: 0.9437034130096436  val: loss: 762610.375 acc: 0.8819090127944946\n",
      "step: 40500 , time : 0.0\n",
      "train: loss: 9172.98828125 acc: 0.9751175045967102  val: loss: 193465.171875 acc: 0.9577935934066772\n",
      "step: 40505 , time : 0.0\n",
      "train: loss: 26980.46484375 acc: 0.9913089871406555  val: loss: 601239.6875 acc: 0.7509366273880005\n",
      "step: 40510 , time : 0.0010008811950683594\n",
      "train: loss: 10663.7880859375 acc: 0.9812658429145813  val: loss: 221412.90625 acc: 0.9538346529006958\n",
      "step: 40515 , time : 0.0\n",
      "train: loss: 48264.796875 acc: 0.9696054458618164  val: loss: 453119.125 acc: 0.9024524092674255\n",
      "step: 40520 , time : 0.0010008811950683594\n",
      "train: loss: 19533.2734375 acc: 0.9282837510108948  val: loss: 857362.9375 acc: 0.8381754159927368\n",
      "step: 40525 , time : 0.0010008811950683594\n",
      "train: loss: 5701.99267578125 acc: 0.9883893132209778  val: loss: 316473.96875 acc: 0.880007266998291\n",
      "step: 40530 , time : 0.001001119613647461\n",
      "train: loss: 6152.6708984375 acc: 0.9803028702735901  val: loss: 653470.875 acc: 0.8641495704650879\n",
      "step: 40535 , time : 0.0\n",
      "train: loss: 114674.1953125 acc: 0.9416524767875671  val: loss: 3460868.25 acc: 0.7452373504638672\n",
      "step: 40540 , time : 0.0\n",
      "train: loss: 6930.71826171875 acc: 0.9820603728294373  val: loss: 1504930.5 acc: 0.7954648733139038\n",
      "step: 40545 , time : 0.0\n",
      "train: loss: 44583.4453125 acc: 0.9422948360443115  val: loss: 1093350.25 acc: 0.9088616371154785\n",
      "step: 40550 , time : 0.0\n",
      "train: loss: 29208.666015625 acc: 0.9701687097549438  val: loss: 1022020.875 acc: 0.10692459344863892\n",
      "step: 40555 , time : 0.0\n",
      "train: loss: 45596.20703125 acc: 0.9725571274757385  val: loss: 549080.0 acc: 0.900867223739624\n",
      "step: 40560 , time : 0.0\n",
      "train: loss: 34551.4453125 acc: 0.9816422462463379  val: loss: 1059517.125 acc: 0.8424044251441956\n",
      "step: 40565 , time : 0.0\n",
      "train: loss: 58765.5 acc: 0.9762529730796814  val: loss: 147567.84375 acc: 0.9329754710197449\n",
      "step: 40570 , time : 0.0\n",
      "train: loss: 14947.833984375 acc: 0.9800682663917542  val: loss: 782753.4375 acc: 0.8837981820106506\n",
      "step: 40575 , time : 0.0010013580322265625\n",
      "train: loss: 15445.9375 acc: 0.974787712097168  val: loss: 819362.0 acc: 0.9285291433334351\n",
      "step: 40580 , time : 0.0\n",
      "train: loss: 30449.15625 acc: 0.9902990460395813  val: loss: 2258169.5 acc: -0.7827179431915283\n",
      "step: 40585 , time : 0.0009999275207519531\n",
      "train: loss: 39376.37109375 acc: 0.9892056584358215  val: loss: 348919.375 acc: 0.975482165813446\n",
      "step: 40590 , time : 0.0010006427764892578\n",
      "train: loss: 62306.1015625 acc: 0.9853547215461731  val: loss: 1424911.125 acc: 0.8029577732086182\n",
      "step: 40595 , time : 0.0\n",
      "train: loss: 82936.2578125 acc: 0.9728699326515198  val: loss: 1771260.5 acc: 0.8174424767494202\n",
      "step: 40600 , time : 0.0\n",
      "train: loss: 109700.8359375 acc: 0.9770387411117554  val: loss: 1255906.25 acc: 0.8049523234367371\n",
      "step: 40605 , time : 0.0\n",
      "train: loss: 204858.40625 acc: 0.8993059396743774  val: loss: 1209892.75 acc: 0.873651385307312\n",
      "step: 40610 , time : 0.0010006427764892578\n",
      "train: loss: 145411.46875 acc: 0.9522664546966553  val: loss: 2641506.25 acc: 0.6092877388000488\n",
      "step: 40615 , time : 0.0\n",
      "train: loss: 125962.6640625 acc: 0.9695004224777222  val: loss: 650100.125 acc: 0.8852428197860718\n",
      "step: 40620 , time : 0.0\n",
      "train: loss: 54405.65625 acc: 0.9939555525779724  val: loss: 1584404.75 acc: 0.8852723240852356\n",
      "step: 40625 , time : 0.0010006427764892578\n",
      "train: loss: 577825.75 acc: 0.930892825126648  val: loss: 1293421.875 acc: 0.6717884540557861\n",
      "step: 40630 , time : 0.0010006427764892578\n",
      "train: loss: 47939.38671875 acc: 0.9937322735786438  val: loss: 899798.5625 acc: 0.42311328649520874\n",
      "step: 40635 , time : 0.0\n",
      "train: loss: 151666.28125 acc: 0.9804101586341858  val: loss: 452562.5625 acc: 0.933954656124115\n",
      "step: 40640 , time : 0.0010008811950683594\n",
      "train: loss: 392064.8125 acc: 0.9628897309303284  val: loss: 2955295.5 acc: -0.06534802913665771\n",
      "step: 40645 , time : 0.0\n",
      "train: loss: 471162.34375 acc: 0.9704347848892212  val: loss: 1070511.25 acc: 0.5738786458969116\n",
      "step: 40650 , time : 0.0\n",
      "train: loss: 448910.34375 acc: 0.9353283643722534  val: loss: 2671228.25 acc: 0.6127411127090454\n",
      "step: 40655 , time : 0.0010004043579101562\n",
      "train: loss: 2038101.875 acc: 0.8784034848213196  val: loss: 1611805.5 acc: 0.8555649518966675\n",
      "step: 40660 , time : 0.0\n",
      "train: loss: 500734.125 acc: 0.9843811988830566  val: loss: 480880.96875 acc: 0.8152814507484436\n",
      "step: 40665 , time : 0.0\n",
      "train: loss: 1930971.625 acc: 0.9537262320518494  val: loss: 1158465.75 acc: 0.8388918042182922\n",
      "step: 40670 , time : 0.0010008811950683594\n",
      "train: loss: 789586.1875 acc: 0.9607898592948914  val: loss: 2879595.75 acc: -0.0469437837600708\n",
      "step: 40675 , time : 0.0\n",
      "train: loss: 587015.6875 acc: 0.9427521228790283  val: loss: 1581117.875 acc: 0.7135323286056519\n",
      "step: 40680 , time : 0.0\n",
      "train: loss: 229882.75 acc: 0.9788339138031006  val: loss: 1659075.5 acc: 0.7651469707489014\n",
      "step: 40685 , time : 0.0\n",
      "train: loss: 534691.4375 acc: 0.9411099553108215  val: loss: 1199601.25 acc: 0.6027966737747192\n",
      "step: 40690 , time : 0.0\n",
      "train: loss: 701268.4375 acc: 0.9006636142730713  val: loss: 1446325.75 acc: 0.6989299058914185\n",
      "step: 40695 , time : 0.0\n",
      "train: loss: 1941247.25 acc: 0.34321773052215576  val: loss: 863663.75 acc: 0.813197135925293\n",
      "step: 40700 , time : 0.0\n",
      "train: loss: 972493.0625 acc: 0.7648147940635681  val: loss: 830015.4375 acc: 0.7687485218048096\n",
      "step: 40705 , time : 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: loss: 999213.8125 acc: 0.36759012937545776  val: loss: 1555331.75 acc: 0.7800824642181396\n",
      "step: 40710 , time : 0.0\n",
      "train: loss: 640024.0 acc: 0.8117551803588867  val: loss: 1203838.25 acc: 0.8582423329353333\n",
      "step: 40715 , time : 0.0\n",
      "train: loss: 1479577.25 acc: 0.49981725215911865  val: loss: 387480.65625 acc: 0.8902071714401245\n",
      "step: 40720 , time : 0.0\n",
      "train: loss: 1521230.25 acc: 0.36812466382980347  val: loss: 735276.0625 acc: 0.7657415866851807\n",
      "step: 40725 , time : 0.0\n",
      "train: loss: 830721.0 acc: 0.3451045751571655  val: loss: 1457066.375 acc: 0.5420665740966797\n",
      "step: 40730 , time : 0.0010008811950683594\n",
      "train: loss: 279848.71875 acc: 0.7807514667510986  val: loss: 666033.5 acc: 0.7101972699165344\n",
      "step: 40735 , time : 0.0010004043579101562\n",
      "train: loss: 276152.3125 acc: 0.7885901927947998  val: loss: 1502447.625 acc: 0.7247767448425293\n",
      "step: 40740 , time : 0.0\n",
      "train: loss: 124234.9375 acc: 0.906582772731781  val: loss: 1556387.75 acc: 0.7433472871780396\n",
      "step: 40745 , time : 0.0010006427764892578\n",
      "train: loss: 75581.9609375 acc: 0.9320598840713501  val: loss: 1503824.625 acc: 0.7047921419143677\n",
      "step: 40750 , time : 0.0010006427764892578\n",
      "train: loss: 126636.5390625 acc: 0.9017258882522583  val: loss: 734287.1875 acc: 0.732710063457489\n",
      "step: 40755 , time : 0.0010006427764892578\n",
      "train: loss: 94422.59375 acc: 0.917241632938385  val: loss: 1133900.5 acc: 0.8064918518066406\n",
      "step: 40760 , time : 0.0\n",
      "train: loss: 429263.03125 acc: 0.7822104692459106  val: loss: 368446.4375 acc: 0.7995630502700806\n",
      "step: 40765 , time : 0.0\n",
      "train: loss: 115764.875 acc: 0.8828508853912354  val: loss: 533426.9375 acc: 0.7496675252914429\n",
      "step: 40770 , time : 0.0\n",
      "train: loss: 129692.78125 acc: 0.9088203310966492  val: loss: 967863.875 acc: 0.7341016530990601\n",
      "step: 40775 , time : 0.0\n",
      "train: loss: 166183.71875 acc: 0.9066177606582642  val: loss: 2501151.25 acc: 0.6516468524932861\n",
      "step: 40780 , time : 0.0\n",
      "train: loss: 475786.0625 acc: 0.7780051231384277  val: loss: 836880.0 acc: 0.6634107828140259\n",
      "step: 40785 , time : 0.0010006427764892578\n",
      "train: loss: 572888.6875 acc: 0.7307705283164978  val: loss: 1930562.5 acc: 0.6311962604522705\n",
      "step: 40790 , time : 0.0\n",
      "train: loss: 370505.96875 acc: 0.7651602029800415  val: loss: 2097675.5 acc: 0.619404673576355\n",
      "step: 40795 , time : 0.0\n",
      "train: loss: 11522.1826171875 acc: 0.9851606488227844  val: loss: 2128403.0 acc: 0.6418642997741699\n",
      "step: 40800 , time : 0.0\n",
      "train: loss: 753722.625 acc: 0.6876623630523682  val: loss: 1921186.75 acc: 0.654974102973938\n",
      "step: 40805 , time : 0.0\n",
      "train: loss: 2325723.25 acc: 0.6815876960754395  val: loss: 582626.5 acc: 0.8117928504943848\n",
      "step: 40810 , time : 0.0\n",
      "train: loss: 691051.8125 acc: 0.8339403867721558  val: loss: 389363.09375 acc: 0.8883118629455566\n",
      "step: 40815 , time : 0.0\n",
      "train: loss: 973876.5 acc: 0.8971232771873474  val: loss: 345000.65625 acc: 0.9209017157554626\n",
      "step: 40820 , time : 0.0\n",
      "train: loss: 721002.5625 acc: 0.928680956363678  val: loss: 240336.640625 acc: 0.9647716283798218\n",
      "step: 40825 , time : 0.0\n",
      "train: loss: 429569.875 acc: 0.8853750228881836  val: loss: 133188.265625 acc: 0.9541051387786865\n",
      "step: 40830 , time : 0.001001119613647461\n",
      "train: loss: 210190.84375 acc: 0.952589213848114  val: loss: 285835.25 acc: 0.9546886086463928\n",
      "step: 40835 , time : 0.0\n",
      "train: loss: 164390.421875 acc: 0.9823428392410278  val: loss: 252283.046875 acc: 0.9489959478378296\n",
      "step: 40840 , time : 0.0\n",
      "train: loss: 78775.2265625 acc: 0.9936593770980835  val: loss: 771837.4375 acc: 0.9086808562278748\n",
      "step: 40845 , time : 0.0\n",
      "train: loss: 96793.0546875 acc: 0.9938474297523499  val: loss: 1115174.5 acc: 0.8634355664253235\n",
      "step: 40850 , time : 0.0010008811950683594\n",
      "train: loss: 102432.4453125 acc: 0.9868586659431458  val: loss: 665525.3125 acc: 0.9270716905593872\n",
      "step: 40855 , time : 0.0\n",
      "train: loss: 81792.40625 acc: 0.9878237843513489  val: loss: 1965926.625 acc: 0.6505805253982544\n",
      "step: 40860 , time : 0.0\n",
      "train: loss: 25331.443359375 acc: 0.9895914196968079  val: loss: 1339242.0 acc: 0.9048707485198975\n",
      "step: 40865 , time : 0.0\n",
      "train: loss: 31334.88671875 acc: 0.9904069304466248  val: loss: 546934.125 acc: 0.9237334132194519\n",
      "step: 40870 , time : 0.0\n",
      "train: loss: 9803.666015625 acc: 0.991148054599762  val: loss: 661331.625 acc: 0.9346317052841187\n",
      "step: 40875 , time : 0.0\n",
      "train: loss: 3310.224365234375 acc: 0.9976829886436462  val: loss: 1125519.375 acc: 0.7558656930923462\n",
      "step: 40880 , time : 0.0\n",
      "train: loss: 181893.953125 acc: 0.8036056756973267  val: loss: 998117.0625 acc: 0.8876028060913086\n",
      "step: 40885 , time : 0.0\n",
      "train: loss: 12646.07421875 acc: 0.9846963882446289  val: loss: 423086.90625 acc: 0.8984887003898621\n",
      "step: 40890 , time : 0.0\n",
      "train: loss: 10609.953125 acc: 0.9813234210014343  val: loss: 1376555.875 acc: 0.6482464075088501\n",
      "step: 40895 , time : 0.0\n",
      "train: loss: 4553.54150390625 acc: 0.9874142408370972  val: loss: 1040250.875 acc: 0.9100217819213867\n",
      "step: 40900 , time : 0.0\n",
      "train: loss: 8513.3056640625 acc: 0.9895757436752319  val: loss: 3113359.25 acc: -0.04826819896697998\n",
      "step: 40905 , time : 0.0\n",
      "train: loss: 8469.3955078125 acc: 0.9813398122787476  val: loss: 1567011.0 acc: 0.6015372276306152\n",
      "step: 40910 , time : 0.0\n",
      "train: loss: 56026.8984375 acc: 0.9619366526603699  val: loss: 710187.8125 acc: 0.7796534895896912\n",
      "step: 40915 , time : 0.0\n",
      "train: loss: 22398.00390625 acc: 0.9858148097991943  val: loss: 1824955.625 acc: 0.8039042949676514\n",
      "step: 40920 , time : 0.0\n",
      "train: loss: 105050.796875 acc: 0.8567317128181458  val: loss: 1226240.75 acc: 0.8133974671363831\n",
      "step: 40925 , time : 0.0\n",
      "train: loss: 24079.64453125 acc: 0.9895883202552795  val: loss: 2324868.0 acc: 0.2984643578529358\n",
      "step: 40930 , time : 0.0\n",
      "train: loss: 6620.4892578125 acc: 0.9928252100944519  val: loss: 1378886.125 acc: -0.7137296199798584\n",
      "step: 40935 , time : 0.0\n",
      "train: loss: 11873.5166015625 acc: 0.9810619950294495  val: loss: 1652532.875 acc: 0.44636285305023193\n",
      "step: 40940 , time : 0.0010008811950683594\n",
      "train: loss: 35234.2421875 acc: 0.9767775535583496  val: loss: 1927785.125 acc: 0.32769811153411865\n",
      "step: 40945 , time : 0.0\n",
      "train: loss: 30602.056640625 acc: 0.9856727123260498  val: loss: 2444206.25 acc: -0.12301135063171387\n",
      "step: 40950 , time : 0.0010008811950683594\n",
      "train: loss: 36993.890625 acc: 0.9940474033355713  val: loss: 1678653.0 acc: 0.530245304107666\n",
      "step: 40955 , time : 0.0010006427764892578\n",
      "train: loss: 27247.212890625 acc: 0.9859530329704285  val: loss: 1258567.75 acc: 0.5342813730239868\n",
      "step: 40960 , time : 0.0\n",
      "train: loss: 33603.03125 acc: 0.9766837358474731  val: loss: 974510.5625 acc: 0.6735848188400269\n",
      "step: 40965 , time : 0.0\n",
      "train: loss: 52703.66015625 acc: 0.9876720309257507  val: loss: 1364847.875 acc: 0.29814112186431885\n",
      "step: 40970 , time : 0.0\n",
      "train: loss: 64326.4765625 acc: 0.9801648259162903  val: loss: 2013775.125 acc: 0.8023077845573425\n",
      "step: 40975 , time : 0.0\n",
      "train: loss: 108642.7421875 acc: 0.9364525079727173  val: loss: 1443728.875 acc: 0.7905908823013306\n",
      "step: 40980 , time : 0.0\n",
      "train: loss: 552993.0625 acc: 0.8408013582229614  val: loss: 1088026.0 acc: 0.7928158044815063\n",
      "step: 40985 , time : 0.0\n",
      "train: loss: 269981.15625 acc: 0.9752915501594543  val: loss: 1009852.3125 acc: 0.5747368335723877\n",
      "step: 40990 , time : 0.0010006427764892578\n",
      "train: loss: 97210.3046875 acc: 0.9914940595626831  val: loss: 3273008.25 acc: -0.7946144342422485\n",
      "step: 40995 , time : 0.0\n",
      "train: loss: 56369.66015625 acc: 0.9930228590965271  val: loss: 3606552.75 acc: 0.17488956451416016\n",
      "step: 41000 , time : 0.0\n",
      "train: loss: 362366.1875 acc: 0.9691307544708252  val: loss: 1208913.25 acc: 0.7423365116119385\n",
      "step: 41005 , time : 0.0010006427764892578\n",
      "train: loss: 267915.21875 acc: 0.9795563220977783  val: loss: 1003065.0 acc: 0.8749790191650391\n",
      "step: 41010 , time : 0.0\n",
      "train: loss: 527811.6875 acc: 0.9536934494972229  val: loss: 788953.9375 acc: 0.8108187317848206\n",
      "step: 41015 , time : 0.0\n",
      "train: loss: 390843.0 acc: 0.9569045901298523  val: loss: 598627.0 acc: 0.8687418699264526\n",
      "step: 41020 , time : 0.0010004043579101562\n",
      "train: loss: 497206.5 acc: 0.9567393064498901  val: loss: 565431.75 acc: 0.8802531957626343\n",
      "step: 41025 , time : 0.0\n",
      "train: loss: 1199498.5 acc: 0.9559528827667236  val: loss: 916850.375 acc: 0.8908076882362366\n",
      "step: 41030 , time : 0.0\n",
      "train: loss: 811202.0 acc: 0.9680806398391724  val: loss: 1954646.75 acc: 0.4150717854499817\n",
      "step: 41035 , time : 0.0\n",
      "train: loss: 1103861.625 acc: 0.9580636620521545  val: loss: 994677.4375 acc: 0.7777484655380249\n",
      "step: 41040 , time : 0.0\n",
      "train: loss: 367609.3125 acc: 0.9679656624794006  val: loss: 969363.625 acc: 0.7284034490585327\n",
      "step: 41045 , time : 0.0010008811950683594\n",
      "train: loss: 970670.0 acc: 0.9363268613815308  val: loss: 343762.5625 acc: 0.9061082005500793\n",
      "step: 41050 , time : 0.0\n",
      "train: loss: 395017.53125 acc: 0.9498862624168396  val: loss: 776649.25 acc: 0.7460191249847412\n",
      "step: 41055 , time : 0.001001119613647461\n",
      "train: loss: 289113.0 acc: 0.9395167231559753  val: loss: 338742.5 acc: 0.919633686542511\n",
      "step: 41060 , time : 0.0010004043579101562\n",
      "train: loss: 2273521.5 acc: 0.21065360307693481  val: loss: 663368.9375 acc: 0.8529032468795776\n",
      "step: 41065 , time : 0.0\n",
      "train: loss: 747448.6875 acc: 0.5190919637680054  val: loss: 1617751.125 acc: 0.8026723861694336\n",
      "step: 41070 , time : 0.0\n",
      "train: loss: 921614.6875 acc: 0.7799370884895325  val: loss: 2062104.625 acc: 0.5317704081535339\n",
      "step: 41075 , time : 0.0010008811950683594\n",
      "train: loss: 1158085.125 acc: 0.4816097021102905  val: loss: 490840.75 acc: 0.8473573923110962\n",
      "step: 41080 , time : 0.0010006427764892578\n",
      "train: loss: 928477.5625 acc: 0.6935064196586609  val: loss: 525150.9375 acc: 0.9167604446411133\n",
      "step: 41085 , time : 0.0\n",
      "train: loss: 1908432.875 acc: 0.5264865756034851  val: loss: 1379372.625 acc: 0.6558188796043396\n",
      "step: 41090 , time : 0.0010004043579101562\n",
      "train: loss: 1635409.25 acc: 0.42202913761138916  val: loss: 1586032.5 acc: 0.48685288429260254\n",
      "step: 41095 , time : 0.0010006427764892578\n",
      "train: loss: 860050.5625 acc: 0.5623058080673218  val: loss: 2179182.75 acc: 0.5471950173377991\n",
      "step: 41100 , time : 0.0\n",
      "train: loss: 439364.8125 acc: 0.5410083532333374  val: loss: 1515280.875 acc: 0.5801635980606079\n",
      "step: 41105 , time : 0.0\n",
      "train: loss: 233854.9375 acc: 0.8427076935768127  val: loss: 908218.4375 acc: 0.9068587422370911\n",
      "step: 41110 , time : 0.0\n",
      "train: loss: 182899.15625 acc: 0.8319789171218872  val: loss: 757437.9375 acc: 0.7983047366142273\n",
      "step: 41115 , time : 0.0010004043579101562\n",
      "train: loss: 137152.46875 acc: 0.9121115803718567  val: loss: 746398.5625 acc: 0.7160618305206299\n",
      "step: 41120 , time : 0.0\n",
      "train: loss: 129976.359375 acc: 0.8972595930099487  val: loss: 1285231.375 acc: 0.7015091776847839\n",
      "step: 41125 , time : 0.0010006427764892578\n",
      "train: loss: 95278.2734375 acc: 0.920985221862793  val: loss: 1055179.25 acc: 0.7503646612167358\n",
      "step: 41130 , time : 0.0\n",
      "train: loss: 221100.78125 acc: 0.849783718585968  val: loss: 3112425.5 acc: 0.4882124066352844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 41135 , time : 0.0\n",
      "train: loss: 87773.0546875 acc: 0.9156449437141418  val: loss: 475207.5625 acc: 0.8393837213516235\n",
      "step: 41140 , time : 0.0\n",
      "train: loss: 157479.875 acc: 0.8712646961212158  val: loss: 194330.78125 acc: 0.8900099992752075\n",
      "step: 41145 , time : 0.0\n",
      "train: loss: 754363.9375 acc: 0.7101712822914124  val: loss: 868540.375 acc: 0.7691867351531982\n",
      "step: 41150 , time : 0.001001119613647461\n",
      "train: loss: 590949.625 acc: 0.7041940689086914  val: loss: 1599521.875 acc: 0.7268063426017761\n",
      "step: 41155 , time : 0.001001119613647461\n",
      "train: loss: 334558.625 acc: 0.616623044013977  val: loss: 3163513.75 acc: 0.6660689115524292\n",
      "step: 41160 , time : 0.0010008811950683594\n",
      "train: loss: 750778.1875 acc: 0.5431679487228394  val: loss: 2774922.5 acc: 0.7151751518249512\n",
      "step: 41165 , time : 0.001001119613647461\n",
      "train: loss: 470774.46875 acc: 0.7359685301780701  val: loss: 2055678.125 acc: 0.6807074546813965\n",
      "step: 41170 , time : 0.0\n",
      "train: loss: 1191054.375 acc: 0.7020686864852905  val: loss: 897373.8125 acc: 0.836693286895752\n",
      "step: 41175 , time : 0.0\n",
      "train: loss: 1018473.25 acc: 0.8035640120506287  val: loss: 811381.0625 acc: 0.8839044570922852\n",
      "step: 41180 , time : 0.001001119613647461\n",
      "train: loss: 1071323.5 acc: 0.8876659274101257  val: loss: 1013490.0625 acc: 0.830876350402832\n",
      "step: 41185 , time : 0.0010004043579101562\n",
      "train: loss: 804067.9375 acc: 0.9148133993148804  val: loss: 1429655.875 acc: 0.8867672681808472\n",
      "step: 41190 , time : 0.0\n",
      "train: loss: 422673.03125 acc: 0.9631171226501465  val: loss: 1048252.1875 acc: 0.6281454563140869\n",
      "step: 41195 , time : 0.0\n",
      "train: loss: 428809.0625 acc: 0.9338033199310303  val: loss: 736027.75 acc: 0.9213393330574036\n",
      "step: 41200 , time : 0.0\n",
      "train: loss: 595688.375 acc: 0.9200421571731567  val: loss: 719758.3125 acc: 0.9270710349082947\n",
      "step: 41205 , time : 0.001001119613647461\n",
      "train: loss: 235556.515625 acc: 0.9817970991134644  val: loss: 3462547.0 acc: 0.6959354877471924\n",
      "step: 41210 , time : 0.0\n",
      "train: loss: 682986.3125 acc: 0.9347175359725952  val: loss: 1658177.5 acc: 0.8096932768821716\n",
      "step: 41215 , time : 0.0\n",
      "train: loss: 236663.21875 acc: 0.9807270765304565  val: loss: 614388.75 acc: 0.8848196864128113\n",
      "step: 41220 , time : 0.0\n",
      "train: loss: 273353.65625 acc: 0.9359996318817139  val: loss: 483924.5625 acc: 0.9236807227134705\n",
      "step: 41225 , time : 0.0010006427764892578\n",
      "train: loss: 295074.65625 acc: 0.9337561130523682  val: loss: 803580.4375 acc: 0.7893352508544922\n",
      "step: 41230 , time : 0.0\n",
      "train: loss: 148866.828125 acc: 0.95717453956604  val: loss: 1277868.375 acc: 0.6909112334251404\n",
      "step: 41235 , time : 0.0010008811950683594\n",
      "train: loss: 3383.895751953125 acc: 0.9888538718223572  val: loss: 838504.3125 acc: 0.8293550610542297\n",
      "step: 41240 , time : 0.0010008811950683594\n",
      "train: loss: 19290.953125 acc: 0.9486159086227417  val: loss: 219519.515625 acc: 0.922488272190094\n",
      "step: 41245 , time : 0.0\n",
      "train: loss: 12857.970703125 acc: 0.9759061932563782  val: loss: 1077590.875 acc: 0.8288838863372803\n",
      "step: 41250 , time : 0.001001119613647461\n",
      "train: loss: 106602.421875 acc: 0.9561604261398315  val: loss: 647253.1875 acc: 0.888733446598053\n",
      "step: 41255 , time : 0.0\n",
      "train: loss: 17607.072265625 acc: 0.9880071878433228  val: loss: 605623.375 acc: 0.8356082439422607\n",
      "step: 41260 , time : 0.0\n",
      "train: loss: 4966.53173828125 acc: 0.9898396730422974  val: loss: 1886666.75 acc: 0.4239068031311035\n",
      "step: 41265 , time : 0.0010008811950683594\n",
      "train: loss: 1735.7327880859375 acc: 0.9959200024604797  val: loss: 617465.875 acc: 0.9238241910934448\n",
      "step: 41270 , time : 0.0010004043579101562\n",
      "train: loss: 8923.544921875 acc: 0.972542405128479  val: loss: 2395933.0 acc: 0.698055624961853\n",
      "step: 41275 , time : 0.0010006427764892578\n",
      "train: loss: 31432.802734375 acc: 0.9504027962684631  val: loss: 630542.125 acc: 0.7242927551269531\n",
      "step: 41280 , time : 0.0010006427764892578\n",
      "train: loss: 69474.734375 acc: 0.9659538865089417  val: loss: 1904175.625 acc: 0.8079304695129395\n",
      "step: 41285 , time : 0.0\n",
      "train: loss: 23433.37890625 acc: 0.9849178791046143  val: loss: 1519476.625 acc: 0.7674570083618164\n",
      "step: 41290 , time : 0.0\n",
      "train: loss: 58432.5390625 acc: 0.9755269885063171  val: loss: 2807266.25 acc: -0.3974236249923706\n",
      "step: 41295 , time : 0.0010008811950683594\n",
      "train: loss: 20106.4140625 acc: 0.975486159324646  val: loss: 3069641.25 acc: 0.027761757373809814\n",
      "step: 41300 , time : 0.0010008811950683594\n",
      "train: loss: 7837.732421875 acc: 0.9749999046325684  val: loss: 1023598.875 acc: 0.8534196615219116\n",
      "step: 41305 , time : 0.0010004043579101562\n",
      "train: loss: 9432.3408203125 acc: 0.9893731474876404  val: loss: 2088480.875 acc: 0.3207164406776428\n",
      "step: 41310 , time : 0.0010006427764892578\n",
      "train: loss: 17724.005859375 acc: 0.9916418194770813  val: loss: 1973122.375 acc: 0.8153286576271057\n",
      "step: 41315 , time : 0.0\n",
      "train: loss: 55791.3203125 acc: 0.9886852502822876  val: loss: 1930340.25 acc: 0.8303937911987305\n",
      "step: 41320 , time : 0.0\n",
      "train: loss: 59470.0078125 acc: 0.9866642355918884  val: loss: 2953853.75 acc: 0.36281245946884155\n",
      "step: 41325 , time : 0.0\n",
      "train: loss: 20837.51171875 acc: 0.9891647100448608  val: loss: 843443.625 acc: 0.9093514084815979\n",
      "step: 41330 , time : 0.0010008811950683594\n",
      "train: loss: 183396.25 acc: 0.9018303155899048  val: loss: 1313775.375 acc: -0.5084495544433594\n",
      "step: 41335 , time : 0.0\n",
      "train: loss: 117830.8046875 acc: 0.9548395872116089  val: loss: 254701.78125 acc: 0.9301019906997681\n",
      "step: 41340 , time : 0.0010006427764892578\n",
      "train: loss: 65906.2578125 acc: 0.9475049376487732  val: loss: 1947394.75 acc: 0.7515383362770081\n",
      "step: 41345 , time : 0.0\n",
      "train: loss: 670700.5625 acc: 0.8536986112594604  val: loss: 2729896.25 acc: -0.23683059215545654\n",
      "step: 41350 , time : 0.0\n",
      "train: loss: 164005.53125 acc: 0.9782253503799438  val: loss: 995377.0 acc: 0.839840829372406\n",
      "step: 41355 , time : 0.0\n",
      "train: loss: 109841.9375 acc: 0.990582287311554  val: loss: 1241506.75 acc: 0.7182385921478271\n",
      "step: 41360 , time : 0.0010006427764892578\n",
      "train: loss: 59660.90234375 acc: 0.9919645190238953  val: loss: 2112663.25 acc: -0.6255463361740112\n",
      "step: 41365 , time : 0.0010004043579101562\n",
      "train: loss: 99154.21875 acc: 0.9866018295288086  val: loss: 1039283.8125 acc: 0.7527493834495544\n",
      "step: 41370 , time : 0.0\n",
      "train: loss: 355360.09375 acc: 0.9817311763763428  val: loss: 820666.0 acc: 0.8359243273735046\n",
      "step: 41375 , time : 0.0010008811950683594\n",
      "train: loss: 848594.875 acc: 0.9619097709655762  val: loss: 1176640.375 acc: 0.808197557926178\n",
      "step: 41380 , time : 0.0\n",
      "train: loss: 194505.5 acc: 0.9798232913017273  val: loss: 2409414.25 acc: 0.46765953302383423\n",
      "step: 41385 , time : 0.0\n",
      "train: loss: 260249.265625 acc: 0.9430270791053772  val: loss: 2194894.75 acc: 0.37528061866760254\n",
      "step: 41390 , time : 0.0\n",
      "train: loss: 393959.0625 acc: 0.9792535901069641  val: loss: 550012.375 acc: 0.8112256526947021\n",
      "step: 41395 , time : 0.0\n",
      "train: loss: 1172907.375 acc: 0.9671573042869568  val: loss: 424148.625 acc: 0.9466718435287476\n",
      "step: 41400 , time : 0.0\n",
      "train: loss: 1081847.0 acc: 0.9692057371139526  val: loss: 264735.65625 acc: 0.8551865816116333\n",
      "step: 41405 , time : 0.0\n",
      "train: loss: 699665.5 acc: 0.8916712999343872  val: loss: 686776.875 acc: 0.43376094102859497\n",
      "step: 41410 , time : 0.001001119613647461\n",
      "train: loss: 1376645.75 acc: 0.8978573679924011  val: loss: 407009.65625 acc: 0.9028275012969971\n",
      "step: 41415 , time : 0.0\n",
      "train: loss: 372018.8125 acc: 0.9308549761772156  val: loss: 578183.5625 acc: 0.9502589106559753\n",
      "step: 41420 , time : 0.0010006427764892578\n",
      "train: loss: 423276.125 acc: 0.9176483750343323  val: loss: 294840.5 acc: 0.8867176175117493\n",
      "step: 41425 , time : 0.0010006427764892578\n",
      "train: loss: 1211081.5 acc: 0.6898103952407837  val: loss: 564715.625 acc: 0.9110609889030457\n",
      "step: 41430 , time : 0.0\n",
      "train: loss: 1648181.25 acc: 0.5980944633483887  val: loss: 1386936.375 acc: 0.7570421099662781\n",
      "step: 41435 , time : 0.0\n",
      "train: loss: 552675.625 acc: 0.7465120553970337  val: loss: 814578.625 acc: 0.7061746716499329\n",
      "step: 41440 , time : 0.0\n",
      "train: loss: 398103.125 acc: 0.7992738485336304  val: loss: 440558.53125 acc: 0.9211838245391846\n",
      "step: 41445 , time : 0.0\n",
      "train: loss: 861182.9375 acc: 0.8016352653503418  val: loss: 223030.421875 acc: 0.8979605436325073\n",
      "step: 41450 , time : 0.0\n",
      "train: loss: 1749480.5 acc: -0.30623626708984375  val: loss: 1436586.125 acc: 0.623633861541748\n",
      "step: 41455 , time : 0.0\n",
      "train: loss: 1358765.75 acc: 0.26829099655151367  val: loss: 1361597.125 acc: 0.6137851476669312\n",
      "step: 41460 , time : 0.0\n",
      "train: loss: 1383277.5 acc: 0.5208255052566528  val: loss: 1876285.625 acc: 0.7291686534881592\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 41465 , time : 0.0010004043579101562\n",
      "train: loss: 516098.8125 acc: 0.5328600406646729  val: loss: 2436809.25 acc: 0.7040262222290039\n",
      "step: 41470 , time : 0.0010006427764892578\n",
      "train: loss: 49922.078125 acc: 0.9559292793273926  val: loss: 4360972.5 acc: 0.6890294551849365\n",
      "step: 41475 , time : 0.0010006427764892578\n",
      "train: loss: 146009.359375 acc: 0.8999086022377014  val: loss: 612524.0 acc: 0.7630766034126282\n",
      "step: 41480 , time : 0.0010008811950683594\n",
      "train: loss: 136891.8125 acc: 0.9142675399780273  val: loss: 1737577.5 acc: 0.6585193872451782\n",
      "step: 41485 , time : 0.0010008811950683594\n",
      "train: loss: 244425.921875 acc: 0.822928786277771  val: loss: 4222488.5 acc: 0.6413758397102356\n",
      "step: 41490 , time : 0.0\n",
      "train: loss: 205349.796875 acc: 0.8725860714912415  val: loss: 2181930.75 acc: 0.7307841777801514\n",
      "step: 41495 , time : 0.0\n",
      "train: loss: 193508.46875 acc: 0.7736655473709106  val: loss: 3388204.75 acc: 0.648650050163269\n",
      "step: 41500 , time : 0.0\n",
      "train: loss: 63164.15625 acc: 0.9330117106437683  val: loss: 1177475.625 acc: 0.6660875082015991\n",
      "step: 41505 , time : 0.0010006427764892578\n",
      "train: loss: 40834.5859375 acc: 0.9460651874542236  val: loss: 3342391.5 acc: 0.6224960684776306\n",
      "step: 41510 , time : 0.0010013580322265625\n",
      "train: loss: 251152.734375 acc: 0.8752415180206299  val: loss: 4000503.75 acc: 0.6156567931175232\n",
      "step: 41515 , time : 0.0010006427764892578\n",
      "train: loss: 263190.53125 acc: 0.8304870128631592  val: loss: 1093655.0 acc: 0.681154727935791\n",
      "step: 41520 , time : 0.0\n",
      "train: loss: 1187672.75 acc: 0.6905711889266968  val: loss: 4618881.5 acc: 0.5185699462890625\n",
      "step: 41525 , time : 0.0\n",
      "train: loss: 285297.6875 acc: 0.7806289196014404  val: loss: 1333043.625 acc: 0.7275421619415283\n",
      "step: 41530 , time : 0.0010006427764892578\n",
      "train: loss: 126567.6796875 acc: 0.9034240245819092  val: loss: 1219782.0 acc: 0.7011794447898865\n",
      "step: 41535 , time : 0.0010008811950683594\n",
      "train: loss: 414675.40625 acc: 0.76624596118927  val: loss: 1015017.6875 acc: 0.727466344833374\n",
      "step: 41540 , time : 0.0\n",
      "train: loss: 1999224.625 acc: 0.7010130286216736  val: loss: 977398.6875 acc: 0.7512623071670532\n",
      "step: 41545 , time : 0.0\n",
      "train: loss: 1862995.0 acc: 0.8224887847900391  val: loss: 514032.84375 acc: 0.8215019702911377\n",
      "step: 41550 , time : 0.0\n",
      "train: loss: 679316.25 acc: 0.9507391452789307  val: loss: 691263.3125 acc: 0.9027562141418457\n",
      "step: 41555 , time : 0.014623880386352539\n",
      "train: loss: 332657.75 acc: 0.9616634249687195  val: loss: 1087915.125 acc: 0.8687353134155273\n",
      "step: 41560 , time : 0.0010004043579101562\n",
      "train: loss: 288530.0625 acc: 0.9629092812538147  val: loss: 1289685.25 acc: 0.6878895163536072\n",
      "step: 41565 , time : 0.0\n",
      "train: loss: 154382.515625 acc: 0.9805334806442261  val: loss: 826923.625 acc: 0.8911314606666565\n",
      "step: 41570 , time : 0.0\n",
      "train: loss: 183042.09375 acc: 0.9834795594215393  val: loss: 1229695.25 acc: 0.7885043025016785\n",
      "step: 41575 , time : 0.0010006427764892578\n",
      "train: loss: 516236.03125 acc: 0.9607803225517273  val: loss: 640276.3125 acc: 0.9104220271110535\n",
      "step: 41580 , time : 0.0010004043579101562\n",
      "train: loss: 94150.6875 acc: 0.9918478727340698  val: loss: 2255771.25 acc: 0.24050092697143555\n",
      "step: 41585 , time : 0.0\n",
      "train: loss: 75268.625 acc: 0.9872921705245972  val: loss: 1077466.25 acc: 0.33234989643096924\n",
      "step: 41590 , time : 0.0\n",
      "train: loss: 26354.39453125 acc: 0.9924741983413696  val: loss: 2586226.25 acc: 0.7714525461196899\n",
      "step: 41595 , time : 0.0010004043579101562\n",
      "train: loss: 36643.484375 acc: 0.9724823236465454  val: loss: 1331595.625 acc: 0.31284552812576294\n",
      "step: 41600 , time : 0.0010008811950683594\n",
      "train: loss: 8103.43408203125 acc: 0.9946923851966858  val: loss: 697344.0625 acc: 0.9244627952575684\n",
      "step: 41605 , time : 0.0010008811950683594\n",
      "train: loss: 14429.607421875 acc: 0.9722040891647339  val: loss: 1706817.0 acc: 0.42586302757263184\n",
      "step: 41610 , time : 0.0\n",
      "train: loss: 13949.837890625 acc: 0.9927181005477905  val: loss: 4983707.0 acc: 0.17348414659500122\n",
      "step: 41615 , time : 0.0\n",
      "train: loss: 9794.228515625 acc: 0.9626756310462952  val: loss: 3447395.0 acc: 0.37653106451034546\n",
      "step: 41620 , time : 0.015624523162841797\n",
      "train: loss: 1808.1461181640625 acc: 0.990389883518219  val: loss: 1347806.5 acc: 0.5081725716590881\n",
      "step: 41625 , time : 0.0\n",
      "train: loss: 11043.763671875 acc: 0.9742683172225952  val: loss: 1313873.625 acc: 0.7614572644233704\n",
      "step: 41630 , time : 0.0\n",
      "train: loss: 6195.2236328125 acc: 0.9859222769737244  val: loss: 933525.8125 acc: 0.7091272473335266\n",
      "step: 41635 , time : 0.0\n",
      "train: loss: 14906.775390625 acc: 0.9744859337806702  val: loss: 2466200.25 acc: 0.5481414794921875\n",
      "step: 41640 , time : 0.0\n",
      "train: loss: 14739.837890625 acc: 0.9877352714538574  val: loss: 1373451.375 acc: 0.8374555110931396\n",
      "step: 41645 , time : 0.0\n",
      "train: loss: 6501.79931640625 acc: 0.9917100667953491  val: loss: 2448029.5 acc: 0.3358972668647766\n",
      "step: 41650 , time : 0.0\n",
      "train: loss: 106119.4609375 acc: 0.936385452747345  val: loss: 651382.0 acc: 0.8855412602424622\n",
      "step: 41655 , time : 0.0\n",
      "train: loss: 27290.880859375 acc: 0.9859161972999573  val: loss: 2881734.25 acc: 0.4264805316925049\n",
      "step: 41660 , time : 0.0\n",
      "train: loss: 6577.33056640625 acc: 0.992813766002655  val: loss: 952882.125 acc: 0.6153451204299927\n",
      "step: 41665 , time : 0.0\n",
      "train: loss: 23390.021484375 acc: 0.9478866457939148  val: loss: 165824.53125 acc: 0.9690545201301575\n",
      "step: 41670 , time : 0.0\n",
      "train: loss: 9400.43359375 acc: 0.986691415309906  val: loss: 2026246.625 acc: 0.5631694793701172\n",
      "step: 41675 , time : 0.0\n",
      "train: loss: 28176.466796875 acc: 0.9871108531951904  val: loss: 502200.1875 acc: 0.8482162952423096\n",
      "step: 41680 , time : 0.0\n",
      "train: loss: 26103.94140625 acc: 0.9954211115837097  val: loss: 290830.4375 acc: 0.9248695373535156\n",
      "step: 41685 , time : 0.0\n",
      "train: loss: 27264.69140625 acc: 0.9942718744277954  val: loss: 2516147.75 acc: 0.6184049844741821\n",
      "step: 41690 , time : 0.0010004043579101562\n",
      "train: loss: 27079.9296875 acc: 0.9877387285232544  val: loss: 381481.90625 acc: 0.959201455116272\n",
      "step: 41695 , time : 0.0010004043579101562\n",
      "train: loss: 40515.7890625 acc: 0.9887052178382874  val: loss: 1352437.125 acc: 0.8186588287353516\n",
      "step: 41700 , time : 0.0\n",
      "train: loss: 154858.453125 acc: 0.9645606875419617  val: loss: 629992.375 acc: 0.8843239545822144\n",
      "step: 41705 , time : 0.001001119613647461\n",
      "train: loss: 98375.4375 acc: 0.9128091931343079  val: loss: 92309.1640625 acc: 0.9668169021606445\n",
      "step: 41710 , time : 0.0\n",
      "train: loss: 133693.875 acc: 0.9549034833908081  val: loss: 616222.875 acc: 0.3244439959526062\n",
      "step: 41715 , time : 0.0\n",
      "train: loss: 127348.390625 acc: 0.9782653450965881  val: loss: 1185274.875 acc: 0.7427202463150024\n",
      "step: 41720 , time : 0.0010004043579101562\n",
      "train: loss: 604786.0 acc: 0.9354137182235718  val: loss: 1108314.5 acc: 0.7045535445213318\n",
      "step: 41725 , time : 0.001001119613647461\n",
      "train: loss: 123472.7578125 acc: 0.9872830510139465  val: loss: 2488144.0 acc: 0.6706396341323853\n",
      "step: 41730 , time : 0.0010004043579101562\n",
      "train: loss: 80652.6328125 acc: 0.9826416373252869  val: loss: 900730.75 acc: 0.7367473840713501\n",
      "step: 41735 , time : 0.0010004043579101562\n",
      "train: loss: 238635.359375 acc: 0.9653205275535583  val: loss: 457984.25 acc: 0.8622144460678101\n",
      "step: 41740 , time : 0.0\n",
      "train: loss: 243055.515625 acc: 0.966704249382019  val: loss: 630660.625 acc: 0.36482948064804077\n",
      "step: 41745 , time : 0.0\n",
      "train: loss: 411586.78125 acc: 0.9711111783981323  val: loss: 1013691.75 acc: 0.6668855547904968\n",
      "step: 41750 , time : 0.0\n",
      "train: loss: 258259.875 acc: 0.959938645362854  val: loss: 597957.625 acc: 0.8907610177993774\n",
      "step: 41755 , time : 0.0\n",
      "train: loss: 713779.6875 acc: 0.9751094579696655  val: loss: 343566.6875 acc: 0.8388562798500061\n",
      "step: 41760 , time : 0.0\n",
      "train: loss: 2216219.25 acc: 0.9215993881225586  val: loss: 569906.0 acc: 0.8762804269790649\n",
      "step: 41765 , time : 0.0\n",
      "train: loss: 1064133.125 acc: 0.9613059759140015  val: loss: 264651.5625 acc: 0.9176453351974487\n",
      "step: 41770 , time : 0.0\n",
      "train: loss: 285680.1875 acc: 0.9818820357322693  val: loss: 193023.6875 acc: 0.957779049873352\n",
      "step: 41775 , time : 0.0\n",
      "train: loss: 521424.90625 acc: 0.9445580244064331  val: loss: 75733.3984375 acc: 0.9742010235786438\n",
      "step: 41780 , time : 0.0010004043579101562\n",
      "train: loss: 191215.640625 acc: 0.9756972193717957  val: loss: 165188.390625 acc: 0.9540515542030334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 41785 , time : 0.0\n",
      "train: loss: 136202.046875 acc: 0.9484226107597351  val: loss: 847618.4375 acc: 0.8410235643386841\n",
      "step: 41790 , time : 0.0\n",
      "train: loss: 3429661.0 acc: 0.49536746740341187  val: loss: 572430.75 acc: 0.9550679922103882\n",
      "step: 41795 , time : 0.0\n",
      "train: loss: 1474700.875 acc: 0.43934279680252075  val: loss: 1538389.75 acc: 0.7250363826751709\n",
      "step: 41800 , time : 0.0\n",
      "train: loss: 1026126.25 acc: 0.5977096557617188  val: loss: 1161417.625 acc: 0.8750720620155334\n",
      "step: 41805 , time : 0.0009999275207519531\n",
      "train: loss: 938299.9375 acc: 0.6004397869110107  val: loss: 494637.90625 acc: 0.9234164953231812\n",
      "step: 41810 , time : 0.0\n",
      "train: loss: 415343.5625 acc: 0.8403892517089844  val: loss: 750022.6875 acc: 0.9176491498947144\n",
      "step: 41815 , time : 0.001001119613647461\n",
      "train: loss: 1569884.875 acc: 0.35396063327789307  val: loss: 813482.9375 acc: 0.721371054649353\n",
      "step: 41820 , time : 0.0\n",
      "train: loss: 1427859.125 acc: 0.44567763805389404  val: loss: 2686795.0 acc: 0.614038348197937\n",
      "step: 41825 , time : 0.0\n",
      "train: loss: 531791.0625 acc: 0.5710695385932922  val: loss: 1957006.375 acc: 0.778144359588623\n",
      "step: 41830 , time : 0.0\n",
      "train: loss: 780212.625 acc: 0.645021915435791  val: loss: 1339975.25 acc: 0.7124972343444824\n",
      "step: 41835 , time : 0.0010006427764892578\n",
      "train: loss: 180692.90625 acc: 0.8472558259963989  val: loss: 1553401.75 acc: 0.864873468875885\n",
      "step: 41840 , time : 0.0\n",
      "train: loss: 197935.25 acc: 0.8459815979003906  val: loss: 884805.625 acc: 0.8621916174888611\n",
      "step: 41845 , time : 0.0\n",
      "train: loss: 111730.40625 acc: 0.9136850237846375  val: loss: 865203.25 acc: 0.76405268907547\n",
      "step: 41850 , time : 0.0010006427764892578\n",
      "train: loss: 207966.53125 acc: 0.860538125038147  val: loss: 1802015.625 acc: 0.7621449828147888\n",
      "step: 41855 , time : 0.0\n",
      "train: loss: 245795.03125 acc: 0.8547950983047485  val: loss: 2331684.5 acc: 0.7328740358352661\n",
      "step: 41860 , time : 0.0010008811950683594\n",
      "train: loss: 100132.875 acc: 0.9216232895851135  val: loss: 667741.8125 acc: 0.7490791082382202\n",
      "step: 41865 , time : 0.0\n",
      "train: loss: 70433.0078125 acc: 0.9056180119514465  val: loss: 803958.4375 acc: 0.7525140047073364\n",
      "step: 41870 , time : 0.0\n",
      "train: loss: 338281.5625 acc: 0.7938458323478699  val: loss: 854881.375 acc: 0.8475096225738525\n",
      "step: 41875 , time : 0.0\n",
      "train: loss: 293276.78125 acc: 0.630113959312439  val: loss: 345879.78125 acc: 0.922339141368866\n",
      "step: 41880 , time : 0.0010008811950683594\n",
      "train: loss: 541496.9375 acc: 0.5550715923309326  val: loss: 3188931.75 acc: 0.6922394037246704\n",
      "step: 41885 , time : 0.0\n",
      "train: loss: 582668.625 acc: 0.5044649243354797  val: loss: 2194994.0 acc: 0.7181097269058228\n",
      "step: 41890 , time : 0.0\n",
      "train: loss: 802857.6875 acc: 0.5329344272613525  val: loss: 821317.0 acc: 0.6770545244216919\n",
      "step: 41895 , time : 0.0\n",
      "train: loss: 449859.25 acc: 0.6637152433395386  val: loss: 4089034.75 acc: 0.709253191947937\n",
      "step: 41900 , time : 0.0\n",
      "train: loss: 1140457.375 acc: 0.7137187719345093  val: loss: 887635.6875 acc: 0.7095357775688171\n",
      "step: 41905 , time : 0.0\n",
      "train: loss: 1338161.625 acc: 0.7676939964294434  val: loss: 1498868.375 acc: 0.7777355909347534\n",
      "step: 41910 , time : 0.0\n",
      "train: loss: 1273238.0 acc: 0.8700927495956421  val: loss: 1406782.0 acc: 0.8206098675727844\n",
      "step: 41915 , time : 0.0\n",
      "train: loss: 786890.5 acc: 0.9253290891647339  val: loss: 1257937.375 acc: 0.7984486818313599\n",
      "step: 41920 , time : 0.0\n",
      "train: loss: 512254.8125 acc: 0.9133946895599365  val: loss: 1861050.625 acc: 0.7114875316619873\n",
      "step: 41925 , time : 0.0\n",
      "train: loss: 678545.25 acc: 0.9077902436256409  val: loss: 1564162.875 acc: -0.3197329044342041\n",
      "step: 41930 , time : 0.0010001659393310547\n",
      "train: loss: 300346.21875 acc: 0.9542651772499084  val: loss: 1151706.25 acc: 0.7284805774688721\n",
      "step: 41935 , time : 0.0010504722595214844\n",
      "train: loss: 394045.5 acc: 0.9624319672584534  val: loss: 3313771.75 acc: -0.2605931758880615\n",
      "step: 41940 , time : 0.0\n",
      "train: loss: 216244.53125 acc: 0.9844964146614075  val: loss: 692575.375 acc: 0.702937126159668\n",
      "step: 41945 , time : 0.0\n",
      "train: loss: 252668.90625 acc: 0.9779270887374878  val: loss: 1808213.625 acc: 0.5376453399658203\n",
      "step: 41950 , time : 0.0\n",
      "train: loss: 250355.46875 acc: 0.9711647033691406  val: loss: 1199924.625 acc: 0.8184239864349365\n",
      "step: 41955 , time : 0.0\n",
      "train: loss: 80826.4609375 acc: 0.9741554856300354  val: loss: 866361.0 acc: 0.886741042137146\n",
      "step: 41960 , time : 0.0\n",
      "train: loss: 95604.0703125 acc: 0.893622100353241  val: loss: 438741.25 acc: 0.8962016701698303\n",
      "step: 41965 , time : 0.0\n",
      "train: loss: 65402.14453125 acc: 0.9455832242965698  val: loss: 142975.125 acc: 0.9636490941047668\n",
      "step: 41970 , time : 0.0\n",
      "train: loss: 22131.365234375 acc: 0.9854434132575989  val: loss: 322938.1875 acc: 0.9054372310638428\n",
      "step: 41975 , time : 0.0\n",
      "train: loss: 71250.390625 acc: 0.9484166502952576  val: loss: 389901.9375 acc: 0.8277040719985962\n",
      "step: 41980 , time : 0.0\n",
      "train: loss: 62613.55078125 acc: 0.9507579207420349  val: loss: 853866.125 acc: 0.3691754937171936\n",
      "step: 41985 , time : 0.0\n",
      "train: loss: 15190.90234375 acc: 0.9470346570014954  val: loss: 1139191.625 acc: 0.6962205171585083\n",
      "step: 41990 , time : 0.0\n",
      "train: loss: 4178.4013671875 acc: 0.9928175806999207  val: loss: 253327.625 acc: 0.9386444687843323\n",
      "step: 41995 , time : 0.0\n",
      "train: loss: 4397.53466796875 acc: 0.9855985045433044  val: loss: 771188.5 acc: 0.8789127469062805\n",
      "step: 42000 , time : 0.0\n",
      "train: loss: 7849.77392578125 acc: 0.9844975471496582  val: loss: 122329.0234375 acc: 0.9729070663452148\n",
      "step: 42005 , time : 0.0\n",
      "train: loss: 17966.45703125 acc: 0.974457859992981  val: loss: 482126.9375 acc: 0.8399239182472229\n",
      "step: 42010 , time : 0.0\n",
      "train: loss: 33532.87109375 acc: 0.9757592082023621  val: loss: 1339890.25 acc: 0.6946725845336914\n",
      "step: 42015 , time : 0.0010008811950683594\n",
      "train: loss: 16192.3076171875 acc: 0.9803326725959778  val: loss: 552963.375 acc: 0.7891554236412048\n",
      "step: 42020 , time : 0.0010008811950683594\n",
      "train: loss: 18427.0625 acc: 0.9858336448669434  val: loss: 1052994.625 acc: 0.726279079914093\n",
      "step: 42025 , time : 0.0010006427764892578\n",
      "train: loss: 14777.9931640625 acc: 0.9859327673912048  val: loss: 1287451.5 acc: 0.8029460906982422\n",
      "step: 42030 , time : 0.0\n",
      "train: loss: 12834.8759765625 acc: 0.9783229231834412  val: loss: 128906.3359375 acc: 0.9183542132377625\n",
      "step: 42035 , time : 0.0\n",
      "train: loss: 19894.513671875 acc: 0.9759241938591003  val: loss: 699873.875 acc: 0.7593532800674438\n",
      "step: 42040 , time : 0.0\n",
      "train: loss: 21938.232421875 acc: 0.9868621826171875  val: loss: 1137132.375 acc: 0.774010956287384\n",
      "step: 42045 , time : 0.0\n",
      "train: loss: 69849.21875 acc: 0.9832364916801453  val: loss: 296544.0 acc: 0.9512667655944824\n",
      "step: 42050 , time : 0.0\n",
      "train: loss: 92385.96875 acc: 0.9797822833061218  val: loss: 328097.6875 acc: 0.8547768592834473\n",
      "step: 42055 , time : 0.0\n",
      "train: loss: 22250.26171875 acc: 0.9872443079948425  val: loss: 169553.828125 acc: 0.9538167119026184\n",
      "step: 42060 , time : 0.0\n",
      "train: loss: 65046.1484375 acc: 0.9811989665031433  val: loss: 683987.75 acc: 0.6884958744049072\n",
      "step: 42065 , time : 0.0\n",
      "train: loss: 63794.171875 acc: 0.9692732095718384  val: loss: 329062.875 acc: 0.9378429055213928\n",
      "step: 42070 , time : 0.0\n",
      "train: loss: 138209.59375 acc: 0.9504982829093933  val: loss: 1130925.875 acc: 0.8184906244277954\n",
      "step: 42075 , time : 0.0\n",
      "train: loss: 70241.671875 acc: 0.974793553352356  val: loss: 316460.03125 acc: 0.9599200487136841\n",
      "step: 42080 , time : 0.0\n",
      "train: loss: 102804.8984375 acc: 0.983058512210846  val: loss: 1003881.6875 acc: 0.674044668674469\n",
      "step: 42085 , time : 0.0\n",
      "train: loss: 66028.4140625 acc: 0.9923450946807861  val: loss: 743777.875 acc: 0.8076849579811096\n",
      "step: 42090 , time : 0.0\n",
      "train: loss: 166007.71875 acc: 0.9798147678375244  val: loss: 259976.59375 acc: 0.934556782245636\n",
      "step: 42095 , time : 0.0\n",
      "train: loss: 334502.8125 acc: 0.9612648487091064  val: loss: 689114.375 acc: 0.8471439480781555\n",
      "step: 42100 , time : 0.0\n",
      "train: loss: 70763.0625 acc: 0.9847802519798279  val: loss: 563793.75 acc: 0.925978422164917\n",
      "step: 42105 , time : 0.0\n",
      "train: loss: 574780.0625 acc: 0.9774909615516663  val: loss: 668691.9375 acc: 0.9174467325210571\n",
      "step: 42110 , time : 0.0\n",
      "train: loss: 289379.53125 acc: 0.9617308974266052  val: loss: 685506.0625 acc: 0.8406280279159546\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 42115 , time : 0.0\n",
      "train: loss: 428219.15625 acc: 0.9491098523139954  val: loss: 613144.125 acc: 0.9094337821006775\n",
      "step: 42120 , time : 0.0010008811950683594\n",
      "train: loss: 750199.25 acc: 0.9637426733970642  val: loss: 2133168.5 acc: 0.8519481420516968\n",
      "step: 42125 , time : 0.0010008811950683594\n",
      "train: loss: 1404446.875 acc: 0.9455722570419312  val: loss: 396851.5 acc: 0.9400312304496765\n",
      "step: 42130 , time : 0.001001119613647461\n",
      "train: loss: 1256812.875 acc: 0.9612967371940613  val: loss: 1202489.0 acc: 0.8639733195304871\n",
      "step: 42135 , time : 0.001001119613647461\n",
      "train: loss: 963940.6875 acc: 0.9465312361717224  val: loss: 710420.5625 acc: 0.9357253313064575\n",
      "step: 42140 , time : 0.0\n",
      "train: loss: 447184.625 acc: 0.9637911319732666  val: loss: 3154632.75 acc: 0.6946674585342407\n",
      "step: 42145 , time : 0.0\n",
      "train: loss: 1219556.375 acc: 0.941192090511322  val: loss: 529039.375 acc: 0.9612414836883545\n",
      "step: 42150 , time : 0.0\n",
      "train: loss: 545439.75 acc: 0.9466681480407715  val: loss: 397388.625 acc: 0.9191325306892395\n",
      "step: 42155 , time : 0.015624761581420898\n",
      "train: loss: 248838.6875 acc: 0.962431788444519  val: loss: 404044.125 acc: 0.9274423122406006\n",
      "step: 42160 , time : 0.0\n",
      "train: loss: 1472512.875 acc: 0.585171103477478  val: loss: 929224.0625 acc: 0.8668258190155029\n",
      "step: 42165 , time : 0.0\n",
      "train: loss: 674974.375 acc: 0.680419385433197  val: loss: 1213073.625 acc: 0.8531234860420227\n",
      "step: 42170 , time : 0.0\n",
      "train: loss: 966870.625 acc: 0.5347713232040405  val: loss: 4447306.5 acc: 0.08265805244445801\n",
      "step: 42175 , time : 0.0\n",
      "train: loss: 366378.25 acc: 0.8401538133621216  val: loss: 710459.4375 acc: 0.8775199055671692\n",
      "step: 42180 , time : 0.0\n",
      "train: loss: 2512779.75 acc: 0.006083667278289795  val: loss: 1084633.375 acc: 0.5006104111671448\n",
      "step: 42185 , time : 0.0\n",
      "train: loss: 2040555.625 acc: 0.4245827794075012  val: loss: 2904521.0 acc: 0.5406169295310974\n",
      "step: 42190 , time : 0.0\n",
      "train: loss: 706009.1875 acc: 0.6937977075576782  val: loss: 1663709.75 acc: 0.491841197013855\n",
      "step: 42195 , time : 0.0\n",
      "train: loss: 737420.75 acc: 0.3700910806655884  val: loss: 1507673.125 acc: 0.7516045570373535\n",
      "step: 42200 , time : 0.0\n",
      "train: loss: 424703.34375 acc: 0.6346180438995361  val: loss: 767688.75 acc: 0.42548418045043945\n",
      "step: 42205 , time : 0.0\n",
      "train: loss: 435471.21875 acc: 0.6841865181922913  val: loss: 1392535.125 acc: 0.7691031694412231\n",
      "step: 42210 , time : 0.0\n",
      "train: loss: 125942.8359375 acc: 0.9140722155570984  val: loss: 812469.3125 acc: 0.7567253112792969\n",
      "step: 42215 , time : 0.0\n",
      "train: loss: 177784.15625 acc: 0.8908755779266357  val: loss: 533688.125 acc: 0.8098869323730469\n",
      "step: 42220 , time : 0.0\n",
      "train: loss: 149896.15625 acc: 0.8960337042808533  val: loss: 752825.1875 acc: 0.47474610805511475\n",
      "step: 42225 , time : 0.0010004043579101562\n",
      "train: loss: 176104.828125 acc: 0.8595094084739685  val: loss: 258554.71875 acc: 0.9097726345062256\n",
      "step: 42230 , time : 0.0010008811950683594\n",
      "train: loss: 298107.46875 acc: 0.8200553059577942  val: loss: 1251201.125 acc: 0.8533679842948914\n",
      "step: 42235 , time : 0.0\n",
      "train: loss: 121871.4453125 acc: 0.8713416457176208  val: loss: 1489743.125 acc: 0.8587446808815002\n",
      "step: 42240 , time : 0.0\n",
      "train: loss: 426743.625 acc: 0.7413206696510315  val: loss: 659969.6875 acc: 0.8725729584693909\n",
      "step: 42245 , time : 0.0\n",
      "train: loss: 436174.96875 acc: 0.7574924826622009  val: loss: 2175466.0 acc: 0.7497447729110718\n",
      "step: 42250 , time : 0.0\n",
      "train: loss: 465461.5625 acc: 0.4032937288284302  val: loss: 1804430.25 acc: 0.7220784425735474\n",
      "step: 42255 , time : 0.0\n",
      "train: loss: 307890.09375 acc: 0.7005040645599365  val: loss: 590157.375 acc: 0.6867644190788269\n",
      "step: 42260 , time : 0.0\n",
      "train: loss: 217802.96875 acc: 0.7227550148963928  val: loss: 769106.8125 acc: 0.7626051902770996\n",
      "step: 42265 , time : 0.0\n",
      "train: loss: 650551.3125 acc: 0.7069958448410034  val: loss: 1138587.625 acc: 0.8319428563117981\n",
      "step: 42270 , time : 0.0\n",
      "train: loss: 1539359.25 acc: 0.7549800276756287  val: loss: 411790.25 acc: 0.7890366911888123\n",
      "step: 42275 , time : 0.001001119613647461\n",
      "train: loss: 1303760.25 acc: 0.8119699358940125  val: loss: 1434190.375 acc: 0.77541583776474\n",
      "step: 42280 , time : 0.0\n",
      "train: loss: 970700.625 acc: 0.8351050615310669  val: loss: 1008793.875 acc: 0.7833064794540405\n",
      "step: 42285 , time : 0.0010006427764892578\n",
      "train: loss: 444427.71875 acc: 0.9530881643295288  val: loss: 928549.625 acc: 0.871639609336853\n",
      "step: 42290 , time : 0.0\n",
      "train: loss: 336500.375 acc: 0.9152460098266602  val: loss: 514539.21875 acc: 0.8600276708602905\n",
      "step: 42295 , time : 0.0\n",
      "train: loss: 155768.078125 acc: 0.9799430966377258  val: loss: 828924.625 acc: 0.851425290107727\n",
      "step: 42300 , time : 0.0\n",
      "train: loss: 221270.015625 acc: 0.981934666633606  val: loss: 812959.75 acc: 0.8978472948074341\n",
      "step: 42305 , time : 0.0\n",
      "train: loss: 273328.4375 acc: 0.9794609546661377  val: loss: 408712.9375 acc: 0.8807554841041565\n",
      "step: 42310 , time : 0.0\n",
      "train: loss: 418365.8125 acc: 0.9578368663787842  val: loss: 2302593.25 acc: 0.7648296356201172\n",
      "step: 42315 , time : 0.0\n",
      "train: loss: 277960.3125 acc: 0.9538185596466064  val: loss: 686486.25 acc: 0.851355254650116\n",
      "step: 42320 , time : 0.0\n",
      "train: loss: 147506.84375 acc: 0.9694647192955017  val: loss: 559198.9375 acc: 0.9005546569824219\n",
      "step: 42325 , time : 0.0\n",
      "train: loss: 31844.06640625 acc: 0.9872560501098633  val: loss: 715496.25 acc: 0.852547287940979\n",
      "step: 42330 , time : 0.0\n",
      "train: loss: 8503.451171875 acc: 0.9835498332977295  val: loss: 281595.0 acc: 0.9333121180534363\n",
      "step: 42335 , time : 0.0\n",
      "train: loss: 3344.77783203125 acc: 0.9830604195594788  val: loss: 219103.125 acc: 0.9326536059379578\n",
      "step: 42340 , time : 0.0010006427764892578\n",
      "train: loss: 31943.232421875 acc: 0.98744136095047  val: loss: 138755.453125 acc: 0.9531517624855042\n",
      "step: 42345 , time : 0.001001119613647461\n",
      "train: loss: 14265.3701171875 acc: 0.9918951392173767  val: loss: 1214898.5 acc: 0.7446528077125549\n",
      "step: 42350 , time : 0.0\n",
      "train: loss: 7285.5546875 acc: 0.9843508005142212  val: loss: 733404.0625 acc: 0.7882922291755676\n",
      "step: 42355 , time : 0.0\n",
      "train: loss: 10728.498046875 acc: 0.9605838060379028  val: loss: 277039.46875 acc: 0.8751448392868042\n",
      "step: 42360 , time : 0.001001119613647461\n",
      "train: loss: 14205.798828125 acc: 0.9683441519737244  val: loss: 555444.125 acc: 0.8257958889007568\n",
      "step: 42365 , time : 0.0\n",
      "train: loss: 7460.953125 acc: 0.99033522605896  val: loss: 479139.75 acc: 0.8426263928413391\n",
      "step: 42370 , time : 0.0\n",
      "train: loss: 5732.08154296875 acc: 0.9885669350624084  val: loss: 227194.5625 acc: 0.9618619084358215\n",
      "step: 42375 , time : 0.0\n",
      "train: loss: 16505.69140625 acc: 0.9792752265930176  val: loss: 465542.25 acc: 0.9181976914405823\n",
      "step: 42380 , time : 0.0\n",
      "train: loss: 37893.71875 acc: 0.9331474900245667  val: loss: 460575.875 acc: 0.8921924233436584\n",
      "step: 42385 , time : 0.0\n",
      "train: loss: 20434.994140625 acc: 0.9874521493911743  val: loss: 427162.09375 acc: 0.6227173805236816\n",
      "step: 42390 , time : 0.0\n",
      "train: loss: 63481.62109375 acc: 0.9754999279975891  val: loss: 1125012.375 acc: 0.8209689855575562\n",
      "step: 42395 , time : 0.0\n",
      "train: loss: 17306.923828125 acc: 0.9770956635475159  val: loss: 1994814.75 acc: 0.7656936645507812\n",
      "step: 42400 , time : 0.0\n",
      "train: loss: 2915.27490234375 acc: 0.9935832619667053  val: loss: 533219.875 acc: 0.9000440239906311\n",
      "step: 42405 , time : 0.0\n",
      "train: loss: 53957.19921875 acc: 0.9818739295005798  val: loss: 685251.625 acc: 0.7820286154747009\n",
      "step: 42410 , time : 0.015624761581420898\n",
      "train: loss: 144365.984375 acc: 0.9610615968704224  val: loss: 755079.25 acc: 0.7449886798858643\n",
      "step: 42415 , time : 0.0\n",
      "train: loss: 30446.28515625 acc: 0.9927812218666077  val: loss: 617532.25 acc: 0.9371713399887085\n",
      "step: 42420 , time : 0.0010004043579101562\n",
      "train: loss: 38576.83203125 acc: 0.9927155375480652  val: loss: 1287169.625 acc: 0.8875272870063782\n",
      "step: 42425 , time : 0.0\n",
      "train: loss: 67113.5078125 acc: 0.9736059308052063  val: loss: 423218.28125 acc: 0.8761553764343262\n",
      "step: 42430 , time : 0.0\n",
      "train: loss: 71460.9296875 acc: 0.9778317809104919  val: loss: 546891.125 acc: 0.8506439924240112\n",
      "step: 42435 , time : 0.0010006427764892578\n",
      "train: loss: 100750.7265625 acc: 0.9592317938804626  val: loss: 612508.4375 acc: 0.9671799540519714\n",
      "step: 42440 , time : 0.0\n",
      "train: loss: 81048.7109375 acc: 0.9692322015762329  val: loss: 538378.6875 acc: 0.940232515335083\n",
      "step: 42445 , time : 0.0010004043579101562\n",
      "train: loss: 134473.796875 acc: 0.9655609726905823  val: loss: 1821677.625 acc: 0.6087602376937866\n",
      "step: 42450 , time : 0.0010008811950683594\n",
      "train: loss: 38984.65625 acc: 0.9969707131385803  val: loss: 3303763.5 acc: 0.584252119064331\n",
      "step: 42455 , time : 0.0\n",
      "train: loss: 81811.328125 acc: 0.9858573079109192  val: loss: 755184.375 acc: 0.8516704440116882\n",
      "step: 42460 , time : 0.0010008811950683594\n",
      "train: loss: 124469.8984375 acc: 0.9778437614440918  val: loss: 702586.9375 acc: 0.7879214882850647\n",
      "step: 42465 , time : 0.0\n",
      "train: loss: 207077.78125 acc: 0.9718248844146729  val: loss: 1070753.75 acc: 0.8557259440422058\n",
      "step: 42470 , time : 0.0\n",
      "train: loss: 2102795.75 acc: 0.8604121804237366  val: loss: 462195.15625 acc: 0.9265620112419128\n",
      "step: 42475 , time : 0.0\n",
      "train: loss: 554813.375 acc: 0.9652721285820007  val: loss: 1474366.625 acc: 0.8947607278823853\n",
      "step: 42480 , time : 0.0\n",
      "train: loss: 504460.3125 acc: 0.9042674899101257  val: loss: 1755519.875 acc: 0.3233568072319031\n",
      "step: 42485 , time : 0.0\n",
      "train: loss: 647998.875 acc: 0.933613121509552  val: loss: 2839349.75 acc: 0.6650039553642273\n",
      "step: 42490 , time : 0.001001119613647461\n",
      "train: loss: 2055063.0 acc: 0.9402660131454468  val: loss: 1051020.625 acc: 0.8396788835525513\n",
      "step: 42495 , time : 0.0\n",
      "train: loss: 1099713.25 acc: 0.9569091796875  val: loss: 759572.25 acc: 0.8953375816345215\n",
      "step: 42500 , time : 0.0\n",
      "train: loss: 1275711.75 acc: 0.9488329291343689  val: loss: 1642365.375 acc: 0.25468510389328003\n",
      "step: 42505 , time : 0.0\n",
      "train: loss: 769074.0625 acc: 0.9614428281784058  val: loss: 197159.203125 acc: 0.9344213604927063\n",
      "step: 42510 , time : 0.0\n",
      "train: loss: 1120647.375 acc: 0.9388430118560791  val: loss: 1940486.625 acc: -0.12703394889831543\n",
      "step: 42515 , time : 0.0\n",
      "train: loss: 492764.0625 acc: 0.8891810774803162  val: loss: 2143145.5 acc: 0.6360799074172974\n",
      "step: 42520 , time : 0.0\n",
      "train: loss: 625054.5 acc: 0.858522355556488  val: loss: 2804210.0 acc: 0.7587878704071045\n",
      "step: 42525 , time : 0.0\n",
      "train: loss: 1117357.0 acc: 0.42647236585617065  val: loss: 1151474.125 acc: 0.7870846390724182\n",
      "step: 42530 , time : 0.0\n",
      "train: loss: 1203901.125 acc: 0.5122799873352051  val: loss: 664400.875 acc: 0.48136115074157715\n",
      "step: 42535 , time : 0.0\n",
      "train: loss: 822502.125 acc: 0.7855379581451416  val: loss: 961263.5 acc: 0.6561176776885986\n",
      "step: 42540 , time : 0.0\n",
      "train: loss: 482006.9375 acc: 0.8359642624855042  val: loss: 735167.1875 acc: 0.8342647552490234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 42545 , time : 0.0010008811950683594\n",
      "train: loss: 1449614.625 acc: 0.7107232809066772  val: loss: 591221.3125 acc: 0.8712947368621826\n",
      "step: 42550 , time : 0.0\n",
      "train: loss: 1152880.875 acc: 0.37412047386169434  val: loss: 1564257.125 acc: 0.3945015072822571\n",
      "step: 42555 , time : 0.0010008811950683594\n",
      "train: loss: 837948.375 acc: 0.5131843090057373  val: loss: 789469.875 acc: 0.6756538152694702\n",
      "step: 42560 , time : 0.001001596450805664\n",
      "train: loss: 569733.75 acc: 0.45863091945648193  val: loss: 2046569.875 acc: 0.722453773021698\n",
      "step: 42565 , time : 0.0\n",
      "train: loss: 306353.5625 acc: 0.7207093834877014  val: loss: 2295199.75 acc: 0.7018916010856628\n",
      "step: 42570 , time : 0.0\n",
      "train: loss: 81670.390625 acc: 0.9331092834472656  val: loss: 1737337.0 acc: 0.7619748711585999\n",
      "step: 42575 , time : 0.0010001659393310547\n",
      "train: loss: 139274.921875 acc: 0.8857886791229248  val: loss: 1815860.875 acc: 0.7677377462387085\n",
      "step: 42580 , time : 0.0\n",
      "train: loss: 223100.109375 acc: 0.8724997043609619  val: loss: 331239.15625 acc: 0.8153591156005859\n",
      "step: 42585 , time : 0.0\n",
      "train: loss: 243974.765625 acc: 0.824951171875  val: loss: 1760598.75 acc: 0.6688791513442993\n",
      "step: 42590 , time : 0.0\n",
      "train: loss: 88081.2734375 acc: 0.9191413521766663  val: loss: 557770.25 acc: 0.8086847066879272\n",
      "step: 42595 , time : 0.0\n",
      "train: loss: 197449.484375 acc: 0.8864704966545105  val: loss: 743491.125 acc: 0.797392725944519\n",
      "step: 42600 , time : 0.0\n",
      "train: loss: 281162.4375 acc: 0.7547323107719421  val: loss: 1192053.875 acc: 0.8185964822769165\n",
      "step: 42605 , time : 0.0\n",
      "train: loss: 232477.78125 acc: 0.8598626255989075  val: loss: 604786.25 acc: 0.874923586845398\n",
      "step: 42610 , time : 0.0\n",
      "train: loss: 784057.625 acc: 0.717877209186554  val: loss: 2980093.25 acc: 0.663652777671814\n",
      "step: 42615 , time : 0.0\n",
      "train: loss: 580171.875 acc: 0.4106765389442444  val: loss: 613905.625 acc: 0.8021257519721985\n",
      "step: 42620 , time : 0.0\n",
      "train: loss: 780903.75 acc: 0.6512884497642517  val: loss: 1145546.5 acc: 0.6147605180740356\n",
      "step: 42625 , time : 0.0\n",
      "train: loss: 227085.640625 acc: 0.8332747220993042  val: loss: 1494780.0 acc: 0.6657678484916687\n",
      "step: 42630 , time : 0.0\n",
      "train: loss: 779104.75 acc: 0.703760027885437  val: loss: 886973.75 acc: 0.6352922916412354\n",
      "step: 42635 , time : 0.0\n",
      "train: loss: 1654564.375 acc: 0.7786084413528442  val: loss: 408452.59375 acc: 0.9044771790504456\n",
      "step: 42640 , time : 0.0\n",
      "train: loss: 1351502.375 acc: 0.815441370010376  val: loss: 1272532.625 acc: 0.8945440649986267\n",
      "step: 42645 , time : 0.0\n",
      "train: loss: 881281.9375 acc: 0.9052116274833679  val: loss: 1229401.0 acc: 0.7535130977630615\n",
      "step: 42650 , time : 0.0\n",
      "train: loss: 459385.625 acc: 0.9445295929908752  val: loss: 1197294.5 acc: 0.7508528828620911\n",
      "step: 42655 , time : 0.0\n",
      "train: loss: 518023.375 acc: 0.9116653800010681  val: loss: 561194.0 acc: 0.875078558921814\n",
      "step: 42660 , time : 0.0\n",
      "train: loss: 356017.96875 acc: 0.9478902220726013  val: loss: 786339.5625 acc: 0.8547323346138\n",
      "step: 42665 , time : 0.001001119613647461\n",
      "train: loss: 372904.71875 acc: 0.9642067551612854  val: loss: 1243005.125 acc: 0.7764557003974915\n",
      "step: 42670 , time : 0.0010006427764892578\n",
      "train: loss: 180878.75 acc: 0.9870389103889465  val: loss: 748210.75 acc: 0.8298981189727783\n",
      "step: 42675 , time : 0.0\n",
      "train: loss: 148349.046875 acc: 0.9883604645729065  val: loss: 1166472.125 acc: 0.5073648691177368\n",
      "step: 42680 , time : 0.0010006427764892578\n",
      "train: loss: 368561.3125 acc: 0.9590018391609192  val: loss: 898744.4375 acc: 0.5328704118728638\n",
      "step: 42685 , time : 0.0\n",
      "train: loss: 282108.3125 acc: 0.9141157269477844  val: loss: 570388.9375 acc: 0.7862192392349243\n",
      "step: 42690 , time : 0.0\n",
      "train: loss: 137537.03125 acc: 0.9655582904815674  val: loss: 474162.71875 acc: 0.8856679201126099\n",
      "step: 42695 , time : 0.0\n",
      "train: loss: 99582.2265625 acc: 0.9824621081352234  val: loss: 640269.375 acc: 0.7176790833473206\n",
      "step: 42700 , time : 0.0\n",
      "train: loss: 12069.65234375 acc: 0.991810142993927  val: loss: 736369.5 acc: 0.8512364625930786\n",
      "step: 42705 , time : 0.0\n",
      "train: loss: 16317.419921875 acc: 0.9702659249305725  val: loss: 639316.0 acc: 0.9320634007453918\n",
      "step: 42710 , time : 0.0\n",
      "train: loss: 12947.6005859375 acc: 0.9762089252471924  val: loss: 655275.1875 acc: 0.681093156337738\n",
      "step: 42715 , time : 0.0\n",
      "train: loss: 54033.4921875 acc: 0.9555187225341797  val: loss: 832222.875 acc: 0.8148612976074219\n",
      "step: 42720 , time : 0.0\n",
      "train: loss: 7290.97119140625 acc: 0.9864752292633057  val: loss: 208528.09375 acc: 0.9524750709533691\n",
      "step: 42725 , time : 0.0\n",
      "train: loss: 8979.884765625 acc: 0.979692280292511  val: loss: 346274.8125 acc: 0.959316074848175\n",
      "step: 42730 , time : 0.0\n",
      "train: loss: 7818.91748046875 acc: 0.9863924980163574  val: loss: 704802.0 acc: 0.8371931314468384\n",
      "step: 42735 , time : 0.0\n",
      "train: loss: 6345.060546875 acc: 0.9904417991638184  val: loss: 1038705.0 acc: 0.875327467918396\n",
      "step: 42740 , time : 0.0\n",
      "train: loss: 68313.8984375 acc: 0.9628696441650391  val: loss: 572548.0625 acc: 0.9229084253311157\n",
      "step: 42745 , time : 0.0\n",
      "train: loss: 60915.2109375 acc: 0.953857421875  val: loss: 330900.1875 acc: 0.9620500802993774\n",
      "step: 42750 , time : 0.0\n",
      "train: loss: 104096.046875 acc: 0.948215126991272  val: loss: 315562.5 acc: 0.9320275187492371\n",
      "step: 42755 , time : 0.0\n",
      "train: loss: 31057.94921875 acc: 0.982324481010437  val: loss: 1328841.125 acc: 0.6775004863739014\n",
      "step: 42760 , time : 0.0\n",
      "train: loss: 24338.970703125 acc: 0.9777664542198181  val: loss: 1255669.625 acc: 0.7588819265365601\n",
      "step: 42765 , time : 0.0\n",
      "train: loss: 18339.876953125 acc: 0.9646530151367188  val: loss: 682940.5 acc: 0.879067063331604\n",
      "step: 42770 , time : 0.0\n",
      "train: loss: 9576.16796875 acc: 0.9954079985618591  val: loss: 1572494.25 acc: 0.6853026151657104\n",
      "step: 42775 , time : 0.0010004043579101562\n",
      "train: loss: 23885.7265625 acc: 0.9925096035003662  val: loss: 1034312.875 acc: 0.8821595907211304\n",
      "step: 42780 , time : 0.0\n",
      "train: loss: 28187.048828125 acc: 0.9922793507575989  val: loss: 986211.875 acc: 0.8678987622261047\n",
      "step: 42785 , time : 0.0010008811950683594\n",
      "train: loss: 24492.162109375 acc: 0.9904417395591736  val: loss: 1494393.875 acc: 0.8844637870788574\n",
      "step: 42790 , time : 0.0\n",
      "train: loss: 37682.76953125 acc: 0.9835978150367737  val: loss: 1153075.375 acc: 0.7534356117248535\n",
      "step: 42795 , time : 0.0\n",
      "train: loss: 265311.46875 acc: 0.9064565300941467  val: loss: 1644318.5 acc: 0.5124626159667969\n",
      "step: 42800 , time : 0.0\n",
      "train: loss: 101678.625 acc: 0.9732133746147156  val: loss: 5010141.0 acc: 0.6344246864318848\n",
      "step: 42805 , time : 0.0\n",
      "train: loss: 51676.1796875 acc: 0.9566876292228699  val: loss: 1397885.375 acc: 0.720655083656311\n",
      "step: 42810 , time : 0.0\n",
      "train: loss: 958408.5 acc: 0.7178283929824829  val: loss: 717025.125 acc: 0.7882717847824097\n",
      "step: 42815 , time : 0.0\n",
      "train: loss: 99822.0234375 acc: 0.9890456199645996  val: loss: 892046.375 acc: 0.8743726015090942\n",
      "step: 42820 , time : 0.0\n",
      "train: loss: 48482.78515625 acc: 0.9945025444030762  val: loss: 721386.875 acc: 0.5286343097686768\n",
      "step: 42825 , time : 0.0\n",
      "train: loss: 483998.4375 acc: 0.8974804282188416  val: loss: 622315.375 acc: 0.9145277738571167\n",
      "step: 42830 , time : 0.0\n",
      "train: loss: 511957.09375 acc: 0.9642446637153625  val: loss: 682599.0 acc: 0.8700684309005737\n",
      "step: 42835 , time : 0.0\n",
      "train: loss: 1652779.75 acc: 0.48569995164871216  val: loss: 1212915.125 acc: 0.8674557209014893\n",
      "step: 42840 , time : 0.0\n",
      "train: loss: 433592.25 acc: 0.9702692627906799  val: loss: 853730.875 acc: 0.6864134073257446\n",
      "step: 42845 , time : 0.0\n",
      "train: loss: 1063550.375 acc: 0.8631435036659241  val: loss: 522935.9375 acc: 0.9431705474853516\n",
      "step: 42850 , time : 0.0\n",
      "train: loss: 1284860.5 acc: 0.9455916881561279  val: loss: 1324470.875 acc: 0.15467029809951782\n",
      "step: 42855 , time : 0.0\n",
      "train: loss: 228636.53125 acc: 0.9923346042633057  val: loss: 2558818.5 acc: -0.029944539070129395\n",
      "step: 42860 , time : 0.001001119613647461\n",
      "train: loss: 725346.75 acc: 0.9756802320480347  val: loss: 821179.8125 acc: 0.8393163681030273\n",
      "step: 42865 , time : 0.0\n",
      "train: loss: 1471109.25 acc: 0.9261633157730103  val: loss: 532174.1875 acc: 0.9484169483184814\n",
      "step: 42870 , time : 0.0010006427764892578\n",
      "train: loss: 568279.3125 acc: 0.9594056606292725  val: loss: 873705.6875 acc: 0.7531791925430298\n",
      "step: 42875 , time : 0.0010004043579101562\n",
      "train: loss: 162047.53125 acc: 0.9823068380355835  val: loss: 2306017.75 acc: 0.29659128189086914\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 42880 , time : 0.001001119613647461\n",
      "train: loss: 206408.46875 acc: 0.9667068123817444  val: loss: 2010879.5 acc: 0.6280561089515686\n",
      "step: 42885 , time : 0.0\n",
      "train: loss: 1206769.625 acc: 0.8146712779998779  val: loss: 272145.03125 acc: 0.9480239748954773\n",
      "step: 42890 , time : 0.0\n",
      "train: loss: 1532015.625 acc: 0.7001467347145081  val: loss: 1473252.625 acc: 0.5614158511161804\n",
      "step: 42895 , time : 0.0\n",
      "train: loss: 516361.0625 acc: 0.798783004283905  val: loss: 1211739.625 acc: 0.8716779351234436\n",
      "step: 42900 , time : 0.001001119613647461\n",
      "train: loss: 796489.5 acc: 0.7012385725975037  val: loss: 983847.6875 acc: 0.8933078646659851\n",
      "step: 42905 , time : 0.001001119613647461\n",
      "train: loss: 863163.5625 acc: 0.7334120273590088  val: loss: 1280240.75 acc: -0.37773072719573975\n",
      "step: 42910 , time : 0.0\n",
      "train: loss: 2264184.5 acc: -0.7067441940307617  val: loss: 1567913.125 acc: 0.7779226899147034\n",
      "step: 42915 , time : 0.0010008811950683594\n",
      "train: loss: 1358137.375 acc: 0.503790557384491  val: loss: 1302394.75 acc: 0.6351959705352783\n",
      "step: 42920 , time : 0.0\n",
      "train: loss: 1217347.875 acc: 0.4012075662612915  val: loss: 3641979.25 acc: 0.6195386648178101\n",
      "step: 42925 , time : 0.0\n",
      "train: loss: 413114.28125 acc: 0.6638964414596558  val: loss: 1179159.875 acc: 0.7379516959190369\n",
      "step: 42930 , time : 0.0\n",
      "train: loss: 616870.0625 acc: 0.5166801810264587  val: loss: 314283.84375 acc: 0.829748272895813\n",
      "step: 42935 , time : 0.0\n",
      "train: loss: 233270.859375 acc: 0.8435238599777222  val: loss: 941505.9375 acc: 0.7925557494163513\n",
      "step: 42940 , time : 0.0\n",
      "train: loss: 138856.359375 acc: 0.8980616331100464  val: loss: 831174.125 acc: 0.7507811784744263\n",
      "step: 42945 , time : 0.0\n",
      "train: loss: 83963.0546875 acc: 0.9355116486549377  val: loss: 497608.21875 acc: 0.7616397142410278\n",
      "step: 42950 , time : 0.0010006427764892578\n",
      "train: loss: 150558.25 acc: 0.9042333960533142  val: loss: 706225.0625 acc: 0.7766804099082947\n",
      "step: 42955 , time : 0.0010004043579101562\n",
      "train: loss: 81174.9609375 acc: 0.9200443625450134  val: loss: 541083.25 acc: 0.8693097233772278\n",
      "step: 42960 , time : 0.0\n",
      "train: loss: 179079.1875 acc: 0.7732670307159424  val: loss: 1071147.875 acc: 0.8032763004302979\n",
      "step: 42965 , time : 0.0\n",
      "train: loss: 84608.359375 acc: 0.9107011556625366  val: loss: 366318.78125 acc: 0.8563835024833679\n",
      "step: 42970 , time : 0.0\n",
      "train: loss: 202136.03125 acc: 0.8590184450149536  val: loss: 173297.875 acc: 0.8218274712562561\n",
      "step: 42975 , time : 0.0\n",
      "train: loss: 486007.59375 acc: 0.6916235685348511  val: loss: 615419.9375 acc: 0.6266584396362305\n",
      "step: 42980 , time : 0.0\n",
      "train: loss: 562304.125 acc: 0.6661118268966675  val: loss: 1222979.375 acc: 0.7019228935241699\n",
      "step: 42985 , time : 0.0\n",
      "train: loss: 523707.03125 acc: 0.7144773602485657  val: loss: 560675.75 acc: 0.6791952848434448\n",
      "step: 42990 , time : 0.0010006427764892578\n",
      "train: loss: 180897.734375 acc: 0.7415556311607361  val: loss: 2029138.625 acc: 0.6643187999725342\n",
      "step: 42995 , time : 0.0\n",
      "train: loss: 878690.75 acc: 0.6353635787963867  val: loss: 560376.75 acc: 0.8163775205612183\n",
      "step: 43000 , time : 0.0010008811950683594\n",
      "train: loss: 1527183.625 acc: 0.7184152603149414  val: loss: 993752.5625 acc: 0.797339677810669\n",
      "step: 43005 , time : 0.0\n",
      "train: loss: 1244736.375 acc: 0.7914721965789795  val: loss: 598890.0625 acc: 0.7955957651138306\n",
      "step: 43010 , time : 0.0010004043579101562\n",
      "train: loss: 781971.8125 acc: 0.9130098223686218  val: loss: 642946.0625 acc: 0.8103965520858765\n",
      "step: 43015 , time : 0.0\n",
      "train: loss: 739831.6875 acc: 0.9304624795913696  val: loss: 1276602.25 acc: 0.547311544418335\n",
      "step: 43020 , time : 0.001001119613647461\n",
      "train: loss: 605282.5625 acc: 0.8202528357505798  val: loss: 497836.4375 acc: 0.8737706542015076\n",
      "step: 43025 , time : 0.0\n",
      "train: loss: 496726.46875 acc: 0.9244160652160645  val: loss: 825056.125 acc: 0.7697448134422302\n",
      "step: 43030 , time : 0.0\n",
      "train: loss: 425406.96875 acc: 0.9609447121620178  val: loss: 1373828.75 acc: 0.8445064425468445\n",
      "step: 43035 , time : 0.0010006427764892578\n",
      "train: loss: 231652.140625 acc: 0.981545090675354  val: loss: 1171333.5 acc: 0.8618285655975342\n",
      "step: 43040 , time : 0.0\n",
      "train: loss: 232067.890625 acc: 0.984257698059082  val: loss: 1272786.75 acc: 0.9340415000915527\n",
      "step: 43045 , time : 0.0010006427764892578\n",
      "train: loss: 286084.53125 acc: 0.9587306976318359  val: loss: 721831.75 acc: 0.7577353119850159\n",
      "step: 43050 , time : 0.0\n",
      "train: loss: 212677.890625 acc: 0.955301821231842  val: loss: 1382026.5 acc: 0.8600174188613892\n",
      "step: 43055 , time : 0.0\n",
      "train: loss: 110615.9140625 acc: 0.9228330850601196  val: loss: 383787.625 acc: 0.9282606840133667\n",
      "step: 43060 , time : 0.0010006427764892578\n",
      "train: loss: 42133.4140625 acc: 0.9061177968978882  val: loss: 948849.875 acc: 0.9167231917381287\n",
      "step: 43065 , time : 0.0\n",
      "train: loss: 11858.6611328125 acc: 0.9641744494438171  val: loss: 337968.40625 acc: 0.9254588484764099\n",
      "step: 43070 , time : 0.0\n",
      "train: loss: 42873.265625 acc: 0.965698778629303  val: loss: 1181883.125 acc: 0.7194133996963501\n",
      "step: 43075 , time : 0.0\n",
      "train: loss: 14294.76953125 acc: 0.9777211546897888  val: loss: 3485911.75 acc: 0.31778454780578613\n",
      "step: 43080 , time : 0.0\n",
      "train: loss: 4240.0576171875 acc: 0.9890429377555847  val: loss: 208107.8125 acc: 0.9606308937072754\n",
      "step: 43085 , time : 0.0\n",
      "train: loss: 9078.2353515625 acc: 0.9452942609786987  val: loss: 886817.8125 acc: 0.9074013829231262\n",
      "step: 43090 , time : 0.0\n",
      "train: loss: 4151.59814453125 acc: 0.9904816150665283  val: loss: 1019772.75 acc: 0.8265531659126282\n",
      "step: 43095 , time : 0.0\n",
      "train: loss: 5280.3720703125 acc: 0.9844558835029602  val: loss: 1203116.75 acc: 0.7057293653488159\n",
      "step: 43100 , time : 0.0\n",
      "train: loss: 2272.773681640625 acc: 0.995501697063446  val: loss: 1221792.375 acc: 0.7125357389450073\n",
      "step: 43105 , time : 0.0010008811950683594\n",
      "train: loss: 52864.1015625 acc: 0.9696534872055054  val: loss: 2737056.0 acc: -0.6680494546890259\n",
      "step: 43110 , time : 0.001001119613647461\n",
      "train: loss: 53569.34765625 acc: 0.970835268497467  val: loss: 2786649.5 acc: 0.8280651569366455\n",
      "step: 43115 , time : 0.0\n",
      "train: loss: 47553.2578125 acc: 0.9724037051200867  val: loss: 2239031.5 acc: 0.36711549758911133\n",
      "step: 43120 , time : 0.0010008811950683594\n",
      "train: loss: 46248.109375 acc: 0.9721680283546448  val: loss: 2971719.5 acc: 0.4540629982948303\n",
      "step: 43125 , time : 0.0\n",
      "train: loss: 20852.0078125 acc: 0.9858378767967224  val: loss: 1151039.125 acc: 0.8812451362609863\n",
      "step: 43130 , time : 0.0\n",
      "train: loss: 32798.1328125 acc: 0.9671571254730225  val: loss: 2829113.5 acc: 0.4671539068222046\n",
      "step: 43135 , time : 0.0\n",
      "train: loss: 19252.064453125 acc: 0.9888454675674438  val: loss: 1176526.375 acc: 0.8822077512741089\n",
      "step: 43140 , time : 0.0\n",
      "train: loss: 47549.21484375 acc: 0.9828087687492371  val: loss: 2267513.5 acc: 0.7770936489105225\n",
      "step: 43145 , time : 0.0\n",
      "train: loss: 68574.375 acc: 0.9835435152053833  val: loss: 2238050.25 acc: -0.11584329605102539\n",
      "step: 43150 , time : 0.015625715255737305\n",
      "train: loss: 22492.896484375 acc: 0.9922388195991516  val: loss: 2293028.0 acc: 0.49807238578796387\n",
      "step: 43155 , time : 0.0010008811950683594\n",
      "train: loss: 55076.3203125 acc: 0.9843004941940308  val: loss: 3150951.5 acc: 0.6996849775314331\n",
      "step: 43160 , time : 0.0\n",
      "train: loss: 153450.953125 acc: 0.9533870816230774  val: loss: 2857321.5 acc: -0.6343716382980347\n",
      "step: 43165 , time : 0.0\n",
      "train: loss: 96182.0546875 acc: 0.9682021141052246  val: loss: 1665983.875 acc: 0.7109194993972778\n",
      "step: 43170 , time : 0.0\n",
      "train: loss: 114216.1484375 acc: 0.9637426733970642  val: loss: 1235225.5 acc: 0.8297457098960876\n",
      "step: 43175 , time : 0.0\n",
      "train: loss: 175858.796875 acc: 0.9701291918754578  val: loss: 507337.78125 acc: 0.9300215244293213\n",
      "step: 43180 , time : 0.0\n",
      "train: loss: 131892.578125 acc: 0.9826586246490479  val: loss: 1084014.375 acc: 0.8056394457817078\n",
      "step: 43185 , time : 0.0\n",
      "train: loss: 136734.21875 acc: 0.9858185052871704  val: loss: 1081924.375 acc: 0.6436049938201904\n",
      "step: 43190 , time : 0.0\n",
      "train: loss: 92551.703125 acc: 0.9888576865196228  val: loss: 1760940.875 acc: 0.73298180103302\n",
      "step: 43195 , time : 0.0\n",
      "train: loss: 191910.125 acc: 0.9825108051300049  val: loss: 856118.4375 acc: 0.7441568374633789\n",
      "step: 43200 , time : 0.0\n",
      "train: loss: 333104.40625 acc: 0.9095430374145508  val: loss: 1637248.375 acc: 0.7697396874427795\n",
      "step: 43205 , time : 0.0\n",
      "train: loss: 401332.28125 acc: 0.9785144329071045  val: loss: 1224296.875 acc: 0.7591614723205566\n",
      "step: 43210 , time : 0.0\n",
      "train: loss: 332421.4375 acc: 0.9055989980697632  val: loss: 632839.875 acc: 0.7888791561126709\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 43215 , time : 0.0\n",
      "train: loss: 486888.25 acc: 0.9729787111282349  val: loss: 1100878.25 acc: 0.7053762674331665\n",
      "step: 43220 , time : 0.0\n",
      "train: loss: 830956.9375 acc: 0.9714273810386658  val: loss: 1424176.125 acc: 0.5627866983413696\n",
      "step: 43225 , time : 0.0010008811950683594\n",
      "train: loss: 2061196.625 acc: 0.9199245572090149  val: loss: 3094165.0 acc: -0.4576040506362915\n",
      "step: 43230 , time : 0.0010006427764892578\n",
      "train: loss: 669453.0625 acc: 0.9727321267127991  val: loss: 300547.96875 acc: 0.8440465927124023\n",
      "step: 43235 , time : 0.0\n",
      "train: loss: 1096900.625 acc: 0.9002670049667358  val: loss: 808971.125 acc: 0.8137654662132263\n",
      "step: 43240 , time : 0.0\n",
      "train: loss: 1547533.25 acc: 0.8888651728630066  val: loss: 227941.953125 acc: 0.9566673636436462\n",
      "step: 43245 , time : 0.0\n",
      "train: loss: 643026.4375 acc: 0.9014394879341125  val: loss: 291894.21875 acc: 0.9032762050628662\n",
      "step: 43250 , time : 0.0\n",
      "train: loss: 637040.625 acc: 0.8244631886482239  val: loss: 1003774.0 acc: 0.7127791047096252\n",
      "step: 43255 , time : 0.0\n",
      "train: loss: 2014933.375 acc: 0.46843022108078003  val: loss: 682396.25 acc: 0.8613288402557373\n",
      "step: 43260 , time : 0.0\n",
      "train: loss: 1070465.75 acc: 0.3463563919067383  val: loss: 1226418.75 acc: 0.4852787256240845\n",
      "step: 43265 , time : 0.0\n",
      "train: loss: 1111714.875 acc: 0.7777899503707886  val: loss: 1048531.75 acc: 0.7399276494979858\n",
      "step: 43270 , time : 0.0\n",
      "train: loss: 1076008.0 acc: 0.6698884963989258  val: loss: 321641.75 acc: 0.883799135684967\n",
      "step: 43275 , time : 0.0\n",
      "train: loss: 876341.5 acc: 0.6584808826446533  val: loss: 371923.6875 acc: 0.813473105430603\n",
      "step: 43280 , time : 0.0\n",
      "train: loss: 1677063.0 acc: 0.5203126668930054  val: loss: 1432878.25 acc: 0.7840951085090637\n",
      "step: 43285 , time : 0.0\n",
      "train: loss: 885324.125 acc: 0.5826281905174255  val: loss: 1657811.75 acc: 0.4578753709793091\n",
      "step: 43290 , time : 0.0\n",
      "train: loss: 413081.78125 acc: 0.6971930265426636  val: loss: 1717249.375 acc: 0.41012996435165405\n",
      "step: 43295 , time : 0.0\n",
      "train: loss: 276330.34375 acc: 0.7401777505874634  val: loss: 2384355.25 acc: 0.6990240812301636\n",
      "step: 43300 , time : 0.0\n",
      "train: loss: 275446.125 acc: 0.780999481678009  val: loss: 1473542.375 acc: 0.7746111154556274\n",
      "step: 43305 , time : 0.0\n",
      "train: loss: 53859.640625 acc: 0.9575067758560181  val: loss: 698785.75 acc: 0.6193777322769165\n",
      "step: 43310 , time : 0.0\n",
      "train: loss: 215965.46875 acc: 0.859801173210144  val: loss: 850823.3125 acc: 0.7473165988922119\n",
      "step: 43315 , time : 0.0\n",
      "train: loss: 321322.6875 acc: 0.8131181597709656  val: loss: 987037.625 acc: 0.7204538583755493\n",
      "step: 43320 , time : 0.0\n",
      "train: loss: 475456.25 acc: 0.7152482271194458  val: loss: 834429.625 acc: 0.7724473476409912\n",
      "step: 43325 , time : 0.0\n",
      "train: loss: 281375.1875 acc: 0.7737995386123657  val: loss: 1210091.625 acc: 0.8075990676879883\n",
      "step: 43330 , time : 0.0\n",
      "train: loss: 185138.4375 acc: 0.8592662215232849  val: loss: 895694.0 acc: 0.8078116774559021\n",
      "step: 43335 , time : 0.0\n",
      "train: loss: 150966.5625 acc: 0.9049293994903564  val: loss: 928029.375 acc: 0.8736461400985718\n",
      "step: 43340 , time : 0.0\n",
      "train: loss: 334482.6875 acc: 0.6661390066146851  val: loss: 1254445.75 acc: 0.8203662633895874\n",
      "step: 43345 , time : 0.0\n",
      "train: loss: 408845.09375 acc: 0.764461100101471  val: loss: 729034.0 acc: 0.7342267632484436\n",
      "step: 43350 , time : 0.0\n",
      "train: loss: 278022.21875 acc: 0.8264734745025635  val: loss: 1482788.125 acc: 0.8133558034896851\n",
      "step: 43355 , time : 0.0\n",
      "train: loss: 239777.859375 acc: 0.7310302257537842  val: loss: 1512203.875 acc: 0.7093477249145508\n",
      "step: 43360 , time : 0.0010004043579101562\n",
      "train: loss: 258063.0 acc: 0.7271295785903931  val: loss: 3397245.75 acc: 0.7371475696563721\n",
      "step: 43365 , time : 0.0\n",
      "train: loss: 795266.125 acc: 0.7407951354980469  val: loss: 2916315.25 acc: 0.7311887741088867\n",
      "step: 43370 , time : 0.0\n",
      "train: loss: 1235137.875 acc: 0.7712752819061279  val: loss: 1836639.875 acc: 0.8568086624145508\n",
      "step: 43375 , time : 0.0\n",
      "train: loss: 1240154.125 acc: 0.8598450422286987  val: loss: 2933898.0 acc: 0.7928899526596069\n",
      "step: 43380 , time : 0.0\n",
      "train: loss: 353262.28125 acc: 0.9665840268135071  val: loss: 729904.625 acc: 0.905333936214447\n",
      "step: 43385 , time : 0.0\n",
      "train: loss: 463063.40625 acc: 0.927571713924408  val: loss: 1005045.625 acc: 0.7592814564704895\n",
      "step: 43390 , time : 0.0\n",
      "train: loss: 828301.5 acc: 0.8715267181396484  val: loss: 778342.5 acc: 0.9240490794181824\n",
      "step: 43395 , time : 0.0\n",
      "train: loss: 385489.96875 acc: 0.9186233878135681  val: loss: 580501.125 acc: 0.9388633370399475\n",
      "step: 43400 , time : 0.0\n",
      "train: loss: 145556.8125 acc: 0.989983081817627  val: loss: 853437.875 acc: 0.941686749458313\n",
      "step: 43405 , time : 0.0\n",
      "train: loss: 360615.15625 acc: 0.9643567800521851  val: loss: 1558466.625 acc: 0.6891118288040161\n",
      "step: 43410 , time : 0.0\n",
      "train: loss: 229877.59375 acc: 0.9790732860565186  val: loss: 2139716.0 acc: 0.1149480938911438\n",
      "step: 43415 , time : 0.0\n",
      "train: loss: 253882.359375 acc: 0.9350064396858215  val: loss: 1362244.25 acc: 0.46154946088790894\n",
      "step: 43420 , time : 0.0\n",
      "train: loss: 108882.7578125 acc: 0.7338560819625854  val: loss: 1570701.875 acc: 0.7990332245826721\n",
      "step: 43425 , time : 0.0\n",
      "train: loss: 97955.078125 acc: 0.9582143425941467  val: loss: 202946.59375 acc: 0.8884759545326233\n",
      "step: 43430 , time : 0.0010008811950683594\n",
      "train: loss: 92815.90625 acc: 0.928402841091156  val: loss: 2301545.75 acc: 0.7074658274650574\n",
      "step: 43435 , time : 0.0\n",
      "train: loss: 6864.61181640625 acc: 0.9742762446403503  val: loss: 1694509.875 acc: 0.7911025285720825\n",
      "step: 43440 , time : 0.0010006427764892578\n",
      "train: loss: 14281.6904296875 acc: 0.971058189868927  val: loss: 532814.5 acc: 0.9482015371322632\n",
      "step: 43445 , time : 0.0\n",
      "train: loss: 24728.578125 acc: 0.9879441261291504  val: loss: 905577.5625 acc: 0.8529336452484131\n",
      "step: 43450 , time : 0.0010006427764892578\n",
      "train: loss: 7633.423828125 acc: 0.981407105922699  val: loss: 559473.3125 acc: 0.9375381469726562\n",
      "step: 43455 , time : 0.001001119613647461\n",
      "train: loss: 9090.90625 acc: 0.9713385701179504  val: loss: 937488.0625 acc: 0.8675419688224792\n",
      "step: 43460 , time : 0.0\n",
      "train: loss: 11786.8271484375 acc: 0.9657633900642395  val: loss: 2698461.5 acc: 0.03779780864715576\n",
      "step: 43465 , time : 0.0\n",
      "train: loss: 5205.6904296875 acc: 0.9905227422714233  val: loss: 2330751.0 acc: -0.1756824254989624\n",
      "step: 43470 , time : 0.0\n",
      "train: loss: 96889.453125 acc: 0.9540436267852783  val: loss: 1434406.125 acc: 0.6470785140991211\n",
      "step: 43475 , time : 0.0\n",
      "train: loss: 36512.1328125 acc: 0.9754716753959656  val: loss: 802730.9375 acc: 0.7968283295631409\n",
      "step: 43480 , time : 0.0\n",
      "train: loss: 22209.48046875 acc: 0.9839157462120056  val: loss: 1850569.125 acc: 0.17162758111953735\n",
      "step: 43485 , time : 0.0\n",
      "train: loss: 21868.689453125 acc: 0.9874081015586853  val: loss: 1358020.75 acc: 0.7979732751846313\n",
      "step: 43490 , time : 0.0\n",
      "train: loss: 61079.0390625 acc: 0.965553879737854  val: loss: 1072721.875 acc: 0.6248888969421387\n",
      "step: 43495 , time : 0.0\n",
      "train: loss: 39812.671875 acc: 0.9681938290596008  val: loss: 2880224.5 acc: 0.3783726692199707\n",
      "step: 43500 , time : 0.0\n",
      "train: loss: 19811.05078125 acc: 0.9895269870758057  val: loss: 306362.15625 acc: 0.9163089990615845\n",
      "step: 43505 , time : 0.0\n",
      "train: loss: 65843.21875 acc: 0.9805459380149841  val: loss: 1039523.1875 acc: 0.8607040643692017\n",
      "step: 43510 , time : 0.0\n",
      "train: loss: 39098.328125 acc: 0.9912036657333374  val: loss: 2376993.0 acc: 0.3564841151237488\n",
      "step: 43515 , time : 0.0\n",
      "train: loss: 38147.58203125 acc: 0.9928162097930908  val: loss: 2475006.0 acc: 0.4040108323097229\n",
      "step: 43520 , time : 0.0\n",
      "train: loss: 53857.359375 acc: 0.9831575751304626  val: loss: 995845.9375 acc: 0.8171190023422241\n",
      "step: 43525 , time : 0.0\n",
      "train: loss: 109302.7734375 acc: 0.9694015383720398  val: loss: 1395372.625 acc: 0.5883736610412598\n",
      "step: 43530 , time : 0.0\n",
      "train: loss: 115232.8671875 acc: 0.9762677550315857  val: loss: 399834.1875 acc: 0.9558653235435486\n",
      "step: 43535 , time : 0.01562643051147461\n",
      "train: loss: 172023.5625 acc: 0.9615059494972229  val: loss: 865876.5625 acc: 0.8169049024581909\n",
      "step: 43540 , time : 0.0\n",
      "train: loss: 179337.59375 acc: 0.9683820009231567  val: loss: 1130490.125 acc: 0.8123456239700317\n",
      "step: 43545 , time : 0.0010008811950683594\n",
      "train: loss: 75441.8125 acc: 0.9882546067237854  val: loss: 1606929.875 acc: 0.4002980589866638\n",
      "step: 43550 , time : 0.0\n",
      "train: loss: 455555.875 acc: 0.9471550583839417  val: loss: 1183788.375 acc: 0.7887970209121704\n",
      "step: 43555 , time : 0.0010008811950683594\n",
      "train: loss: 54798.80078125 acc: 0.9928227066993713  val: loss: 863296.125 acc: 0.8198643922805786\n",
      "step: 43560 , time : 0.0010008811950683594\n",
      "train: loss: 324680.28125 acc: 0.974391758441925  val: loss: 765636.9375 acc: 0.8843244314193726\n",
      "step: 43565 , time : 0.0010008811950683594\n",
      "train: loss: 196567.46875 acc: 0.9681081771850586  val: loss: 431699.34375 acc: 0.8493661284446716\n",
      "step: 43570 , time : 0.0\n",
      "train: loss: 590114.3125 acc: 0.9600331783294678  val: loss: 1907827.125 acc: 0.59991854429245\n",
      "step: 43575 , time : 0.0\n",
      "train: loss: 190434.046875 acc: 0.9432536959648132  val: loss: 1313944.75 acc: 0.016415774822235107\n",
      "step: 43580 , time : 0.0\n",
      "train: loss: 295750.375 acc: 0.96747887134552  val: loss: 2998639.0 acc: 0.5781750082969666\n",
      "step: 43585 , time : 0.0\n",
      "train: loss: 614829.875 acc: 0.9723210334777832  val: loss: 313598.5 acc: 0.8460016250610352\n",
      "step: 43590 , time : 0.0\n",
      "train: loss: 992460.1875 acc: 0.9479668140411377  val: loss: 374896.84375 acc: 0.8718616962432861\n",
      "step: 43595 , time : 0.0\n",
      "train: loss: 879712.5625 acc: 0.958530068397522  val: loss: 851987.0625 acc: 0.8009947538375854\n",
      "step: 43600 , time : 0.0\n",
      "train: loss: 1030857.0625 acc: 0.8264130353927612  val: loss: 185338.78125 acc: 0.8859489560127258\n",
      "step: 43605 , time : 0.015625953674316406\n",
      "train: loss: 340783.71875 acc: 0.9762378334999084  val: loss: 635727.6875 acc: 0.8788019418716431\n",
      "step: 43610 , time : 0.0\n",
      "train: loss: 710331.5 acc: 0.9406247735023499  val: loss: 1021649.9375 acc: 0.8542648553848267\n",
      "step: 43615 , time : 0.0\n",
      "train: loss: 479000.96875 acc: 0.9339309930801392  val: loss: 1028372.9375 acc: 0.7412465810775757\n",
      "step: 43620 , time : 0.0\n",
      "train: loss: 915601.25 acc: 0.8386338353157043  val: loss: 1175921.25 acc: 0.7924654483795166\n",
      "step: 43625 , time : 0.0\n",
      "train: loss: 1607755.875 acc: -0.026676535606384277  val: loss: 1003176.0 acc: 0.8357468247413635\n",
      "step: 43630 , time : 0.0\n",
      "train: loss: 820493.625 acc: 0.6705499887466431  val: loss: 749274.375 acc: 0.822919487953186\n",
      "step: 43635 , time : 0.0\n",
      "train: loss: 524549.875 acc: 0.8590674996376038  val: loss: 355201.375 acc: 0.7323664426803589\n",
      "step: 43640 , time : 0.0\n",
      "train: loss: 848498.5 acc: 0.87129807472229  val: loss: 304475.0 acc: 0.798068106174469\n",
      "step: 43645 , time : 0.0\n",
      "train: loss: 1807789.5 acc: 0.34257692098617554  val: loss: 838417.125 acc: 0.7640691995620728\n",
      "step: 43650 , time : 0.0\n",
      "train: loss: 972233.4375 acc: 0.5361626148223877  val: loss: 1796328.75 acc: 0.48136526346206665\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 43655 , time : 0.0\n",
      "train: loss: 577832.4375 acc: 0.5095329284667969  val: loss: 2295438.25 acc: 0.4853333830833435\n",
      "step: 43660 , time : 0.0\n",
      "train: loss: 448515.71875 acc: 0.5866703987121582  val: loss: 923749.0 acc: 0.7679088115692139\n",
      "step: 43665 , time : 0.0\n",
      "train: loss: 486489.53125 acc: 0.7509394884109497  val: loss: 1142288.5 acc: 0.7009221315383911\n",
      "step: 43670 , time : 0.0010008811950683594\n",
      "train: loss: 175451.265625 acc: 0.880464494228363  val: loss: 2008485.875 acc: 0.8385897874832153\n",
      "step: 43675 , time : 0.0010004043579101562\n",
      "train: loss: 204355.65625 acc: 0.8219693899154663  val: loss: 896983.4375 acc: 0.897040843963623\n",
      "step: 43680 , time : 0.0\n",
      "train: loss: 433023.71875 acc: 0.8074686527252197  val: loss: 1070235.625 acc: 0.794032096862793\n",
      "step: 43685 , time : 0.0\n",
      "train: loss: 116397.9609375 acc: 0.9011745452880859  val: loss: 1474501.625 acc: 0.7351630926132202\n",
      "step: 43690 , time : 0.0010006427764892578\n",
      "train: loss: 125790.6171875 acc: 0.8824955224990845  val: loss: 1475036.75 acc: 0.8399404883384705\n",
      "step: 43695 , time : 0.0\n",
      "train: loss: 238835.578125 acc: 0.8495467901229858  val: loss: 556575.0625 acc: 0.8291410803794861\n",
      "step: 43700 , time : 0.015625\n",
      "train: loss: 50194.44921875 acc: 0.9449752569198608  val: loss: 1853945.375 acc: 0.8460890650749207\n",
      "step: 43705 , time : 0.0\n",
      "train: loss: 565236.625 acc: 0.7614372968673706  val: loss: 329877.21875 acc: 0.9553022384643555\n",
      "step: 43710 , time : 0.0\n",
      "train: loss: 492568.375 acc: 0.6179684400558472  val: loss: 1327935.625 acc: 0.7765771746635437\n",
      "step: 43715 , time : 0.0\n",
      "train: loss: 754603.75 acc: 0.666648268699646  val: loss: 3287966.5 acc: 0.7201606631278992\n",
      "step: 43720 , time : 0.0\n",
      "train: loss: 541657.0625 acc: 0.6129034757614136  val: loss: 1313867.375 acc: 0.6977828145027161\n",
      "step: 43725 , time : 0.0\n",
      "train: loss: 553685.875 acc: 0.6618047952651978  val: loss: 1458739.625 acc: 0.7356797456741333\n",
      "step: 43730 , time : 0.0\n",
      "train: loss: 725396.6875 acc: 0.758226215839386  val: loss: 1113916.625 acc: 0.6790316104888916\n",
      "step: 43735 , time : 0.0\n",
      "train: loss: 1076217.25 acc: 0.808192253112793  val: loss: 1207335.875 acc: 0.7730116844177246\n",
      "step: 43740 , time : 0.0\n",
      "train: loss: 679909.0 acc: 0.9249082803726196  val: loss: 1241645.5 acc: 0.7830213904380798\n",
      "step: 43745 , time : 0.0\n",
      "train: loss: 803448.4375 acc: 0.9380871057510376  val: loss: 1009307.375 acc: 0.8751404285430908\n",
      "step: 43750 , time : 0.0\n",
      "train: loss: 472185.0625 acc: 0.8344913721084595  val: loss: 2351894.25 acc: 0.7108291387557983\n",
      "step: 43755 , time : 0.0\n",
      "train: loss: 495638.15625 acc: 0.9272132515907288  val: loss: 1389413.375 acc: 0.7619616985321045\n",
      "step: 43760 , time : 0.0\n",
      "train: loss: 228021.796875 acc: 0.9613064527511597  val: loss: 749523.625 acc: 0.8140550255775452\n",
      "step: 43765 , time : 0.0\n",
      "train: loss: 198983.6875 acc: 0.980993390083313  val: loss: 735586.0625 acc: 0.8153849244117737\n",
      "step: 43770 , time : 0.0\n",
      "train: loss: 146472.703125 acc: 0.9894044399261475  val: loss: 1632167.125 acc: 0.7685960531234741\n",
      "step: 43775 , time : 0.0\n",
      "train: loss: 311267.40625 acc: 0.9626930952072144  val: loss: 1902840.5 acc: 0.7576212882995605\n",
      "step: 43780 , time : 0.0\n",
      "train: loss: 169790.5625 acc: 0.9768772721290588  val: loss: 1186550.25 acc: -0.014140129089355469\n",
      "step: 43785 , time : 0.0\n",
      "train: loss: 178300.6875 acc: 0.9221485257148743  val: loss: 421417.9375 acc: 0.7681453824043274\n",
      "step: 43790 , time : 0.0010008811950683594\n",
      "train: loss: 17645.818359375 acc: 0.9895136952400208  val: loss: 3786590.5 acc: 0.6913022994995117\n",
      "step: 43795 , time : 0.0\n",
      "train: loss: 67125.171875 acc: 0.9735007286071777  val: loss: 827026.0625 acc: 0.7863657474517822\n",
      "step: 43800 , time : 0.0\n",
      "train: loss: 13248.46875 acc: 0.9704575538635254  val: loss: 1110810.125 acc: 0.8940414786338806\n",
      "step: 43805 , time : 0.0\n",
      "train: loss: 20311.439453125 acc: 0.9905939102172852  val: loss: 1798935.375 acc: 0.4772452712059021\n",
      "step: 43810 , time : 0.0\n",
      "train: loss: 66988.0546875 acc: 0.9497808218002319  val: loss: 1056560.375 acc: 0.6923730373382568\n",
      "step: 43815 , time : 0.0156252384185791\n",
      "train: loss: 18206.171875 acc: 0.9538975358009338  val: loss: 931903.125 acc: 0.8775042295455933\n",
      "step: 43820 , time : 0.0\n",
      "train: loss: 23748.5390625 acc: 0.9599984884262085  val: loss: 172185.3125 acc: 0.9463249444961548\n",
      "step: 43825 , time : 0.0\n",
      "train: loss: 3895.078369140625 acc: 0.989281415939331  val: loss: 1056035.0 acc: 0.7262341976165771\n",
      "step: 43830 , time : 0.0\n",
      "train: loss: 4860.03564453125 acc: 0.9780375957489014  val: loss: 1142104.0 acc: 0.8176402449607849\n",
      "step: 43835 , time : 0.0\n",
      "train: loss: 57035.4453125 acc: 0.9656448364257812  val: loss: 689229.3125 acc: 0.8338375687599182\n",
      "step: 43840 , time : 0.0\n",
      "train: loss: 17369.703125 acc: 0.9806833863258362  val: loss: 1813313.375 acc: 0.6179184317588806\n",
      "step: 43845 , time : 0.0\n",
      "train: loss: 84116.546875 acc: 0.9551994800567627  val: loss: 811040.5625 acc: 0.8794630169868469\n",
      "step: 43850 , time : 0.0\n",
      "train: loss: 20875.67578125 acc: 0.9873454570770264  val: loss: 1147175.5 acc: 0.5841274261474609\n",
      "step: 43855 , time : 0.0\n",
      "train: loss: 81183.859375 acc: 0.9656402468681335  val: loss: 1410603.125 acc: 0.5888833999633789\n",
      "step: 43860 , time : 0.0\n",
      "train: loss: 13496.3681640625 acc: 0.9652175903320312  val: loss: 1354525.875 acc: 0.5050724148750305\n",
      "step: 43865 , time : 0.0\n",
      "train: loss: 10024.068359375 acc: 0.9917629361152649  val: loss: 1930320.375 acc: 0.11019229888916016\n",
      "step: 43870 , time : 0.0\n",
      "train: loss: 38724.2421875 acc: 0.9769434928894043  val: loss: 1658598.875 acc: 0.7098016738891602\n",
      "step: 43875 , time : 0.0\n",
      "train: loss: 22965.830078125 acc: 0.99612957239151  val: loss: 310088.46875 acc: 0.7639944553375244\n",
      "step: 43880 , time : 0.0\n",
      "train: loss: 80411.0234375 acc: 0.9854615926742554  val: loss: 1269466.625 acc: 0.7036476135253906\n",
      "step: 43885 , time : 0.0\n",
      "train: loss: 37695.99609375 acc: 0.9884978532791138  val: loss: 682784.625 acc: 0.9207814931869507\n",
      "step: 43890 , time : 0.0\n",
      "train: loss: 94553.859375 acc: 0.9432124495506287  val: loss: 913236.6875 acc: 0.7403349876403809\n",
      "step: 43895 , time : 0.0\n",
      "train: loss: 94825.0703125 acc: 0.9589427709579468  val: loss: 772111.625 acc: 0.14128637313842773\n",
      "step: 43900 , time : 0.0\n",
      "train: loss: 88222.4921875 acc: 0.9431591033935547  val: loss: 1677105.375 acc: 0.24054259061813354\n",
      "step: 43905 , time : 0.0\n",
      "train: loss: 1222513.5 acc: 0.5696710348129272  val: loss: 263614.40625 acc: 0.9500601291656494\n",
      "step: 43910 , time : 0.0\n",
      "train: loss: 144552.53125 acc: 0.9743527173995972  val: loss: 1310817.375 acc: 0.6372882127761841\n",
      "step: 43915 , time : 0.0\n",
      "train: loss: 66729.890625 acc: 0.9935564994812012  val: loss: 817638.875 acc: 0.397965669631958\n",
      "step: 43920 , time : 0.01562643051147461\n",
      "train: loss: 85320.0703125 acc: 0.992088258266449  val: loss: 656160.5 acc: 0.7331360578536987\n",
      "step: 43925 , time : 0.0\n",
      "train: loss: 83422.2421875 acc: 0.9878470301628113  val: loss: 560960.125 acc: 0.9239240288734436\n",
      "step: 43930 , time : 0.0\n",
      "train: loss: 303882.5 acc: 0.9556266665458679  val: loss: 638349.9375 acc: 0.7563247680664062\n",
      "step: 43935 , time : 0.0\n",
      "train: loss: 368465.71875 acc: 0.9725663661956787  val: loss: 1233542.125 acc: 0.5862672924995422\n",
      "step: 43940 , time : 0.0\n",
      "train: loss: 300043.96875 acc: 0.9718250036239624  val: loss: 955171.4375 acc: 0.6157204508781433\n",
      "step: 43945 , time : 0.0\n",
      "train: loss: 362682.25 acc: 0.9274268746376038  val: loss: 1015056.9375 acc: 0.7233113646507263\n",
      "step: 43950 , time : 0.0\n",
      "train: loss: 2188140.75 acc: 0.841959536075592  val: loss: 789914.8125 acc: 0.7955267429351807\n",
      "step: 43955 , time : 0.0\n",
      "train: loss: 2252247.5 acc: 0.9247333407402039  val: loss: 569610.375 acc: 0.8700202703475952\n",
      "step: 43960 , time : 0.0\n",
      "train: loss: 816523.6875 acc: 0.9747198224067688  val: loss: 980854.75 acc: 0.8081925511360168\n",
      "step: 43965 , time : 0.0\n",
      "train: loss: 1240299.875 acc: 0.95648193359375  val: loss: 1384362.25 acc: 0.3064422607421875\n",
      "step: 43970 , time : 0.0\n",
      "train: loss: 266635.65625 acc: 0.9791778326034546  val: loss: 560939.0 acc: 0.9342824816703796\n",
      "step: 43975 , time : 0.0\n",
      "train: loss: 462833.96875 acc: 0.9489670991897583  val: loss: 385205.65625 acc: 0.9542674422264099\n",
      "step: 43980 , time : 0.0\n",
      "train: loss: 612280.625 acc: 0.9580856561660767  val: loss: 203621.8125 acc: 0.9664733409881592\n",
      "step: 43985 , time : 0.0\n",
      "train: loss: 698378.0 acc: 0.6887502074241638  val: loss: 648484.5 acc: 0.9056204557418823\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 43990 , time : 0.0\n",
      "train: loss: 1588029.875 acc: 0.5742835998535156  val: loss: 1652105.375 acc: 0.7355809211730957\n",
      "step: 43995 , time : 0.0\n",
      "train: loss: 1129764.375 acc: 0.295526385307312  val: loss: 2823193.25 acc: 0.7963215112686157\n",
      "step: 44000 , time : 0.0\n",
      "train: loss: 734513.625 acc: 0.7018305063247681  val: loss: 1027686.5625 acc: 0.8624891042709351\n",
      "step: 44005 , time : 0.0\n",
      "train: loss: 569525.9375 acc: 0.8712689876556396  val: loss: 1216461.625 acc: 0.8273493051528931\n",
      "step: 44010 , time : 0.0\n",
      "train: loss: 1850478.0 acc: 0.12184512615203857  val: loss: 1983431.0 acc: 0.7598309516906738\n",
      "step: 44015 , time : 0.0010004043579101562\n",
      "train: loss: 1149605.125 acc: 0.1748661994934082  val: loss: 2078606.625 acc: 0.7051461935043335\n",
      "step: 44020 , time : 0.0010008811950683594\n",
      "train: loss: 789302.8125 acc: 0.6335658431053162  val: loss: 2229536.0 acc: 0.8133533000946045\n",
      "step: 44025 , time : 0.0\n",
      "train: loss: 898346.375 acc: 0.4948025941848755  val: loss: 1088271.25 acc: 0.7079957723617554\n",
      "step: 44030 , time : 0.0\n",
      "train: loss: 284684.4375 acc: 0.725054144859314  val: loss: 2137479.0 acc: 0.775132954120636\n",
      "step: 44035 , time : 0.0\n",
      "train: loss: 420996.4375 acc: 0.6379815340042114  val: loss: 1104952.375 acc: 0.7681826949119568\n",
      "step: 44040 , time : 0.0\n",
      "train: loss: 363232.0625 acc: 0.8280247449874878  val: loss: 527805.8125 acc: 0.8005313873291016\n",
      "step: 44045 , time : 0.0010004043579101562\n",
      "train: loss: 214200.046875 acc: 0.8519319295883179  val: loss: 2318631.5 acc: 0.7071551084518433\n",
      "step: 44050 , time : 0.0\n",
      "train: loss: 319731.1875 acc: 0.8023149371147156  val: loss: 842217.0625 acc: 0.8621379137039185\n",
      "step: 44055 , time : 0.0010008811950683594\n",
      "train: loss: 207714.9375 acc: 0.8686326742172241  val: loss: 759875.4375 acc: 0.8290239572525024\n",
      "step: 44060 , time : 0.0\n",
      "train: loss: 233238.328125 acc: 0.7805831432342529  val: loss: 1800227.625 acc: 0.7535961866378784\n",
      "step: 44065 , time : 0.0\n",
      "train: loss: 88606.6796875 acc: 0.9059252142906189  val: loss: 633506.6875 acc: 0.8758726119995117\n",
      "step: 44070 , time : 0.0\n",
      "train: loss: 292644.625 acc: 0.6394299864768982  val: loss: 1102156.875 acc: 0.7808074355125427\n",
      "step: 44075 , time : 0.0\n",
      "train: loss: 1435492.375 acc: 0.5825169086456299  val: loss: 3440251.5 acc: 0.7050071358680725\n",
      "step: 44080 , time : 0.0\n",
      "train: loss: 714486.75 acc: 0.6987107992172241  val: loss: 2218884.5 acc: 0.7252758741378784\n",
      "step: 44085 , time : 0.0\n",
      "train: loss: 633480.625 acc: 0.7030946612358093  val: loss: 851129.125 acc: 0.8317714929580688\n",
      "step: 44090 , time : 0.0\n",
      "train: loss: 359574.34375 acc: 0.6837236285209656  val: loss: 1533406.375 acc: 0.7480994462966919\n",
      "step: 44095 , time : 0.0\n",
      "train: loss: 881830.5625 acc: 0.6572624444961548  val: loss: 1210439.625 acc: 0.802901029586792\n",
      "step: 44100 , time : 0.0\n",
      "train: loss: 820558.625 acc: 0.7681528329849243  val: loss: 367459.8125 acc: 0.8137691020965576\n",
      "step: 44105 , time : 0.0\n",
      "train: loss: 1077590.5 acc: 0.8318320512771606  val: loss: 1005199.3125 acc: 0.6845852136611938\n",
      "step: 44110 , time : 0.0010004043579101562\n",
      "train: loss: 581421.625 acc: 0.9611061811447144  val: loss: 1432505.75 acc: 0.4705008864402771\n",
      "step: 44115 , time : 0.0\n",
      "train: loss: 629301.1875 acc: 0.9049873948097229  val: loss: 2151924.0 acc: 0.664394736289978\n",
      "step: 44120 , time : 0.0\n",
      "train: loss: 642633.375 acc: 0.9144920706748962  val: loss: 1301431.375 acc: 0.8092992305755615\n",
      "step: 44125 , time : 0.0\n",
      "train: loss: 404093.71875 acc: 0.9407059550285339  val: loss: 826607.9375 acc: 0.5440613627433777\n",
      "step: 44130 , time : 0.001001119613647461\n",
      "train: loss: 308002.28125 acc: 0.9755827784538269  val: loss: 676190.0 acc: 0.8539509177207947\n",
      "step: 44135 , time : 0.0\n",
      "train: loss: 304975.625 acc: 0.9748573303222656  val: loss: 726262.125 acc: 0.9060825109481812\n",
      "step: 44140 , time : 0.00024271011352539062\n",
      "train: loss: 328556.15625 acc: 0.9676389098167419  val: loss: 1439512.375 acc: 0.7495735883712769\n",
      "step: 44145 , time : 0.0\n",
      "train: loss: 159493.484375 acc: 0.9671525955200195  val: loss: 1184979.5 acc: 0.3316414952278137\n",
      "step: 44150 , time : 0.0\n",
      "train: loss: 219844.015625 acc: 0.9178107976913452  val: loss: 509520.34375 acc: 0.8883445262908936\n",
      "step: 44155 , time : 0.0\n",
      "train: loss: 306608.53125 acc: 0.9298379421234131  val: loss: 540306.5 acc: 0.8404399752616882\n",
      "step: 44160 , time : 0.0\n",
      "train: loss: 100411.1953125 acc: 0.9198045134544373  val: loss: 434086.59375 acc: 0.777315080165863\n",
      "step: 44165 , time : 0.0\n",
      "train: loss: 10747.9072265625 acc: 0.9793399572372437  val: loss: 760344.6875 acc: 0.6974483728408813\n",
      "step: 44170 , time : 0.0\n",
      "train: loss: 21127.568359375 acc: 0.9645024538040161  val: loss: 397514.25 acc: 0.8796114325523376\n",
      "step: 44175 , time : 0.0\n",
      "train: loss: 16368.79296875 acc: 0.989874541759491  val: loss: 384014.53125 acc: 0.9465102553367615\n",
      "step: 44180 , time : 0.0\n",
      "train: loss: 6823.51025390625 acc: 0.9802679419517517  val: loss: 1513068.625 acc: 0.7440235614776611\n",
      "step: 44185 , time : 0.0\n",
      "train: loss: 20317.505859375 acc: 0.9476802945137024  val: loss: 1581080.875 acc: 0.3684324622154236\n",
      "step: 44190 , time : 0.0\n",
      "train: loss: 14175.6689453125 acc: 0.9650852680206299  val: loss: 1345684.625 acc: 0.6411056518554688\n",
      "step: 44195 , time : 0.0\n",
      "train: loss: 2845.793701171875 acc: 0.9914628863334656  val: loss: 408659.65625 acc: 0.9324911236763\n",
      "step: 44200 , time : 0.0\n",
      "train: loss: 59251.7578125 acc: 0.9655542373657227  val: loss: 2231692.25 acc: 0.7040636539459229\n",
      "step: 44205 , time : 0.0\n",
      "train: loss: 113436.265625 acc: 0.9596762657165527  val: loss: 2279271.75 acc: 0.6766287088394165\n",
      "step: 44210 , time : 0.0\n",
      "train: loss: 69252.59375 acc: 0.8939570188522339  val: loss: 642327.875 acc: 0.9303137063980103\n",
      "step: 44215 , time : 0.0010008811950683594\n",
      "train: loss: 57572.12109375 acc: 0.9723032116889954  val: loss: 1684672.0 acc: 0.29032307863235474\n",
      "step: 44220 , time : 0.0\n",
      "train: loss: 35518.06640625 acc: 0.9750616550445557  val: loss: 805975.4375 acc: 0.7267738580703735\n",
      "step: 44225 , time : 0.0\n",
      "train: loss: 29665.76953125 acc: 0.9809994697570801  val: loss: 880907.4375 acc: 0.878103494644165\n",
      "step: 44230 , time : 0.0010006427764892578\n",
      "train: loss: 15667.345703125 acc: 0.9665170907974243  val: loss: 557348.5625 acc: 0.6102263927459717\n",
      "step: 44235 , time : 0.0\n",
      "train: loss: 57725.41796875 acc: 0.9840803146362305  val: loss: 475818.03125 acc: 0.9461795091629028\n",
      "step: 44240 , time : 0.0\n",
      "train: loss: 42706.02734375 acc: 0.9915923476219177  val: loss: 873283.5 acc: 0.6230114698410034\n",
      "step: 44245 , time : 0.0\n",
      "train: loss: 25903.03125 acc: 0.9891314506530762  val: loss: 309455.8125 acc: 0.9219668507575989\n",
      "step: 44250 , time : 0.015625\n",
      "train: loss: 55828.0859375 acc: 0.9827673435211182  val: loss: 206310.203125 acc: 0.9598020911216736\n",
      "step: 44255 , time : 0.0\n",
      "train: loss: 56580.6015625 acc: 0.9732266664505005  val: loss: 288893.34375 acc: 0.9028887152671814\n",
      "step: 44260 , time : 0.0002741813659667969\n",
      "train: loss: 121983.3671875 acc: 0.9710215926170349  val: loss: 856036.75 acc: 0.7267626523971558\n",
      "step: 44265 , time : 0.0\n",
      "train: loss: 218536.84375 acc: 0.8708091378211975  val: loss: 271297.28125 acc: 0.9463956952095032\n",
      "step: 44270 , time : 0.0\n",
      "train: loss: 795743.4375 acc: 0.8299849033355713  val: loss: 1136826.875 acc: 0.5361530780792236\n",
      "step: 44275 , time : 0.0\n",
      "train: loss: 123346.5 acc: 0.9850050210952759  val: loss: 1419282.5 acc: 0.7602111101150513\n",
      "step: 44280 , time : 0.0\n",
      "train: loss: 77068.34375 acc: 0.9918828010559082  val: loss: 242991.671875 acc: 0.9695793390274048\n",
      "step: 44285 , time : 0.0010008811950683594\n",
      "train: loss: 66711.0078125 acc: 0.9913786053657532  val: loss: 257349.890625 acc: 0.9725409150123596\n",
      "step: 44290 , time : 0.0010004043579101562\n",
      "train: loss: 187356.703125 acc: 0.9692129492759705  val: loss: 1365330.875 acc: 0.6068664789199829\n",
      "step: 44295 , time : 0.0\n",
      "train: loss: 331055.71875 acc: 0.98085618019104  val: loss: 983443.5 acc: 0.941855788230896\n",
      "step: 44300 , time : 0.0\n",
      "train: loss: 339826.21875 acc: 0.9745892286300659  val: loss: 931295.125 acc: 0.8776676058769226\n",
      "step: 44305 , time : 0.0\n",
      "train: loss: 475692.6875 acc: 0.9675139784812927  val: loss: 795556.0625 acc: 0.8764190077781677\n",
      "step: 44310 , time : 0.0\n",
      "train: loss: 237036.53125 acc: 0.9462562799453735  val: loss: 651496.125 acc: 0.845889151096344\n",
      "step: 44315 , time : 0.0\n",
      "train: loss: 1078585.0 acc: 0.9441896677017212  val: loss: 1086325.25 acc: 0.9009436964988708\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 44320 , time : 0.0\n",
      "train: loss: 845458.5625 acc: 0.971718430519104  val: loss: 1189750.375 acc: 0.8254921436309814\n",
      "step: 44325 , time : 0.0\n",
      "train: loss: 1331309.125 acc: 0.9419850707054138  val: loss: 630509.0625 acc: 0.891830325126648\n",
      "step: 44330 , time : 0.0010006427764892578\n",
      "train: loss: 606900.6875 acc: 0.9582631587982178  val: loss: 909149.4375 acc: 0.8034537434577942\n",
      "step: 44335 , time : 0.0\n",
      "train: loss: 1206979.25 acc: 0.8016993999481201  val: loss: 1135490.125 acc: 0.9165768027305603\n",
      "step: 44340 , time : 0.0\n",
      "train: loss: 669218.8125 acc: 0.8909927010536194  val: loss: 533787.25 acc: 0.8796313405036926\n",
      "step: 44345 , time : 0.0010006427764892578\n",
      "train: loss: 661469.0 acc: 0.9488622546195984  val: loss: 1264508.25 acc: 0.8692518472671509\n",
      "step: 44350 , time : 0.0010008811950683594\n",
      "train: loss: 1172219.875 acc: 0.7352594137191772  val: loss: 1116003.625 acc: 0.7973662614822388\n",
      "step: 44355 , time : 0.0\n",
      "train: loss: 946216.5625 acc: 0.6607710719108582  val: loss: 1675977.375 acc: 0.8552553057670593\n",
      "step: 44360 , time : 0.0\n",
      "train: loss: 933910.8125 acc: 0.5044537782669067  val: loss: 1484001.375 acc: 0.8901444673538208\n",
      "step: 44365 , time : 0.0\n",
      "train: loss: 681885.25 acc: 0.7306069135665894  val: loss: 554959.0 acc: 0.5498367547988892\n",
      "step: 44370 , time : 0.0\n",
      "train: loss: 461235.3125 acc: 0.8676092624664307  val: loss: 1461634.75 acc: 0.7307533025741577\n",
      "step: 44375 , time : 0.0\n",
      "train: loss: 1928928.875 acc: 0.3546938896179199  val: loss: 954129.625 acc: 0.5425246357917786\n",
      "step: 44380 , time : 0.0\n",
      "train: loss: 1145089.875 acc: 0.5029595494270325  val: loss: 1147000.875 acc: 0.4744851589202881\n",
      "step: 44385 , time : 0.0\n",
      "train: loss: 457501.875 acc: 0.6160564422607422  val: loss: 1636040.625 acc: 0.7056318521499634\n",
      "step: 44390 , time : 0.0\n",
      "train: loss: 513689.6875 acc: 0.7340432405471802  val: loss: 3939873.5 acc: 0.32016420364379883\n",
      "step: 44395 , time : 0.0\n",
      "train: loss: 444724.9375 acc: 0.7780500650405884  val: loss: 912933.375 acc: 0.7807052135467529\n",
      "step: 44400 , time : 0.015625953674316406\n",
      "train: loss: 142141.96875 acc: 0.8870609998703003  val: loss: 2132605.25 acc: 0.7084284424781799\n",
      "step: 44405 , time : 0.0\n",
      "train: loss: 69873.25 acc: 0.9516790509223938  val: loss: 585285.875 acc: 0.7582624554634094\n",
      "step: 44410 , time : 0.0\n",
      "train: loss: 300111.90625 acc: 0.8537337779998779  val: loss: 1925322.375 acc: 0.7377128005027771\n",
      "step: 44415 , time : 0.0\n",
      "train: loss: 120256.4609375 acc: 0.8816428184509277  val: loss: 2572595.75 acc: 0.6431694030761719\n",
      "step: 44420 , time : 0.0\n",
      "train: loss: 52715.8359375 acc: 0.942031741142273  val: loss: 4098461.5 acc: 0.493660032749176\n",
      "step: 44425 , time : 0.0\n",
      "train: loss: 137427.328125 acc: 0.8554835319519043  val: loss: 588276.625 acc: 0.8388442993164062\n",
      "step: 44430 , time : 0.0\n",
      "train: loss: 47647.30859375 acc: 0.927200436592102  val: loss: 1437362.5 acc: 0.6770616769790649\n",
      "step: 44435 , time : 0.0\n",
      "train: loss: 173999.6875 acc: 0.8423669338226318  val: loss: 1742203.375 acc: 0.6516371965408325\n",
      "step: 44440 , time : 0.0\n",
      "train: loss: 778756.9375 acc: 0.7184445858001709  val: loss: 912651.375 acc: 0.7247034311294556\n",
      "step: 44445 , time : 0.0\n",
      "train: loss: 838666.6875 acc: 0.0796155333518982  val: loss: 997324.875 acc: 0.7489625215530396\n",
      "step: 44450 , time : 0.0010008811950683594\n",
      "train: loss: 96248.203125 acc: 0.8713611364364624  val: loss: 3306391.25 acc: 0.5864177346229553\n",
      "step: 44455 , time : 0.0010006427764892578\n",
      "train: loss: 377314.375 acc: 0.8071787357330322  val: loss: 516866.46875 acc: 0.8066980838775635\n",
      "step: 44460 , time : 0.001001119613647461\n",
      "train: loss: 914085.75 acc: 0.7260744571685791  val: loss: 1013338.625 acc: 0.7147935628890991\n",
      "step: 44465 , time : 0.0010008811950683594\n",
      "train: loss: 1370962.375 acc: 0.7442967891693115  val: loss: 1880744.75 acc: 0.7856210470199585\n",
      "step: 44470 , time : 0.0\n",
      "train: loss: 733453.1875 acc: 0.8361309170722961  val: loss: 714551.125 acc: 0.7719405889511108\n",
      "step: 44475 , time : 0.0\n",
      "train: loss: 481951.0625 acc: 0.960917592048645  val: loss: 970694.8125 acc: 0.789493203163147\n",
      "step: 44480 , time : 0.0\n",
      "train: loss: 353927.65625 acc: 0.9697815775871277  val: loss: 509713.375 acc: 0.8189558982849121\n",
      "step: 44485 , time : 0.0010006427764892578\n",
      "train: loss: 222273.578125 acc: 0.9671032428741455  val: loss: 1457967.5 acc: 0.4804961681365967\n",
      "step: 44490 , time : 0.0\n",
      "train: loss: 1812135.25 acc: 0.6312311887741089  val: loss: 1807609.0 acc: 0.63202965259552\n",
      "step: 44495 , time : 0.0\n",
      "train: loss: 165387.78125 acc: 0.9777475595474243  val: loss: 1431640.5 acc: 0.5401273965835571\n",
      "step: 44500 , time : 0.0\n",
      "train: loss: 124863.0078125 acc: 0.9906906485557556  val: loss: 1693936.75 acc: 0.4706386923789978\n",
      "step: 44505 , time : 0.0\n",
      "train: loss: 160995.8125 acc: 0.9870694875717163  val: loss: 1086327.0 acc: 0.6854705810546875\n",
      "step: 44510 , time : 0.0\n",
      "train: loss: 86923.21875 acc: 0.9890878200531006  val: loss: 1160804.75 acc: 0.6184216737747192\n",
      "step: 44515 , time : 0.0\n",
      "train: loss: 111629.234375 acc: 0.9778490662574768  val: loss: 621090.625 acc: 0.8841150403022766\n",
      "step: 44520 , time : 0.0\n",
      "train: loss: 13513.693359375 acc: 0.9915375709533691  val: loss: 853442.3125 acc: 0.821434736251831\n",
      "step: 44525 , time : 0.0\n",
      "train: loss: 7938.76513671875 acc: 0.9886907339096069  val: loss: 470443.0625 acc: 0.911914587020874\n",
      "step: 44530 , time : 0.0\n",
      "train: loss: 17426.380859375 acc: 0.9879379272460938  val: loss: 780925.6875 acc: 0.8817811012268066\n",
      "step: 44535 , time : 0.015626192092895508\n",
      "train: loss: 15291.236328125 acc: 0.9656620025634766  val: loss: 130404.859375 acc: 0.9622194766998291\n",
      "step: 44540 , time : 0.0\n",
      "train: loss: 56420.1875 acc: 0.9779694676399231  val: loss: 312223.25 acc: 0.9567922353744507\n",
      "step: 44545 , time : 0.0\n",
      "train: loss: 4496.60205078125 acc: 0.9828118681907654  val: loss: 360747.0625 acc: 0.8683630228042603\n",
      "step: 44550 , time : 0.0010004043579101562\n",
      "train: loss: 12953.7353515625 acc: 0.9354682564735413  val: loss: 776846.5625 acc: 0.5915663242340088\n",
      "step: 44555 , time : 0.0010008811950683594\n",
      "train: loss: 7158.6376953125 acc: 0.9824007749557495  val: loss: 457452.28125 acc: 0.8275020122528076\n",
      "step: 44560 , time : 0.0\n",
      "train: loss: 3819.272705078125 acc: 0.9952195286750793  val: loss: 670094.75 acc: 0.8763955235481262\n",
      "step: 44565 , time : 0.0010006427764892578\n",
      "train: loss: 14835.1953125 acc: 0.9694595336914062  val: loss: 45690.14453125 acc: 0.9779495596885681\n",
      "step: 44570 , time : 0.0003464221954345703\n",
      "train: loss: 26586.7578125 acc: 0.9842877388000488  val: loss: 315902.28125 acc: 0.8468741774559021\n",
      "step: 44575 , time : 0.0\n",
      "train: loss: 35756.36328125 acc: 0.9742748141288757  val: loss: 815006.6875 acc: 0.8952673673629761\n",
      "step: 44580 , time : 0.0\n",
      "train: loss: 23613.060546875 acc: 0.9836145043373108  val: loss: 273647.96875 acc: 0.9487082362174988\n",
      "step: 44585 , time : 0.0\n",
      "train: loss: 36446.7421875 acc: 0.9864883422851562  val: loss: 561869.1875 acc: 0.8576037883758545\n",
      "step: 44590 , time : 0.0\n",
      "train: loss: 16432.203125 acc: 0.9809426069259644  val: loss: 767911.0625 acc: 0.6208097338676453\n",
      "step: 44595 , time : 0.0\n",
      "train: loss: 8080.734375 acc: 0.9912846684455872  val: loss: 485450.03125 acc: 0.8703761696815491\n",
      "step: 44600 , time : 0.0\n",
      "train: loss: 21388.029296875 acc: 0.9903915524482727  val: loss: 493629.96875 acc: 0.9053219556808472\n",
      "step: 44605 , time : 0.0\n",
      "train: loss: 20613.533203125 acc: 0.9941807985305786  val: loss: 1467257.625 acc: 0.7145422697067261\n",
      "step: 44610 , time : 0.0\n",
      "train: loss: 75380.546875 acc: 0.981171190738678  val: loss: 756639.6875 acc: 0.9446377754211426\n",
      "step: 44615 , time : 0.0010004043579101562\n",
      "train: loss: 20082.51953125 acc: 0.9934308528900146  val: loss: 441484.96875 acc: 0.9523577690124512\n",
      "step: 44620 , time : 0.0\n",
      "train: loss: 47354.58203125 acc: 0.9499807953834534  val: loss: 242491.609375 acc: 0.9065114259719849\n",
      "step: 44625 , time : 0.0\n",
      "train: loss: 161470.078125 acc: 0.9677926898002625  val: loss: 632123.25 acc: 0.8943812847137451\n",
      "step: 44630 , time : 0.0\n",
      "train: loss: 118169.109375 acc: 0.9810596108436584  val: loss: 2726894.0 acc: 0.4764719009399414\n",
      "step: 44635 , time : 0.0\n",
      "train: loss: 65208.8203125 acc: 0.9764938354492188  val: loss: 1217329.625 acc: 0.8708134889602661\n",
      "step: 44640 , time : 0.0\n",
      "train: loss: 318755.09375 acc: 0.9374023675918579  val: loss: 705665.8125 acc: 0.9294090867042542\n",
      "step: 44645 , time : 0.0\n",
      "train: loss: 110460.671875 acc: 0.991913914680481  val: loss: 2696881.0 acc: 0.3384121060371399\n",
      "step: 44650 , time : 0.0\n",
      "train: loss: 82228.234375 acc: 0.9904837608337402  val: loss: 3685749.75 acc: 0.7464781999588013\n",
      "step: 44655 , time : 0.0\n",
      "train: loss: 107569.7109375 acc: 0.9842440485954285  val: loss: 1102809.75 acc: 0.8530629873275757\n",
      "step: 44660 , time : 0.0\n",
      "train: loss: 178837.9375 acc: 0.9815898537635803  val: loss: 338243.15625 acc: 0.9510580897331238\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 44665 , time : 0.0\n",
      "train: loss: 706386.5 acc: 0.9389902353286743  val: loss: 521999.46875 acc: 0.825026273727417\n",
      "step: 44670 , time : 0.001001119613647461\n",
      "train: loss: 309506.4375 acc: 0.9799299240112305  val: loss: 622242.5625 acc: 0.881925642490387\n",
      "step: 44675 , time : 0.0010008811950683594\n",
      "train: loss: 273797.34375 acc: 0.9652853608131409  val: loss: 2325267.5 acc: 0.6682062149047852\n",
      "step: 44680 , time : 0.0\n",
      "train: loss: 995048.5 acc: 0.9684273600578308  val: loss: 1843478.125 acc: 0.67733234167099\n",
      "step: 44685 , time : 0.0010006427764892578\n",
      "train: loss: 1050671.875 acc: 0.9691295027732849  val: loss: 783600.25 acc: 0.8676609992980957\n",
      "step: 44690 , time : 0.0010004043579101562\n",
      "train: loss: 960955.25 acc: 0.9703669548034668  val: loss: 466292.28125 acc: 0.8946407437324524\n",
      "step: 44695 , time : 0.0\n",
      "train: loss: 990180.375 acc: 0.9494464993476868  val: loss: 1215019.5 acc: 0.8297160267829895\n",
      "step: 44700 , time : 0.0\n",
      "train: loss: 446221.65625 acc: 0.9741258025169373  val: loss: 473813.96875 acc: 0.7545839548110962\n",
      "step: 44705 , time : 0.0\n",
      "train: loss: 991277.0 acc: 0.8875673413276672  val: loss: 1094730.0 acc: -0.23923885822296143\n",
      "step: 44710 , time : 0.0\n",
      "train: loss: 777310.4375 acc: 0.7780675292015076  val: loss: 4470787.5 acc: 0.027529656887054443\n",
      "step: 44715 , time : 0.0\n",
      "train: loss: 602816.1875 acc: 0.8898640871047974  val: loss: 1587022.25 acc: -0.009966135025024414\n",
      "step: 44720 , time : 0.0006463527679443359\n",
      "train: loss: 622111.9375 acc: 0.7920525074005127  val: loss: 590526.4375 acc: 0.8607288599014282\n",
      "step: 44725 , time : 0.0\n",
      "train: loss: 255825.765625 acc: 0.8827850222587585  val: loss: 685333.0625 acc: 0.8220292329788208\n",
      "step: 44730 , time : 0.0010006427764892578\n",
      "train: loss: 288912.1875 acc: 0.8772286772727966  val: loss: 628360.0625 acc: 0.8888528943061829\n",
      "step: 44735 , time : 0.001001119613647461\n",
      "train: loss: 1575125.125 acc: 0.7643600106239319  val: loss: 643455.1875 acc: 0.8158483505249023\n",
      "step: 44740 , time : 0.0\n",
      "train: loss: 1116623.625 acc: 0.5683498382568359  val: loss: 953100.5625 acc: 0.768054187297821\n",
      "step: 44745 , time : 0.0\n",
      "train: loss: 743009.125 acc: 0.7065809369087219  val: loss: 665176.1875 acc: 0.6869980096817017\n",
      "step: 44750 , time : 0.0\n",
      "train: loss: 337531.71875 acc: 0.8103231191635132  val: loss: 3479531.0 acc: 0.6992509365081787\n",
      "step: 44755 , time : 0.0\n",
      "train: loss: 285193.65625 acc: 0.8171548247337341  val: loss: 2555915.25 acc: 0.6544637084007263\n",
      "step: 44760 , time : 0.0\n",
      "train: loss: 69653.1484375 acc: 0.9390769600868225  val: loss: 864309.3125 acc: 0.724418044090271\n",
      "step: 44765 , time : 0.0\n",
      "train: loss: 77656.6171875 acc: 0.9267081022262573  val: loss: 1719095.375 acc: 0.6483449935913086\n",
      "step: 44770 , time : 0.0\n",
      "train: loss: 7898.46533203125 acc: 0.9932051301002502  val: loss: 2064217.375 acc: 0.5817198753356934\n",
      "step: 44775 , time : 0.0\n",
      "train: loss: 7732.6865234375 acc: 0.9937030673027039  val: loss: 1968829.0 acc: 0.6954046487808228\n",
      "step: 44780 , time : 0.0\n",
      "train: loss: 226797.53125 acc: 0.8674687743186951  val: loss: 2175423.25 acc: 0.7436343431472778\n",
      "step: 44785 , time : 0.0010006427764892578\n",
      "train: loss: 231192.921875 acc: 0.8493282198905945  val: loss: 832132.75 acc: 0.7245092988014221\n",
      "step: 44790 , time : 0.0010008811950683594\n",
      "train: loss: 43137.08984375 acc: 0.951036810874939  val: loss: 3277656.25 acc: 0.6257989406585693\n",
      "step: 44795 , time : 0.0010004043579101562\n",
      "train: loss: 76116.375 acc: 0.922005832195282  val: loss: 2222535.75 acc: 0.5816847085952759\n",
      "step: 44800 , time : 0.0\n",
      "train: loss: 152956.1875 acc: 0.8787423372268677  val: loss: 1122109.375 acc: 0.7554424405097961\n",
      "step: 44805 , time : 0.0\n",
      "train: loss: 467476.75 acc: 0.7569568157196045  val: loss: 888477.0 acc: 0.7026777267456055\n",
      "step: 44810 , time : 0.0\n",
      "train: loss: 305166.5625 acc: 0.8375856876373291  val: loss: 3232403.75 acc: 0.5726528167724609\n",
      "step: 44815 , time : 0.0\n",
      "train: loss: 266493.125 acc: 0.7821413278579712  val: loss: 1868706.5 acc: 0.6928931474685669\n",
      "step: 44820 , time : 0.0\n",
      "train: loss: 137918.921875 acc: 0.8880859017372131  val: loss: 847304.0625 acc: 0.7226324081420898\n",
      "step: 44825 , time : 0.0\n",
      "train: loss: 323439.90625 acc: 0.825268566608429  val: loss: 1339581.875 acc: 0.7586513757705688\n",
      "step: 44830 , time : 0.0\n",
      "train: loss: 1192326.625 acc: 0.7625499963760376  val: loss: 226069.796875 acc: 0.8489349484443665\n",
      "step: 44835 , time : 0.0\n",
      "train: loss: 1317304.25 acc: 0.8115760684013367  val: loss: 952328.125 acc: 0.8178948760032654\n",
      "step: 44840 , time : 0.0\n",
      "train: loss: 524417.6875 acc: 0.9533902406692505  val: loss: 1889997.25 acc: 0.7102752923965454\n",
      "step: 44845 , time : 0.0\n",
      "train: loss: 1165434.75 acc: 0.8165984153747559  val: loss: 575849.0625 acc: 0.7022982835769653\n",
      "step: 44850 , time : 0.0\n",
      "train: loss: 227124.0625 acc: 0.9709986448287964  val: loss: 509902.65625 acc: 0.8659014701843262\n",
      "step: 44855 , time : 0.0\n",
      "train: loss: 269846.875 acc: 0.9594493508338928  val: loss: 1096706.25 acc: 0.7947852611541748\n",
      "step: 44860 , time : 0.0010008811950683594\n",
      "train: loss: 115949.421875 acc: 0.9886674880981445  val: loss: 1584674.5 acc: 0.560839056968689\n",
      "step: 44865 , time : 0.0\n",
      "train: loss: 397522.75 acc: 0.9713162779808044  val: loss: 52657.72265625 acc: 0.9468533992767334\n",
      "step: 44870 , time : 0.0\n",
      "train: loss: 76036.703125 acc: 0.9941725134849548  val: loss: 179397.265625 acc: 0.9081093668937683\n",
      "step: 44875 , time : 0.0010004043579101562\n",
      "train: loss: 88810.6640625 acc: 0.9912694096565247  val: loss: 760738.125 acc: 0.7466650009155273\n",
      "step: 44880 , time : 0.0\n",
      "train: loss: 37608.12890625 acc: 0.9930943846702576  val: loss: 278665.125 acc: 0.9663347005844116\n",
      "step: 44885 , time : 0.0\n",
      "train: loss: 28444.853515625 acc: 0.9874279499053955  val: loss: 867538.5625 acc: 0.807180643081665\n",
      "step: 44890 , time : 0.0010008811950683594\n",
      "train: loss: 60509.75390625 acc: 0.9808509349822998  val: loss: 570822.8125 acc: 0.7978443503379822\n",
      "step: 44895 , time : 0.0010008811950683594\n",
      "train: loss: 39129.5 acc: 0.9820858836174011  val: loss: 417526.46875 acc: 0.7357462048530579\n",
      "step: 44900 , time : 0.0010008811950683594\n",
      "train: loss: 10915.5654296875 acc: 0.9811304211616516  val: loss: 228142.296875 acc: 0.9588668346405029\n",
      "step: 44905 , time : 0.0010008811950683594\n",
      "train: loss: 14243.095703125 acc: 0.9920393228530884  val: loss: 397112.71875 acc: 0.9415665864944458\n",
      "step: 44910 , time : 0.0\n",
      "train: loss: 8246.052734375 acc: 0.9656809568405151  val: loss: 307693.3125 acc: 0.9726184010505676\n",
      "step: 44915 , time : 0.0\n",
      "train: loss: 1906.6749267578125 acc: 0.9963217377662659  val: loss: 219004.046875 acc: 0.9711355566978455\n",
      "step: 44920 , time : 0.0\n",
      "train: loss: 183252.234375 acc: 0.5873598456382751  val: loss: 402305.09375 acc: 0.9252210259437561\n",
      "step: 44925 , time : 0.0\n",
      "train: loss: 3995.34033203125 acc: 0.9863225817680359  val: loss: 253973.078125 acc: 0.9716381430625916\n",
      "step: 44930 , time : 0.0\n",
      "train: loss: 24355.025390625 acc: 0.9693009257316589  val: loss: 311809.8125 acc: 0.960088849067688\n",
      "step: 44935 , time : 0.0\n",
      "train: loss: 52825.8203125 acc: 0.9797776937484741  val: loss: 435195.34375 acc: 0.9605777859687805\n",
      "step: 44940 , time : 0.0\n",
      "train: loss: 27355.576171875 acc: 0.9823898673057556  val: loss: 1713200.0 acc: 0.7445918321609497\n",
      "step: 44945 , time : 0.0\n",
      "train: loss: 16004.8271484375 acc: 0.988461434841156  val: loss: 790792.8125 acc: 0.7302837371826172\n",
      "step: 44950 , time : 0.0\n",
      "train: loss: 15281.8466796875 acc: 0.9927476048469543  val: loss: 824175.875 acc: 0.9273517727851868\n",
      "step: 44955 , time : 0.015625476837158203\n",
      "train: loss: 11490.1005859375 acc: 0.988222062587738  val: loss: 1134950.75 acc: 0.644512951374054\n",
      "step: 44960 , time : 0.0\n",
      "train: loss: 7425.365234375 acc: 0.9942249655723572  val: loss: 2289741.5 acc: 0.7375873327255249\n",
      "step: 44965 , time : 0.0\n",
      "train: loss: 11456.0009765625 acc: 0.9952113628387451  val: loss: 2673342.25 acc: 0.2702140212059021\n",
      "step: 44970 , time : 0.0\n",
      "train: loss: 27595.525390625 acc: 0.9931918382644653  val: loss: 1013005.5 acc: 0.6708979606628418\n",
      "step: 44975 , time : 0.0\n",
      "train: loss: 62346.34765625 acc: 0.9882768392562866  val: loss: 3557800.75 acc: 0.603654146194458\n",
      "step: 44980 , time : 0.0\n",
      "train: loss: 28487.900390625 acc: 0.9803703427314758  val: loss: 1364970.375 acc: 0.7842668294906616\n",
      "step: 44985 , time : 0.0\n",
      "train: loss: 62676.1015625 acc: 0.9801319241523743  val: loss: 1398738.5 acc: 0.7346593737602234\n",
      "step: 44990 , time : 0.0\n",
      "train: loss: 183873.265625 acc: 0.9614024758338928  val: loss: 1125026.875 acc: 0.8636558055877686\n",
      "step: 44995 , time : 0.0\n",
      "train: loss: 66061.1015625 acc: 0.9804593920707703  val: loss: 2132583.75 acc: 0.6999120712280273\n",
      "step: 45000 , time : 0.0\n",
      "train: loss: 64997.60546875 acc: 0.9554256200790405  val: loss: 1195931.5 acc: 0.847199559211731\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 45005 , time : 0.0\n",
      "train: loss: 823460.0 acc: 0.687725841999054  val: loss: 2633512.5 acc: 0.6232628226280212\n",
      "step: 45010 , time : 0.0\n",
      "train: loss: 56808.62890625 acc: 0.9930029511451721  val: loss: 417635.28125 acc: 0.6210941076278687\n",
      "step: 45015 , time : 0.0010006427764892578\n",
      "train: loss: 191889.328125 acc: 0.981001079082489  val: loss: 1455728.625 acc: 0.06223100423812866\n",
      "step: 45020 , time : 0.001001119613647461\n",
      "train: loss: 679171.9375 acc: 0.9160296320915222  val: loss: 1123248.25 acc: 0.6354957818984985\n",
      "step: 45025 , time : 0.0010006427764892578\n",
      "train: loss: 373326.78125 acc: 0.9614258408546448  val: loss: 818656.25 acc: 0.6814257502555847\n",
      "step: 45030 , time : 0.0\n",
      "train: loss: 589955.5 acc: 0.9636772871017456  val: loss: 1281964.25 acc: 0.7158902883529663\n",
      "step: 45035 , time : 0.0\n",
      "train: loss: 816124.6875 acc: 0.9352148771286011  val: loss: 1281150.25 acc: 0.7981497049331665\n",
      "step: 45040 , time : 0.0\n",
      "train: loss: 245436.09375 acc: 0.968908429145813  val: loss: 4920957.0 acc: 0.6511681079864502\n",
      "step: 45045 , time : 0.001001119613647461\n",
      "train: loss: 1048552.8125 acc: 0.9609872102737427  val: loss: 369374.53125 acc: 0.9498891234397888\n",
      "step: 45050 , time : 0.001001119613647461\n",
      "train: loss: 1074822.0 acc: 0.974765956401825  val: loss: 1857362.0 acc: 0.6248493194580078\n",
      "step: 45055 , time : 0.0\n",
      "train: loss: 666457.0 acc: 0.969231128692627  val: loss: 1253426.625 acc: 0.7908691763877869\n",
      "step: 45060 , time : 0.0\n",
      "train: loss: 528991.5625 acc: 0.9643263816833496  val: loss: 339353.71875 acc: 0.8173442482948303\n",
      "step: 45065 , time : 0.0\n",
      "train: loss: 196722.71875 acc: 0.983894407749176  val: loss: 1149841.125 acc: 0.8580262660980225\n",
      "step: 45070 , time : 0.0\n",
      "train: loss: 228507.734375 acc: 0.9788646101951599  val: loss: 328318.96875 acc: 0.9354055523872375\n",
      "step: 45075 , time : 0.0\n",
      "train: loss: 857017.375 acc: 0.8677447438240051  val: loss: 832670.625 acc: 0.8253472447395325\n",
      "step: 45080 , time : 0.0\n",
      "train: loss: 400434.1875 acc: 0.9056075811386108  val: loss: 3578903.5 acc: 0.3047880530357361\n",
      "step: 45085 , time : 0.0\n",
      "train: loss: 2434118.5 acc: -0.4664421081542969  val: loss: 1191190.0 acc: 0.7094171643257141\n",
      "step: 45090 , time : 0.0\n",
      "train: loss: 926286.25 acc: 0.7609353065490723  val: loss: 1851781.75 acc: 0.7323253750801086\n",
      "step: 45095 , time : 0.0\n",
      "train: loss: 351596.875 acc: 0.8262240886688232  val: loss: 1207630.375 acc: 0.8090268969535828\n",
      "step: 45100 , time : 0.0\n",
      "train: loss: 475790.6875 acc: 0.8223412036895752  val: loss: 595965.875 acc: 0.8418304920196533\n",
      "step: 45105 , time : 0.0\n",
      "train: loss: 1289631.375 acc: 0.7122517824172974  val: loss: 796246.0 acc: 0.810950756072998\n",
      "step: 45110 , time : 0.0\n",
      "train: loss: 1625594.0 acc: 0.19517409801483154  val: loss: 1475358.75 acc: 0.5986417531967163\n",
      "step: 45115 , time : 0.0\n",
      "train: loss: 993439.8125 acc: 0.8221255540847778  val: loss: 1904585.5 acc: 0.5632873773574829\n",
      "step: 45120 , time : 0.0\n",
      "train: loss: 430373.1875 acc: 0.6673673987388611  val: loss: 2208885.0 acc: 0.7926332950592041\n",
      "step: 45125 , time : 0.0\n",
      "train: loss: 292325.625 acc: 0.786769688129425  val: loss: 1490515.75 acc: 0.7556459307670593\n",
      "step: 45130 , time : 0.0\n",
      "train: loss: 155128.5625 acc: 0.8753734827041626  val: loss: 1192091.5 acc: 0.8439622521400452\n",
      "step: 45135 , time : 0.0010004043579101562\n",
      "train: loss: 186888.8125 acc: 0.8864301443099976  val: loss: 429756.25 acc: 0.9170637726783752\n",
      "step: 45140 , time : 0.0\n",
      "train: loss: 234972.0 acc: 0.8646572828292847  val: loss: 1279916.0 acc: 0.8231199979782104\n",
      "step: 45145 , time : 0.0\n",
      "train: loss: 259870.359375 acc: 0.8514535427093506  val: loss: 598874.5625 acc: 0.6823926568031311\n",
      "step: 45150 , time : 0.0\n",
      "train: loss: 103069.5546875 acc: 0.9108670353889465  val: loss: 552831.0625 acc: 0.7068524360656738\n",
      "step: 45155 , time : 0.0\n",
      "train: loss: 206826.71875 acc: 0.7797480821609497  val: loss: 196858.71875 acc: 0.9143714308738708\n",
      "step: 45160 , time : 0.0\n",
      "train: loss: 55392.43359375 acc: 0.9245073795318604  val: loss: 2072334.25 acc: 0.7889084219932556\n",
      "step: 45165 , time : 0.0\n",
      "train: loss: 265727.5 acc: 0.8206914663314819  val: loss: 1330092.25 acc: 0.8122985363006592\n",
      "step: 45170 , time : 0.0\n",
      "train: loss: 529901.0 acc: 0.7649126648902893  val: loss: 823696.75 acc: 0.7078369855880737\n",
      "step: 45175 , time : 0.0\n",
      "train: loss: 339425.09375 acc: 0.7222402095794678  val: loss: 958542.75 acc: 0.7483325004577637\n",
      "step: 45180 , time : 0.0\n",
      "train: loss: 535047.9375 acc: 0.6302890181541443  val: loss: 1114302.25 acc: 0.6918714046478271\n",
      "step: 45185 , time : 0.0\n",
      "train: loss: 573259.875 acc: 0.7448337078094482  val: loss: 407959.09375 acc: 0.7941986918449402\n",
      "step: 45190 , time : 0.0\n",
      "train: loss: 1000641.375 acc: 0.7090374827384949  val: loss: 1452812.0 acc: 0.6764637231826782\n",
      "step: 45195 , time : 0.0\n",
      "train: loss: 1555498.875 acc: 0.70426344871521  val: loss: 1269409.125 acc: 0.79613196849823\n",
      "step: 45200 , time : 0.0\n",
      "train: loss: 883623.0625 acc: 0.7109681367874146  val: loss: 424417.125 acc: 0.8269375562667847\n",
      "step: 45205 , time : 0.0\n",
      "train: loss: 976853.375 acc: 0.9284155964851379  val: loss: 468677.15625 acc: 0.9035048484802246\n",
      "step: 45210 , time : 0.0\n",
      "train: loss: 496348.84375 acc: 0.9293694496154785  val: loss: 990204.5625 acc: 0.904119610786438\n",
      "step: 45215 , time : 0.0\n",
      "train: loss: 361866.25 acc: 0.9214476346969604  val: loss: 660928.75 acc: 0.872363805770874\n",
      "step: 45220 , time : 0.0\n",
      "train: loss: 246911.4375 acc: 0.8861884474754333  val: loss: 1855294.5 acc: 0.8192931413650513\n",
      "step: 45225 , time : 0.0\n",
      "train: loss: 167177.078125 acc: 0.9834606051445007  val: loss: 1132466.25 acc: 0.8080692291259766\n",
      "step: 45230 , time : 0.0\n",
      "train: loss: 172025.796875 acc: 0.9871225953102112  val: loss: 602735.5 acc: 0.9134721159934998\n",
      "step: 45235 , time : 0.0\n",
      "train: loss: 114374.265625 acc: 0.9907075762748718  val: loss: 776974.625 acc: 0.9120666980743408\n",
      "step: 45240 , time : 0.0\n",
      "train: loss: 265423.1875 acc: 0.9740517735481262  val: loss: 1822424.0 acc: 0.8346040844917297\n",
      "step: 45245 , time : 0.0010006427764892578\n",
      "train: loss: 281516.1875 acc: 0.9452006220817566  val: loss: 513541.90625 acc: 0.9508609175682068\n",
      "step: 45250 , time : 0.0\n",
      "train: loss: 90799.828125 acc: 0.9426258206367493  val: loss: 417041.9375 acc: 0.8942644596099854\n",
      "step: 45255 , time : 0.0\n",
      "train: loss: 102523.265625 acc: 0.9003464579582214  val: loss: 1503721.625 acc: 0.660372257232666\n",
      "step: 45260 , time : 0.0\n",
      "train: loss: 74024.4765625 acc: 0.9677668809890747  val: loss: 2036457.0 acc: 0.6960793733596802\n",
      "step: 45265 , time : 0.0\n",
      "train: loss: 39586.83984375 acc: 0.984116792678833  val: loss: 1419654.75 acc: 0.7863547801971436\n",
      "step: 45270 , time : 0.0\n",
      "train: loss: 106999.3203125 acc: 0.8806555271148682  val: loss: 1039276.9375 acc: 0.6225744485855103\n",
      "step: 45275 , time : 0.0\n",
      "train: loss: 17457.869140625 acc: 0.9702461957931519  val: loss: 1517896.25 acc: 0.6943178176879883\n",
      "step: 45280 , time : 0.0\n",
      "train: loss: 59418.734375 acc: 0.9187635183334351  val: loss: 1084510.375 acc: 0.8791009187698364\n",
      "step: 45285 , time : 0.0\n",
      "train: loss: 17871.697265625 acc: 0.9672223925590515  val: loss: 957393.75 acc: 0.9037473201751709\n",
      "step: 45290 , time : 0.0\n",
      "train: loss: 10355.1416015625 acc: 0.9792491793632507  val: loss: 1309196.625 acc: 0.801268994808197\n",
      "step: 45295 , time : 0.0\n",
      "train: loss: 8923.09375 acc: 0.988884687423706  val: loss: 590407.8125 acc: 0.9280645251274109\n",
      "step: 45300 , time : 0.0\n",
      "train: loss: 56156.80078125 acc: 0.9645471572875977  val: loss: 1485471.625 acc: 0.6242095232009888\n",
      "step: 45305 , time : 0.0\n",
      "train: loss: 17024.27734375 acc: 0.9840781092643738  val: loss: 361964.09375 acc: 0.9483841061592102\n",
      "step: 45310 , time : 0.0\n",
      "train: loss: 29488.873046875 acc: 0.9812254905700684  val: loss: 3941361.0 acc: -0.6597295999526978\n",
      "step: 45315 , time : 0.0010004043579101562\n",
      "train: loss: 28366.486328125 acc: 0.9845350384712219  val: loss: 2169682.25 acc: 0.5913663506507874\n",
      "step: 45320 , time : 0.0\n",
      "train: loss: 16381.494140625 acc: 0.980423092842102  val: loss: 572714.5 acc: 0.8096747398376465\n",
      "step: 45325 , time : 0.0\n",
      "train: loss: 35550.84375 acc: 0.9761698246002197  val: loss: 2641808.0 acc: 0.6630550622940063\n",
      "step: 45330 , time : 0.0\n",
      "train: loss: 3900.440673828125 acc: 0.998071014881134  val: loss: 1111407.25 acc: 0.8757888674736023\n",
      "step: 45335 , time : 0.0\n",
      "train: loss: 40060.86328125 acc: 0.9876531362533569  val: loss: 1251796.5 acc: 0.8439164757728577\n",
      "step: 45340 , time : 0.0010006427764892578\n",
      "train: loss: 18412.080078125 acc: 0.9931679964065552  val: loss: 974869.375 acc: 0.7527226209640503\n",
      "step: 45345 , time : 0.0010006427764892578\n",
      "train: loss: 29635.009765625 acc: 0.9905351996421814  val: loss: 768535.0625 acc: 0.8133183717727661\n",
      "step: 45350 , time : 0.0\n",
      "train: loss: 34494.921875 acc: 0.9938386678695679  val: loss: 1231587.5 acc: 0.47852760553359985\n",
      "step: 45355 , time : 0.0\n",
      "train: loss: 62543.39453125 acc: 0.9867647886276245  val: loss: 2851924.5 acc: -0.0573960542678833\n",
      "step: 45360 , time : 0.0010004043579101562\n",
      "train: loss: 175341.703125 acc: 0.94605553150177  val: loss: 380259.125 acc: 0.9204682111740112\n",
      "step: 45365 , time : 0.0\n",
      "train: loss: 259325.875 acc: 0.9147616624832153  val: loss: 889443.75 acc: 0.8587735891342163\n",
      "step: 45370 , time : 0.0\n",
      "train: loss: 528221.0 acc: 0.8773654699325562  val: loss: 889523.75 acc: 0.6531058549880981\n",
      "step: 45375 , time : 0.0\n",
      "train: loss: 397290.96875 acc: 0.9603641033172607  val: loss: 1358012.0 acc: 0.5499629974365234\n",
      "step: 45380 , time : 0.0\n",
      "train: loss: 675946.6875 acc: 0.8768227100372314  val: loss: 1411854.75 acc: 0.8598630428314209\n",
      "step: 45385 , time : 0.0\n",
      "train: loss: 59547.28515625 acc: 0.9889843463897705  val: loss: 4384532.0 acc: -0.7323414087295532\n",
      "step: 45390 , time : 0.0\n",
      "train: loss: 189465.421875 acc: 0.9664835333824158  val: loss: 2316243.25 acc: 0.7009532451629639\n",
      "step: 45395 , time : 0.0\n",
      "train: loss: 498450.78125 acc: 0.9610579013824463  val: loss: 1288671.5 acc: 0.786567211151123\n",
      "step: 45400 , time : 0.0\n",
      "train: loss: 2143614.25 acc: 0.7651626467704773  val: loss: 1017183.875 acc: 0.6091967821121216\n",
      "step: 45405 , time : 0.0\n",
      "train: loss: 258803.890625 acc: 0.9578126668930054  val: loss: 2232412.0 acc: 0.4872371554374695\n",
      "step: 45410 , time : 0.01562643051147461\n",
      "train: loss: 680978.75 acc: 0.9290277361869812  val: loss: 781483.8125 acc: 0.7789586186408997\n",
      "step: 45415 , time : 0.0\n",
      "train: loss: 493717.28125 acc: 0.9781636595726013  val: loss: 751304.5 acc: 0.889569103717804\n",
      "step: 45420 , time : 0.0\n",
      "train: loss: 826698.125 acc: 0.9650430679321289  val: loss: 220649.40625 acc: 0.9581478238105774\n",
      "step: 45425 , time : 0.0\n",
      "train: loss: 1021098.0 acc: 0.9624174237251282  val: loss: 1051441.25 acc: 0.8931673169136047\n",
      "step: 45430 , time : 0.0\n",
      "train: loss: 1474034.0 acc: 0.9161368012428284  val: loss: 1082176.125 acc: 0.709067165851593\n",
      "step: 45435 , time : 0.0\n",
      "train: loss: 519234.03125 acc: 0.9417065978050232  val: loss: 305393.65625 acc: 0.8707062005996704\n",
      "step: 45440 , time : 0.0\n",
      "train: loss: 267533.59375 acc: 0.9560937881469727  val: loss: 870086.625 acc: 0.6388115882873535\n",
      "step: 45445 , time : 0.0\n",
      "train: loss: 208349.296875 acc: 0.9542089104652405  val: loss: 368221.28125 acc: 0.8655450344085693\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 45450 , time : 0.0\n",
      "train: loss: 928564.3125 acc: 0.520804226398468  val: loss: 855567.875 acc: 0.9009393453598022\n",
      "step: 45455 , time : 0.001001119613647461\n",
      "train: loss: 864003.1875 acc: 0.7433575391769409  val: loss: 995780.25 acc: 0.6909515261650085\n",
      "step: 45460 , time : 0.0010006427764892578\n",
      "train: loss: 1349871.75 acc: 0.5991889238357544  val: loss: 1396643.75 acc: 0.8432878255844116\n",
      "step: 45465 , time : 0.0010004043579101562\n",
      "train: loss: 820061.1875 acc: 0.8064302206039429  val: loss: 785380.75 acc: 0.7688511610031128\n",
      "step: 45470 , time : 0.0010004043579101562\n",
      "train: loss: 805070.5625 acc: 0.6614132523536682  val: loss: 504551.28125 acc: 0.7501309514045715\n",
      "step: 45475 , time : 0.0\n",
      "train: loss: 1690967.375 acc: -0.00502169132232666  val: loss: 1305037.25 acc: 0.2568298578262329\n",
      "step: 45480 , time : 0.0010008811950683594\n",
      "train: loss: 1525526.625 acc: 0.4573765993118286  val: loss: 1631821.25 acc: 0.4468117356300354\n",
      "step: 45485 , time : 0.0\n",
      "train: loss: 955278.375 acc: 0.23101770877838135  val: loss: 1911813.25 acc: 0.6689332723617554\n",
      "step: 45490 , time : 0.0\n",
      "train: loss: 911091.4375 acc: 0.1685890555381775  val: loss: 795239.125 acc: 0.8263532519340515\n",
      "step: 45495 , time : 0.0\n",
      "train: loss: 239546.5 acc: 0.8240329027175903  val: loss: 577996.375 acc: 0.8126819133758545\n",
      "step: 45500 , time : 0.0\n",
      "train: loss: 367948.375 acc: 0.7089996337890625  val: loss: 1472577.125 acc: 0.7691746354103088\n",
      "step: 45505 , time : 0.0\n",
      "train: loss: 197275.609375 acc: 0.8765099048614502  val: loss: 663162.125 acc: 0.825743556022644\n",
      "step: 45510 , time : 0.0\n",
      "train: loss: 406842.9375 acc: 0.8055008053779602  val: loss: 1522979.75 acc: 0.8180116415023804\n",
      "step: 45515 , time : 0.0\n",
      "train: loss: 156747.5625 acc: 0.8693742156028748  val: loss: 425607.0625 acc: 0.7917494773864746\n",
      "step: 45520 , time : 0.0\n",
      "train: loss: 137231.0 acc: 0.8535591959953308  val: loss: 1491103.5 acc: 0.7723487615585327\n",
      "step: 45525 , time : 0.0\n",
      "train: loss: 234932.34375 acc: 0.7461061477661133  val: loss: 882373.125 acc: 0.7655850648880005\n",
      "step: 45530 , time : 0.0\n",
      "train: loss: 107216.1171875 acc: 0.9176956415176392  val: loss: 294635.03125 acc: 0.8657283782958984\n",
      "step: 45535 , time : 0.0\n",
      "train: loss: 335076.875 acc: 0.6748719811439514  val: loss: 3483289.75 acc: 0.4413224458694458\n",
      "step: 45540 , time : 0.0\n",
      "train: loss: 514142.1875 acc: 0.48441416025161743  val: loss: 2695903.75 acc: 0.7584023475646973\n",
      "step: 45545 , time : 0.0\n",
      "train: loss: 783853.75 acc: 0.6637014150619507  val: loss: 2319972.0 acc: 0.7428274154663086\n",
      "step: 45550 , time : 0.0\n",
      "train: loss: 364559.3125 acc: 0.618450403213501  val: loss: 366328.78125 acc: 0.7960084080696106\n",
      "step: 45555 , time : 0.0\n",
      "train: loss: 266103.875 acc: 0.8054080605506897  val: loss: 2131411.25 acc: 0.7020704746246338\n",
      "step: 45560 , time : 0.0\n",
      "train: loss: 781743.5625 acc: 0.7548500895500183  val: loss: 1422226.75 acc: 0.793052613735199\n",
      "step: 45565 , time : 0.0\n",
      "train: loss: 958737.875 acc: 0.8366299867630005  val: loss: 1193719.25 acc: 0.8762000799179077\n",
      "step: 45570 , time : 0.0\n",
      "train: loss: 1084633.5 acc: 0.8992627859115601  val: loss: 2640528.0 acc: 0.7573758959770203\n",
      "step: 45575 , time : 0.0010004043579101562\n",
      "train: loss: 894886.1875 acc: 0.8930099010467529  val: loss: 1230283.0 acc: 0.7901979684829712\n",
      "step: 45580 , time : 0.0\n",
      "train: loss: 879840.75 acc: 0.8914952874183655  val: loss: 915216.0 acc: 0.7916086316108704\n",
      "step: 45585 , time : 0.0\n",
      "train: loss: 403711.3125 acc: 0.9480751156806946  val: loss: 1261446.25 acc: 0.8724408745765686\n",
      "step: 45590 , time : 0.0\n",
      "train: loss: 356322.90625 acc: 0.946797251701355  val: loss: 2107929.5 acc: -0.029628396034240723\n",
      "step: 45595 , time : 0.0\n",
      "train: loss: 184685.890625 acc: 0.9860721826553345  val: loss: 460426.71875 acc: 0.9432573914527893\n",
      "step: 45600 , time : 0.0\n",
      "train: loss: 197534.15625 acc: 0.9865005016326904  val: loss: 2797756.5 acc: -0.6864043474197388\n",
      "step: 45605 , time : 0.0\n",
      "train: loss: 374322.84375 acc: 0.9535282850265503  val: loss: 1963350.5 acc: 0.798250675201416\n",
      "step: 45610 , time : 0.0\n",
      "train: loss: 163942.640625 acc: 0.9632651209831238  val: loss: 787083.125 acc: 0.9105013012886047\n",
      "step: 45615 , time : 0.0\n",
      "train: loss: 79159.203125 acc: 0.966765820980072  val: loss: 899035.625 acc: 0.8835147619247437\n",
      "step: 45620 , time : 0.0\n",
      "train: loss: 65488.9609375 acc: 0.9693282842636108  val: loss: 1287523.0 acc: 0.7996776700019836\n",
      "step: 45625 , time : 0.0\n",
      "train: loss: 59154.00390625 acc: 0.9740522503852844  val: loss: 1170888.75 acc: 0.7024117708206177\n",
      "step: 45630 , time : 0.0\n",
      "train: loss: 82758.671875 acc: 0.9476615786552429  val: loss: 937697.5 acc: 0.883956789970398\n",
      "step: 45635 , time : 0.0\n",
      "train: loss: 22106.982421875 acc: 0.9850946068763733  val: loss: 1618607.75 acc: 0.6421533823013306\n",
      "step: 45640 , time : 0.0\n",
      "train: loss: 6356.45458984375 acc: 0.9733350872993469  val: loss: 777859.125 acc: 0.7055773138999939\n",
      "step: 45645 , time : 0.0\n",
      "train: loss: 16227.5224609375 acc: 0.9512524008750916  val: loss: 1261158.25 acc: 0.7152892351150513\n",
      "step: 45650 , time : 0.0\n",
      "train: loss: 5811.3232421875 acc: 0.9839521646499634  val: loss: 1991370.75 acc: 0.0025652050971984863\n",
      "step: 45655 , time : 0.0\n",
      "train: loss: 6654.0908203125 acc: 0.9887864589691162  val: loss: 752568.625 acc: 0.8705778121948242\n",
      "step: 45660 , time : 0.0\n",
      "train: loss: 21688.490234375 acc: 0.9770860075950623  val: loss: 991771.0 acc: 0.8660668730735779\n",
      "step: 45665 , time : 0.0\n",
      "train: loss: 50289.4375 acc: 0.9602706432342529  val: loss: 1027235.625 acc: 0.7062475681304932\n",
      "step: 45670 , time : 0.0\n",
      "train: loss: 75291.234375 acc: 0.9703415036201477  val: loss: 2560706.5 acc: 0.663152813911438\n",
      "step: 45675 , time : 0.0010006427764892578\n",
      "train: loss: 36703.265625 acc: 0.97727370262146  val: loss: 2116857.0 acc: 0.4685378074645996\n",
      "step: 45680 , time : 0.0\n",
      "train: loss: 147113.40625 acc: 0.884914755821228  val: loss: 1099322.375 acc: 0.7720392942428589\n",
      "step: 45685 , time : 0.0\n",
      "train: loss: 22967.962890625 acc: 0.9886675477027893  val: loss: 299841.09375 acc: 0.9573583602905273\n",
      "step: 45690 , time : 0.0\n",
      "train: loss: 29817.37109375 acc: 0.9767258167266846  val: loss: 708280.125 acc: 0.6905587911605835\n",
      "step: 45695 , time : 0.0\n",
      "train: loss: 14865.38671875 acc: 0.9918074011802673  val: loss: 1396111.5 acc: 0.6852880120277405\n",
      "step: 45700 , time : 0.0\n",
      "train: loss: 58538.09765625 acc: 0.9740529656410217  val: loss: 1713014.875 acc: 0.16559511423110962\n",
      "step: 45705 , time : 0.0\n",
      "train: loss: 26035.94140625 acc: 0.9949879050254822  val: loss: 3259877.75 acc: 0.5331677794456482\n",
      "step: 45710 , time : 0.0\n",
      "train: loss: 29958.2578125 acc: 0.9920620918273926  val: loss: 1235310.75 acc: 0.6126393675804138\n",
      "step: 45715 , time : 0.0\n",
      "train: loss: 28670.271484375 acc: 0.9857180118560791  val: loss: 701879.9375 acc: 0.8629966974258423\n",
      "step: 45720 , time : 0.0\n",
      "train: loss: 337212.5 acc: 0.8831926584243774  val: loss: 2204404.0 acc: 0.7405064105987549\n",
      "step: 45725 , time : 0.0\n",
      "train: loss: 124694.296875 acc: 0.973244309425354  val: loss: 1553100.875 acc: 0.5210697650909424\n",
      "step: 45730 , time : 0.0\n",
      "train: loss: 109725.0078125 acc: 0.8905147314071655  val: loss: 232510.25 acc: 0.97170490026474\n",
      "step: 45735 , time : 0.0\n",
      "train: loss: 166957.765625 acc: 0.9655318260192871  val: loss: 706090.75 acc: 0.855840802192688\n",
      "step: 45740 , time : 0.0\n",
      "train: loss: 142533.46875 acc: 0.9854261875152588  val: loss: 386268.78125 acc: 0.9317061305046082\n",
      "step: 45745 , time : 0.0\n",
      "train: loss: 134542.75 acc: 0.9867101907730103  val: loss: 722535.75 acc: 0.8203049898147583\n",
      "step: 45750 , time : 0.0\n",
      "train: loss: 121784.59375 acc: 0.9847273826599121  val: loss: 1608374.75 acc: -0.569695234298706\n",
      "step: 45755 , time : 0.0\n",
      "train: loss: 205974.109375 acc: 0.9714574217796326  val: loss: 779677.3125 acc: 0.7996774315834045\n",
      "step: 45760 , time : 0.015625953674316406\n",
      "train: loss: 191604.9375 acc: 0.9780518412590027  val: loss: 403737.21875 acc: 0.9527702927589417\n",
      "step: 45765 , time : 0.0\n",
      "train: loss: 929000.25 acc: 0.937762975692749  val: loss: 1391148.25 acc: 0.7418510317802429\n",
      "step: 45770 , time : 0.0\n",
      "train: loss: 278941.65625 acc: 0.9150971174240112  val: loss: 742575.625 acc: 0.6070278882980347\n",
      "step: 45775 , time : 0.0\n",
      "train: loss: 1068946.75 acc: 0.948290228843689  val: loss: 489276.0 acc: 0.9407486319541931\n",
      "step: 45780 , time : 0.0\n",
      "train: loss: 461830.34375 acc: 0.98182213306427  val: loss: 368814.71875 acc: 0.8918927907943726\n",
      "step: 45785 , time : 0.0010004043579101562\n",
      "train: loss: 1701474.75 acc: 0.9494799971580505  val: loss: 586589.375 acc: 0.7046289443969727\n",
      "step: 45790 , time : 0.0010006427764892578\n",
      "train: loss: 791043.75 acc: 0.948448657989502  val: loss: 1629865.625 acc: 0.5195813179016113\n",
      "step: 45795 , time : 0.0\n",
      "train: loss: 811478.875 acc: 0.9282077550888062  val: loss: 1455772.0 acc: 0.7692660093307495\n",
      "step: 45800 , time : 0.0\n",
      "train: loss: 400731.03125 acc: 0.9579041600227356  val: loss: 3062898.0 acc: 0.7191042900085449\n",
      "step: 45805 , time : 0.0\n",
      "train: loss: 471898.65625 acc: 0.9428794384002686  val: loss: 422193.71875 acc: 0.8861067295074463\n",
      "step: 45810 , time : 0.0\n",
      "train: loss: 204889.296875 acc: 0.9534406661987305  val: loss: 1036408.8125 acc: 0.5947434306144714\n",
      "step: 45815 , time : 0.0\n",
      "train: loss: 2492150.5 acc: -0.09689009189605713  val: loss: 1331093.25 acc: 0.7814123630523682\n",
      "step: 45820 , time : 0.0\n",
      "train: loss: 1198448.75 acc: 0.49267297983169556  val: loss: 1152827.25 acc: 0.5475903749465942\n",
      "step: 45825 , time : 0.0\n",
      "train: loss: 1147920.25 acc: 0.861777663230896  val: loss: 662086.25 acc: 0.6444391012191772\n",
      "step: 45830 , time : 0.0\n",
      "train: loss: 523404.03125 acc: 0.8178125023841858  val: loss: 511078.625 acc: 0.8566572070121765\n",
      "step: 45835 , time : 0.0\n",
      "train: loss: 1078115.75 acc: 0.3221031427383423  val: loss: 885356.1875 acc: 0.8190560340881348\n",
      "step: 45840 , time : 0.0\n",
      "train: loss: 2448106.0 acc: 0.05938005447387695  val: loss: 1309201.0 acc: 0.6969009637832642\n",
      "step: 45845 , time : 0.0\n",
      "train: loss: 945019.5625 acc: 0.35336703062057495  val: loss: 2475211.5 acc: 0.47156715393066406\n",
      "step: 45850 , time : 0.0\n",
      "train: loss: 466997.125 acc: 0.6081640720367432  val: loss: 1028511.5625 acc: 0.6857901811599731\n",
      "step: 45855 , time : 0.0\n",
      "train: loss: 455215.78125 acc: 0.6889926195144653  val: loss: 684205.3125 acc: 0.6534233093261719\n",
      "step: 45860 , time : 0.0\n",
      "train: loss: 423316.03125 acc: 0.69291752576828  val: loss: 2951253.0 acc: 0.7277750372886658\n",
      "step: 45865 , time : 0.0\n",
      "train: loss: 380064.0 acc: 0.7876301407814026  val: loss: 931571.8125 acc: 0.8656232357025146\n",
      "step: 45870 , time : 0.0\n",
      "train: loss: 176665.328125 acc: 0.8666235208511353  val: loss: 672091.625 acc: 0.801247239112854\n",
      "step: 45875 , time : 0.0\n",
      "train: loss: 161214.75 acc: 0.8627954125404358  val: loss: 1581932.25 acc: 0.8664483428001404\n",
      "step: 45880 , time : 0.0\n",
      "train: loss: 259670.921875 acc: 0.7899995446205139  val: loss: 800684.25 acc: 0.7509980201721191\n",
      "step: 45885 , time : 0.0\n",
      "train: loss: 520892.3125 acc: 0.7084928750991821  val: loss: 1770549.5 acc: 0.7893289923667908\n",
      "step: 45890 , time : 0.0\n",
      "train: loss: 68418.1953125 acc: 0.916789174079895  val: loss: 1223457.75 acc: 0.6139479279518127\n",
      "step: 45895 , time : 0.0010006427764892578\n",
      "train: loss: 143219.09375 acc: 0.8992102146148682  val: loss: 564224.0625 acc: 0.7950114011764526\n",
      "step: 45900 , time : 0.0\n",
      "train: loss: 608721.4375 acc: 0.7384730577468872  val: loss: 2618600.0 acc: 0.7740510702133179\n",
      "step: 45905 , time : 0.0\n",
      "train: loss: 1041753.0 acc: 0.6354019641876221  val: loss: 1074282.25 acc: 0.6250214576721191\n",
      "step: 45910 , time : 0.0010004043579101562\n",
      "train: loss: 619403.0 acc: 0.4530583620071411  val: loss: 565709.375 acc: 0.7831234335899353\n",
      "step: 45915 , time : 0.0\n",
      "train: loss: 704292.1875 acc: 0.6255797147750854  val: loss: 662371.9375 acc: 0.768657922744751\n",
      "step: 45920 , time : 0.0\n",
      "train: loss: 229951.09375 acc: 0.8082426190376282  val: loss: 1027576.875 acc: 0.7645000219345093\n",
      "step: 45925 , time : 0.0\n",
      "train: loss: 1213477.0 acc: 0.7231916189193726  val: loss: 788518.0 acc: 0.6420047879219055\n",
      "step: 45930 , time : 0.0\n",
      "train: loss: 1254907.0 acc: 0.826920747756958  val: loss: 631446.375 acc: 0.9487270712852478\n",
      "step: 45935 , time : 0.0\n",
      "train: loss: 1000561.9375 acc: 0.8375287652015686  val: loss: 757396.25 acc: 0.8815866708755493\n",
      "step: 45940 , time : 0.0\n",
      "train: loss: 770174.625 acc: 0.9476956129074097  val: loss: 1883048.625 acc: 0.8127043843269348\n",
      "step: 45945 , time : 0.0\n",
      "train: loss: 341383.5 acc: 0.9508863091468811  val: loss: 1413607.25 acc: 0.7293369770050049\n",
      "step: 45950 , time : 0.0\n",
      "train: loss: 321344.59375 acc: 0.9403777122497559  val: loss: 1017321.5625 acc: 0.8714656829833984\n",
      "step: 45955 , time : 0.0\n",
      "train: loss: 237250.78125 acc: 0.9744726419448853  val: loss: 914656.875 acc: 0.30707401037216187\n",
      "step: 45960 , time : 0.0\n",
      "train: loss: 200852.3125 acc: 0.9845210909843445  val: loss: 1333526.5 acc: 0.35519939661026\n",
      "step: 45965 , time : 0.0\n",
      "train: loss: 335935.71875 acc: 0.9740678071975708  val: loss: 598504.0625 acc: 0.8630680441856384\n",
      "step: 45970 , time : 0.015625715255737305\n",
      "train: loss: 286807.78125 acc: 0.9680501818656921  val: loss: 2884648.5 acc: 0.6249907612800598\n",
      "step: 45975 , time : 0.0\n",
      "train: loss: 103402.2890625 acc: 0.9809088706970215  val: loss: 750895.9375 acc: 0.7757028937339783\n",
      "step: 45980 , time : 0.0\n",
      "train: loss: 307939.90625 acc: 0.9544345736503601  val: loss: 1351449.125 acc: 0.8177566528320312\n",
      "step: 45985 , time : 0.0\n",
      "train: loss: 250515.671875 acc: 0.8490507006645203  val: loss: 1644206.0 acc: 0.8096908926963806\n",
      "step: 45990 , time : 0.0\n",
      "train: loss: 77412.8203125 acc: 0.9400034546852112  val: loss: 1081911.5 acc: 0.781484842300415\n",
      "step: 45995 , time : 0.0\n",
      "train: loss: 18196.4609375 acc: 0.9774612188339233  val: loss: 1453055.875 acc: 0.7301592230796814\n",
      "step: 46000 , time : 0.0\n",
      "train: loss: 59981.875 acc: 0.9725534319877625  val: loss: 1941927.5 acc: 0.5928688049316406\n",
      "step: 46005 , time : 0.0\n",
      "train: loss: 122887.7734375 acc: 0.8487177491188049  val: loss: 1703269.625 acc: 0.5280687808990479\n",
      "step: 46010 , time : 0.0010004043579101562\n",
      "train: loss: 8914.97265625 acc: 0.983625590801239  val: loss: 773354.625 acc: 0.9185367822647095\n",
      "step: 46015 , time : 0.0\n",
      "train: loss: 20487.4375 acc: 0.960424542427063  val: loss: 1016500.5 acc: 0.8009704947471619\n",
      "step: 46020 , time : 0.0010008811950683594\n",
      "train: loss: 6586.87158203125 acc: 0.978927731513977  val: loss: 979584.625 acc: 0.8445883989334106\n",
      "step: 46025 , time : 0.0\n",
      "train: loss: 26337.494140625 acc: 0.9711646437644958  val: loss: 843308.5625 acc: 0.7858729362487793\n",
      "step: 46030 , time : 0.0\n",
      "train: loss: 58436.51953125 acc: 0.950111448764801  val: loss: 1055146.5 acc: 0.7606524229049683\n",
      "step: 46035 , time : 0.0\n",
      "train: loss: 27051.0625 acc: 0.983954131603241  val: loss: 1007463.0625 acc: 0.6849170327186584\n",
      "step: 46040 , time : 0.0\n",
      "train: loss: 38240.66015625 acc: 0.9840872287750244  val: loss: 1139844.625 acc: 0.5871564149856567\n",
      "step: 46045 , time : 0.0010008811950683594\n",
      "train: loss: 77164.0546875 acc: 0.9515060186386108  val: loss: 599224.1875 acc: 0.870110273361206\n",
      "step: 46050 , time : 0.0\n",
      "train: loss: 15480.6328125 acc: 0.9902843832969666  val: loss: 2139126.75 acc: 0.4473041296005249\n",
      "step: 46055 , time : 0.0010006427764892578\n",
      "train: loss: 25943.521484375 acc: 0.9774889945983887  val: loss: 989464.625 acc: 0.8902219533920288\n",
      "step: 46060 , time : 0.0\n",
      "train: loss: 16664.375 acc: 0.9879084825515747  val: loss: 163789.109375 acc: 0.9796116352081299\n",
      "step: 46065 , time : 0.0\n",
      "train: loss: 30360.5234375 acc: 0.9923017024993896  val: loss: 1825493.5 acc: 0.7327672243118286\n",
      "step: 46070 , time : 0.0\n",
      "train: loss: 39439.26171875 acc: 0.9873477220535278  val: loss: 1176435.75 acc: 0.7048746943473816\n",
      "step: 46075 , time : 0.0\n",
      "train: loss: 39509.40234375 acc: 0.9904022216796875  val: loss: 671768.125 acc: 0.8918592929840088\n",
      "step: 46080 , time : 0.0\n",
      "train: loss: 36810.5234375 acc: 0.9717540144920349  val: loss: 3057310.25 acc: 0.4414491057395935\n",
      "step: 46085 , time : 0.0\n",
      "train: loss: 114934.6875 acc: 0.9734343886375427  val: loss: 1150016.375 acc: 0.33841657638549805\n",
      "step: 46090 , time : 0.0\n",
      "train: loss: 353318.03125 acc: 0.9170452356338501  val: loss: 358348.21875 acc: 0.8174244165420532\n",
      "step: 46095 , time : 0.0\n",
      "train: loss: 140087.40625 acc: 0.949209451675415  val: loss: 227540.265625 acc: 0.9508914351463318\n",
      "step: 46100 , time : 0.0\n",
      "train: loss: 219386.84375 acc: 0.9237726926803589  val: loss: 960562.75 acc: 0.893214225769043\n",
      "step: 46105 , time : 0.0\n",
      "train: loss: 138481.65625 acc: 0.984621524810791  val: loss: 1124904.75 acc: 0.42941761016845703\n",
      "step: 46110 , time : 0.0\n",
      "train: loss: 166513.671875 acc: 0.9844397306442261  val: loss: 1046424.5 acc: 0.8192442655563354\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 46115 , time : 0.0010004043579101562\n",
      "train: loss: 45316.875 acc: 0.993299126625061  val: loss: 442191.625 acc: 0.8942062854766846\n",
      "step: 46120 , time : 0.0\n",
      "train: loss: 102801.546875 acc: 0.9895145893096924  val: loss: 276872.21875 acc: 0.942226767539978\n",
      "step: 46125 , time : 0.0\n",
      "train: loss: 190438.4375 acc: 0.9779440760612488  val: loss: 329952.8125 acc: 0.9206984043121338\n",
      "step: 46130 , time : 0.0010008811950683594\n",
      "train: loss: 581179.6875 acc: 0.9680069088935852  val: loss: 393752.59375 acc: 0.8250413537025452\n",
      "step: 46135 , time : 0.0\n",
      "train: loss: 310818.03125 acc: 0.9465843439102173  val: loss: 582081.5 acc: 0.7998409271240234\n",
      "step: 46140 , time : 0.0\n",
      "train: loss: 254789.984375 acc: 0.8910123109817505  val: loss: 481296.96875 acc: 0.9014057517051697\n",
      "step: 46145 , time : 0.0\n",
      "train: loss: 725595.5625 acc: 0.9725099802017212  val: loss: 486046.09375 acc: 0.9093148708343506\n",
      "step: 46150 , time : 0.0\n",
      "train: loss: 905997.4375 acc: 0.9669767022132874  val: loss: 1651704.75 acc: 0.1860024333000183\n",
      "step: 46155 , time : 0.0\n",
      "train: loss: 1149752.625 acc: 0.9660328030586243  val: loss: 723388.9375 acc: 0.8145594596862793\n",
      "step: 46160 , time : 0.015626192092895508\n",
      "train: loss: 639648.8125 acc: 0.9558990001678467  val: loss: 258766.921875 acc: 0.9519640207290649\n",
      "step: 46165 , time : 0.0\n",
      "train: loss: 354921.0 acc: 0.9535834193229675  val: loss: 423518.53125 acc: 0.9276313185691833\n",
      "step: 46170 , time : 0.0\n",
      "train: loss: 554570.5625 acc: 0.9422845840454102  val: loss: 470090.875 acc: 0.9461859464645386\n",
      "step: 46175 , time : 0.0\n",
      "train: loss: 1075350.25 acc: 0.7940080165863037  val: loss: 328386.0 acc: 0.8787400722503662\n",
      "step: 46180 , time : 0.0\n",
      "train: loss: 452559.3125 acc: 0.9260189533233643  val: loss: 921578.75 acc: 0.9174742698669434\n",
      "step: 46185 , time : 0.0\n",
      "train: loss: 956689.375 acc: 0.504355788230896  val: loss: 581319.375 acc: 0.7921105623245239\n",
      "step: 46190 , time : 0.0\n",
      "train: loss: 402841.875 acc: 0.9100664854049683  val: loss: 398842.4375 acc: 0.901931643486023\n",
      "step: 46195 , time : 0.0\n",
      "train: loss: 871910.0 acc: 0.6642706394195557  val: loss: 1993375.5 acc: 0.7499082088470459\n",
      "step: 46200 , time : 0.0\n",
      "train: loss: 578378.5 acc: 0.8654927611351013  val: loss: 1536632.75 acc: 0.8268496990203857\n",
      "step: 46205 , time : 0.0\n",
      "train: loss: 508307.5625 acc: 0.4783749580383301  val: loss: 896051.6875 acc: 0.8041688799858093\n",
      "step: 46210 , time : 0.0\n",
      "train: loss: 551917.3125 acc: 0.7675461769104004  val: loss: 2938450.5 acc: 0.5908414125442505\n",
      "step: 46215 , time : 0.0\n",
      "train: loss: 592814.875 acc: 0.7268909811973572  val: loss: 2629399.5 acc: 0.6869886517524719\n",
      "step: 46220 , time : 0.0\n",
      "train: loss: 105126.34375 acc: 0.9299358129501343  val: loss: 772865.75 acc: 0.788303792476654\n",
      "step: 46225 , time : 0.0\n",
      "train: loss: 47641.41015625 acc: 0.9639433026313782  val: loss: 1575336.625 acc: 0.6270856857299805\n",
      "step: 46230 , time : 0.0\n",
      "train: loss: 14284.748046875 acc: 0.9885519742965698  val: loss: 4584105.0 acc: 0.6500961780548096\n",
      "step: 46235 , time : 0.0\n",
      "train: loss: 500338.75 acc: 0.8016536235809326  val: loss: 1756317.25 acc: 0.7612512111663818\n",
      "step: 46240 , time : 0.0\n",
      "train: loss: 115498.796875 acc: 0.9282586574554443  val: loss: 968043.0 acc: 0.7239152193069458\n",
      "step: 46245 , time : 0.001001119613647461\n",
      "train: loss: 322931.71875 acc: 0.8076781034469604  val: loss: 922854.25 acc: 0.7460924386978149\n",
      "step: 46250 , time : 0.0\n",
      "train: loss: 38115.7890625 acc: 0.971226155757904  val: loss: 1020150.5 acc: 0.6918987035751343\n",
      "step: 46255 , time : 0.0010004043579101562\n",
      "train: loss: 158839.09375 acc: 0.8152253031730652  val: loss: 2551677.5 acc: 0.7065045833587646\n",
      "step: 46260 , time : 0.0\n",
      "train: loss: 35433.51171875 acc: 0.9563477039337158  val: loss: 642244.3125 acc: 0.7902688980102539\n",
      "step: 46265 , time : 0.0\n",
      "train: loss: 99745.8125 acc: 0.9038203954696655  val: loss: 1557791.25 acc: 0.701077401638031\n",
      "step: 46270 , time : 0.0\n",
      "train: loss: 276314.65625 acc: 0.8064804673194885  val: loss: 715319.875 acc: 0.79117351770401\n",
      "step: 46275 , time : 0.0\n",
      "train: loss: 493578.15625 acc: 0.6930280923843384  val: loss: 782650.3125 acc: 0.7679603695869446\n",
      "step: 46280 , time : 0.0\n",
      "train: loss: 166976.046875 acc: 0.7991183996200562  val: loss: 1978339.0 acc: 0.6960618495941162\n",
      "step: 46285 , time : 0.0\n",
      "train: loss: 232423.21875 acc: 0.8131162524223328  val: loss: 2538009.75 acc: 0.5515695810317993\n",
      "step: 46290 , time : 0.0156252384185791\n",
      "train: loss: 245201.640625 acc: 0.8319203853607178  val: loss: 1529021.0 acc: 0.6546344757080078\n",
      "step: 46295 , time : 0.0\n",
      "train: loss: 1746464.0 acc: 0.7722688913345337  val: loss: 2132511.25 acc: 0.7345961332321167\n",
      "step: 46300 , time : 0.0\n",
      "train: loss: 1280516.5 acc: 0.853622317314148  val: loss: 623854.875 acc: 0.41015714406967163\n",
      "step: 46305 , time : 0.0\n",
      "train: loss: 310836.3125 acc: 0.9755245447158813  val: loss: 1764417.25 acc: 0.8714714050292969\n",
      "step: 46310 , time : 0.0\n",
      "train: loss: 163406.171875 acc: 0.9824532866477966  val: loss: 1388899.625 acc: 0.32598280906677246\n",
      "step: 46315 , time : 0.0\n",
      "train: loss: 256854.015625 acc: 0.9443228244781494  val: loss: 1309825.5 acc: 0.8452472686767578\n",
      "step: 46320 , time : 0.0\n",
      "train: loss: 176527.703125 acc: 0.9766749143600464  val: loss: 1314696.75 acc: 0.8774528503417969\n",
      "step: 46325 , time : 0.0\n",
      "train: loss: 99794.6328125 acc: 0.9920120239257812  val: loss: 320320.96875 acc: 0.9352788925170898\n",
      "step: 46330 , time : 0.0\n",
      "train: loss: 107632.890625 acc: 0.9908850193023682  val: loss: 2456640.0 acc: 0.6988399028778076\n",
      "step: 46335 , time : 0.0\n",
      "train: loss: 76508.1171875 acc: 0.9938293099403381  val: loss: 1935107.0 acc: 0.6871868968009949\n",
      "step: 46340 , time : 0.0\n",
      "train: loss: 50762.42578125 acc: 0.9920174479484558  val: loss: 1238629.0 acc: 0.6769838333129883\n",
      "step: 46345 , time : 0.0\n",
      "train: loss: 19225.140625 acc: 0.9953014850616455  val: loss: 603750.8125 acc: 0.9388513565063477\n",
      "step: 46350 , time : 0.0\n",
      "train: loss: 15558.3994140625 acc: 0.960105836391449  val: loss: 1014697.3125 acc: 0.8945080637931824\n",
      "step: 46355 , time : 0.001001119613647461\n",
      "train: loss: 18120.7109375 acc: 0.9907628297805786  val: loss: 2288082.5 acc: 0.41551923751831055\n",
      "step: 46360 , time : 0.0010008811950683594\n",
      "train: loss: 20322.2265625 acc: 0.9847044348716736  val: loss: 386214.28125 acc: 0.9351736307144165\n",
      "step: 46365 , time : 0.0010008811950683594\n",
      "train: loss: 10701.2216796875 acc: 0.9861311316490173  val: loss: 867687.4375 acc: 0.8711243867874146\n",
      "step: 46370 , time : 0.0\n",
      "train: loss: 5250.11767578125 acc: 0.9909480214118958  val: loss: 951728.875 acc: 0.8108540773391724\n",
      "step: 46375 , time : 0.0\n",
      "train: loss: 20332.572265625 acc: 0.9564195871353149  val: loss: 1087812.875 acc: 0.3827238082885742\n",
      "step: 46380 , time : 0.0\n",
      "train: loss: 11929.7734375 acc: 0.9603164196014404  val: loss: 1409028.625 acc: 0.7362569570541382\n",
      "step: 46385 , time : 0.0\n",
      "train: loss: 1556.9830322265625 acc: 0.9956205487251282  val: loss: 524930.5625 acc: 0.8487204313278198\n",
      "step: 46390 , time : 0.0\n",
      "train: loss: 16396.77734375 acc: 0.9870361089706421  val: loss: 861631.375 acc: 0.9187760353088379\n",
      "step: 46395 , time : 0.0\n",
      "train: loss: 28912.76171875 acc: 0.9792541265487671  val: loss: 150401.265625 acc: 0.9205577969551086\n",
      "step: 46400 , time : 0.0\n",
      "train: loss: 48374.11328125 acc: 0.9731051921844482  val: loss: 2626150.0 acc: 0.5819454193115234\n",
      "step: 46405 , time : 0.0\n",
      "train: loss: 120600.8125 acc: 0.9270437955856323  val: loss: 695705.1875 acc: -0.04873216152191162\n",
      "step: 46410 , time : 0.0010004043579101562\n",
      "train: loss: 35773.4375 acc: 0.9821394681930542  val: loss: 1268013.875 acc: 0.6379458904266357\n",
      "step: 46415 , time : 0.0010004043579101562\n",
      "train: loss: 43105.72265625 acc: 0.9732338786125183  val: loss: 677292.875 acc: 0.6367673277854919\n",
      "step: 46420 , time : 0.0\n",
      "train: loss: 6315.85791015625 acc: 0.9877795577049255  val: loss: 1300919.875 acc: 0.5152596235275269\n",
      "step: 46425 , time : 0.0\n",
      "train: loss: 6558.70556640625 acc: 0.9940817952156067  val: loss: 295249.53125 acc: 0.8951361179351807\n",
      "step: 46430 , time : 0.0\n",
      "train: loss: 18313.703125 acc: 0.9936923980712891  val: loss: 1083796.375 acc: 0.7556698322296143\n",
      "step: 46435 , time : 0.0\n",
      "train: loss: 31902.416015625 acc: 0.9924027919769287  val: loss: 1032263.0625 acc: 0.7447812557220459\n",
      "step: 46440 , time : 0.0\n",
      "train: loss: 26888.66796875 acc: 0.9906808137893677  val: loss: 1133823.875 acc: 0.7866973876953125\n",
      "step: 46445 , time : 0.0\n",
      "train: loss: 22492.228515625 acc: 0.9939844012260437  val: loss: 257415.4375 acc: 0.9471340775489807\n",
      "step: 46450 , time : 0.0\n",
      "train: loss: 19839.46875 acc: 0.9951297044754028  val: loss: 819514.1875 acc: 0.6897754669189453\n",
      "step: 46455 , time : 0.0\n",
      "train: loss: 286333.0 acc: 0.8943821787834167  val: loss: 518099.65625 acc: 0.8641459345817566\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 46460 , time : 0.0\n",
      "train: loss: 87067.234375 acc: 0.8650462031364441  val: loss: 242191.421875 acc: 0.9543433785438538\n",
      "step: 46465 , time : 0.0010006427764892578\n",
      "train: loss: 114379.421875 acc: 0.9622778296470642  val: loss: 226513.84375 acc: 0.9466677904129028\n",
      "step: 46470 , time : 0.0010006427764892578\n",
      "train: loss: 327894.09375 acc: 0.9674131274223328  val: loss: 1315751.625 acc: 0.5125819444656372\n",
      "step: 46475 , time : 0.0\n",
      "train: loss: 70735.140625 acc: 0.9924072027206421  val: loss: 196207.359375 acc: 0.9520767331123352\n",
      "step: 46480 , time : 0.0010006427764892578\n",
      "train: loss: 51435.12890625 acc: 0.9931561350822449  val: loss: 346824.65625 acc: 0.9475140571594238\n",
      "step: 46485 , time : 0.0\n",
      "train: loss: 237771.359375 acc: 0.9741555452346802  val: loss: 2080047.875 acc: 0.8273414373397827\n",
      "step: 46490 , time : 0.0010006427764892578\n",
      "train: loss: 172769.96875 acc: 0.9725747108459473  val: loss: 372835.28125 acc: 0.9625056982040405\n",
      "step: 46495 , time : 0.0010006427764892578\n",
      "train: loss: 1115162.625 acc: 0.9494280219078064  val: loss: 679469.5 acc: 0.792199969291687\n",
      "step: 46500 , time : 0.0\n",
      "train: loss: 334021.46875 acc: 0.9685646891593933  val: loss: 591958.8125 acc: 0.8564202785491943\n",
      "step: 46505 , time : 0.0\n",
      "train: loss: 349498.09375 acc: 0.9052439332008362  val: loss: 556267.25 acc: 0.9638310670852661\n",
      "step: 46510 , time : 0.0\n",
      "train: loss: 358504.9375 acc: 0.9841241836547852  val: loss: 1649498.875 acc: 0.6927558183670044\n",
      "step: 46515 , time : 0.0\n",
      "train: loss: 620566.25 acc: 0.9803996086120605  val: loss: 357669.15625 acc: 0.8991318345069885\n",
      "step: 46520 , time : 0.0\n",
      "train: loss: 552765.75 acc: 0.9790072441101074  val: loss: 3009446.25 acc: 0.6492229700088501\n",
      "step: 46525 , time : 0.0\n",
      "train: loss: 258326.578125 acc: 0.9807827472686768  val: loss: 1845541.875 acc: 0.8593959212303162\n",
      "step: 46530 , time : 0.0\n",
      "train: loss: 446652.875 acc: 0.9650311470031738  val: loss: 3107070.0 acc: 0.22488093376159668\n",
      "step: 46535 , time : 0.0\n",
      "train: loss: 779892.125 acc: 0.9388500452041626  val: loss: 1234003.375 acc: 0.858482837677002\n",
      "step: 46540 , time : 0.0\n",
      "train: loss: 348378.21875 acc: 0.9378305673599243  val: loss: 558523.0 acc: 0.920879065990448\n",
      "step: 46545 , time : 0.0\n",
      "train: loss: 2869587.5 acc: 0.6851917505264282  val: loss: 1472792.25 acc: 0.7318097352981567\n",
      "step: 46550 , time : 0.0\n",
      "train: loss: 516966.84375 acc: 0.7417498826980591  val: loss: 1029127.4375 acc: 0.6189507842063904\n",
      "step: 46555 , time : 0.0\n",
      "train: loss: 607884.0625 acc: 0.8362565040588379  val: loss: 676269.8125 acc: 0.8451404571533203\n",
      "step: 46560 , time : 0.0\n",
      "train: loss: 166377.375 acc: 0.8925062417984009  val: loss: 979633.3125 acc: 0.6890872716903687\n",
      "step: 46565 , time : 0.001001119613647461\n",
      "train: loss: 405121.03125 acc: 0.8486404418945312  val: loss: 2129501.5 acc: 0.7122598886489868\n",
      "step: 46570 , time : 0.0\n",
      "train: loss: 1325975.625 acc: 0.14115285873413086  val: loss: 1832746.875 acc: 0.6741853952407837\n",
      "step: 46575 , time : 0.0010008811950683594\n",
      "train: loss: 686221.9375 acc: 0.7629393339157104  val: loss: 1508452.75 acc: 0.7230978012084961\n",
      "step: 46580 , time : 0.0\n",
      "train: loss: 94193.671875 acc: 0.9268057942390442  val: loss: 2551532.5 acc: 0.5324746966362\n",
      "step: 46585 , time : 0.001001119613647461\n",
      "train: loss: 82004.7421875 acc: 0.9332993626594543  val: loss: 460014.21875 acc: 0.7916272282600403\n",
      "step: 46590 , time : 0.0010006427764892578\n",
      "train: loss: 19022.431640625 acc: 0.983980119228363  val: loss: 1134418.5 acc: 0.6858525276184082\n",
      "step: 46595 , time : 0.0\n",
      "train: loss: 68289.6484375 acc: 0.9415110945701599  val: loss: 1631899.0 acc: 0.7217317819595337\n",
      "step: 46600 , time : 0.0\n",
      "train: loss: 67006.6484375 acc: 0.9550255537033081  val: loss: 2847097.25 acc: 0.5854331254959106\n",
      "step: 46605 , time : 0.0\n",
      "train: loss: 157131.484375 acc: 0.9109290838241577  val: loss: 1380478.875 acc: 0.6775350570678711\n",
      "step: 46610 , time : 0.0\n",
      "train: loss: 49705.76953125 acc: 0.9550554752349854  val: loss: 792728.5625 acc: 0.6846061944961548\n",
      "step: 46615 , time : 0.0\n",
      "train: loss: 27866.587890625 acc: 0.9741140007972717  val: loss: 3374472.5 acc: 0.6060171127319336\n",
      "step: 46620 , time : 0.0\n",
      "train: loss: 117715.2421875 acc: 0.8925802707672119  val: loss: 1845564.0 acc: 0.6312204003334045\n",
      "step: 46625 , time : 0.0\n",
      "train: loss: 157990.765625 acc: 0.8090497851371765  val: loss: 3250547.75 acc: 0.6824589371681213\n",
      "step: 46630 , time : 0.0\n",
      "train: loss: 291394.21875 acc: 0.8545265793800354  val: loss: 698298.625 acc: 0.7599825263023376\n",
      "step: 46635 , time : 0.0\n",
      "train: loss: 190814.328125 acc: 0.8650690317153931  val: loss: 987520.125 acc: 0.7702324986457825\n",
      "step: 46640 , time : 0.015625715255737305\n",
      "train: loss: 809017.3125 acc: 0.7291803359985352  val: loss: 3863780.75 acc: 0.5131300687789917\n",
      "step: 46645 , time : 0.0\n",
      "train: loss: 188904.546875 acc: 0.7765699028968811  val: loss: 915073.8125 acc: 0.7312876582145691\n",
      "step: 46650 , time : 0.0\n",
      "train: loss: 137316.65625 acc: 0.8962569832801819  val: loss: 4871487.0 acc: 0.5427541732788086\n",
      "step: 46655 , time : 0.0\n",
      "train: loss: 499317.375 acc: 0.7974743843078613  val: loss: 512720.40625 acc: 0.7260798215866089\n",
      "step: 46660 , time : 0.0010006427764892578\n",
      "train: loss: 2956996.5 acc: 0.7470349073410034  val: loss: 712943.0625 acc: 0.7194693088531494\n",
      "step: 46665 , time : 0.0\n",
      "train: loss: 1679163.75 acc: 0.8329735994338989  val: loss: 1007927.8125 acc: 0.6118019819259644\n",
      "step: 46670 , time : 0.0\n",
      "train: loss: 668599.8125 acc: 0.936430811882019  val: loss: 1304619.25 acc: 0.8386024832725525\n",
      "step: 46675 , time : 0.0\n",
      "train: loss: 359062.71875 acc: 0.9579358100891113  val: loss: 730289.9375 acc: 0.8623100519180298\n",
      "step: 46680 , time : 0.0010004043579101562\n",
      "train: loss: 77321.1484375 acc: 0.986366868019104  val: loss: 1913668.625 acc: 0.7362054586410522\n",
      "step: 46685 , time : 0.0010001659393310547\n",
      "train: loss: 106492.71875 acc: 0.9735599160194397  val: loss: 772658.625 acc: 0.8663910031318665\n",
      "step: 46690 , time : 0.0\n",
      "train: loss: 60586.9140625 acc: 0.9946895837783813  val: loss: 796543.6875 acc: 0.915107011795044\n",
      "step: 46695 , time : 0.0\n",
      "train: loss: 110994.859375 acc: 0.9922202229499817  val: loss: 1315498.875 acc: 0.8256757259368896\n",
      "step: 46700 , time : 0.0\n",
      "train: loss: 100794.2578125 acc: 0.9924464225769043  val: loss: 181118.8125 acc: 0.9053509831428528\n",
      "step: 46705 , time : 0.0\n",
      "train: loss: 118668.5390625 acc: 0.9810637831687927  val: loss: 158136.0 acc: 0.950069785118103\n",
      "step: 46710 , time : 0.0\n",
      "train: loss: 35538.671875 acc: 0.9912266731262207  val: loss: 221879.671875 acc: 0.9507818818092346\n",
      "step: 46715 , time : 0.0\n",
      "train: loss: 23708.763671875 acc: 0.9918627142906189  val: loss: 452362.625 acc: 0.908889889717102\n",
      "step: 46720 , time : 0.0\n",
      "train: loss: 8395.0751953125 acc: 0.9938107132911682  val: loss: 159857.3125 acc: 0.9416413307189941\n",
      "step: 46725 , time : 0.0\n",
      "train: loss: 9823.4453125 acc: 0.9936777353286743  val: loss: 1197741.75 acc: 0.2471572756767273\n",
      "step: 46730 , time : 0.0\n",
      "train: loss: 17737.767578125 acc: 0.9933987855911255  val: loss: 310350.75 acc: 0.9320380091667175\n",
      "step: 46735 , time : 0.0\n",
      "train: loss: 4801.017578125 acc: 0.9878018498420715  val: loss: 227220.53125 acc: 0.9411647319793701\n",
      "step: 46740 , time : 0.0\n",
      "train: loss: 21872.54296875 acc: 0.9416313767433167  val: loss: 265701.5 acc: 0.8951082229614258\n",
      "step: 46745 , time : 0.0\n",
      "train: loss: 6829.71826171875 acc: 0.9800634384155273  val: loss: 105065.0625 acc: 0.9122548699378967\n",
      "step: 46750 , time : 0.015625\n",
      "train: loss: 9083.0625 acc: 0.9708030819892883  val: loss: 266239.78125 acc: 0.9554665684700012\n",
      "step: 46755 , time : 0.0\n",
      "train: loss: 7360.54345703125 acc: 0.9843315482139587  val: loss: 668930.5 acc: 0.8941596150398254\n",
      "step: 46760 , time : 0.0\n",
      "train: loss: 27283.154296875 acc: 0.9756951332092285  val: loss: 124194.5625 acc: 0.9446977376937866\n",
      "step: 46765 , time : 0.0\n",
      "train: loss: 17568.064453125 acc: 0.9768545031547546  val: loss: 604497.0625 acc: 0.9057838320732117\n",
      "step: 46770 , time : 0.0\n",
      "train: loss: 10902.984375 acc: 0.9941279888153076  val: loss: 2378789.5 acc: 0.4898488521575928\n",
      "step: 46775 , time : 0.0\n",
      "train: loss: 31469.87109375 acc: 0.9857134819030762  val: loss: 512316.6875 acc: 0.9359102249145508\n",
      "step: 46780 , time : 0.0\n",
      "train: loss: 10460.71484375 acc: 0.9915585517883301  val: loss: 1971897.0 acc: 0.3501858711242676\n",
      "step: 46785 , time : 0.0\n",
      "train: loss: 5064.0927734375 acc: 0.9899459481239319  val: loss: 373064.40625 acc: 0.9289816617965698\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 46790 , time : 0.0\n",
      "train: loss: 97446.4609375 acc: 0.9405629634857178  val: loss: 347898.09375 acc: 0.9703711271286011\n",
      "step: 46795 , time : 0.0010004043579101562\n",
      "train: loss: 26556.15625 acc: 0.9910039305686951  val: loss: 1412988.875 acc: 0.7662058472633362\n",
      "step: 46800 , time : 0.001001596450805664\n",
      "train: loss: 24141.564453125 acc: 0.9948648810386658  val: loss: 930110.125 acc: 0.8933302760124207\n",
      "step: 46805 , time : 0.0010008811950683594\n",
      "train: loss: 36187.50390625 acc: 0.9684569835662842  val: loss: 896136.625 acc: 0.9046467542648315\n",
      "step: 46810 , time : 0.0\n",
      "train: loss: 27914.60546875 acc: 0.9944555759429932  val: loss: 597547.625 acc: 0.9526302218437195\n",
      "step: 46815 , time : 0.0010008811950683594\n",
      "train: loss: 40087.5078125 acc: 0.9910363554954529  val: loss: 833144.125 acc: 0.8326237201690674\n",
      "step: 46820 , time : 0.0\n",
      "train: loss: 128761.71875 acc: 0.9400224685668945  val: loss: 840619.125 acc: 0.9291180968284607\n",
      "step: 46825 , time : 0.0\n",
      "train: loss: 133142.15625 acc: 0.8789703249931335  val: loss: 1112220.875 acc: 0.7318685054779053\n",
      "step: 46830 , time : 0.0\n",
      "train: loss: 1065494.125 acc: 0.7377461194992065  val: loss: 749401.125 acc: 0.9180995225906372\n",
      "step: 46835 , time : 0.0\n",
      "train: loss: 51308.63671875 acc: 0.9914363622665405  val: loss: 664496.6875 acc: 0.9369759559631348\n",
      "step: 46840 , time : 0.0\n",
      "train: loss: 90612.1015625 acc: 0.9916703104972839  val: loss: 1233845.0 acc: 0.6520819664001465\n",
      "step: 46845 , time : 0.0\n",
      "train: loss: 170546.390625 acc: 0.9766272306442261  val: loss: 2591904.5 acc: 0.7839359641075134\n",
      "step: 46850 , time : 0.0\n",
      "train: loss: 87997.328125 acc: 0.9883365035057068  val: loss: 1754707.125 acc: 0.6028413772583008\n",
      "step: 46855 , time : 0.0\n",
      "train: loss: 365419.65625 acc: 0.9672598838806152  val: loss: 387989.5625 acc: 0.914199709892273\n",
      "step: 46860 , time : 0.0\n",
      "train: loss: 606151.125 acc: 0.9579648971557617  val: loss: 411901.59375 acc: 0.8715365529060364\n",
      "step: 46865 , time : 0.0\n",
      "train: loss: 384950.03125 acc: 0.9607870578765869  val: loss: 2178448.5 acc: 0.04202568531036377\n",
      "step: 46870 , time : 0.0\n",
      "train: loss: 254737.265625 acc: 0.9614924788475037  val: loss: 1806434.375 acc: 0.8056310415267944\n",
      "step: 46875 , time : 0.0\n",
      "train: loss: 687403.25 acc: 0.9795445203781128  val: loss: 2073928.875 acc: 0.6825724244117737\n",
      "step: 46880 , time : 0.0\n",
      "train: loss: 881683.5 acc: 0.9672083258628845  val: loss: 1376708.625 acc: 0.8874466419219971\n",
      "step: 46885 , time : 0.0\n",
      "train: loss: 1660859.875 acc: 0.9156308174133301  val: loss: 406617.3125 acc: 0.8489497900009155\n",
      "step: 46890 , time : 0.0\n",
      "train: loss: 1099454.375 acc: 0.9451991319656372  val: loss: 1355395.625 acc: 0.7769197821617126\n",
      "step: 46895 , time : 0.0\n",
      "train: loss: 265280.21875 acc: 0.9774835705757141  val: loss: 966954.5 acc: 0.7797685265541077\n",
      "step: 46900 , time : 0.0\n",
      "train: loss: 709330.5625 acc: 0.9459030032157898  val: loss: 1421794.375 acc: 0.07647734880447388\n",
      "step: 46905 , time : 0.0\n",
      "train: loss: 476217.15625 acc: 0.9482605457305908  val: loss: 795813.25 acc: 0.7792344093322754\n",
      "step: 46910 , time : 0.0010008811950683594\n",
      "train: loss: 457135.40625 acc: 0.9092435240745544  val: loss: 1441615.875 acc: 0.40188151597976685\n",
      "step: 46915 , time : 0.0010004043579101562\n",
      "train: loss: 1569858.625 acc: 0.09611189365386963  val: loss: 886834.8125 acc: 0.7555973529815674\n",
      "step: 46920 , time : 0.0\n",
      "train: loss: 667446.25 acc: 0.6355386972427368  val: loss: 1567209.875 acc: 0.27290135622024536\n",
      "step: 46925 , time : 0.0\n",
      "train: loss: 1015746.375 acc: 0.6245667934417725  val: loss: 312326.1875 acc: 0.945905864238739\n",
      "step: 46930 , time : 0.0010008811950683594\n",
      "train: loss: 382687.5625 acc: 0.8925593495368958  val: loss: 780463.5625 acc: 0.8154897689819336\n",
      "step: 46935 , time : 0.0\n",
      "train: loss: 2135514.75 acc: 0.27260899543762207  val: loss: 1083746.625 acc: 0.8821616768836975\n",
      "step: 46940 , time : 0.0\n",
      "train: loss: 1300527.25 acc: 0.28834623098373413  val: loss: 2924832.25 acc: 0.4442713260650635\n",
      "step: 46945 , time : 0.0\n",
      "train: loss: 1140419.25 acc: 0.3906499743461609  val: loss: 2999761.5 acc: 0.7496962547302246\n",
      "step: 46950 , time : 0.0\n",
      "train: loss: 640390.625 acc: 0.6627182960510254  val: loss: 647907.0625 acc: 0.8818627595901489\n",
      "step: 46955 , time : 0.0\n",
      "train: loss: 488205.34375 acc: 0.6599911451339722  val: loss: 808243.6875 acc: 0.8062134981155396\n",
      "step: 46960 , time : 0.0\n",
      "train: loss: 286651.0625 acc: 0.7669587731361389  val: loss: 548390.125 acc: 0.8748024702072144\n",
      "step: 46965 , time : 0.0\n",
      "train: loss: 273849.84375 acc: 0.7455113530158997  val: loss: 691770.0 acc: 0.7785860300064087\n",
      "step: 46970 , time : 0.0\n",
      "train: loss: 195517.9375 acc: 0.865007758140564  val: loss: 933362.875 acc: 0.7847129106521606\n",
      "step: 46975 , time : 0.0\n",
      "train: loss: 401634.15625 acc: 0.7382485866546631  val: loss: 1277151.125 acc: 0.5395476222038269\n",
      "step: 46980 , time : 0.0\n",
      "train: loss: 174120.390625 acc: 0.7988344430923462  val: loss: 1047784.625 acc: 0.8680194616317749\n",
      "step: 46985 , time : 0.0\n",
      "train: loss: 152119.265625 acc: 0.8824458718299866  val: loss: 680512.875 acc: 0.9060102105140686\n",
      "step: 46990 , time : 0.0\n",
      "train: loss: 144682.4375 acc: 0.8600660562515259  val: loss: 520442.875 acc: 0.7322505712509155\n",
      "step: 46995 , time : 0.0\n",
      "train: loss: 562351.9375 acc: 0.44983750581741333  val: loss: 587748.1875 acc: 0.7719796299934387\n",
      "step: 47000 , time : 0.0\n",
      "train: loss: 832772.0 acc: 0.5804418325424194  val: loss: 938441.0 acc: 0.7803826928138733\n",
      "step: 47005 , time : 0.0\n",
      "train: loss: 876173.6875 acc: 0.613824725151062  val: loss: 340388.0625 acc: 0.8214417099952698\n",
      "step: 47010 , time : 0.0\n",
      "train: loss: 362501.9375 acc: 0.7841256856918335  val: loss: 3733141.0 acc: 0.436980664730072\n",
      "step: 47015 , time : 0.0\n",
      "train: loss: 347852.75 acc: 0.7442607879638672  val: loss: 638422.5625 acc: 0.8024381399154663\n",
      "step: 47020 , time : 0.0\n",
      "train: loss: 271394.125 acc: 0.7672793865203857  val: loss: 2254284.5 acc: 0.6946720480918884\n",
      "step: 47025 , time : 0.001001119613647461\n",
      "train: loss: 1347358.625 acc: 0.769094705581665  val: loss: 1197020.5 acc: 0.7808048725128174\n",
      "step: 47030 , time : 0.0010004043579101562\n",
      "train: loss: 1296135.625 acc: 0.83360755443573  val: loss: 329231.84375 acc: 0.8329172134399414\n",
      "step: 47035 , time : 0.0010008811950683594\n",
      "train: loss: 884104.625 acc: 0.9453624486923218  val: loss: 850066.25 acc: 0.8227714896202087\n",
      "step: 47040 , time : 0.0\n",
      "train: loss: 525996.75 acc: 0.9385191798210144  val: loss: 1350395.625 acc: 0.82231605052948\n",
      "step: 47045 , time : 0.0\n",
      "train: loss: 874386.125 acc: 0.8752050399780273  val: loss: 796052.25 acc: 0.885719895362854\n",
      "step: 47050 , time : 0.0\n",
      "train: loss: 793572.8125 acc: 0.8747522234916687  val: loss: 883930.4375 acc: 0.8179497718811035\n",
      "step: 47055 , time : 0.0\n",
      "train: loss: 407002.5 acc: 0.9632797837257385  val: loss: 1100992.5 acc: 0.8399801254272461\n",
      "step: 47060 , time : 0.0\n",
      "train: loss: 358628.90625 acc: 0.9707756638526917  val: loss: 382582.03125 acc: 0.9440609812736511\n",
      "step: 47065 , time : 0.0\n",
      "train: loss: 382839.875 acc: 0.9661498069763184  val: loss: 798861.4375 acc: 0.8549808263778687\n",
      "step: 47070 , time : 0.0\n",
      "train: loss: 406533.625 acc: 0.9420174360275269  val: loss: 286370.1875 acc: 0.9608376026153564\n",
      "step: 47075 , time : 0.0\n",
      "train: loss: 219961.1875 acc: 0.9688506722450256  val: loss: 262921.84375 acc: 0.924186110496521\n",
      "step: 47080 , time : 0.0\n",
      "train: loss: 138620.71875 acc: 0.9117087125778198  val: loss: 154195.1875 acc: 0.9519825577735901\n",
      "step: 47085 , time : 0.0\n",
      "train: loss: 6930.61328125 acc: 0.9947235584259033  val: loss: 58529.421875 acc: 0.9811114072799683\n",
      "step: 47090 , time : 0.0\n",
      "train: loss: 37607.73828125 acc: 0.9509010314941406  val: loss: 362487.96875 acc: 0.9114030599594116\n",
      "step: 47095 , time : 0.0\n",
      "train: loss: 77006.28125 acc: 0.9685784578323364  val: loss: 169753.890625 acc: 0.8864032030105591\n",
      "step: 47100 , time : 0.0\n",
      "train: loss: 13349.0458984375 acc: 0.9815475344657898  val: loss: 717369.0625 acc: 0.7991800904273987\n",
      "step: 47105 , time : 0.015625476837158203\n",
      "train: loss: 22410.173828125 acc: 0.9749337434768677  val: loss: 1191067.875 acc: 0.7469501495361328\n",
      "step: 47110 , time : 0.0\n",
      "train: loss: 30801.337890625 acc: 0.9506211876869202  val: loss: 196341.75 acc: 0.9383025765419006\n",
      "step: 47115 , time : 0.0\n",
      "train: loss: 6081.0009765625 acc: 0.9870449900627136  val: loss: 449111.21875 acc: 0.8810399174690247\n",
      "step: 47120 , time : 0.0\n",
      "train: loss: 12149.8271484375 acc: 0.9794167280197144  val: loss: 1063709.375 acc: 0.7230851650238037\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 47125 , time : 0.0\n",
      "train: loss: 32919.30859375 acc: 0.9709510803222656  val: loss: 764162.6875 acc: 0.9202214479446411\n",
      "step: 47130 , time : 0.0010004043579101562\n",
      "train: loss: 75631.3828125 acc: 0.9607804417610168  val: loss: 2121071.25 acc: 0.6576052904129028\n",
      "step: 47135 , time : 0.0010008811950683594\n",
      "train: loss: 8337.658203125 acc: 0.9883326888084412  val: loss: 1907485.625 acc: 0.5193272829055786\n",
      "step: 47140 , time : 0.0\n",
      "train: loss: 21766.95703125 acc: 0.9877296090126038  val: loss: 594531.9375 acc: 0.9401354789733887\n",
      "step: 47145 , time : 0.0010004043579101562\n",
      "train: loss: 48820.6015625 acc: 0.973943829536438  val: loss: 941471.5 acc: 0.9330748915672302\n",
      "step: 47150 , time : 0.0\n",
      "train: loss: 14167.689453125 acc: 0.9890670776367188  val: loss: 669480.6875 acc: 0.8832259178161621\n",
      "step: 47155 , time : 0.0\n",
      "train: loss: 23308.36328125 acc: 0.9763554930686951  val: loss: 2018209.875 acc: 0.2234364151954651\n",
      "step: 47160 , time : 0.0\n",
      "train: loss: 24060.5625 acc: 0.9882386922836304  val: loss: 2522299.0 acc: 0.13844138383865356\n",
      "step: 47165 , time : 0.0\n",
      "train: loss: 26693.443359375 acc: 0.9939348697662354  val: loss: 1494443.0 acc: 0.5457693934440613\n",
      "step: 47170 , time : 0.0\n",
      "train: loss: 37109.875 acc: 0.9930102229118347  val: loss: 1428467.125 acc: 0.436474084854126\n",
      "step: 47175 , time : 0.0\n",
      "train: loss: 34245.23046875 acc: 0.9860199093818665  val: loss: 461115.84375 acc: 0.9223349094390869\n",
      "step: 47180 , time : 0.0\n",
      "train: loss: 51504.70703125 acc: 0.9863202571868896  val: loss: 2698200.75 acc: 0.6678410768508911\n",
      "step: 47185 , time : 0.0\n",
      "train: loss: 28298.73046875 acc: 0.9893587827682495  val: loss: 2028182.0 acc: 0.726502001285553\n",
      "step: 47190 , time : 0.0\n",
      "train: loss: 92408.90625 acc: 0.9799282550811768  val: loss: 4187200.25 acc: 0.410133957862854\n",
      "step: 47195 , time : 0.0\n",
      "train: loss: 113813.4765625 acc: 0.9543627500534058  val: loss: 1022197.5 acc: 0.6713396906852722\n",
      "step: 47200 , time : 0.0\n",
      "train: loss: 111886.03125 acc: 0.9784041047096252  val: loss: 1616228.875 acc: 0.8184471130371094\n",
      "step: 47205 , time : 0.0\n",
      "train: loss: 174072.28125 acc: 0.976149320602417  val: loss: 439689.0 acc: 0.9179558753967285\n",
      "step: 47210 , time : 0.0\n",
      "train: loss: 217589.71875 acc: 0.9700478315353394  val: loss: 580169.125 acc: 0.9082024693489075\n",
      "step: 47215 , time : 0.0\n",
      "train: loss: 72077.2421875 acc: 0.9932035803794861  val: loss: 2074755.875 acc: 0.22993505001068115\n",
      "step: 47220 , time : 0.0\n",
      "train: loss: 454386.21875 acc: 0.9672801494598389  val: loss: 780787.25 acc: 0.903019905090332\n",
      "step: 47225 , time : 0.0\n",
      "train: loss: 782823.5 acc: 0.9622791409492493  val: loss: 847149.25 acc: 0.8356724381446838\n",
      "step: 47230 , time : 0.0\n",
      "train: loss: 521874.59375 acc: 0.9642181396484375  val: loss: 4291080.5 acc: -0.7460106611251831\n",
      "step: 47235 , time : 0.0\n",
      "train: loss: 331752.3125 acc: 0.9734724164009094  val: loss: 1231532.0 acc: 0.8415205478668213\n",
      "step: 47240 , time : 0.0\n",
      "train: loss: 542891.0 acc: 0.9758648872375488  val: loss: 2918314.0 acc: -0.5125410556793213\n",
      "step: 47245 , time : 0.0\n",
      "train: loss: 1952890.875 acc: 0.9297105669975281  val: loss: 1169573.5 acc: 0.8527237772941589\n",
      "step: 47250 , time : 0.001001119613647461\n",
      "train: loss: 644423.3125 acc: 0.9738327860832214  val: loss: 597071.5 acc: 0.63618004322052\n",
      "step: 47255 , time : 0.0\n",
      "train: loss: 537276.875 acc: 0.9645372629165649  val: loss: 2419153.5 acc: 0.23816049098968506\n",
      "step: 47260 , time : 0.0010004043579101562\n",
      "train: loss: 424584.8125 acc: 0.9717219471931458  val: loss: 1813244.0 acc: 0.7513847351074219\n",
      "step: 47265 , time : 0.0\n",
      "train: loss: 270757.0 acc: 0.983197808265686  val: loss: 763883.9375 acc: 0.9024715423583984\n",
      "step: 47270 , time : 0.0\n",
      "train: loss: 224021.578125 acc: 0.9468093514442444  val: loss: 601178.25 acc: 0.9119073152542114\n",
      "step: 47275 , time : 0.0\n",
      "train: loss: 1013988.75 acc: 0.8325076699256897  val: loss: 716486.6875 acc: 0.869775116443634\n",
      "step: 47280 , time : 0.0\n",
      "train: loss: 1031638.25 acc: 0.7321192026138306  val: loss: 1258023.125 acc: 0.7307494878768921\n",
      "step: 47285 , time : 0.0\n",
      "train: loss: 1252702.25 acc: 0.8426491022109985  val: loss: 1283402.875 acc: 0.8415997624397278\n",
      "step: 47290 , time : 0.0\n",
      "train: loss: 554757.5625 acc: 0.7177004814147949  val: loss: 517992.9375 acc: 0.7464985847473145\n",
      "step: 47295 , time : 0.0\n",
      "train: loss: 322421.0 acc: 0.8669758439064026  val: loss: 910917.125 acc: 0.48752278089523315\n",
      "step: 47300 , time : 0.0\n",
      "train: loss: 1343427.875 acc: 0.6553860306739807  val: loss: 453652.75 acc: 0.86898273229599\n",
      "step: 47305 , time : 0.0\n",
      "train: loss: 1120233.75 acc: 0.4721683859825134  val: loss: 1810779.75 acc: 0.4724571704864502\n",
      "step: 47310 , time : 0.001001119613647461\n",
      "train: loss: 1227060.375 acc: 0.5926851034164429  val: loss: 2196147.5 acc: 0.7780680656433105\n",
      "step: 47315 , time : 0.0\n",
      "train: loss: 843117.0625 acc: 0.6451042890548706  val: loss: 2286899.0 acc: 0.6816725134849548\n",
      "step: 47320 , time : 0.0\n",
      "train: loss: 540216.3125 acc: 0.6379275321960449  val: loss: 1331056.125 acc: 0.6526147127151489\n",
      "step: 47325 , time : 0.0\n",
      "train: loss: 497319.3125 acc: 0.6377965211868286  val: loss: 1724431.125 acc: 0.716113269329071\n",
      "step: 47330 , time : 0.0\n",
      "train: loss: 119638.1171875 acc: 0.8928362727165222  val: loss: 806418.5 acc: 0.6712623238563538\n",
      "step: 47335 , time : 0.0\n",
      "train: loss: 304470.71875 acc: 0.7828372120857239  val: loss: 989109.75 acc: 0.8685382604598999\n",
      "step: 47340 , time : 0.0\n",
      "train: loss: 202901.375 acc: 0.8580289483070374  val: loss: 1854441.75 acc: 0.8404375910758972\n",
      "step: 47345 , time : 0.0\n",
      "train: loss: 146384.296875 acc: 0.8839178681373596  val: loss: 524278.1875 acc: 0.7413955926895142\n",
      "step: 47350 , time : 0.015626192092895508\n",
      "train: loss: 90154.734375 acc: 0.9200490117073059  val: loss: 540430.125 acc: 0.7377655506134033\n",
      "step: 47355 , time : 0.0\n",
      "train: loss: 155819.921875 acc: 0.8172290325164795  val: loss: 702971.5625 acc: 0.7709773778915405\n",
      "step: 47360 , time : 0.0\n",
      "train: loss: 323399.78125 acc: 0.7905282378196716  val: loss: 472043.0625 acc: 0.813473105430603\n",
      "step: 47365 , time : 0.0\n",
      "train: loss: 398847.1875 acc: 0.8039553761482239  val: loss: 1155142.125 acc: 0.647653341293335\n",
      "step: 47370 , time : 0.0\n",
      "train: loss: 630296.0 acc: 0.702917218208313  val: loss: 856117.375 acc: 0.7165632247924805\n",
      "step: 47375 , time : 0.0\n",
      "train: loss: 640495.375 acc: 0.44928449392318726  val: loss: 806643.5 acc: 0.6885106563568115\n",
      "step: 47380 , time : 0.0\n",
      "train: loss: 441572.71875 acc: 0.7638070583343506  val: loss: 804644.6875 acc: 0.6172451972961426\n",
      "step: 47385 , time : 0.0\n",
      "train: loss: 494457.375 acc: 0.6731507778167725  val: loss: 1765542.625 acc: 0.6618157625198364\n",
      "step: 47390 , time : 0.0\n",
      "train: loss: 1249560.125 acc: 0.797884464263916  val: loss: 581135.4375 acc: 0.737692654132843\n",
      "step: 47395 , time : 0.015624761581420898\n",
      "train: loss: 1538410.625 acc: 0.7650001645088196  val: loss: 638731.375 acc: 0.87554931640625\n",
      "step: 47400 , time : 0.0\n",
      "train: loss: 869614.5 acc: 0.9284806847572327  val: loss: 909106.125 acc: 0.8259958624839783\n",
      "step: 47405 , time : 0.015625476837158203\n",
      "train: loss: 535948.8125 acc: 0.9376316070556641  val: loss: 598986.75 acc: 0.7179434895515442\n",
      "step: 47410 , time : 0.0\n",
      "train: loss: 313004.21875 acc: 0.9416421055793762  val: loss: 690736.0 acc: 0.8942428827285767\n",
      "step: 47415 , time : 0.0\n",
      "train: loss: 314879.375 acc: 0.9579776525497437  val: loss: 854966.25 acc: 0.6210348010063171\n",
      "step: 47420 , time : 0.0\n",
      "train: loss: 224612.828125 acc: 0.966238796710968  val: loss: 1189394.5 acc: 0.882387638092041\n",
      "step: 47425 , time : 0.0\n",
      "train: loss: 181312.734375 acc: 0.9852901101112366  val: loss: 872613.1875 acc: 0.9076929688453674\n",
      "step: 47430 , time : 0.0\n",
      "train: loss: 105174.7109375 acc: 0.9932377338409424  val: loss: 737732.4375 acc: 0.9357116222381592\n",
      "step: 47435 , time : 0.0\n",
      "train: loss: 237055.984375 acc: 0.9668293595314026  val: loss: 1483935.125 acc: 0.8912975192070007\n",
      "step: 47440 , time : 0.0\n",
      "train: loss: 147725.953125 acc: 0.9621022939682007  val: loss: 385347.09375 acc: 0.8517107367515564\n",
      "step: 47445 , time : 0.0\n",
      "train: loss: 150828.75 acc: 0.9464196562767029  val: loss: 452374.96875 acc: 0.9361164569854736\n",
      "step: 47450 , time : 0.0\n",
      "train: loss: 41090.19140625 acc: 0.9801561236381531  val: loss: 694288.5 acc: 0.9321664571762085\n",
      "step: 47455 , time : 0.0\n",
      "train: loss: 88176.1171875 acc: 0.9598497152328491  val: loss: 1552644.75 acc: 0.8732916116714478\n",
      "step: 47460 , time : 0.0\n",
      "train: loss: 12643.599609375 acc: 0.992353081703186  val: loss: 722470.8125 acc: 0.35225772857666016\n",
      "step: 47465 , time : 0.0\n",
      "train: loss: 13603.234375 acc: 0.9821680188179016  val: loss: 1005531.75 acc: 0.9213849902153015\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 47470 , time : 0.0\n",
      "train: loss: 52694.8515625 acc: 0.9478318691253662  val: loss: 1621296.375 acc: 0.7353185415267944\n",
      "step: 47475 , time : 0.0\n",
      "train: loss: 13666.9140625 acc: 0.9564691781997681  val: loss: 1954751.875 acc: 0.8149823546409607\n",
      "step: 47480 , time : 0.0\n",
      "train: loss: 11866.7060546875 acc: 0.9805184006690979  val: loss: 2770397.0 acc: 0.39123690128326416\n",
      "step: 47485 , time : 0.0010008811950683594\n",
      "train: loss: 9437.0341796875 acc: 0.9784938097000122  val: loss: 1662996.5 acc: 0.6478093862533569\n",
      "step: 47490 , time : 0.0\n",
      "train: loss: 16037.7607421875 acc: 0.9779565930366516  val: loss: 1912548.125 acc: 0.6698123216629028\n",
      "step: 47495 , time : 0.0\n",
      "train: loss: 18611.04296875 acc: 0.9832128882408142  val: loss: 1877505.125 acc: 0.7247340679168701\n",
      "step: 47500 , time : 0.0\n",
      "train: loss: 28775.4453125 acc: 0.9867684841156006  val: loss: 1336838.375 acc: 0.8011693954467773\n",
      "step: 47505 , time : 0.015624523162841797\n",
      "train: loss: 20099.859375 acc: 0.9829853177070618  val: loss: 1590252.75 acc: 0.4566774368286133\n",
      "step: 47510 , time : 0.0\n",
      "train: loss: 25663.107421875 acc: 0.9828997254371643  val: loss: 1615904.375 acc: 0.7074804306030273\n",
      "step: 47515 , time : 0.0\n",
      "train: loss: 11853.056640625 acc: 0.984827995300293  val: loss: 4480602.0 acc: 0.13159799575805664\n",
      "step: 47520 , time : 0.0\n",
      "train: loss: 18251.10546875 acc: 0.9784221053123474  val: loss: 548404.625 acc: 0.8872233033180237\n",
      "step: 47525 , time : 0.0\n",
      "train: loss: 26806.052734375 acc: 0.9793951511383057  val: loss: 789958.0 acc: 0.8426791429519653\n",
      "step: 47530 , time : 0.0\n",
      "train: loss: 41131.3984375 acc: 0.9891366362571716  val: loss: 3738033.75 acc: 0.5983480215072632\n",
      "step: 47535 , time : 0.0\n",
      "train: loss: 58491.6953125 acc: 0.9862610101699829  val: loss: 1480053.5 acc: 0.7945946455001831\n",
      "step: 47540 , time : 0.0\n",
      "train: loss: 59071.1328125 acc: 0.9831345677375793  val: loss: 1505021.5 acc: 0.026774823665618896\n",
      "step: 47545 , time : 0.0\n",
      "train: loss: 43271.79296875 acc: 0.9645344614982605  val: loss: 1031951.75 acc: 0.6258279085159302\n",
      "step: 47550 , time : 0.0\n",
      "train: loss: 128125.65625 acc: 0.9687129259109497  val: loss: 1857286.0 acc: 0.4578321576118469\n",
      "step: 47555 , time : 0.0\n",
      "train: loss: 54355.8671875 acc: 0.9616432785987854  val: loss: 1011261.5 acc: 0.2747318148612976\n",
      "step: 47560 , time : 0.0\n",
      "train: loss: 86743.4140625 acc: 0.9594271779060364  val: loss: 556267.25 acc: 0.9444692134857178\n",
      "step: 47565 , time : 0.0\n",
      "train: loss: 153173.5625 acc: 0.9583202600479126  val: loss: 2570294.0 acc: 0.4517441987991333\n",
      "step: 47570 , time : 0.0\n",
      "train: loss: 353921.15625 acc: 0.9671215415000916  val: loss: 1644988.875 acc: 0.7294220924377441\n",
      "step: 47575 , time : 0.0\n",
      "train: loss: 60242.078125 acc: 0.9930502772331238  val: loss: 1958439.125 acc: 0.7654630541801453\n",
      "step: 47580 , time : 0.0\n",
      "train: loss: 361743.5625 acc: 0.9659225940704346  val: loss: 1684457.75 acc: 0.726433277130127\n",
      "step: 47585 , time : 0.0\n",
      "train: loss: 260430.453125 acc: 0.9259419441223145  val: loss: 2312318.75 acc: 0.21999484300613403\n",
      "step: 47590 , time : 0.0\n",
      "train: loss: 797139.1875 acc: 0.9553824067115784  val: loss: 1088026.375 acc: 0.7822921276092529\n",
      "step: 47595 , time : 0.001001119613647461\n",
      "train: loss: 610047.875 acc: 0.9748944640159607  val: loss: 1033485.1875 acc: 0.5462417602539062\n",
      "step: 47600 , time : 0.0\n",
      "train: loss: 160757.1875 acc: 0.9711732268333435  val: loss: 757784.3125 acc: 0.8056238889694214\n",
      "step: 47605 , time : 0.0\n",
      "train: loss: 1270272.125 acc: 0.9390383958816528  val: loss: 2620461.75 acc: 0.5269854068756104\n",
      "step: 47610 , time : 0.0\n",
      "train: loss: 642710.625 acc: 0.9817869663238525  val: loss: 421587.6875 acc: 0.942167341709137\n",
      "step: 47615 , time : 0.0\n",
      "train: loss: 807783.375 acc: 0.9703267216682434  val: loss: 972308.375 acc: 0.6218206882476807\n",
      "step: 47620 , time : 0.0\n",
      "train: loss: 2138583.0 acc: 0.9020892381668091  val: loss: 609114.5625 acc: 0.8875397443771362\n",
      "step: 47625 , time : 0.0\n",
      "train: loss: 692644.375 acc: 0.88301682472229  val: loss: 1657662.875 acc: 0.7480341196060181\n",
      "step: 47630 , time : 0.0\n",
      "train: loss: 447483.59375 acc: 0.9329746961593628  val: loss: 287963.4375 acc: 0.8902987241744995\n",
      "step: 47635 , time : 0.0\n",
      "train: loss: 382128.6875 acc: 0.9336796402931213  val: loss: 908651.125 acc: 0.8381454944610596\n",
      "step: 47640 , time : 0.0\n",
      "train: loss: 499754.21875 acc: 0.9332595467567444  val: loss: 890704.1875 acc: 0.865314245223999\n",
      "step: 47645 , time : 0.0\n",
      "train: loss: 943640.25 acc: 0.7059366703033447  val: loss: 707258.4375 acc: 0.7229009866714478\n",
      "step: 47650 , time : 0.0\n",
      "train: loss: 802741.1875 acc: 0.8584005832672119  val: loss: 1605958.125 acc: 0.40903007984161377\n",
      "step: 47655 , time : 0.0\n",
      "train: loss: 1234061.625 acc: 0.482238233089447  val: loss: 677304.5625 acc: 0.8606008291244507\n",
      "step: 47660 , time : 0.0\n",
      "train: loss: 427439.875 acc: 0.8426240682601929  val: loss: 369406.90625 acc: 0.8222136497497559\n",
      "step: 47665 , time : 0.0\n",
      "train: loss: 1110931.0 acc: 0.4965502619743347  val: loss: 521684.8125 acc: 0.785700798034668\n",
      "step: 47670 , time : 0.0\n",
      "train: loss: 1332254.5 acc: 0.47120916843414307  val: loss: 1614214.375 acc: 0.32368725538253784\n",
      "step: 47675 , time : 0.0\n",
      "train: loss: 581346.625 acc: 0.5837762355804443  val: loss: 2195642.0 acc: -0.1009742021560669\n",
      "step: 47680 , time : 0.0\n",
      "train: loss: 729823.3125 acc: 0.6418787837028503  val: loss: 1445822.625 acc: 0.4873926639556885\n",
      "step: 47685 , time : 0.0\n",
      "train: loss: 693452.9375 acc: 0.6247272491455078  val: loss: 996627.4375 acc: 0.8517894744873047\n",
      "step: 47690 , time : 0.0\n",
      "train: loss: 104624.5234375 acc: 0.920625627040863  val: loss: 952181.375 acc: 0.613737165927887\n",
      "step: 47695 , time : 0.0010013580322265625\n",
      "train: loss: 164345.390625 acc: 0.870090126991272  val: loss: 877838.5 acc: 0.8463485240936279\n",
      "step: 47700 , time : 0.0010004043579101562\n",
      "train: loss: 271560.34375 acc: 0.8287420272827148  val: loss: 758822.375 acc: 0.7366401553153992\n",
      "step: 47705 , time : 0.0010004043579101562\n",
      "train: loss: 211136.640625 acc: 0.8412523865699768  val: loss: 440556.34375 acc: 0.8566598892211914\n",
      "step: 47710 , time : 0.0010004043579101562\n",
      "train: loss: 108357.9921875 acc: 0.8964071869850159  val: loss: 888954.125 acc: 0.7967695593833923\n",
      "step: 47715 , time : 0.0\n",
      "train: loss: 237613.015625 acc: 0.7138890027999878  val: loss: 659294.0625 acc: 0.8225563764572144\n",
      "step: 47720 , time : 0.0\n",
      "train: loss: 257779.421875 acc: 0.6577636003494263  val: loss: 395987.1875 acc: 0.8766483664512634\n",
      "step: 47725 , time : 0.0\n",
      "train: loss: 131440.34375 acc: 0.8671970367431641  val: loss: 2037955.375 acc: 0.8096938729286194\n",
      "step: 47730 , time : 0.0\n",
      "train: loss: 486777.21875 acc: 0.6112979650497437  val: loss: 4771215.5 acc: 0.7304456233978271\n",
      "step: 47735 , time : 0.0\n",
      "train: loss: 539478.3125 acc: 0.6170917749404907  val: loss: 684311.5 acc: 0.8394505977630615\n",
      "step: 47740 , time : 0.0\n",
      "train: loss: 586734.1875 acc: 0.6661972999572754  val: loss: 3845695.5 acc: 0.45371532440185547\n",
      "step: 47745 , time : 0.0\n",
      "train: loss: 303677.46875 acc: 0.7716472148895264  val: loss: 602627.0625 acc: 0.8785966634750366\n",
      "step: 47750 , time : 0.0\n",
      "train: loss: 737409.75 acc: 0.6963933706283569  val: loss: 1193365.875 acc: 0.7325289249420166\n",
      "step: 47755 , time : 0.0\n",
      "train: loss: 592478.3125 acc: 0.7939594388008118  val: loss: 1049241.125 acc: 0.809967041015625\n",
      "step: 47760 , time : 0.0\n",
      "train: loss: 917370.5625 acc: 0.8455245494842529  val: loss: 1365288.125 acc: 0.813867449760437\n",
      "step: 47765 , time : 0.0\n",
      "train: loss: 858905.3125 acc: 0.8779042959213257  val: loss: 816105.25 acc: 0.8944495916366577\n",
      "step: 47770 , time : 0.0\n",
      "train: loss: 325992.625 acc: 0.9687758088111877  val: loss: 1717250.0 acc: 0.6677470207214355\n",
      "step: 47775 , time : 0.0\n",
      "train: loss: 372750.40625 acc: 0.9095686078071594  val: loss: 1098621.375 acc: 0.828197717666626\n",
      "step: 47780 , time : 0.0\n",
      "train: loss: 170608.296875 acc: 0.9686521291732788  val: loss: 1144381.375 acc: 0.8112431764602661\n",
      "step: 47785 , time : 0.0\n",
      "train: loss: 285689.28125 acc: 0.9767475128173828  val: loss: 1415476.5 acc: 0.7811340093612671\n",
      "step: 47790 , time : 0.0\n",
      "train: loss: 158671.234375 acc: 0.9883451461791992  val: loss: 1802151.375 acc: 0.28115683794021606\n",
      "step: 47795 , time : 0.0\n",
      "train: loss: 219674.71875 acc: 0.9848049879074097  val: loss: 2788376.75 acc: 0.4310287833213806\n",
      "step: 47800 , time : 0.0\n",
      "train: loss: 206683.65625 acc: 0.9798967838287354  val: loss: 1954594.25 acc: 0.6745122671127319\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 47805 , time : 0.0\n",
      "train: loss: 114191.5 acc: 0.9832667708396912  val: loss: 1212172.875 acc: 0.8484349250793457\n",
      "step: 47810 , time : 0.001001119613647461\n",
      "train: loss: 237571.53125 acc: 0.9400272965431213  val: loss: 274637.875 acc: 0.9517107605934143\n",
      "step: 47815 , time : 0.0010008811950683594\n",
      "train: loss: 14859.6474609375 acc: 0.9635301232337952  val: loss: 2568198.75 acc: 0.8416796326637268\n",
      "step: 47820 , time : 0.0\n",
      "train: loss: 36151.7109375 acc: 0.9727662205696106  val: loss: 660454.25 acc: 0.8439508676528931\n",
      "step: 47825 , time : 0.0\n",
      "train: loss: 72693.546875 acc: 0.9629355669021606  val: loss: 278515.71875 acc: 0.9232344627380371\n",
      "step: 47830 , time : 0.001001119613647461\n",
      "train: loss: 80434.234375 acc: 0.9532575607299805  val: loss: 949203.125 acc: 0.7621475458145142\n",
      "step: 47835 , time : 0.0\n",
      "train: loss: 13359.2060546875 acc: 0.9731342792510986  val: loss: 1367081.625 acc: 0.9132629632949829\n",
      "step: 47840 , time : 0.0\n",
      "train: loss: 11832.4501953125 acc: 0.9400726556777954  val: loss: 450586.3125 acc: 0.810981810092926\n",
      "step: 47845 , time : 0.0010006427764892578\n",
      "train: loss: 10682.7587890625 acc: 0.9849679470062256  val: loss: 760381.875 acc: 0.7236330509185791\n",
      "step: 47850 , time : 0.0\n",
      "train: loss: 4023.843017578125 acc: 0.9910954833030701  val: loss: 1020210.0 acc: 0.6633520126342773\n",
      "step: 47855 , time : 0.0\n",
      "train: loss: 5565.22998046875 acc: 0.9849499464035034  val: loss: 1973451.375 acc: 0.804824709892273\n",
      "step: 47860 , time : 0.0\n",
      "train: loss: 40007.80078125 acc: 0.9509015679359436  val: loss: 1245745.25 acc: 0.7188714742660522\n",
      "step: 47865 , time : 0.0010006427764892578\n",
      "train: loss: 107567.1796875 acc: 0.9531408548355103  val: loss: 695916.125 acc: 0.9043776392936707\n",
      "step: 47870 , time : 0.0\n",
      "train: loss: 40269.296875 acc: 0.9761082530021667  val: loss: 1207889.625 acc: 0.6252343058586121\n",
      "step: 47875 , time : 0.0010008811950683594\n",
      "train: loss: 82466.8515625 acc: 0.9630369544029236  val: loss: 820871.9375 acc: 0.7351772785186768\n",
      "step: 47880 , time : 0.0\n",
      "train: loss: 24862.4609375 acc: 0.9819216728210449  val: loss: 767463.25 acc: 0.8118219375610352\n",
      "step: 47885 , time : 0.0\n",
      "train: loss: 10135.3115234375 acc: 0.9914640784263611  val: loss: 1263625.5 acc: 0.7706568837165833\n",
      "step: 47890 , time : 0.0\n",
      "train: loss: 12370.9951171875 acc: 0.9946508407592773  val: loss: 2468343.75 acc: 0.528567910194397\n",
      "step: 47895 , time : 0.0\n",
      "train: loss: 50351.93359375 acc: 0.9835212826728821  val: loss: 1461059.25 acc: 0.8820189237594604\n",
      "step: 47900 , time : 0.0\n",
      "train: loss: 51033.02734375 acc: 0.986605703830719  val: loss: 271684.8125 acc: 0.92635178565979\n",
      "step: 47905 , time : 0.0\n",
      "train: loss: 31883.4453125 acc: 0.992607831954956  val: loss: 2170453.75 acc: 0.6232818365097046\n",
      "step: 47910 , time : 0.0\n",
      "train: loss: 70924.1015625 acc: 0.9327392578125  val: loss: 1585625.375 acc: 0.5536762475967407\n",
      "step: 47915 , time : 0.0\n",
      "train: loss: 65616.453125 acc: 0.9748172163963318  val: loss: 284363.90625 acc: 0.9007625579833984\n",
      "step: 47920 , time : 0.0\n",
      "train: loss: 190700.328125 acc: 0.9187304377555847  val: loss: 1081260.875 acc: 0.6105562448501587\n",
      "step: 47925 , time : 0.0010006427764892578\n",
      "train: loss: 45544.09765625 acc: 0.9799652099609375  val: loss: 704324.875 acc: 0.9026092290878296\n",
      "step: 47930 , time : 0.0010008811950683594\n",
      "train: loss: 102456.6875 acc: 0.9800244569778442  val: loss: 1284958.0 acc: -0.4417741298675537\n",
      "step: 47935 , time : 0.0\n",
      "train: loss: 151877.15625 acc: 0.9869356751441956  val: loss: 1618890.875 acc: 0.4688591957092285\n",
      "step: 47940 , time : 0.001001119613647461\n",
      "train: loss: 551338.6875 acc: 0.932023286819458  val: loss: 196328.78125 acc: 0.9377965927124023\n",
      "step: 47945 , time : 0.0\n",
      "train: loss: 123066.4609375 acc: 0.9746850728988647  val: loss: 774793.1875 acc: 0.8599705100059509\n",
      "step: 47950 , time : 0.0\n",
      "train: loss: 222982.59375 acc: 0.9654209613800049  val: loss: 1320129.625 acc: 0.7319632768630981\n",
      "step: 47955 , time : 0.0\n",
      "train: loss: 139792.265625 acc: 0.9778732061386108  val: loss: 1190524.0 acc: 0.479939341545105\n",
      "step: 47960 , time : 0.0\n",
      "train: loss: 432825.90625 acc: 0.9403864741325378  val: loss: 970434.75 acc: 0.6648419499397278\n",
      "step: 47965 , time : 0.0\n",
      "train: loss: 212973.015625 acc: 0.952300488948822  val: loss: 1357703.375 acc: 0.6248939037322998\n",
      "step: 47970 , time : 0.0\n",
      "train: loss: 1224289.125 acc: 0.832506000995636  val: loss: 968279.1875 acc: 0.7204453349113464\n",
      "step: 47975 , time : 0.0010004043579101562\n",
      "train: loss: 528102.75 acc: 0.9825026988983154  val: loss: 654599.8125 acc: 0.7986398935317993\n",
      "step: 47980 , time : 0.0010006427764892578\n",
      "train: loss: 1401026.625 acc: 0.9477133750915527  val: loss: 494225.25 acc: 0.9181075096130371\n",
      "step: 47985 , time : 0.0001671314239501953\n",
      "train: loss: 821340.0625 acc: 0.9663501977920532  val: loss: 463666.21875 acc: 0.9520149230957031\n",
      "step: 47990 , time : 0.0\n",
      "train: loss: 988579.3125 acc: 0.9309003949165344  val: loss: 308394.0625 acc: 0.9193471074104309\n",
      "step: 47995 , time : 0.0\n",
      "train: loss: 1811493.125 acc: 0.8956523537635803  val: loss: 235551.84375 acc: 0.9484485983848572\n",
      "step: 48000 , time : 0.0\n",
      "train: loss: 336927.96875 acc: 0.96978759765625  val: loss: 1122531.375 acc: 0.720576822757721\n",
      "step: 48005 , time : 0.0\n",
      "train: loss: 503674.71875 acc: 0.9051473140716553  val: loss: 243543.9375 acc: 0.9003880023956299\n",
      "step: 48010 , time : 0.0\n",
      "train: loss: 1845428.875 acc: 0.38189369440078735  val: loss: 1237959.875 acc: 0.6836308240890503\n",
      "step: 48015 , time : 0.0\n",
      "train: loss: 958348.5 acc: 0.5000506043434143  val: loss: 1004688.875 acc: 0.7112512588500977\n",
      "step: 48020 , time : 0.0\n",
      "train: loss: 1369530.125 acc: 0.43393874168395996  val: loss: 1018131.0 acc: 0.867953360080719\n",
      "step: 48025 , time : 0.0\n",
      "train: loss: 451012.71875 acc: 0.7580493688583374  val: loss: 1656365.125 acc: 0.4100712537765503\n",
      "step: 48030 , time : 0.0010006427764892578\n",
      "train: loss: 1275017.625 acc: 0.42093580961227417  val: loss: 707253.1875 acc: 0.883502185344696\n",
      "step: 48035 , time : 0.0\n",
      "train: loss: 1541652.25 acc: 0.4725964665412903  val: loss: 1579966.375 acc: 0.3113516569137573\n",
      "step: 48040 , time : 0.0010006427764892578\n",
      "train: loss: 1815750.125 acc: 0.47954481840133667  val: loss: 2479923.75 acc: 0.3474063277244568\n",
      "step: 48045 , time : 0.0\n",
      "train: loss: 874820.25 acc: 0.41900932788848877  val: loss: 1315934.75 acc: 0.831243634223938\n",
      "step: 48050 , time : 0.0010006427764892578\n",
      "train: loss: 272294.15625 acc: 0.7320523858070374  val: loss: 2093409.625 acc: 0.8247573375701904\n",
      "step: 48055 , time : 0.0\n",
      "train: loss: 174156.765625 acc: 0.8648353815078735  val: loss: 642891.3125 acc: 0.8053238987922668\n",
      "step: 48060 , time : 0.0\n",
      "train: loss: 491443.28125 acc: 0.5850152969360352  val: loss: 1932821.75 acc: 0.7277894020080566\n",
      "step: 48065 , time : 0.0\n",
      "train: loss: 105879.578125 acc: 0.9173104166984558  val: loss: 1255954.625 acc: 0.8486122488975525\n",
      "step: 48070 , time : 0.0\n",
      "train: loss: 224099.234375 acc: 0.8266988396644592  val: loss: 1184442.125 acc: 0.7295008897781372\n",
      "step: 48075 , time : 0.0\n",
      "train: loss: 537615.875 acc: 0.7664673328399658  val: loss: 602323.4375 acc: 0.8940267562866211\n",
      "step: 48080 , time : 0.0\n",
      "train: loss: 81283.828125 acc: 0.9289975166320801  val: loss: 2360033.75 acc: 0.8360230922698975\n",
      "step: 48085 , time : 0.0\n",
      "train: loss: 267699.84375 acc: 0.8075083494186401  val: loss: 324204.125 acc: 0.8791044354438782\n",
      "step: 48090 , time : 0.0\n",
      "train: loss: 98488.671875 acc: 0.9292148351669312  val: loss: 3993506.0 acc: 0.7215390205383301\n",
      "step: 48095 , time : 0.0\n",
      "train: loss: 586951.3125 acc: 0.7619760036468506  val: loss: 2718274.75 acc: 0.7703444957733154\n",
      "step: 48100 , time : 0.0\n",
      "train: loss: 1318118.625 acc: 0.6565216779708862  val: loss: 1225002.5 acc: 0.8040525913238525\n",
      "step: 48105 , time : 0.0\n",
      "train: loss: 550838.0625 acc: 0.6687396168708801  val: loss: 2375992.25 acc: 0.6813098788261414\n",
      "step: 48110 , time : 0.0009996891021728516\n",
      "train: loss: 790671.0625 acc: 0.6796495318412781  val: loss: 2839367.75 acc: 0.6854993104934692\n",
      "step: 48115 , time : 0.0\n",
      "train: loss: 627735.0625 acc: 0.7333253622055054  val: loss: 1380619.25 acc: 0.7766301035881042\n",
      "step: 48120 , time : 0.0010006427764892578\n",
      "train: loss: 1518214.375 acc: 0.7500779032707214  val: loss: 976764.375 acc: 0.802651584148407\n",
      "step: 48125 , time : 0.0010008811950683594\n",
      "train: loss: 1458978.125 acc: 0.8001528978347778  val: loss: 1820573.875 acc: 0.7971215844154358\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 48130 , time : 0.0\n",
      "train: loss: 1407083.125 acc: 0.888445258140564  val: loss: 1208490.25 acc: 0.7900012135505676\n",
      "step: 48135 , time : 0.0010006427764892578\n",
      "train: loss: 451260.46875 acc: 0.9583088755607605  val: loss: 1920124.875 acc: 0.28024572134017944\n",
      "step: 48140 , time : 0.0\n",
      "train: loss: 710368.125 acc: 0.9012517929077148  val: loss: 2388981.0 acc: 0.4985172152519226\n",
      "step: 48145 , time : 0.0\n",
      "train: loss: 177010.34375 acc: 0.9736195802688599  val: loss: 2171760.5 acc: 0.7873190641403198\n",
      "step: 48150 , time : 0.0\n",
      "train: loss: 230107.578125 acc: 0.973052442073822  val: loss: 1532257.625 acc: 0.6886630058288574\n",
      "step: 48155 , time : 0.0\n",
      "train: loss: 194285.421875 acc: 0.9836275577545166  val: loss: 1814969.625 acc: 0.4146292805671692\n",
      "step: 48160 , time : 0.0\n",
      "train: loss: 294692.15625 acc: 0.9806239008903503  val: loss: 890513.625 acc: 0.866420328617096\n",
      "step: 48165 , time : 0.0\n",
      "train: loss: 314730.21875 acc: 0.9613901376724243  val: loss: 1939506.125 acc: 0.8007254004478455\n",
      "step: 48170 , time : 0.0\n",
      "train: loss: 251824.0625 acc: 0.9559922218322754  val: loss: 413162.71875 acc: 0.7473687529563904\n",
      "step: 48175 , time : 0.0\n",
      "train: loss: 169868.8125 acc: 0.9357964992523193  val: loss: 975350.375 acc: 0.9029375314712524\n",
      "step: 48180 , time : 0.0\n",
      "train: loss: 99392.0625 acc: 0.9713276624679565  val: loss: 1832984.375 acc: 0.7400503158569336\n",
      "step: 48185 , time : 0.0\n",
      "train: loss: 17228.01953125 acc: 0.9869763851165771  val: loss: 1827573.375 acc: 0.7951259613037109\n",
      "step: 48190 , time : 0.0\n",
      "train: loss: 98900.03125 acc: 0.9559000730514526  val: loss: 384601.53125 acc: 0.9428566098213196\n",
      "step: 48195 , time : 0.0\n",
      "train: loss: 23234.65234375 acc: 0.9843907952308655  val: loss: 1311422.875 acc: 0.676122784614563\n",
      "step: 48200 , time : 0.0\n",
      "train: loss: 14332.4619140625 acc: 0.9796395897865295  val: loss: 1496991.875 acc: 0.7673807144165039\n",
      "step: 48205 , time : 0.0\n",
      "train: loss: 10426.5703125 acc: 0.9741380214691162  val: loss: 2016005.25 acc: -0.9935539960861206\n",
      "step: 48210 , time : 0.0\n",
      "train: loss: 8034.6875 acc: 0.9745561480522156  val: loss: 1204764.0 acc: 0.7329050302505493\n",
      "step: 48215 , time : 0.0\n",
      "train: loss: 2706.454833984375 acc: 0.9909484386444092  val: loss: 779008.25 acc: 0.6867902874946594\n",
      "step: 48220 , time : 0.0\n",
      "train: loss: 10982.447265625 acc: 0.9583582878112793  val: loss: 690518.4375 acc: 0.9069523215293884\n",
      "step: 48225 , time : 0.0\n",
      "train: loss: 57287.89453125 acc: 0.9629765152931213  val: loss: 570134.1875 acc: 0.9362707734107971\n",
      "step: 48230 , time : 0.0\n",
      "train: loss: 9940.46484375 acc: 0.9865574836730957  val: loss: 412063.28125 acc: 0.9123040437698364\n",
      "step: 48235 , time : 0.0010008811950683594\n",
      "train: loss: 66610.59375 acc: 0.9766374826431274  val: loss: 362628.75 acc: 0.9039550423622131\n",
      "step: 48240 , time : 0.0010006427764892578\n",
      "train: loss: 20582.658203125 acc: 0.9896981716156006  val: loss: 897030.25 acc: 0.5803213715553284\n",
      "step: 48245 , time : 0.0010004043579101562\n",
      "train: loss: 27468.796875 acc: 0.9767546653747559  val: loss: 1608029.625 acc: 0.5890828371047974\n",
      "step: 48250 , time : 0.0\n",
      "train: loss: 32065.6328125 acc: 0.9781107306480408  val: loss: 1029341.5 acc: 0.8618871569633484\n",
      "step: 48255 , time : 0.0010008811950683594\n",
      "train: loss: 8588.98828125 acc: 0.9953863024711609  val: loss: 348446.34375 acc: 0.9361886382102966\n",
      "step: 48260 , time : 0.0\n",
      "train: loss: 35100.8671875 acc: 0.990909993648529  val: loss: 168659.53125 acc: 0.9334836006164551\n",
      "step: 48265 , time : 0.0\n",
      "train: loss: 57393.453125 acc: 0.974942684173584  val: loss: 236863.234375 acc: 0.9439004063606262\n",
      "step: 48270 , time : 0.0\n",
      "train: loss: 30244.06640625 acc: 0.9923572540283203  val: loss: 2793476.0 acc: -0.8914401531219482\n",
      "step: 48275 , time : 0.0\n",
      "train: loss: 32867.3984375 acc: 0.9761021733283997  val: loss: 370505.5625 acc: 0.9324557185173035\n",
      "step: 48280 , time : 0.0010006427764892578\n",
      "train: loss: 242879.359375 acc: 0.8988818526268005  val: loss: 71983.0390625 acc: 0.9207345843315125\n",
      "step: 48285 , time : 0.0\n",
      "train: loss: 53507.28515625 acc: 0.980198860168457  val: loss: 695330.8125 acc: 0.6867458820343018\n",
      "step: 48290 , time : 0.0\n",
      "train: loss: 100174.6953125 acc: 0.957511842250824  val: loss: 1279925.5 acc: 0.4477245807647705\n",
      "step: 48295 , time : 0.0\n",
      "train: loss: 47174.66796875 acc: 0.9872863292694092  val: loss: 1799131.125 acc: 0.647240161895752\n",
      "step: 48300 , time : 0.0\n",
      "train: loss: 168510.65625 acc: 0.9817230105400085  val: loss: 203960.84375 acc: 0.7989859580993652\n",
      "step: 48305 , time : 0.0\n",
      "train: loss: 29628.068359375 acc: 0.9973940849304199  val: loss: 1121681.375 acc: 0.6768332123756409\n",
      "step: 48310 , time : 0.0\n",
      "train: loss: 56495.19921875 acc: 0.9938320517539978  val: loss: 408584.21875 acc: 0.9217520356178284\n",
      "step: 48315 , time : 0.0\n",
      "train: loss: 154592.375 acc: 0.9809032082557678  val: loss: 149586.46875 acc: 0.9620401263237\n",
      "step: 48320 , time : 0.0\n",
      "train: loss: 171164.265625 acc: 0.9795436859130859  val: loss: 2800003.5 acc: -0.4588671922683716\n",
      "step: 48325 , time : 0.01562643051147461\n",
      "train: loss: 469300.6875 acc: 0.9688668847084045  val: loss: 1999555.375 acc: 0.38865840435028076\n",
      "step: 48330 , time : 0.0\n",
      "train: loss: 1513478.875 acc: 0.672420859336853  val: loss: 771039.8125 acc: 0.8386218547821045\n",
      "step: 48335 , time : 0.0\n",
      "train: loss: 234410.484375 acc: 0.9813673496246338  val: loss: 301891.71875 acc: 0.8081251382827759\n",
      "step: 48340 , time : 0.0\n",
      "train: loss: 396960.125 acc: 0.9830750823020935  val: loss: 869494.5625 acc: 0.8092503547668457\n",
      "step: 48345 , time : 0.0\n",
      "train: loss: 2588119.75 acc: 0.9253308176994324  val: loss: 716338.5625 acc: 0.8505754470825195\n",
      "step: 48350 , time : 0.0\n",
      "train: loss: 921148.1875 acc: 0.9541678428649902  val: loss: 277373.28125 acc: 0.9681503772735596\n",
      "step: 48355 , time : 0.001001119613647461\n",
      "train: loss: 1337852.875 acc: 0.9296689033508301  val: loss: 419398.125 acc: 0.9477376341819763\n",
      "step: 48360 , time : 0.0\n",
      "train: loss: 308318.0625 acc: 0.9702014923095703  val: loss: 153291.15625 acc: 0.9487011432647705\n",
      "step: 48365 , time : 0.0\n",
      "train: loss: 576620.4375 acc: 0.9572386145591736  val: loss: 168908.578125 acc: 0.9754629731178284\n",
      "step: 48370 , time : 0.0\n",
      "train: loss: 252801.84375 acc: 0.9199919700622559  val: loss: 237993.15625 acc: 0.9659314751625061\n",
      "step: 48375 , time : 0.0\n",
      "train: loss: 1652505.375 acc: 0.3797485828399658  val: loss: 1041064.4375 acc: 0.8556365370750427\n",
      "step: 48380 , time : 0.0\n",
      "train: loss: 1376782.625 acc: 0.7431434392929077  val: loss: 1591960.0 acc: 0.8477637767791748\n",
      "step: 48385 , time : 0.0\n",
      "train: loss: 584956.6875 acc: 0.7427524328231812  val: loss: 7241930.0 acc: 0.6194124221801758\n",
      "step: 48390 , time : 0.0\n",
      "train: loss: 606821.625 acc: 0.7802608013153076  val: loss: 1012234.25 acc: 0.8517530560493469\n",
      "step: 48395 , time : 0.0\n",
      "train: loss: 360552.84375 acc: 0.8964513540267944  val: loss: 1874136.375 acc: 0.8361178040504456\n",
      "step: 48400 , time : 0.0\n",
      "train: loss: 1523007.5 acc: 0.7453645467758179  val: loss: 2309019.0 acc: 0.693379282951355\n",
      "step: 48405 , time : 0.0\n",
      "train: loss: 494348.21875 acc: 0.665019154548645  val: loss: 400482.34375 acc: 0.7788260579109192\n",
      "step: 48410 , time : 0.0010004043579101562\n",
      "train: loss: 54141.21875 acc: 0.9459950923919678  val: loss: 865381.875 acc: 0.6988178491592407\n",
      "step: 48415 , time : 0.0\n",
      "train: loss: 120403.2890625 acc: 0.8687652945518494  val: loss: 993549.25 acc: 0.7206337451934814\n",
      "step: 48420 , time : 0.0\n",
      "train: loss: 65899.7890625 acc: 0.9542208313941956  val: loss: 1493743.25 acc: 0.7049746513366699\n",
      "step: 48425 , time : 0.0\n",
      "train: loss: 38996.453125 acc: 0.9681438207626343  val: loss: 1163116.125 acc: 0.7791410088539124\n",
      "step: 48430 , time : 0.0\n",
      "train: loss: 498139.1875 acc: 0.8089057803153992  val: loss: 775793.125 acc: 0.7602852582931519\n",
      "step: 48435 , time : 0.0\n",
      "train: loss: 86704.3203125 acc: 0.9414074420928955  val: loss: 3529652.75 acc: 0.6397405862808228\n",
      "step: 48440 , time : 0.0\n",
      "train: loss: 449199.65625 acc: 0.7590533494949341  val: loss: 4285463.0 acc: 0.6300452947616577\n",
      "step: 48445 , time : 0.0\n",
      "train: loss: 203542.46875 acc: 0.8424145579338074  val: loss: 1641945.625 acc: 0.7433856725692749\n",
      "step: 48450 , time : 0.0\n",
      "train: loss: 83916.96875 acc: 0.9264823198318481  val: loss: 1553312.75 acc: 0.7777755856513977\n",
      "step: 48455 , time : 0.0\n",
      "train: loss: 253715.25 acc: 0.8227558135986328  val: loss: 2527470.0 acc: 0.700423538684845\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 48460 , time : 0.0010013580322265625\n",
      "train: loss: 89176.96875 acc: 0.9005239009857178  val: loss: 214059.9375 acc: 0.8359405398368835\n",
      "step: 48465 , time : 0.0\n",
      "train: loss: 161026.90625 acc: 0.8861470222473145  val: loss: 4949080.0 acc: 0.518706202507019\n",
      "step: 48470 , time : 0.0\n",
      "train: loss: 425743.0625 acc: 0.7942018508911133  val: loss: 2306133.25 acc: 0.6503201127052307\n",
      "step: 48475 , time : 0.0010006427764892578\n",
      "train: loss: 195305.640625 acc: 0.8114562630653381  val: loss: 1437473.375 acc: 0.7249886989593506\n",
      "step: 48480 , time : 0.0010006427764892578\n",
      "train: loss: 142072.546875 acc: 0.8492273688316345  val: loss: 1068285.75 acc: 0.7758457064628601\n",
      "step: 48485 , time : 0.001001119613647461\n",
      "train: loss: 663848.125 acc: 0.766127347946167  val: loss: 1730429.25 acc: 0.7091784477233887\n",
      "step: 48490 , time : 0.0\n",
      "train: loss: 1444773.25 acc: 0.7660607099533081  val: loss: 700397.1875 acc: 0.7885182499885559\n",
      "step: 48495 , time : 0.015625476837158203\n",
      "train: loss: 928542.0625 acc: 0.8890373110771179  val: loss: 1985718.25 acc: 0.332463800907135\n",
      "step: 48500 , time : 0.0\n",
      "train: loss: 455342.625 acc: 0.9631323218345642  val: loss: 625790.9375 acc: 0.8064391613006592\n",
      "step: 48505 , time : 0.0\n",
      "train: loss: 146766.5 acc: 0.9847646951675415  val: loss: 819521.75 acc: 0.9066056609153748\n",
      "step: 48510 , time : 0.0\n",
      "train: loss: 151778.03125 acc: 0.9836977124214172  val: loss: 592134.9375 acc: 0.9291030764579773\n",
      "step: 48515 , time : 0.0\n",
      "train: loss: 48877.78515625 acc: 0.9933317303657532  val: loss: 1292041.0 acc: 0.26037198305130005\n",
      "step: 48520 , time : 0.0\n",
      "train: loss: 65789.484375 acc: 0.9932067394256592  val: loss: 1641549.25 acc: 0.6582735776901245\n",
      "step: 48525 , time : 0.0\n",
      "train: loss: 78499.265625 acc: 0.9940757751464844  val: loss: 1248897.25 acc: 0.5847306251525879\n",
      "step: 48530 , time : 0.0\n",
      "train: loss: 84673.0859375 acc: 0.9909921288490295  val: loss: 849594.25 acc: 0.8869555592536926\n",
      "step: 48535 , time : 0.0\n",
      "train: loss: 66674.71875 acc: 0.9871938824653625  val: loss: 874183.75 acc: 0.889181911945343\n",
      "step: 48540 , time : 0.0\n",
      "train: loss: 21457.560546875 acc: 0.9901173710823059  val: loss: 1650415.75 acc: 0.6847100257873535\n",
      "step: 48545 , time : 0.0\n",
      "train: loss: 21010.388671875 acc: 0.984043538570404  val: loss: 1506767.25 acc: 0.7534477114677429\n",
      "step: 48550 , time : 0.0\n",
      "train: loss: 20699.9921875 acc: 0.9854853749275208  val: loss: 880773.1875 acc: 0.6712965369224548\n",
      "step: 48555 , time : 0.0\n",
      "train: loss: 15003.49609375 acc: 0.9924667477607727  val: loss: 1119527.875 acc: 0.7374135255813599\n",
      "step: 48560 , time : 0.0\n",
      "train: loss: 5026.31640625 acc: 0.9974126219749451  val: loss: 208358.984375 acc: 0.9524741768836975\n",
      "step: 48565 , time : 0.0\n",
      "train: loss: 8064.873046875 acc: 0.9858826994895935  val: loss: 341984.78125 acc: 0.9070616364479065\n",
      "step: 48570 , time : 0.0\n",
      "train: loss: 13999.75 acc: 0.9912408590316772  val: loss: 1263391.5 acc: 0.6120080947875977\n",
      "step: 48575 , time : 0.0\n",
      "train: loss: 8523.27734375 acc: 0.9845749139785767  val: loss: 369322.28125 acc: 0.8478054404258728\n",
      "step: 48580 , time : 0.0\n",
      "train: loss: 5627.73486328125 acc: 0.985808253288269  val: loss: 380349.4375 acc: 0.8913702964782715\n",
      "step: 48585 , time : 0.0010006427764892578\n",
      "train: loss: 13875.494140625 acc: 0.9766205549240112  val: loss: 1523980.75 acc: 0.7387768030166626\n",
      "step: 48590 , time : 0.0\n",
      "train: loss: 40210.46875 acc: 0.9775221943855286  val: loss: 346530.5625 acc: 0.8604046106338501\n",
      "step: 48595 , time : 0.0\n",
      "train: loss: 41850.828125 acc: 0.9801321625709534  val: loss: 362502.375 acc: 0.8528550267219543\n",
      "step: 48600 , time : 0.0010006427764892578\n",
      "train: loss: 26891.52734375 acc: 0.9854332208633423  val: loss: 551150.125 acc: 0.7466586232185364\n",
      "step: 48605 , time : 0.0\n",
      "train: loss: 14031.5126953125 acc: 0.9867251515388489  val: loss: 856782.375 acc: 0.74788498878479\n",
      "step: 48610 , time : 0.0\n",
      "train: loss: 25613.15234375 acc: 0.9785888195037842  val: loss: 1240703.75 acc: 0.7191533446311951\n",
      "step: 48615 , time : 0.0\n",
      "train: loss: 20168.244140625 acc: 0.9713680148124695  val: loss: 98699.0859375 acc: 0.9718303084373474\n",
      "step: 48620 , time : 0.0\n",
      "train: loss: 25990.80859375 acc: 0.9805114269256592  val: loss: 1645404.75 acc: 0.7104775905609131\n",
      "step: 48625 , time : 0.0\n",
      "train: loss: 40630.19921875 acc: 0.9868628978729248  val: loss: 773832.8125 acc: 0.9003956913948059\n",
      "step: 48630 , time : 0.0\n",
      "train: loss: 50015.87109375 acc: 0.9875049591064453  val: loss: 172909.59375 acc: 0.9756755828857422\n",
      "step: 48635 , time : 0.0\n",
      "train: loss: 48913.2734375 acc: 0.9941855669021606  val: loss: 415630.5 acc: 0.9041961431503296\n",
      "step: 48640 , time : 0.0\n",
      "train: loss: 19653.57421875 acc: 0.9898515343666077  val: loss: 981530.625 acc: 0.7701519727706909\n",
      "step: 48645 , time : 0.015625715255737305\n",
      "train: loss: 106985.4140625 acc: 0.9760032892227173  val: loss: 1044972.875 acc: 0.770715594291687\n",
      "step: 48650 , time : 0.0\n",
      "train: loss: 106386.21875 acc: 0.9720202088356018  val: loss: 343031.3125 acc: 0.9099481105804443\n",
      "step: 48655 , time : 0.0\n",
      "train: loss: 70175.125 acc: 0.9662312269210815  val: loss: 173079.234375 acc: 0.9714060425758362\n",
      "step: 48660 , time : 0.0\n",
      "train: loss: 1455709.25 acc: 0.3196899890899658  val: loss: 730834.875 acc: 0.9459343552589417\n",
      "step: 48665 , time : 0.0\n",
      "train: loss: 170902.703125 acc: 0.9556130766868591  val: loss: 921837.1875 acc: 0.7200786471366882\n",
      "step: 48670 , time : 0.0\n",
      "train: loss: 100156.8203125 acc: 0.9918095469474792  val: loss: 322833.25 acc: 0.9738092422485352\n",
      "step: 48675 , time : 0.0\n",
      "train: loss: 53148.42578125 acc: 0.9931309819221497  val: loss: 143775.1875 acc: 0.9807267189025879\n",
      "step: 48680 , time : 0.0\n",
      "train: loss: 49206.94921875 acc: 0.9924555420875549  val: loss: 1404403.625 acc: 0.5602360963821411\n",
      "step: 48685 , time : 0.0\n",
      "train: loss: 171809.671875 acc: 0.9834294319152832  val: loss: 675807.125 acc: 0.8824588060379028\n",
      "step: 48690 , time : 0.0\n",
      "train: loss: 810268.125 acc: 0.9536901712417603  val: loss: 441306.40625 acc: 0.8066527843475342\n",
      "step: 48695 , time : 0.0010006427764892578\n",
      "train: loss: 328768.125 acc: 0.9022486805915833  val: loss: 1812948.125 acc: 0.6886707544326782\n",
      "step: 48700 , time : 0.0\n",
      "train: loss: 418174.0 acc: 0.9708529114723206  val: loss: 4283112.5 acc: 0.6648159027099609\n",
      "step: 48705 , time : 0.0\n",
      "train: loss: 1265624.375 acc: 0.9555346965789795  val: loss: 612686.875 acc: 0.8951599597930908\n",
      "step: 48710 , time : 0.0\n",
      "train: loss: 457992.625 acc: 0.9792144298553467  val: loss: 969186.1875 acc: 0.9168336391448975\n",
      "step: 48715 , time : 0.0\n",
      "train: loss: 990475.375 acc: 0.9728462100028992  val: loss: 157658.4375 acc: 0.8726527690887451\n",
      "step: 48720 , time : 0.0\n",
      "train: loss: 1246864.25 acc: 0.9545198678970337  val: loss: 1369111.875 acc: 0.8018807172775269\n",
      "step: 48725 , time : 0.0\n",
      "train: loss: 324074.8125 acc: 0.9597650766372681  val: loss: 2528105.25 acc: 0.8024324774742126\n",
      "step: 48730 , time : 0.0\n",
      "train: loss: 474333.8125 acc: 0.919109582901001  val: loss: 1221892.75 acc: 0.8237660527229309\n",
      "step: 48735 , time : 0.0\n",
      "train: loss: 532451.5625 acc: 0.9259248375892639  val: loss: 656138.0625 acc: 0.9394097328186035\n",
      "step: 48740 , time : 0.0\n",
      "train: loss: 1961410.625 acc: 0.4988449215888977  val: loss: 772699.625 acc: 0.7951499223709106\n",
      "step: 48745 , time : 0.0\n",
      "train: loss: 1142667.375 acc: 0.7993320226669312  val: loss: 681104.0625 acc: 0.7461854219436646\n",
      "step: 48750 , time : 0.0\n",
      "train: loss: 517008.84375 acc: 0.6210353374481201  val: loss: 1456570.25 acc: 0.633456826210022\n",
      "step: 48755 , time : 0.0\n",
      "train: loss: 501010.03125 acc: 0.7622045874595642  val: loss: 675372.375 acc: 0.8426641225814819\n",
      "step: 48760 , time : 0.0\n",
      "train: loss: 843487.1875 acc: 0.7519668936729431  val: loss: 915484.5625 acc: 0.8114514946937561\n",
      "step: 48765 , time : 0.0\n",
      "train: loss: 657545.3125 acc: 0.7053941488265991  val: loss: 883667.6875 acc: 0.7701237201690674\n",
      "step: 48770 , time : 0.0\n",
      "train: loss: 292814.875 acc: 0.8547834157943726  val: loss: 2574608.75 acc: 0.6751493215560913\n",
      "step: 48775 , time : 0.0\n",
      "train: loss: 282822.0 acc: 0.8375386595726013  val: loss: 3265585.0 acc: 0.5857864022254944\n",
      "step: 48780 , time : 0.0\n",
      "train: loss: 34296.74609375 acc: 0.9702783823013306  val: loss: 2530861.75 acc: 0.5846501588821411\n",
      "step: 48785 , time : 0.0\n",
      "train: loss: 400227.6875 acc: 0.7930959463119507  val: loss: 163226.9375 acc: 0.8573296666145325\n",
      "step: 48790 , time : 0.0\n",
      "train: loss: 29157.5 acc: 0.9773240685462952  val: loss: 580130.8125 acc: 0.7952214479446411\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 48795 , time : 0.0\n",
      "train: loss: 115454.53125 acc: 0.9315481781959534  val: loss: 2867130.0 acc: 0.6859367489814758\n",
      "step: 48800 , time : 0.001001119613647461\n",
      "train: loss: 174933.609375 acc: 0.886979341506958  val: loss: 1302677.75 acc: 0.6883336901664734\n",
      "step: 48805 , time : 0.0\n",
      "train: loss: 97910.6796875 acc: 0.9334127306938171  val: loss: 3491488.0 acc: 0.5527271032333374\n",
      "step: 48810 , time : 0.0\n",
      "train: loss: 64234.33984375 acc: 0.9264572858810425  val: loss: 3828961.25 acc: 0.517196774482727\n",
      "step: 48815 , time : 0.0\n",
      "train: loss: 9838.75 acc: 0.9870732426643372  val: loss: 2584950.75 acc: 0.7168159484863281\n",
      "step: 48820 , time : 0.0\n",
      "train: loss: 50869.5703125 acc: 0.9420531392097473  val: loss: 550560.625 acc: 0.7732546925544739\n",
      "step: 48825 , time : 0.0\n",
      "train: loss: 154154.671875 acc: 0.901404857635498  val: loss: 2242758.25 acc: 0.6716887950897217\n",
      "step: 48830 , time : 0.0\n",
      "train: loss: 136058.0 acc: 0.9011818170547485  val: loss: 813842.25 acc: 0.7750858068466187\n",
      "step: 48835 , time : 0.0\n",
      "train: loss: 653622.125 acc: 0.6814410090446472  val: loss: 3880020.75 acc: 0.5330935120582581\n",
      "step: 48840 , time : 0.0\n",
      "train: loss: 249735.953125 acc: 0.8270658254623413  val: loss: 2320187.75 acc: 0.6990969181060791\n",
      "step: 48845 , time : 0.0\n",
      "train: loss: 188203.25 acc: 0.8511167764663696  val: loss: 2749696.25 acc: 0.5802649855613708\n",
      "step: 48850 , time : 0.0\n",
      "train: loss: 943024.0625 acc: 0.6641498804092407  val: loss: 2082684.25 acc: 0.5897232294082642\n",
      "step: 48855 , time : 0.0\n",
      "train: loss: 1421857.75 acc: 0.6895166635513306  val: loss: 1062943.375 acc: 0.7596559524536133\n",
      "step: 48860 , time : 0.0\n",
      "train: loss: 1008744.125 acc: 0.8705872297286987  val: loss: 305096.4375 acc: 0.9155778884887695\n",
      "step: 48865 , time : 0.0\n",
      "train: loss: 620846.375 acc: 0.9455965757369995  val: loss: 866462.625 acc: 0.7711224555969238\n",
      "step: 48870 , time : 0.0\n",
      "train: loss: 607027.0625 acc: 0.9215779900550842  val: loss: 629915.75 acc: 0.8329616785049438\n",
      "step: 48875 , time : 0.0\n",
      "train: loss: 50930.96875 acc: 0.9854041934013367  val: loss: 904212.375 acc: 0.630640983581543\n",
      "step: 48880 , time : 0.0010008811950683594\n",
      "train: loss: 69368.703125 acc: 0.9913159608840942  val: loss: 403977.03125 acc: 0.9036800861358643\n",
      "step: 48885 , time : 0.0\n",
      "train: loss: 88459.3515625 acc: 0.9889370799064636  val: loss: 941210.3125 acc: 0.5735504031181335\n",
      "step: 48890 , time : 0.0\n",
      "train: loss: 75889.4375 acc: 0.9953774213790894  val: loss: 1843434.25 acc: 0.5106183886528015\n",
      "step: 48895 , time : 0.0\n",
      "train: loss: 48672.9609375 acc: 0.9933869242668152  val: loss: 570176.6875 acc: 0.912226140499115\n",
      "step: 48900 , time : 0.0\n",
      "train: loss: 61558.765625 acc: 0.9873846173286438  val: loss: 574002.875 acc: 0.8964963555335999\n",
      "step: 48905 , time : 0.0\n",
      "train: loss: 38962.984375 acc: 0.9908491969108582  val: loss: 836172.6875 acc: 0.7875655889511108\n",
      "step: 48910 , time : 0.0\n",
      "train: loss: 47972.390625 acc: 0.9903964400291443  val: loss: 614075.4375 acc: 0.807875394821167\n",
      "step: 48915 , time : 0.0010004043579101562\n",
      "train: loss: 9717.7197265625 acc: 0.9886295199394226  val: loss: 864971.9375 acc: 0.682708203792572\n",
      "step: 48920 , time : 0.0\n",
      "train: loss: 23675.052734375 acc: 0.9938045144081116  val: loss: 275276.84375 acc: 0.8149794936180115\n",
      "step: 48925 , time : 0.0010006427764892578\n",
      "train: loss: 2456.671630859375 acc: 0.9926120638847351  val: loss: 103103.875 acc: 0.9507228136062622\n",
      "step: 48930 , time : 0.0\n",
      "train: loss: 13081.27734375 acc: 0.9810774922370911  val: loss: 1561129.125 acc: 0.5563620328903198\n",
      "step: 48935 , time : 0.0010006427764892578\n",
      "train: loss: 17832.033203125 acc: 0.979522705078125  val: loss: 1354995.5 acc: 0.5387991070747375\n",
      "step: 48940 , time : 0.0\n",
      "train: loss: 11808.1357421875 acc: 0.9915108680725098  val: loss: 856089.125 acc: 0.8273568153381348\n",
      "step: 48945 , time : 0.0\n",
      "train: loss: 6550.0244140625 acc: 0.9702284932136536  val: loss: 1136169.125 acc: 0.7832108736038208\n",
      "step: 48950 , time : 0.0\n",
      "train: loss: 6675.61474609375 acc: 0.9869214296340942  val: loss: 613249.75 acc: 0.6141203045845032\n",
      "step: 48955 , time : 0.0\n",
      "train: loss: 30244.658203125 acc: 0.97572922706604  val: loss: 1304564.125 acc: 0.6804869771003723\n",
      "step: 48960 , time : 0.0\n",
      "train: loss: 19603.369140625 acc: 0.9883982539176941  val: loss: 196065.421875 acc: 0.9471991658210754\n",
      "step: 48965 , time : 0.0\n",
      "train: loss: 13502.048828125 acc: 0.9874691367149353  val: loss: 758047.3125 acc: 0.7178614139556885\n",
      "step: 48970 , time : 0.0\n",
      "train: loss: 19246.1328125 acc: 0.9899023771286011  val: loss: 437802.875 acc: 0.9441483616828918\n",
      "step: 48975 , time : 0.0\n",
      "train: loss: 15493.9990234375 acc: 0.9854017496109009  val: loss: 1005041.9375 acc: 0.8574478030204773\n",
      "step: 48980 , time : 0.0\n",
      "train: loss: 5532.73779296875 acc: 0.9937490224838257  val: loss: 91982.6171875 acc: 0.9882693886756897\n",
      "step: 48985 , time : 0.0\n",
      "train: loss: 8750.4560546875 acc: 0.9922746419906616  val: loss: 331413.0625 acc: 0.9262991547584534\n",
      "step: 48990 , time : 0.0\n",
      "train: loss: 21572.896484375 acc: 0.988461434841156  val: loss: 1270471.75 acc: 0.6732308864593506\n",
      "step: 48995 , time : 0.0\n",
      "train: loss: 25718.41796875 acc: 0.99385666847229  val: loss: 359318.40625 acc: 0.9397642016410828\n",
      "step: 49000 , time : 0.0\n",
      "train: loss: 52477.14453125 acc: 0.9897108674049377  val: loss: 1252950.375 acc: 0.7419224977493286\n",
      "step: 49005 , time : 0.01562666893005371\n",
      "train: loss: 19826.974609375 acc: 0.9888195395469666  val: loss: 407960.375 acc: 0.9560062885284424\n",
      "step: 49010 , time : 0.0\n",
      "train: loss: 27420.013671875 acc: 0.9883925914764404  val: loss: 2102106.75 acc: 0.6218914985656738\n",
      "step: 49015 , time : 0.0\n",
      "train: loss: 449983.21875 acc: 0.8779308199882507  val: loss: 1601524.25 acc: 0.5891641974449158\n",
      "step: 49020 , time : 0.0\n",
      "train: loss: 100803.6953125 acc: 0.9635921716690063  val: loss: 1246646.625 acc: 0.732236385345459\n",
      "step: 49025 , time : 0.0\n",
      "train: loss: 304867.875 acc: 0.9417387843132019  val: loss: 1171039.25 acc: 0.7961382269859314\n",
      "step: 49030 , time : 0.0\n",
      "train: loss: 85889.25 acc: 0.9892447590827942  val: loss: 1440596.375 acc: 0.807767391204834\n",
      "step: 49035 , time : 0.0010004043579101562\n",
      "train: loss: 43822.4921875 acc: 0.9961875081062317  val: loss: 573879.3125 acc: 0.8862641453742981\n",
      "step: 49040 , time : 0.0\n",
      "train: loss: 533643.0625 acc: 0.9243485927581787  val: loss: 1154157.75 acc: 0.4183209538459778\n",
      "step: 49045 , time : 0.0\n",
      "train: loss: 270932.59375 acc: 0.9651683568954468  val: loss: 229965.921875 acc: 0.9736025333404541\n",
      "step: 49050 , time : 0.0\n",
      "train: loss: 283036.3125 acc: 0.9650013446807861  val: loss: 1186621.5 acc: 0.8689192533493042\n",
      "step: 49055 , time : 0.0\n",
      "train: loss: 463237.375 acc: 0.9683815836906433  val: loss: 2126144.75 acc: 0.7985058426856995\n",
      "step: 49060 , time : 0.0\n",
      "train: loss: 695184.375 acc: 0.9651691913604736  val: loss: 2140652.0 acc: 0.829727292060852\n",
      "step: 49065 , time : 0.0\n",
      "train: loss: 259182.84375 acc: 0.958036482334137  val: loss: 623279.6875 acc: 0.8926662802696228\n",
      "step: 49070 , time : 0.0\n",
      "train: loss: 603041.8125 acc: 0.9745563268661499  val: loss: 962176.375 acc: 0.6716476678848267\n",
      "step: 49075 , time : 0.0\n",
      "train: loss: 1941508.125 acc: 0.9411112666130066  val: loss: 1664578.875 acc: 0.6259865760803223\n",
      "step: 49080 , time : 0.0\n",
      "train: loss: 1687375.5 acc: 0.9304994940757751  val: loss: 1616779.5 acc: 0.7804611921310425\n",
      "step: 49085 , time : 0.0\n",
      "train: loss: 756754.6875 acc: 0.9535985589027405  val: loss: 1349033.25 acc: 0.6118398904800415\n",
      "step: 49090 , time : 0.0\n",
      "train: loss: 151807.359375 acc: 0.989822268486023  val: loss: 1795254.75 acc: -0.4489995241165161\n",
      "step: 49095 , time : 0.0\n",
      "train: loss: 858576.4375 acc: 0.9267988204956055  val: loss: 881308.1875 acc: 0.6769504547119141\n",
      "step: 49100 , time : 0.0\n",
      "train: loss: 136315.609375 acc: 0.9684813022613525  val: loss: 732376.625 acc: 0.6280030012130737\n",
      "step: 49105 , time : 0.0\n",
      "train: loss: 251585.28125 acc: 0.9692125916481018  val: loss: 899975.6875 acc: 0.4364185929298401\n",
      "step: 49110 , time : 0.0\n",
      "train: loss: 511301.09375 acc: 0.8415217995643616  val: loss: 1026275.0 acc: 0.8741183876991272\n",
      "step: 49115 , time : 0.0\n",
      "train: loss: 1359170.5 acc: 0.32344353199005127  val: loss: 1498283.375 acc: 0.7708672881126404\n",
      "step: 49120 , time : 0.0\n",
      "train: loss: 325172.9375 acc: 0.8060272932052612  val: loss: 1805453.0 acc: 0.773708701133728\n",
      "step: 49125 , time : 0.0\n",
      "train: loss: 552699.5 acc: 0.8261069059371948  val: loss: 1968274.75 acc: 0.777506947517395\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 49130 , time : 0.0\n",
      "train: loss: 832921.25 acc: 0.6239160299301147  val: loss: 871091.0 acc: 0.7507984638214111\n",
      "step: 49135 , time : 0.0010006427764892578\n",
      "train: loss: 235819.0 acc: 0.8288556337356567  val: loss: 2058228.875 acc: 0.7073975801467896\n",
      "step: 49140 , time : 0.0010008811950683594\n",
      "train: loss: 230080.390625 acc: 0.8657752275466919  val: loss: 1942027.0 acc: 0.6670013666152954\n",
      "step: 49145 , time : 0.0\n",
      "train: loss: 364694.0 acc: 0.8163894414901733  val: loss: 1232840.375 acc: 0.6945863962173462\n",
      "step: 49150 , time : 0.0\n",
      "train: loss: 192436.9375 acc: 0.8773866891860962  val: loss: 1415935.25 acc: 0.6497139930725098\n",
      "step: 49155 , time : 0.0\n",
      "train: loss: 497159.15625 acc: 0.7513449788093567  val: loss: 6079247.5 acc: 0.4241069555282593\n",
      "step: 49160 , time : 0.0\n",
      "train: loss: 58584.20703125 acc: 0.9589979648590088  val: loss: 3104699.75 acc: 0.5027216076850891\n",
      "step: 49165 , time : 0.0\n",
      "train: loss: 289731.96875 acc: 0.8625164031982422  val: loss: 1174141.375 acc: 0.6692491769790649\n",
      "step: 49170 , time : 0.0\n",
      "train: loss: 248373.109375 acc: 0.8564881086349487  val: loss: 4485137.5 acc: 0.6118505001068115\n",
      "step: 49175 , time : 0.0\n",
      "train: loss: 7579.20166015625 acc: 0.9917084574699402  val: loss: 630021.25 acc: 0.7666065692901611\n",
      "step: 49180 , time : 0.0\n",
      "train: loss: 24024.150390625 acc: 0.9790421724319458  val: loss: 1492106.25 acc: 0.685929536819458\n",
      "step: 49185 , time : 0.0\n",
      "train: loss: 82450.1171875 acc: 0.9236258268356323  val: loss: 1478915.625 acc: 0.5870646238327026\n",
      "step: 49190 , time : 0.0\n",
      "train: loss: 70906.3359375 acc: 0.9414010047912598  val: loss: 492030.5625 acc: 0.7610181570053101\n",
      "step: 49195 , time : 0.0\n",
      "train: loss: 980493.5625 acc: 0.661320686340332  val: loss: 979180.125 acc: 0.7207225561141968\n",
      "step: 49200 , time : 0.0\n",
      "train: loss: 392326.25 acc: 0.7875882983207703  val: loss: 973070.625 acc: 0.7214407324790955\n",
      "step: 49205 , time : 0.0\n",
      "train: loss: 278341.46875 acc: 0.8272344470024109  val: loss: 1276784.75 acc: 0.7081328630447388\n",
      "step: 49210 , time : 0.0\n",
      "train: loss: 241647.484375 acc: 0.8286468982696533  val: loss: 709108.0625 acc: 0.7669637799263\n",
      "step: 49215 , time : 0.0\n",
      "train: loss: 584444.5 acc: 0.7575615644454956  val: loss: 44180.4296875 acc: 0.9478737115859985\n",
      "step: 49220 , time : 0.0\n",
      "train: loss: 1579566.25 acc: 0.7470707297325134  val: loss: 1724430.75 acc: 0.7317909002304077\n",
      "step: 49225 , time : 0.0\n",
      "train: loss: 1042942.125 acc: 0.8582655787467957  val: loss: 875757.0 acc: 0.7857034802436829\n",
      "step: 49230 , time : 0.0\n",
      "train: loss: 274867.375 acc: 0.9727181792259216  val: loss: 1041600.4375 acc: 0.8303250670433044\n",
      "step: 49235 , time : 0.0\n",
      "train: loss: 357617.09375 acc: 0.9611093401908875  val: loss: 69335.359375 acc: 0.9712253212928772\n",
      "step: 49240 , time : 0.0\n",
      "train: loss: 219604.625 acc: 0.9570640921592712  val: loss: 330492.375 acc: 0.9207101464271545\n",
      "step: 49245 , time : 0.0\n",
      "train: loss: 100157.359375 acc: 0.9860591292381287  val: loss: 1847060.375 acc: 0.6004928946495056\n",
      "step: 49250 , time : 0.0\n",
      "train: loss: 180473.828125 acc: 0.9736066460609436  val: loss: 497963.125 acc: 0.880517840385437\n",
      "step: 49255 , time : 0.0010006427764892578\n",
      "train: loss: 74384.171875 acc: 0.9951260089874268  val: loss: 792698.1875 acc: 0.6554925441741943\n",
      "step: 49260 , time : 0.0\n",
      "train: loss: 97614.34375 acc: 0.9925429224967957  val: loss: 156208.265625 acc: 0.9706117510795593\n",
      "step: 49265 , time : 0.0\n",
      "train: loss: 97788.3125 acc: 0.9874899983406067  val: loss: 155953.34375 acc: 0.9764672517776489\n",
      "step: 49270 , time : 0.0\n",
      "train: loss: 32169.728515625 acc: 0.9945616126060486  val: loss: 792387.25 acc: 0.794249415397644\n",
      "step: 49275 , time : 0.0\n",
      "train: loss: 10486.154296875 acc: 0.9975658059120178  val: loss: 770669.9375 acc: 0.7664631605148315\n",
      "step: 49280 , time : 0.0\n",
      "train: loss: 6280.736328125 acc: 0.9888219833374023  val: loss: 123298.734375 acc: 0.9729220271110535\n",
      "step: 49285 , time : 0.0\n",
      "train: loss: 9336.9677734375 acc: 0.9723849296569824  val: loss: 1299445.875 acc: 0.7701916694641113\n",
      "step: 49290 , time : 0.0\n",
      "train: loss: 12797.0126953125 acc: 0.9805115461349487  val: loss: 461329.84375 acc: 0.7544434666633606\n",
      "step: 49295 , time : 0.0\n",
      "train: loss: 9670.814453125 acc: 0.9834378361701965  val: loss: 297469.78125 acc: 0.9575086236000061\n",
      "step: 49300 , time : 0.0\n",
      "train: loss: 10327.017578125 acc: 0.9860292673110962  val: loss: 611588.9375 acc: 0.9497002363204956\n",
      "step: 49305 , time : 0.0\n",
      "train: loss: 10704.8447265625 acc: 0.9840717315673828  val: loss: 2522967.75 acc: 0.5670004487037659\n",
      "step: 49310 , time : 0.0\n",
      "train: loss: 5975.31005859375 acc: 0.9899811744689941  val: loss: 753217.1875 acc: 0.9448614716529846\n",
      "step: 49315 , time : 0.0\n",
      "train: loss: 18707.66015625 acc: 0.9755614995956421  val: loss: 1221254.125 acc: 0.6103299856185913\n",
      "step: 49320 , time : 0.0\n",
      "train: loss: 9940.4931640625 acc: 0.988997757434845  val: loss: 705902.1875 acc: 0.8336924910545349\n",
      "step: 49325 , time : 0.0\n",
      "train: loss: 9041.23046875 acc: 0.9864045977592468  val: loss: 784701.5 acc: 0.9138028621673584\n",
      "step: 49330 , time : 0.0\n",
      "train: loss: 8195.7734375 acc: 0.9933913946151733  val: loss: 979599.625 acc: 0.7461882829666138\n",
      "step: 49335 , time : 0.0\n",
      "train: loss: 22634.89453125 acc: 0.9843098521232605  val: loss: 1881909.375 acc: -0.15704596042633057\n",
      "step: 49340 , time : 0.0\n",
      "train: loss: 21231.458984375 acc: 0.9835789203643799  val: loss: 1725738.0 acc: 0.8484139442443848\n",
      "step: 49345 , time : 0.0\n",
      "train: loss: 5059.3837890625 acc: 0.9893411993980408  val: loss: 937354.0625 acc: 0.9065909385681152\n",
      "step: 49350 , time : 0.0\n",
      "train: loss: 16969.908203125 acc: 0.9770190119743347  val: loss: 3291105.25 acc: 0.4126499891281128\n",
      "step: 49355 , time : 0.0010006427764892578\n",
      "train: loss: 5001.80615234375 acc: 0.9972056746482849  val: loss: 1519284.125 acc: 0.6868860721588135\n",
      "step: 49360 , time : 0.0010006427764892578\n",
      "train: loss: 40044.65234375 acc: 0.9927763342857361  val: loss: 2043792.625 acc: 0.5252193808555603\n",
      "step: 49365 , time : 0.0\n",
      "train: loss: 31333.986328125 acc: 0.9918494820594788  val: loss: 3632298.25 acc: -0.6783699989318848\n",
      "step: 49370 , time : 0.0\n",
      "train: loss: 35442.0859375 acc: 0.9833495616912842  val: loss: 1283481.625 acc: 0.8266737461090088\n",
      "step: 49375 , time : 0.001001119613647461\n",
      "train: loss: 40679.9765625 acc: 0.9884141683578491  val: loss: 312250.21875 acc: 0.9417202472686768\n",
      "step: 49380 , time : 0.0\n",
      "train: loss: 59038.7109375 acc: 0.9752834439277649  val: loss: 1665492.625 acc: 0.7083138823509216\n",
      "step: 49385 , time : 0.0\n",
      "train: loss: 130201.5546875 acc: 0.9493664503097534  val: loss: 1580009.75 acc: 0.7378630638122559\n",
      "step: 49390 , time : 0.0\n",
      "train: loss: 921473.8125 acc: 0.46122121810913086  val: loss: 606194.4375 acc: 0.8750032782554626\n",
      "step: 49395 , time : 0.0\n",
      "train: loss: 108736.515625 acc: 0.9823675155639648  val: loss: 749998.3125 acc: 0.06838589906692505\n",
      "step: 49400 , time : 0.0\n",
      "train: loss: 890198.4375 acc: 0.9043887257575989  val: loss: 2752000.25 acc: 0.6354089975357056\n",
      "step: 49405 , time : 0.0\n",
      "train: loss: 520958.03125 acc: 0.9395164847373962  val: loss: 1544937.0 acc: 0.382331907749176\n",
      "step: 49410 , time : 0.0\n",
      "train: loss: 166661.6875 acc: 0.9826988577842712  val: loss: 4691958.5 acc: 0.12766462564468384\n",
      "step: 49415 , time : 0.0\n",
      "train: loss: 134951.609375 acc: 0.9879896640777588  val: loss: 1005019.9375 acc: 0.9102377891540527\n",
      "step: 49420 , time : 0.0\n",
      "train: loss: 1692128.25 acc: 0.8771463632583618  val: loss: 474414.375 acc: 0.930321216583252\n",
      "step: 49425 , time : 0.0\n",
      "train: loss: 684346.8125 acc: 0.9633749127388  val: loss: 1252392.125 acc: 0.5571673512458801\n",
      "step: 49430 , time : 0.0\n",
      "train: loss: 302911.59375 acc: 0.9580069184303284  val: loss: 1621204.375 acc: 0.7354139685630798\n",
      "step: 49435 , time : 0.0\n",
      "train: loss: 1247289.75 acc: 0.9302219748497009  val: loss: 1990096.875 acc: 0.7869967222213745\n",
      "step: 49440 , time : 0.0\n",
      "train: loss: 887751.1875 acc: 0.976219654083252  val: loss: 1766359.5 acc: 0.81272292137146\n",
      "step: 49445 , time : 0.0\n",
      "train: loss: 984211.4375 acc: 0.9618810415267944  val: loss: 1293943.25 acc: 0.7784376740455627\n",
      "step: 49450 , time : 0.0\n",
      "train: loss: 675927.1875 acc: 0.9541767239570618  val: loss: 436740.03125 acc: 0.9342362284660339\n",
      "step: 49455 , time : 0.0\n",
      "train: loss: 1124248.75 acc: 0.9401780962944031  val: loss: 2249797.25 acc: 0.7574447989463806\n",
      "step: 49460 , time : 0.0\n",
      "train: loss: 438910.34375 acc: 0.9694966077804565  val: loss: 369959.90625 acc: 0.8054540157318115\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 49465 , time : 0.0\n",
      "train: loss: 193048.875 acc: 0.9827277660369873  val: loss: 846328.25 acc: 0.8186891674995422\n",
      "step: 49470 , time : 0.0010008811950683594\n",
      "train: loss: 1819653.125 acc: 0.37983548641204834  val: loss: 1002671.625 acc: 0.763115644454956\n",
      "step: 49475 , time : 0.0010008811950683594\n",
      "train: loss: 2732268.25 acc: -0.9515210390090942  val: loss: 1224543.75 acc: 0.6913001537322998\n",
      "step: 49480 , time : 0.0\n",
      "train: loss: 1112609.5 acc: 0.6904131174087524  val: loss: 1231159.0 acc: 0.8392201662063599\n",
      "step: 49485 , time : 0.0010008811950683594\n",
      "train: loss: 968705.9375 acc: 0.4563469886779785  val: loss: 1369689.75 acc: 0.8053243160247803\n",
      "step: 49490 , time : 0.0\n",
      "train: loss: 1512589.0 acc: 0.812563955783844  val: loss: 806789.1875 acc: 0.8607295751571655\n",
      "step: 49495 , time : 0.0010004043579101562\n",
      "train: loss: 1692593.125 acc: 0.4300018548965454  val: loss: 1419447.5 acc: 0.6910252571105957\n",
      "step: 49500 , time : 0.0\n",
      "train: loss: 1220212.25 acc: 0.42296934127807617  val: loss: 2152490.25 acc: 0.7389695644378662\n",
      "step: 49505 , time : 0.0\n",
      "train: loss: 1187080.5 acc: 0.28396886587142944  val: loss: 1675429.625 acc: 0.430549681186676\n",
      "step: 49510 , time : 0.0\n",
      "train: loss: 622868.375 acc: 0.4427962303161621  val: loss: 987840.0 acc: 0.4222022294998169\n",
      "step: 49515 , time : 0.0\n",
      "train: loss: 500543.40625 acc: 0.5240926146507263  val: loss: 739596.5625 acc: 0.8142581582069397\n",
      "step: 49520 , time : 0.0\n",
      "train: loss: 85215.2265625 acc: 0.9281718730926514  val: loss: 247490.90625 acc: 0.8859022259712219\n",
      "step: 49525 , time : 0.0\n",
      "train: loss: 512951.625 acc: 0.6762319803237915  val: loss: 1424740.625 acc: 0.8204184770584106\n",
      "step: 49530 , time : 0.0\n",
      "train: loss: 152770.109375 acc: 0.8503050804138184  val: loss: 1635591.0 acc: 0.7905662655830383\n",
      "step: 49535 , time : 0.0\n",
      "train: loss: 290072.0625 acc: 0.8403995037078857  val: loss: 856180.8125 acc: 0.8499910831451416\n",
      "step: 49540 , time : 0.0010006427764892578\n",
      "train: loss: 216197.765625 acc: 0.8690860867500305  val: loss: 798129.1875 acc: 0.8212147951126099\n",
      "step: 49545 , time : 0.0\n",
      "train: loss: 146488.796875 acc: 0.8892605304718018  val: loss: 641138.75 acc: 0.8482035398483276\n",
      "step: 49550 , time : 0.0\n",
      "train: loss: 48546.4140625 acc: 0.902332067489624  val: loss: 1483655.0 acc: 0.8009202480316162\n",
      "step: 49555 , time : 0.0\n",
      "train: loss: 721397.3125 acc: 0.4980241060256958  val: loss: 255124.671875 acc: 0.7250759601593018\n",
      "step: 49560 , time : 0.0\n",
      "train: loss: 540051.9375 acc: 0.5216305255889893  val: loss: 569156.1875 acc: 0.7257274389266968\n",
      "step: 49565 , time : 0.0\n",
      "train: loss: 471026.5 acc: 0.5374780893325806  val: loss: 1061595.25 acc: 0.7501302361488342\n",
      "step: 49570 , time : 0.0\n",
      "train: loss: 853889.5 acc: 0.48047858476638794  val: loss: 847777.125 acc: 0.6516897082328796\n",
      "step: 49575 , time : 0.0\n",
      "train: loss: 354845.28125 acc: 0.7480521202087402  val: loss: 1265680.125 acc: 0.6503998637199402\n",
      "step: 49580 , time : 0.0010006427764892578\n",
      "train: loss: 952419.0625 acc: 0.7455382347106934  val: loss: 1112691.5 acc: 0.7173192501068115\n",
      "step: 49585 , time : 0.0010004043579101562\n",
      "train: loss: 1624267.0 acc: 0.7481025457382202  val: loss: 248745.515625 acc: 0.8385264873504639\n",
      "step: 49590 , time : 0.0010006427764892578\n",
      "train: loss: 735560.0625 acc: 0.8459005355834961  val: loss: 565280.75 acc: 0.8378946185112\n",
      "step: 49595 , time : 0.0010008811950683594\n",
      "train: loss: 584655.125 acc: 0.9572982788085938  val: loss: 1877340.625 acc: 0.6151255369186401\n",
      "step: 49600 , time : 0.0010008811950683594\n",
      "train: loss: 856234.8125 acc: 0.9045845866203308  val: loss: 1278375.5 acc: 0.8386779427528381\n",
      "step: 49605 , time : 0.0\n",
      "train: loss: 301437.59375 acc: 0.9117563962936401  val: loss: 1118715.875 acc: 0.9133283495903015\n",
      "step: 49610 , time : 0.0\n",
      "train: loss: 407594.5 acc: 0.9386258125305176  val: loss: 395778.5625 acc: 0.9401509165763855\n",
      "step: 49615 , time : 0.0\n",
      "train: loss: 396672.40625 acc: 0.9500066041946411  val: loss: 1293814.75 acc: 0.870885968208313\n",
      "step: 49620 , time : 0.0\n",
      "train: loss: 193078.34375 acc: 0.9872273802757263  val: loss: 994928.25 acc: 0.8123294711112976\n",
      "step: 49625 , time : 0.0\n",
      "train: loss: 216596.484375 acc: 0.983837902545929  val: loss: 1350061.375 acc: 0.7984208464622498\n",
      "step: 49630 , time : 0.0\n",
      "train: loss: 320271.75 acc: 0.9559764862060547  val: loss: 571210.9375 acc: 0.9082961082458496\n",
      "step: 49635 , time : 0.0\n",
      "train: loss: 171302.375 acc: 0.9611353278160095  val: loss: 332954.5625 acc: 0.9416933655738831\n",
      "step: 49640 , time : 0.0\n",
      "train: loss: 87351.546875 acc: 0.9794509410858154  val: loss: 362976.6875 acc: 0.8895102739334106\n",
      "step: 49645 , time : 0.0010004043579101562\n",
      "train: loss: 157488.734375 acc: 0.7902261018753052  val: loss: 1292047.125 acc: -0.31275200843811035\n",
      "step: 49650 , time : 0.0010008811950683594\n",
      "train: loss: 4324.00927734375 acc: 0.986666202545166  val: loss: 1101782.375 acc: 0.686590313911438\n",
      "step: 49655 , time : 0.0\n",
      "train: loss: 19637.697265625 acc: 0.9890283346176147  val: loss: 4219008.0 acc: 0.5611981153488159\n",
      "step: 49660 , time : 0.0\n",
      "train: loss: 16778.048828125 acc: 0.9747952222824097  val: loss: 1465517.375 acc: 0.7993677854537964\n",
      "step: 49665 , time : 0.0\n",
      "train: loss: 23724.07421875 acc: 0.9738271236419678  val: loss: 2721468.75 acc: 0.6741769313812256\n",
      "step: 49670 , time : 0.0\n",
      "train: loss: 7753.1357421875 acc: 0.9814340472221375  val: loss: 1502402.25 acc: 0.8178667426109314\n",
      "step: 49675 , time : 0.0\n",
      "train: loss: 8420.1640625 acc: 0.984410285949707  val: loss: 2219067.5 acc: 0.7361726760864258\n",
      "step: 49680 , time : 0.0\n",
      "train: loss: 12058.2548828125 acc: 0.9810656309127808  val: loss: 516604.6875 acc: 0.8321003913879395\n",
      "step: 49685 , time : 0.0\n",
      "train: loss: 14604.5927734375 acc: 0.9783961772918701  val: loss: 469920.875 acc: 0.7952007055282593\n",
      "step: 49690 , time : 0.0010004043579101562\n",
      "train: loss: 59397.9453125 acc: 0.9727303385734558  val: loss: 3701391.25 acc: -0.1833420991897583\n",
      "step: 49695 , time : 0.0\n",
      "train: loss: 18521.47265625 acc: 0.9885589480400085  val: loss: 1186021.625 acc: 0.8681992292404175\n",
      "step: 49700 , time : 0.0010004043579101562\n",
      "train: loss: 36529.89453125 acc: 0.9758846163749695  val: loss: 1670717.125 acc: 0.5538656711578369\n",
      "step: 49705 , time : 0.0\n",
      "train: loss: 114412.8203125 acc: 0.8947276473045349  val: loss: 295211.21875 acc: 0.9415056109428406\n",
      "step: 49710 , time : 0.001001119613647461\n",
      "train: loss: 10406.65234375 acc: 0.9826722741127014  val: loss: 1588392.125 acc: 0.7415395975112915\n",
      "step: 49715 , time : 0.0\n",
      "train: loss: 26956.8984375 acc: 0.9786342978477478  val: loss: 1871246.625 acc: 0.4107322096824646\n",
      "step: 49720 , time : 0.0\n",
      "train: loss: 14882.130859375 acc: 0.9892556071281433  val: loss: 2373964.75 acc: 0.6871474981307983\n",
      "step: 49725 , time : 0.0\n",
      "train: loss: 34572.9140625 acc: 0.9925322532653809  val: loss: 2811689.75 acc: 0.4234919548034668\n",
      "step: 49730 , time : 0.0\n",
      "train: loss: 16705.3046875 acc: 0.995357871055603  val: loss: 934672.6875 acc: 0.8332271575927734\n",
      "step: 49735 , time : 0.0\n",
      "train: loss: 23638.12890625 acc: 0.9938443303108215  val: loss: 1215080.875 acc: 0.8743200898170471\n",
      "step: 49740 , time : 0.0\n",
      "train: loss: 56500.28515625 acc: 0.972339391708374  val: loss: 2648695.25 acc: 0.08323568105697632\n",
      "step: 49745 , time : 0.0\n",
      "train: loss: 176321.171875 acc: 0.966248631477356  val: loss: 699024.4375 acc: 0.9068928956985474\n",
      "step: 49750 , time : 0.0\n",
      "train: loss: 57854.171875 acc: 0.9770409464836121  val: loss: 948143.75 acc: 0.9051156640052795\n",
      "step: 49755 , time : 0.0\n",
      "train: loss: 40131.390625 acc: 0.9881054759025574  val: loss: 1082191.75 acc: 0.7702468037605286\n",
      "step: 49760 , time : 0.0\n",
      "train: loss: 255691.65625 acc: 0.9444758892059326  val: loss: 531011.3125 acc: 0.8794898986816406\n",
      "step: 49765 , time : 0.0\n",
      "train: loss: 74292.1953125 acc: 0.9936344027519226  val: loss: 1169638.75 acc: 0.8038589358329773\n",
      "step: 49770 , time : 0.0\n",
      "train: loss: 159021.953125 acc: 0.9792551398277283  val: loss: 2074944.625 acc: 0.5667597055435181\n",
      "step: 49775 , time : 0.0\n",
      "train: loss: 52213.94921875 acc: 0.9914749264717102  val: loss: 586291.0 acc: 0.7742963433265686\n",
      "step: 49780 , time : 0.0\n",
      "train: loss: 217543.3125 acc: 0.9580652713775635  val: loss: 2129176.5 acc: 0.574825644493103\n",
      "step: 49785 , time : 0.0\n",
      "train: loss: 782559.1875 acc: 0.946646511554718  val: loss: 452384.53125 acc: 0.9575303792953491\n",
      "step: 49790 , time : 0.0\n",
      "train: loss: 284868.84375 acc: 0.982460618019104  val: loss: 1709303.375 acc: 0.7057943344116211\n",
      "step: 49795 , time : 0.0\n",
      "train: loss: 451965.9375 acc: 0.9306500554084778  val: loss: 763562.125 acc: 0.9090109467506409\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 49800 , time : 0.0\n",
      "train: loss: 1180057.625 acc: 0.9582871198654175  val: loss: 408911.4375 acc: 0.7375682592391968\n",
      "step: 49805 , time : 0.0010006427764892578\n",
      "train: loss: 510618.34375 acc: 0.9838995933532715  val: loss: 407550.65625 acc: 0.8154562711715698\n",
      "step: 49810 , time : 0.0\n",
      "train: loss: 1231396.0 acc: 0.9590417742729187  val: loss: 826389.125 acc: 0.7122573852539062\n",
      "step: 49815 , time : 0.0\n",
      "train: loss: 681083.375 acc: 0.9666386246681213  val: loss: 1001246.0625 acc: 0.5767036080360413\n",
      "step: 49820 , time : 0.0\n",
      "train: loss: 347484.09375 acc: 0.9594876170158386  val: loss: 130578.1171875 acc: 0.9316458702087402\n",
      "step: 49825 , time : 0.0010008811950683594\n",
      "train: loss: 868559.0 acc: 0.8918445706367493  val: loss: 286434.65625 acc: 0.9429804086685181\n",
      "step: 49830 , time : 0.0\n",
      "train: loss: 952660.75 acc: 0.8274353742599487  val: loss: 426699.84375 acc: 0.9281381368637085\n",
      "step: 49835 , time : 0.0\n",
      "train: loss: 610580.4375 acc: 0.9235734939575195  val: loss: 464885.6875 acc: 0.9323192238807678\n",
      "step: 49840 , time : 0.0\n",
      "train: loss: 2897331.25 acc: 0.4133344292640686  val: loss: 762530.25 acc: 0.7051467895507812\n",
      "step: 49845 , time : 0.0\n",
      "train: loss: 911535.4375 acc: 0.5336031913757324  val: loss: 4924528.0 acc: 0.5394977331161499\n",
      "step: 49850 , time : 0.0\n",
      "train: loss: 626984.4375 acc: 0.7870762348175049  val: loss: 651369.5625 acc: 0.8395237326622009\n",
      "step: 49855 , time : 0.0\n",
      "train: loss: 542609.8125 acc: 0.8910261392593384  val: loss: 355781.9375 acc: 0.7156444787979126\n",
      "step: 49860 , time : 0.0\n",
      "train: loss: 1551379.5 acc: 0.5398721694946289  val: loss: 1219679.25 acc: 0.789084255695343\n",
      "step: 49865 , time : 0.0\n",
      "train: loss: 552809.9375 acc: 0.6398409605026245  val: loss: 1657299.5 acc: 0.6422501802444458\n",
      "step: 49870 , time : 0.0\n",
      "train: loss: 552616.0 acc: 0.7704263925552368  val: loss: 961036.0625 acc: 0.7162477970123291\n",
      "step: 49875 , time : 0.0\n",
      "train: loss: 51136.21484375 acc: 0.952663004398346  val: loss: 2416437.5 acc: 0.6310437917709351\n",
      "step: 49880 , time : 0.0\n",
      "train: loss: 43805.1484375 acc: 0.9644016027450562  val: loss: 643387.625 acc: 0.7615772485733032\n",
      "step: 49885 , time : 0.0\n",
      "train: loss: 46544.5703125 acc: 0.9591742753982544  val: loss: 4473549.0 acc: 0.4496527910232544\n",
      "step: 49890 , time : 0.0\n",
      "train: loss: 47883.05078125 acc: 0.965109646320343  val: loss: 3725222.25 acc: 0.6084431409835815\n",
      "step: 49895 , time : 0.0\n",
      "train: loss: 168202.578125 acc: 0.9055263996124268  val: loss: 3045364.5 acc: 0.5773860216140747\n",
      "step: 49900 , time : 0.0\n",
      "train: loss: 33170.125 acc: 0.9751037955284119  val: loss: 901870.9375 acc: 0.6888649463653564\n",
      "step: 49905 , time : 0.0\n",
      "train: loss: 413520.59375 acc: 0.7948251962661743  val: loss: 381648.3125 acc: 0.8110489249229431\n",
      "step: 49910 , time : 0.0\n",
      "train: loss: 80261.0078125 acc: 0.8968926072120667  val: loss: 2280036.5 acc: 0.6606480479240417\n",
      "step: 49915 , time : 0.0010008811950683594\n",
      "train: loss: 447656.0 acc: 0.713463306427002  val: loss: 4542721.5 acc: 0.607017993927002\n",
      "step: 49920 , time : 0.0\n",
      "train: loss: 168101.5 acc: 0.9070878624916077  val: loss: 3640980.25 acc: 0.5707104206085205\n",
      "step: 49925 , time : 0.0\n",
      "train: loss: 62295.5703125 acc: 0.9461643695831299  val: loss: 6389551.0 acc: 0.49473780393600464\n",
      "step: 49930 , time : 0.0\n",
      "train: loss: 68891.578125 acc: 0.9326705932617188  val: loss: 7933548.5 acc: 0.5661793947219849\n",
      "step: 49935 , time : 0.0010006427764892578\n",
      "train: loss: 337324.8125 acc: 0.7140457630157471  val: loss: 2807080.0 acc: 0.6186428070068359\n",
      "step: 49940 , time : 0.0\n",
      "train: loss: 182957.5625 acc: 0.8797634840011597  val: loss: 3825531.25 acc: 0.5404369831085205\n",
      "step: 49945 , time : 0.0\n",
      "train: loss: 641664.25 acc: 0.7542288899421692  val: loss: 2544553.75 acc: 0.7165465354919434\n",
      "step: 49950 , time : 0.015624761581420898\n",
      "train: loss: 2886748.0 acc: 0.6526232957839966  val: loss: 1133736.375 acc: 0.7819702625274658\n",
      "step: 49955 , time : 0.0\n",
      "train: loss: 542711.0625 acc: 0.8637480139732361  val: loss: 1155880.625 acc: 0.8176634311676025\n",
      "step: 49960 , time : 0.0\n",
      "train: loss: 847319.8125 acc: 0.902970552444458  val: loss: 1582601.875 acc: 0.8363555073738098\n",
      "step: 49965 , time : 0.0\n",
      "train: loss: 252661.546875 acc: 0.9711902141571045  val: loss: 2057253.5 acc: 0.7506685853004456\n",
      "step: 49970 , time : 0.0\n",
      "train: loss: 59827.8125 acc: 0.9911115169525146  val: loss: 895428.9375 acc: 0.8536187410354614\n",
      "step: 49975 , time : 0.0\n",
      "train: loss: 76100.7421875 acc: 0.9868104457855225  val: loss: 853242.125 acc: 0.7889299392700195\n",
      "step: 49980 , time : 0.0\n",
      "train: loss: 75933.09375 acc: 0.992542028427124  val: loss: 3359569.75 acc: 0.649946928024292\n",
      "step: 49985 , time : 0.0\n",
      "train: loss: 92507.078125 acc: 0.9921752214431763  val: loss: 1704487.5 acc: 0.7995201349258423\n",
      "step: 49990 , time : 0.0\n",
      "train: loss: 76920.140625 acc: 0.9945993423461914  val: loss: 1144594.875 acc: 0.8568424582481384\n",
      "step: 49995 , time : 0.0\n",
      "train: loss: 91849.4921875 acc: 0.9912883639335632  val: loss: 1196867.875 acc: 0.8632523417472839\n",
      "final time ： 86.63586497306824\n"
     ]
    }
   ],
   "source": [
    "min_loss=10000\n",
    "sess=tf.Session()\n",
    "# train_writer=tf.summary.FileWriter('C:/graph/fitted/train/',sess.graph)\n",
    "# test_writer = tf.summary.FileWriter('C:/graph/fitted/test/', sess.graph)\n",
    "# saver=tf.train.Saver(max_to_keep=3)\n",
    "\n",
    "init_op = tf.global_variables_initializer()\n",
    "sess.run(init_op)\n",
    "coord = tf.train.Coordinator()\n",
    "threads = tf.train.start_queue_runners(sess, coord)\n",
    "t1=time.time()\n",
    "for training_itr in range(50000):\n",
    "    x1, y1 = sess.run([batch_xt,batch_yt])\n",
    "    feed_dict1 = {x:x1,y:y1}\n",
    "    _, loss1,acc1,summaries1 = sess.run([train_op, loss,acc,merged_summary], feed_dict1)\n",
    "\n",
    "#     train_writer.add_summary(summaries1, training_itr)\n",
    "    if training_itr %5==0:\n",
    "#             saver.save(sess=sess, save_path='model/hand_landmark_v6.1_model/model.ckpt',global_step=(global_step + 1))\n",
    "        mean_val_loss = 0\n",
    "\n",
    "        x2,y2=sess.run([batch_xv,batch_yv])\n",
    "        feed_dict2 = {x:x2,y:y2}\n",
    "        tt=time.time()\n",
    "        loss2,acc2,summaries2 = sess.run([loss,acc,merged_summary], feed_dict2)\n",
    "\n",
    "        print('step: {} , time : {}'.format(training_itr,time.time()-tt))\n",
    "        print('train: loss: {} acc: {}  val: loss: {} acc: {}'.format(loss1,acc1,loss2,acc2))\n",
    "#         test_writer.add_summary(summaries2, training_itr)\n",
    "#         if loss1 < min_loss:\n",
    "#             min_loss=loss1\n",
    "#             saver.save(sess=sess, save_path='D:/model/fitted/model.ckpt',global_step=(training_itr + 1))\n",
    "print('final time ： {}'.format(time.time()-t1))\n",
    "sess.close()\n",
    "coord.request_stop()\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
