{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import h5py\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#读取数据\n",
    "f=h5py.File('D:/data/wind/tdata.h5','r')\n",
    "data_x=f['x']\n",
    "data_y=f['y']\n",
    "#分配训练测试集\n",
    "num=list(range(data_x.shape[0]))\n",
    "num1=random.sample(num,200)\n",
    "num2=set(num)-set(num1)\n",
    "num2=list(num2)\n",
    "num1.sort()\n",
    "\n",
    "\n",
    "x_train=data_x[num2]\n",
    "y_train=data_y[num2]\n",
    "\n",
    "x_test=data_x[num1]\n",
    "y_test=data_y[num1]\n",
    "#生成批次\n",
    "train_queue = tf.train.slice_input_producer([x_train,y_train],shuffle=None)\n",
    "val_queue = tf.train.slice_input_producer([x_test,y_test],shuffle=None)\n",
    "batch_xt,batch_yt=tf.train.shuffle_batch(train_queue,batch_size=16,capacity=500,min_after_dequeue=150)\n",
    "batch_xv,batch_yv=tf.train.shuffle_batch(val_queue,batch_size=16,capacity=500,min_after_dequeue=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 搭建模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 超参数设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = batch_xt.shape[1]  # Time series will have the same past and future (to be predicted) lenght. \n",
    "batch_size = batch_xt.shape[0]  # Low value used for live demo purposes - 100 and 1000 would be possible too, crank that up!\n",
    "\n",
    "output_dim = 1\n",
    "input_dim = batch_xt.shape[-1]  # Output dimension (e.g.: multiple signals at once, tied in time)\n",
    "hidden_dim = 10  # Count of hidden neurons in the recurrent units. \n",
    "layers_stacked_count = 3  # Number of stacked recurrent cells, on the neural depth axis. \n",
    "\n",
    "# Optmizer: \n",
    "learning_rate = 0.007  # Small lr helps not to diverge during training. \n",
    "nb_iters = 10000  # How many times we perform a training step (therefore how many times we show a batch). \n",
    "lr_decay = 0.92  # default: 0.9 . Simulated annealing.\n",
    "momentum = 0.5  # default: 0.0 . Momentum technique in weights update\n",
    "lambda_l2_reg = 0.003  # L2 regularization of weights - avoids overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 搭建s2s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('Seq2seq'):\n",
    "\n",
    "    # Encoder: inputs\n",
    "    enc_inp = [\n",
    "        tf.placeholder(tf.float32, shape=(None, input_dim), name=\"inp_{}\".format(t))\n",
    "           for t in range(seq_length)\n",
    "    ]\n",
    "\n",
    "    # Decoder: expected outputs\n",
    "    expected_sparse_output = [\n",
    "        tf.placeholder(tf.float32, shape=(None, output_dim), name=\"expected_sparse_output_\".format(t))\n",
    "          for t in range(seq_length)\n",
    "    ]\n",
    "    \n",
    "    # Give a \"GO\" token to the decoder. \n",
    "    # Note: we might want to fill the encoder with zeros or its own feedback rather than with \"+ enc_inp[:-1]\"\n",
    "    dec_inp = [ tf.zeros_like(enc_inp[0], dtype=np.float32, name=\"GO\") ] + enc_inp[:-1]\n",
    "\n",
    "    # Create a `layers_stacked_count` of stacked RNNs (GRU cells here). \n",
    "    cells = []\n",
    "    for i in range(layers_stacked_count):\n",
    "        with tf.variable_scope('RNN_{}'.format(i)):\n",
    "            cells.append(tf.contrib.rnn.GRUCell(hidden_dim))\n",
    "            # cells.append(tf.nn.rnn_cell.BasicLSTMCell(...))\n",
    "    cell = tf.contrib.rnn.MultiRNNCell(cells)\n",
    "    \n",
    "    # Here, the encoder and the decoder uses the same cell, HOWEVER,\n",
    "    # the weights aren't shared among the encoder and decoder, we have two\n",
    "    # sets of weights created under the hood according to that function's def. \n",
    "    dec_outputs, dec_memory = tf.contrib.legacy_seq2seq.basic_rnn_seq2seq(\n",
    "        enc_inp, \n",
    "        dec_inp, \n",
    "        cell\n",
    "    )\n",
    "    \n",
    "    # For reshaping the output dimensions of the seq2seq RNN: \n",
    "    w_out = tf.Variable(tf.random_normal([hidden_dim, output_dim]))\n",
    "    b_out = tf.Variable(tf.random_normal([output_dim]))\n",
    "    \n",
    "    # Final outputs: with linear rescaling for enabling possibly large and unrestricted output values.\n",
    "    output_scale_factor = tf.Variable(1.0, name=\"Output_ScaleFactor\")\n",
    "    \n",
    "    reshaped_outputs = [output_scale_factor*(tf.matmul(i, w_out) + b_out) for i in dec_outputs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 搭建损失函数 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loss and optimizer\n",
    "\n",
    "with tf.variable_scope('Loss'):\n",
    "    # L2 loss\n",
    "    output_loss = 0\n",
    "    for _y, _Y in zip(reshaped_outputs, expected_sparse_output):\n",
    "        output_loss += tf.reduce_mean(tf.nn.l2_loss(_y - _Y))\n",
    "        \n",
    "    # L2 regularization (to avoid overfitting and to have a  better generalization capacity)\n",
    "    reg_loss = 0\n",
    "    for tf_var in tf.trainable_variables():\n",
    "        if not (\"Bias\" in tf_var.name or \"Output_\" in tf_var.name):\n",
    "            reg_loss += tf.reduce_mean(tf.nn.l2_loss(tf_var))\n",
    "            \n",
    "    loss = output_loss + lambda_l2_reg * reg_loss\n",
    "    tf.summary.scalar('loss',loss)\n",
    "\n",
    "with tf.variable_scope('acc'):\n",
    "    acc=0\n",
    "    num=0\n",
    "    for _y, _Y in zip(reshaped_outputs, expected_sparse_output):\n",
    "        acc += tf.reduce_mean(tf.nn.l2_loss(_y - _Y))/tf.reduce_mean(tf.nn.l2_loss(_Y-0))\n",
    "        num=num+1\n",
    "    acc=acc/num\n",
    "    acc=1-acc\n",
    "    tf.summary.scalar('acc',acc)\n",
    "\n",
    "with tf.variable_scope('Optimizer'):\n",
    "    optimizer = tf.train.RMSPropOptimizer(learning_rate, decay=lr_decay, momentum=momentum)\n",
    "    train_op = optimizer.minimize(loss)\n",
    "    merged_summary = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0\n",
      "train: loss: 34476684.0 acc: -0.0028951168060302734  val: loss: 111415208.0 acc: 0.0031946301460266113\n",
      "step: 5\n",
      "train: loss: 21001980.0 acc: 0.03326618671417236  val: loss: 60552792.0 acc: 0.01745396852493286\n",
      "step: 10\n",
      "train: loss: 38419348.0 acc: 0.03302466869354248  val: loss: 25809712.0 acc: 0.04363095760345459\n",
      "step: 15\n",
      "train: loss: 48217588.0 acc: 0.03744661808013916  val: loss: 22910484.0 acc: 0.052390098571777344\n",
      "step: 20\n",
      "train: loss: 40999248.0 acc: 0.04620206356048584  val: loss: 77671336.0 acc: 0.030995547771453857\n",
      "step: 25\n",
      "train: loss: 37681016.0 acc: 0.057683587074279785  val: loss: 29605200.0 acc: 0.06563699245452881\n",
      "step: 30\n",
      "train: loss: 59751852.0 acc: 0.04483175277709961  val: loss: 36920576.0 acc: 0.06155937910079956\n",
      "step: 35\n",
      "train: loss: 45183564.0 acc: 0.06259810924530029  val: loss: 33717584.0 acc: 0.06251060962677002\n",
      "step: 40\n",
      "train: loss: 38245072.0 acc: 0.06138259172439575  val: loss: 16956422.0 acc: 0.10719603300094604\n",
      "step: 45\n",
      "train: loss: 43996880.0 acc: 0.0685470700263977  val: loss: 44033508.0 acc: 0.06678974628448486\n",
      "step: 50\n",
      "train: loss: 97539320.0 acc: 0.05231219530105591  val: loss: 103197568.0 acc: 0.044065117835998535\n",
      "step: 55\n",
      "train: loss: 72095544.0 acc: 0.058937907218933105  val: loss: 43892476.0 acc: 0.07067281007766724\n",
      "step: 60\n",
      "train: loss: 15346218.0 acc: 0.16009312868118286  val: loss: 42117360.0 acc: 0.09630191326141357\n",
      "step: 65\n",
      "train: loss: 80013624.0 acc: 0.06639730930328369  val: loss: 39607328.0 acc: 0.10602474212646484\n",
      "step: 70\n",
      "train: loss: 276835040.0 acc: 0.04607737064361572  val: loss: 72184416.0 acc: 0.07967013120651245\n",
      "step: 75\n",
      "train: loss: 108948936.0 acc: 0.06683892011642456  val: loss: 19276976.0 acc: 0.14955425262451172\n",
      "step: 80\n",
      "train: loss: 213073232.0 acc: 0.05952531099319458  val: loss: 137418688.0 acc: 0.0668150782585144\n",
      "step: 85\n",
      "train: loss: 213856736.0 acc: 0.06227731704711914  val: loss: 56659612.0 acc: 0.10188353061676025\n",
      "step: 90\n",
      "train: loss: 195917632.0 acc: 0.0701562762260437  val: loss: 8579066.0 acc: 0.27333080768585205\n",
      "step: 95\n",
      "train: loss: 106099504.0 acc: 0.09470236301422119  val: loss: 61936800.0 acc: 0.1021890640258789\n",
      "step: 100\n",
      "train: loss: 131460504.0 acc: 0.07908296585083008  val: loss: 39329700.0 acc: 0.11891031265258789\n",
      "step: 105\n",
      "train: loss: 69206528.0 acc: 0.12078553438186646  val: loss: 73372712.0 acc: 0.10438907146453857\n",
      "step: 110\n",
      "train: loss: 45720876.0 acc: 0.16055631637573242  val: loss: 30276534.0 acc: 0.1827109456062317\n",
      "step: 115\n",
      "train: loss: 86599344.0 acc: 0.11575138568878174  val: loss: 47568880.0 acc: 0.13306266069412231\n",
      "step: 120\n",
      "train: loss: 29543798.0 acc: 0.20311063528060913  val: loss: 38341736.0 acc: 0.16716116666793823\n",
      "step: 125\n",
      "train: loss: 42713560.0 acc: 0.18147289752960205  val: loss: 36756160.0 acc: 0.14518803358078003\n",
      "step: 130\n",
      "train: loss: 18872118.0 acc: 0.26896166801452637  val: loss: 21018064.0 acc: 0.2527611255645752\n",
      "step: 135\n",
      "train: loss: 65268968.0 acc: 0.14068031311035156  val: loss: 56160096.0 acc: 0.15347397327423096\n",
      "step: 140\n",
      "train: loss: 21428554.0 acc: 0.26518720388412476  val: loss: 28026406.0 acc: 0.23232603073120117\n",
      "step: 145\n",
      "train: loss: 49917504.0 acc: 0.18943393230438232  val: loss: 56939060.0 acc: 0.1614972949028015\n",
      "step: 150\n",
      "train: loss: 15593569.0 acc: 0.323905348777771  val: loss: 42195560.0 acc: 0.20770853757858276\n",
      "step: 155\n",
      "train: loss: 8350082.5 acc: 0.45909470319747925  val: loss: 30763768.0 acc: 0.21481531858444214\n",
      "step: 160\n",
      "train: loss: 36336908.0 acc: 0.24520176649093628  val: loss: 78904992.0 acc: 0.15425527095794678\n",
      "step: 165\n",
      "train: loss: 6053948.0 acc: 0.5165465474128723  val: loss: 63088172.0 acc: 0.16581356525421143\n",
      "step: 170\n",
      "train: loss: 13461853.0 acc: 0.42188727855682373  val: loss: 49989016.0 acc: 0.18893909454345703\n",
      "step: 175\n",
      "train: loss: 19800440.0 acc: 0.3499157428741455  val: loss: 74249360.0 acc: 0.17095822095870972\n",
      "step: 180\n",
      "train: loss: 15600971.0 acc: 0.38118278980255127  val: loss: 29727572.0 acc: 0.2663489580154419\n",
      "step: 185\n",
      "train: loss: 18553214.0 acc: 0.3775904178619385  val: loss: 12686601.0 acc: 0.4145551323890686\n",
      "step: 190\n",
      "train: loss: 12750627.0 acc: 0.45606303215026855  val: loss: 37875748.0 acc: 0.2665945887565613\n",
      "step: 195\n",
      "train: loss: 18266008.0 acc: 0.40699464082717896  val: loss: 21091168.0 acc: 0.29890745878219604\n",
      "step: 200\n",
      "train: loss: 16623873.0 acc: 0.4276307225227356  val: loss: 68711512.0 acc: 0.20679128170013428\n",
      "step: 205\n",
      "train: loss: 9609073.0 acc: 0.5116384029388428  val: loss: 6766306.0 acc: 0.5369924306869507\n",
      "step: 210\n",
      "train: loss: 5523093.0 acc: 0.6062079668045044  val: loss: 62230424.0 acc: 0.21713727712631226\n",
      "step: 215\n",
      "train: loss: 8456577.0 acc: 0.5166832208633423  val: loss: 62728784.0 acc: 0.2063238024711609\n",
      "step: 220\n",
      "train: loss: 12156649.0 acc: 0.4802383780479431  val: loss: 39877008.0 acc: 0.2680717706680298\n",
      "step: 225\n",
      "train: loss: 10242024.0 acc: 0.5144975185394287  val: loss: 25727164.0 acc: 0.3495197296142578\n",
      "step: 230\n",
      "train: loss: 11012620.0 acc: 0.5278463363647461  val: loss: 41613000.0 acc: 0.2394850254058838\n",
      "step: 235\n",
      "train: loss: 17233860.0 acc: 0.46327871084213257  val: loss: 65464932.0 acc: 0.25766098499298096\n",
      "step: 240\n",
      "train: loss: 17946896.0 acc: 0.4146817922592163  val: loss: 9815500.0 acc: 0.45170706510543823\n",
      "step: 245\n",
      "train: loss: 3153730.0 acc: 0.7614691853523254  val: loss: 27296556.0 acc: 0.3398755192756653\n",
      "step: 250\n",
      "train: loss: 13276971.0 acc: 0.497910737991333  val: loss: 39336240.0 acc: 0.29102200269699097\n",
      "step: 255\n",
      "train: loss: 17706916.0 acc: 0.4648471474647522  val: loss: 23036140.0 acc: 0.434606671333313\n",
      "step: 260\n",
      "train: loss: 9147649.0 acc: 0.5732253789901733  val: loss: 67424856.0 acc: 0.25424134731292725\n",
      "step: 265\n",
      "train: loss: 20144300.0 acc: 0.4489036798477173  val: loss: 13577516.0 acc: 0.4478759169578552\n",
      "step: 270\n",
      "train: loss: 20058570.0 acc: 0.43855804204940796  val: loss: 32171552.0 acc: 0.31470251083374023\n",
      "step: 275\n",
      "train: loss: 24824512.0 acc: 0.4388049840927124  val: loss: 21771898.0 acc: 0.36066997051239014\n",
      "step: 280\n",
      "train: loss: 25736358.0 acc: 0.42984479665756226  val: loss: 23944444.0 acc: 0.42006582021713257\n",
      "step: 285\n",
      "train: loss: 43766416.0 acc: 0.3850744962692261  val: loss: 29518744.0 acc: 0.39088547229766846\n",
      "step: 290\n",
      "train: loss: 35976088.0 acc: 0.43203985691070557  val: loss: 19202714.0 acc: 0.47109872102737427\n",
      "step: 295\n",
      "train: loss: 27702438.0 acc: 0.44965362548828125  val: loss: 62015336.0 acc: 0.3338659405708313\n",
      "step: 300\n",
      "train: loss: 28995354.0 acc: 0.43806034326553345  val: loss: 6174019.0 acc: 0.6448080539703369\n",
      "step: 305\n",
      "train: loss: 35169240.0 acc: 0.41218101978302  val: loss: 47740532.0 acc: 0.3524233102798462\n",
      "step: 310\n",
      "train: loss: 35853556.0 acc: 0.42102575302124023  val: loss: 10339736.0 acc: 0.5128949880599976\n",
      "step: 315\n",
      "train: loss: 52604960.0 acc: 0.4425308108329773  val: loss: 83887264.0 acc: 0.35221588611602783\n",
      "step: 320\n",
      "train: loss: 58248768.0 acc: 0.4501687288284302  val: loss: 38208584.0 acc: 0.3323410749435425\n",
      "step: 325\n",
      "train: loss: 22340938.0 acc: 0.46212106943130493  val: loss: 9675172.0 acc: 0.6437894105911255\n",
      "step: 330\n",
      "train: loss: 33136564.0 acc: 0.45750248432159424  val: loss: 26252702.0 acc: 0.4789001941680908\n",
      "step: 335\n",
      "train: loss: 25506612.0 acc: 0.5093387365341187  val: loss: 5661825.0 acc: 0.7030670046806335\n",
      "step: 340\n",
      "train: loss: 30394544.0 acc: 0.4869617223739624  val: loss: 6749360.5 acc: 0.7025951147079468\n",
      "step: 345\n",
      "train: loss: 15931679.0 acc: 0.5352178812026978  val: loss: 7496298.5 acc: 0.6349114775657654\n",
      "step: 350\n",
      "train: loss: 34296292.0 acc: 0.5095284581184387  val: loss: 55770964.0 acc: 0.3993402123451233\n",
      "step: 355\n",
      "train: loss: 6698331.0 acc: 0.5480342507362366  val: loss: 33038718.0 acc: 0.49149078130722046\n",
      "step: 360\n",
      "train: loss: 6068020.5 acc: 0.5041634440422058  val: loss: 4550406.0 acc: 0.7768543362617493\n",
      "step: 365\n",
      "train: loss: 8121742.5 acc: 0.5647243857383728  val: loss: 28971058.0 acc: 0.4592229127883911\n",
      "step: 370\n",
      "train: loss: 12082338.0 acc: 0.5157390832901001  val: loss: 22189844.0 acc: 0.5324430465698242\n",
      "step: 375\n",
      "train: loss: 2760764.0 acc: 0.7239248752593994  val: loss: 37962208.0 acc: 0.4886901378631592\n",
      "step: 380\n",
      "train: loss: 1422434.25 acc: 0.7998855710029602  val: loss: 17553408.0 acc: 0.5536036491394043\n",
      "step: 385\n",
      "train: loss: 6307056.0 acc: 0.5929072499275208  val: loss: 9559577.0 acc: 0.674113392829895\n",
      "step: 390\n",
      "train: loss: 5119035.5 acc: 0.5669388771057129  val: loss: 35032300.0 acc: 0.46136486530303955\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 395\n",
      "train: loss: 657596.875 acc: 0.8703495264053345  val: loss: 12843689.0 acc: 0.6107110977172852\n",
      "step: 400\n",
      "train: loss: 791167.9375 acc: 0.8167446851730347  val: loss: 9812804.0 acc: 0.6725301742553711\n",
      "step: 405\n",
      "train: loss: 12004883.0 acc: 0.5388555526733398  val: loss: 21943198.0 acc: 0.57313072681427\n",
      "step: 410\n",
      "train: loss: 457540.1875 acc: 0.8321945667266846  val: loss: 14565798.0 acc: 0.6498714089393616\n",
      "step: 415\n",
      "train: loss: 1697061.0 acc: 0.7756757736206055  val: loss: 26879370.0 acc: 0.574226975440979\n",
      "step: 420\n",
      "train: loss: 5320892.0 acc: 0.7487071752548218  val: loss: 21576758.0 acc: 0.5949835181236267\n",
      "step: 425\n",
      "train: loss: 2908818.25 acc: 0.8236672878265381  val: loss: 17367756.0 acc: 0.6008491516113281\n",
      "step: 430\n",
      "train: loss: 1752172.25 acc: 0.8509324193000793  val: loss: 39467256.0 acc: 0.4986134171485901\n",
      "step: 435\n",
      "train: loss: 1043892.875 acc: 0.8795480728149414  val: loss: 10743271.0 acc: 0.6727398633956909\n",
      "step: 440\n",
      "train: loss: 7152224.0 acc: 0.6864197850227356  val: loss: 32559804.0 acc: 0.4939899444580078\n",
      "step: 445\n",
      "train: loss: 1809459.875 acc: 0.8357820510864258  val: loss: 12619692.0 acc: 0.6535925269126892\n",
      "step: 450\n",
      "train: loss: 6347984.0 acc: 0.6271302700042725  val: loss: 31732046.0 acc: 0.5590732097625732\n",
      "step: 455\n",
      "train: loss: 614225.8125 acc: 0.848130464553833  val: loss: 32744618.0 acc: 0.4926115870475769\n",
      "step: 460\n",
      "train: loss: 616537.625 acc: 0.8947578072547913  val: loss: 14041336.0 acc: 0.623592734336853\n",
      "step: 465\n",
      "train: loss: 228830.328125 acc: 0.8823197484016418  val: loss: 6411015.0 acc: 0.7292376160621643\n",
      "step: 470\n",
      "train: loss: 836278.1875 acc: 0.8906904458999634  val: loss: 20235006.0 acc: 0.6079077124595642\n",
      "step: 475\n",
      "train: loss: 1186748.875 acc: 0.8916569948196411  val: loss: 9369761.0 acc: 0.6836314797401428\n",
      "step: 480\n",
      "train: loss: 1179289.5 acc: 0.8987241983413696  val: loss: 60854536.0 acc: 0.4388134479522705\n",
      "step: 485\n",
      "train: loss: 1800553.125 acc: 0.9026211500167847  val: loss: 50924676.0 acc: 0.4488237500190735\n",
      "step: 490\n",
      "train: loss: 1612871.5 acc: 0.8780226111412048  val: loss: 10226548.0 acc: 0.7197037935256958\n",
      "step: 495\n",
      "train: loss: 3247628.5 acc: 0.8723775148391724  val: loss: 13918450.0 acc: 0.6704384088516235\n",
      "step: 500\n",
      "train: loss: 7059357.5 acc: 0.8349317312240601  val: loss: 10181078.0 acc: 0.6749457716941833\n",
      "step: 505\n",
      "train: loss: 4851269.0 acc: 0.7996723651885986  val: loss: 24462574.0 acc: 0.5750352144241333\n",
      "step: 510\n",
      "train: loss: 2988862.25 acc: 0.8122526407241821  val: loss: 1970096.375 acc: 0.8370349407196045\n",
      "step: 515\n",
      "train: loss: 2111123.5 acc: 0.8699381947517395  val: loss: 42666768.0 acc: 0.47446393966674805\n",
      "step: 520\n",
      "train: loss: 5573389.0 acc: 0.8075918555259705  val: loss: 21744836.0 acc: 0.662895143032074\n",
      "step: 525\n",
      "train: loss: 10257180.0 acc: 0.7078852653503418  val: loss: 7776375.0 acc: 0.7429830431938171\n",
      "step: 530\n",
      "train: loss: 12160968.0 acc: 0.7661519646644592  val: loss: 3561757.25 acc: 0.8668327331542969\n",
      "step: 535\n",
      "train: loss: 11806779.0 acc: 0.7818584442138672  val: loss: 45980872.0 acc: 0.4832313656806946\n",
      "step: 540\n",
      "train: loss: 13589460.0 acc: 0.7486604452133179  val: loss: 22043194.0 acc: 0.684864342212677\n",
      "step: 545\n",
      "train: loss: 15147345.0 acc: 0.755128800868988  val: loss: 39195316.0 acc: 0.5613651275634766\n",
      "step: 550\n",
      "train: loss: 11715223.0 acc: 0.7661911249160767  val: loss: 4925048.5 acc: 0.8094947934150696\n",
      "step: 555\n",
      "train: loss: 17167462.0 acc: 0.7580955624580383  val: loss: 8824006.0 acc: 0.6905333995819092\n",
      "step: 560\n",
      "train: loss: 20463706.0 acc: 0.6723151803016663  val: loss: 8814604.0 acc: 0.76896733045578\n",
      "step: 565\n",
      "train: loss: 12629626.0 acc: 0.7751671075820923  val: loss: 20004916.0 acc: 0.6840064525604248\n",
      "step: 570\n",
      "train: loss: 18279176.0 acc: 0.7227309346199036  val: loss: 7040083.0 acc: 0.7012259364128113\n",
      "step: 575\n",
      "train: loss: 18352772.0 acc: 0.7036480903625488  val: loss: 47491268.0 acc: 0.5639989972114563\n",
      "step: 580\n",
      "train: loss: 44028728.0 acc: 0.5386313199996948  val: loss: 15450562.0 acc: 0.721656858921051\n",
      "step: 585\n",
      "train: loss: 31418338.0 acc: 0.6656176447868347  val: loss: 13286152.0 acc: 0.7431418299674988\n",
      "step: 590\n",
      "train: loss: 91066368.0 acc: 0.5439104437828064  val: loss: 10125675.0 acc: 0.7659104466438293\n",
      "step: 595\n",
      "train: loss: 43444108.0 acc: 0.597227931022644  val: loss: 44985120.0 acc: 0.6028010249137878\n",
      "step: 600\n",
      "train: loss: 90497848.0 acc: 0.5765029191970825  val: loss: 25342908.0 acc: 0.6375218629837036\n",
      "step: 605\n",
      "train: loss: 77066320.0 acc: 0.5729858875274658  val: loss: 8410195.0 acc: 0.7950674891471863\n",
      "step: 610\n",
      "train: loss: 80277352.0 acc: 0.6070672869682312  val: loss: 21321056.0 acc: 0.7195470333099365\n",
      "step: 615\n",
      "train: loss: 46374604.0 acc: 0.6368640661239624  val: loss: 13887335.0 acc: 0.6971186399459839\n",
      "step: 620\n",
      "train: loss: 28944444.0 acc: 0.699906051158905  val: loss: 24254298.0 acc: 0.654529869556427\n",
      "step: 625\n",
      "train: loss: 13684478.0 acc: 0.8091443777084351  val: loss: 4727861.0 acc: 0.8866396546363831\n",
      "step: 630\n",
      "train: loss: 20876508.0 acc: 0.6862767934799194  val: loss: 4869827.5 acc: 0.8453252911567688\n",
      "step: 635\n",
      "train: loss: 16556457.0 acc: 0.7544924020767212  val: loss: 22931626.0 acc: 0.5957437753677368\n",
      "step: 640\n",
      "train: loss: 8157809.0 acc: 0.817035436630249  val: loss: 6279616.0 acc: 0.7075579762458801\n",
      "step: 645\n",
      "train: loss: 5469490.5 acc: 0.8391156792640686  val: loss: 21871928.0 acc: 0.6788045763969421\n",
      "step: 650\n",
      "train: loss: 14431354.0 acc: 0.7240921258926392  val: loss: 13871196.0 acc: 0.6990278363227844\n",
      "step: 655\n",
      "train: loss: 6505451.0 acc: 0.7932010889053345  val: loss: 13979542.0 acc: 0.70119309425354\n",
      "step: 660\n",
      "train: loss: 6485137.5 acc: 0.6968635320663452  val: loss: 45007424.0 acc: 0.6439798474311829\n",
      "step: 665\n",
      "train: loss: 25783408.0 acc: 0.5863088369369507  val: loss: 23868054.0 acc: 0.61361163854599\n",
      "step: 670\n",
      "train: loss: 8844382.0 acc: 0.7500053644180298  val: loss: 26668382.0 acc: 0.6101844906806946\n",
      "step: 675\n",
      "train: loss: 3343812.5 acc: 0.8282582759857178  val: loss: 13763626.0 acc: 0.6889369487762451\n",
      "step: 680\n",
      "train: loss: 2166423.0 acc: 0.8643803596496582  val: loss: 21676966.0 acc: 0.6869113445281982\n",
      "step: 685\n",
      "train: loss: 10097892.0 acc: 0.6849433779716492  val: loss: 18025332.0 acc: 0.6002104878425598\n",
      "step: 690\n",
      "train: loss: 3352800.0 acc: 0.8098293542861938  val: loss: 9839684.0 acc: 0.660617470741272\n",
      "step: 695\n",
      "train: loss: 2542459.25 acc: 0.8468624353408813  val: loss: 20011388.0 acc: 0.6018694043159485\n",
      "step: 700\n",
      "train: loss: 1812718.625 acc: 0.8707311749458313  val: loss: 13223940.0 acc: 0.7393430471420288\n",
      "step: 705\n",
      "train: loss: 4228134.0 acc: 0.8149123191833496  val: loss: 27196078.0 acc: 0.6971681118011475\n",
      "step: 710\n",
      "train: loss: 6157145.5 acc: 0.7211743593215942  val: loss: 16360932.0 acc: 0.6867117285728455\n",
      "step: 715\n",
      "train: loss: 4855303.5 acc: 0.8508738875389099  val: loss: 27965262.0 acc: 0.6828948259353638\n",
      "step: 720\n",
      "train: loss: 7167212.5 acc: 0.791589617729187  val: loss: 16400249.0 acc: 0.7347304821014404\n",
      "step: 725\n",
      "train: loss: 3875340.25 acc: 0.7628353834152222  val: loss: 7210771.5 acc: 0.7918733954429626\n",
      "step: 730\n",
      "train: loss: 2465157.75 acc: 0.8625444173812866  val: loss: 11062383.0 acc: 0.6766341328620911\n",
      "step: 735\n",
      "train: loss: 1659525.0 acc: 0.863559901714325  val: loss: 32882866.0 acc: 0.5981760621070862\n",
      "step: 740\n",
      "train: loss: 1705089.75 acc: 0.8530904650688171  val: loss: 9592123.0 acc: 0.6844385862350464\n",
      "step: 745\n",
      "train: loss: 9589775.0 acc: 0.7289651036262512  val: loss: 6871113.0 acc: 0.7937703728675842\n",
      "step: 750\n",
      "train: loss: 7800219.0 acc: 0.7191765904426575  val: loss: 25395792.0 acc: 0.6451877951622009\n",
      "step: 755\n",
      "train: loss: 5364974.5 acc: 0.7811834216117859  val: loss: 16637836.0 acc: 0.7687761187553406\n",
      "step: 760\n",
      "train: loss: 2036456.875 acc: 0.8296467661857605  val: loss: 10476926.0 acc: 0.6966939568519592\n",
      "step: 765\n",
      "train: loss: 5898814.0 acc: 0.7629342079162598  val: loss: 25977934.0 acc: 0.5530512928962708\n",
      "step: 770\n",
      "train: loss: 5193550.0 acc: 0.7423703670501709  val: loss: 12433785.0 acc: 0.693122386932373\n",
      "step: 775\n",
      "train: loss: 7391539.0 acc: 0.7271270751953125  val: loss: 8802849.0 acc: 0.7015981674194336\n",
      "step: 780\n",
      "train: loss: 12277352.0 acc: 0.7069369554519653  val: loss: 10200794.0 acc: 0.7175136804580688\n",
      "step: 785\n",
      "train: loss: 11677144.0 acc: 0.7482656240463257  val: loss: 28032992.0 acc: 0.6297072172164917\n",
      "step: 790\n",
      "train: loss: 11806392.0 acc: 0.7614607214927673  val: loss: 9420389.0 acc: 0.8323068022727966\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 795\n",
      "train: loss: 13621606.0 acc: 0.8140578269958496  val: loss: 7752155.5 acc: 0.7840930819511414\n",
      "step: 800\n",
      "train: loss: 11016167.0 acc: 0.851718544960022  val: loss: 24159706.0 acc: 0.759017288684845\n",
      "step: 805\n",
      "train: loss: 10624907.0 acc: 0.810798168182373  val: loss: 14717858.0 acc: 0.7258673310279846\n",
      "step: 810\n",
      "train: loss: 6988831.5 acc: 0.8599867224693298  val: loss: 28224802.0 acc: 0.6758928298950195\n",
      "step: 815\n",
      "train: loss: 6207606.0 acc: 0.7778469920158386  val: loss: 31435044.0 acc: 0.6754424571990967\n",
      "step: 820\n",
      "train: loss: 6697622.0 acc: 0.6852985620498657  val: loss: 16261066.0 acc: 0.8241550922393799\n",
      "step: 825\n",
      "train: loss: 6733633.0 acc: 0.8863685131072998  val: loss: 30530534.0 acc: 0.6989288330078125\n",
      "step: 830\n",
      "train: loss: 8541499.0 acc: 0.8869803547859192  val: loss: 10085547.0 acc: 0.7516784071922302\n",
      "step: 835\n",
      "train: loss: 10998383.0 acc: 0.8809555768966675  val: loss: 3376403.5 acc: 0.9150993227958679\n",
      "step: 840\n",
      "train: loss: 7327928.5 acc: 0.927736759185791  val: loss: 5206803.0 acc: 0.49465417861938477\n",
      "step: 845\n",
      "train: loss: 5111945.0 acc: 0.924500048160553  val: loss: 6869137.0 acc: 0.8167088627815247\n",
      "step: 850\n",
      "train: loss: 5438380.5 acc: 0.9272854328155518  val: loss: 9032559.0 acc: 0.7365326881408691\n",
      "step: 855\n",
      "train: loss: 2644867.0 acc: 0.9485023021697998  val: loss: 12197087.0 acc: 0.6811545491218567\n",
      "step: 860\n",
      "train: loss: 3285228.25 acc: 0.8817868828773499  val: loss: 12374035.0 acc: 0.7728514671325684\n",
      "step: 865\n",
      "train: loss: 4084387.75 acc: 0.8816123008728027  val: loss: 7585168.5 acc: 0.7486067414283752\n",
      "step: 870\n",
      "train: loss: 4901751.5 acc: 0.8175562620162964  val: loss: 17895250.0 acc: 0.6777480840682983\n",
      "step: 875\n",
      "train: loss: 1401062.25 acc: 0.9534140825271606  val: loss: 7193419.5 acc: 0.8377260565757751\n",
      "step: 880\n",
      "train: loss: 1358577.25 acc: 0.9009351134300232  val: loss: 28737496.0 acc: 0.7093961834907532\n",
      "step: 885\n",
      "train: loss: 2070038.625 acc: 0.9124302268028259  val: loss: 31250756.0 acc: 0.6614885330200195\n",
      "step: 890\n",
      "train: loss: 946792.75 acc: 0.8671894669532776  val: loss: 2097620.75 acc: 0.8258376717567444\n",
      "step: 895\n",
      "train: loss: 2535096.5 acc: 0.8807014226913452  val: loss: 7378464.0 acc: 0.815570592880249\n",
      "step: 900\n",
      "train: loss: 1641713.75 acc: 0.9374924302101135  val: loss: 41902144.0 acc: 0.6767932772636414\n",
      "step: 905\n",
      "train: loss: 869445.4375 acc: 0.7835666537284851  val: loss: 19204774.0 acc: 0.5504835844039917\n",
      "step: 910\n",
      "train: loss: 1043633.125 acc: 0.9288852214813232  val: loss: 21486690.0 acc: 0.7325451970100403\n",
      "step: 915\n",
      "train: loss: 920515.375 acc: 0.9354496598243713  val: loss: 9024059.0 acc: 0.8155754804611206\n",
      "step: 920\n",
      "train: loss: 767041.5625 acc: 0.9369058012962341  val: loss: 6098024.0 acc: 0.7643656730651855\n",
      "step: 925\n",
      "train: loss: 362079.875 acc: 0.9015673995018005  val: loss: 3787224.5 acc: 0.8742061257362366\n",
      "step: 930\n",
      "train: loss: 190050.1875 acc: 0.928416907787323  val: loss: 8811039.0 acc: 0.564257025718689\n",
      "step: 935\n",
      "train: loss: 513094.21875 acc: 0.8911548852920532  val: loss: 19249984.0 acc: 0.7279735207557678\n",
      "step: 940\n",
      "train: loss: 2004476.25 acc: 0.9177796840667725  val: loss: 14096025.0 acc: 0.7631040215492249\n",
      "step: 945\n",
      "train: loss: 1038090.25 acc: 0.9092780351638794  val: loss: 6888783.5 acc: 0.7693272829055786\n",
      "step: 950\n",
      "train: loss: 1322164.25 acc: 0.8796216249465942  val: loss: 17462254.0 acc: 0.7298817038536072\n",
      "step: 955\n",
      "train: loss: 1834952.125 acc: 0.8717440366744995  val: loss: 16230947.0 acc: 0.7623240351676941\n",
      "step: 960\n",
      "train: loss: 1648232.375 acc: 0.8553544282913208  val: loss: 21465922.0 acc: 0.7903913259506226\n",
      "step: 965\n",
      "train: loss: 1197241.875 acc: 0.8453223705291748  val: loss: 4855877.5 acc: 0.891910195350647\n",
      "step: 970\n",
      "train: loss: 967383.625 acc: 0.9208744764328003  val: loss: 20759508.0 acc: 0.617597758769989\n",
      "step: 975\n",
      "train: loss: 712520.5625 acc: 0.9367574453353882  val: loss: 14426765.0 acc: 0.6701458692550659\n",
      "step: 980\n",
      "train: loss: 466686.1875 acc: 0.9022526741027832  val: loss: 11319040.0 acc: 0.6938268542289734\n",
      "step: 985\n",
      "train: loss: 183093.046875 acc: 0.9589281678199768  val: loss: 14705517.0 acc: 0.7898592352867126\n",
      "step: 990\n",
      "train: loss: 1512499.5 acc: 0.88777095079422  val: loss: 16336034.0 acc: 0.7462039589881897\n",
      "step: 995\n",
      "train: loss: 1186774.375 acc: 0.9461960196495056  val: loss: 14875355.0 acc: 0.7528249621391296\n",
      "step: 1000\n",
      "train: loss: 1624457.0 acc: 0.8945711851119995  val: loss: 6304100.0 acc: 0.6864039301872253\n",
      "step: 1005\n",
      "train: loss: 2491611.25 acc: 0.9081357717514038  val: loss: 28997588.0 acc: 0.7036145925521851\n",
      "step: 1010\n",
      "train: loss: 1215366.125 acc: 0.9412331581115723  val: loss: 6669963.5 acc: 0.889030396938324\n",
      "step: 1015\n",
      "train: loss: 923197.0625 acc: 0.9329329133033752  val: loss: 9866401.0 acc: 0.47603297233581543\n",
      "step: 1020\n",
      "train: loss: 1739908.125 acc: 0.941612720489502  val: loss: 12292584.0 acc: 0.8066668510437012\n",
      "step: 1025\n",
      "train: loss: 4735220.5 acc: 0.8240240216255188  val: loss: 3497537.75 acc: 0.8993383646011353\n",
      "step: 1030\n",
      "train: loss: 2898142.25 acc: 0.9058588743209839  val: loss: 17316216.0 acc: 0.815260648727417\n",
      "step: 1035\n",
      "train: loss: 2090086.875 acc: 0.9086531400680542  val: loss: 5123607.0 acc: 0.9267939329147339\n",
      "step: 1040\n",
      "train: loss: 960226.6875 acc: 0.9532924890518188  val: loss: 5382881.0 acc: 0.6750354170799255\n",
      "step: 1045\n",
      "train: loss: 1545725.875 acc: 0.9544066190719604  val: loss: 4720556.5 acc: 0.7676427960395813\n",
      "step: 1050\n",
      "train: loss: 1935453.25 acc: 0.9371182322502136  val: loss: 6784930.0 acc: -0.29028499126434326\n",
      "step: 1055\n",
      "train: loss: 3541466.0 acc: 0.9467057585716248  val: loss: 16513844.0 acc: 0.7493394613265991\n",
      "step: 1060\n",
      "train: loss: 5367724.0 acc: 0.8931167721748352  val: loss: 7756753.0 acc: 0.792657196521759\n",
      "step: 1065\n",
      "train: loss: 1526612.625 acc: 0.9821831583976746  val: loss: 3597855.5 acc: 0.8991972208023071\n",
      "step: 1070\n",
      "train: loss: 3120452.75 acc: 0.9429808855056763  val: loss: 19897332.0 acc: 0.6377655267715454\n",
      "step: 1075\n",
      "train: loss: 7336485.0 acc: 0.8751832246780396  val: loss: 16331814.0 acc: 0.6907967925071716\n",
      "step: 1080\n",
      "train: loss: 7403346.5 acc: 0.8843637704849243  val: loss: 16445928.0 acc: 0.7186476588249207\n",
      "step: 1085\n",
      "train: loss: 26131574.0 acc: 0.7773343920707703  val: loss: 11440301.0 acc: 0.7885044813156128\n",
      "step: 1090\n",
      "train: loss: 9537594.0 acc: 0.8857520222663879  val: loss: 3524294.25 acc: 0.9271005988121033\n",
      "step: 1095\n",
      "train: loss: 9143623.0 acc: 0.8698215484619141  val: loss: 21410532.0 acc: 0.679327666759491\n",
      "step: 1100\n",
      "train: loss: 11701296.0 acc: 0.8063445091247559  val: loss: 5587257.5 acc: 0.8793269991874695\n",
      "step: 1105\n",
      "train: loss: 37472836.0 acc: 0.7827351093292236  val: loss: 6780936.0 acc: 0.7792226672172546\n",
      "step: 1110\n",
      "train: loss: 43269064.0 acc: 0.7687221765518188  val: loss: 15388958.0 acc: 0.783248782157898\n",
      "step: 1115\n",
      "train: loss: 43680752.0 acc: 0.7546983361244202  val: loss: 13295901.0 acc: 0.4899377226829529\n",
      "step: 1120\n",
      "train: loss: 19809746.0 acc: 0.8836440443992615  val: loss: 13146014.0 acc: 0.782726526260376\n",
      "step: 1125\n",
      "train: loss: 15627437.0 acc: 0.8337327837944031  val: loss: 11299896.0 acc: 0.8012875318527222\n",
      "step: 1130\n",
      "train: loss: 27274154.0 acc: 0.8312035799026489  val: loss: 26804000.0 acc: 0.5907692909240723\n",
      "step: 1135\n",
      "train: loss: 13355333.0 acc: 0.8792377710342407  val: loss: 21645036.0 acc: 0.8412119150161743\n",
      "step: 1140\n",
      "train: loss: 20895176.0 acc: 0.8564306497573853  val: loss: 8196752.0 acc: 0.8808058500289917\n",
      "step: 1145\n",
      "train: loss: 17157888.0 acc: 0.8537387847900391  val: loss: 7582813.5 acc: 0.7826561331748962\n",
      "step: 1150\n",
      "train: loss: 11231193.0 acc: 0.9037804007530212  val: loss: 2293668.5 acc: 0.8608160018920898\n",
      "step: 1155\n",
      "train: loss: 6597055.0 acc: 0.8392197489738464  val: loss: 10723051.0 acc: 0.830878496170044\n",
      "step: 1160\n",
      "train: loss: 29276620.0 acc: 0.574729323387146  val: loss: 9559553.0 acc: 0.8425565958023071\n",
      "step: 1165\n",
      "train: loss: 16103146.0 acc: 0.662002444267273  val: loss: 10579435.0 acc: 0.7524247765541077\n",
      "step: 1170\n",
      "train: loss: 19059182.0 acc: 0.7979452013969421  val: loss: 13428592.0 acc: 0.746335506439209\n",
      "step: 1175\n",
      "train: loss: 12448996.0 acc: 0.852279782295227  val: loss: 5860356.5 acc: 0.43841540813446045\n",
      "step: 1180\n",
      "train: loss: 3763345.25 acc: 0.768244206905365  val: loss: 8269538.0 acc: 0.7923933267593384\n",
      "step: 1185\n",
      "train: loss: 23602956.0 acc: 0.6366124153137207  val: loss: 10977323.0 acc: 0.7823268175125122\n",
      "step: 1190\n",
      "train: loss: 3949371.0 acc: 0.7873841524124146  val: loss: 12303175.0 acc: 0.7037253379821777\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 1195\n",
      "train: loss: 13263848.0 acc: 0.6836504936218262  val: loss: 11498497.0 acc: 0.7481537461280823\n",
      "step: 1200\n",
      "train: loss: 1596824.375 acc: 0.8771691918373108  val: loss: 11988598.0 acc: 0.7967002987861633\n",
      "step: 1205\n",
      "train: loss: 8044863.5 acc: 0.8425007462501526  val: loss: 30371768.0 acc: 0.6689443588256836\n",
      "step: 1210\n",
      "train: loss: 3015543.25 acc: 0.8434683084487915  val: loss: 8056881.0 acc: 0.8442305326461792\n",
      "step: 1215\n",
      "train: loss: 6731596.5 acc: 0.85820472240448  val: loss: 6732558.0 acc: 0.7318225502967834\n",
      "step: 1220\n",
      "train: loss: 8916610.0 acc: 0.7350637912750244  val: loss: 12781706.0 acc: 0.7900791764259338\n",
      "step: 1225\n",
      "train: loss: 5195051.0 acc: 0.8322274684906006  val: loss: 12170383.0 acc: 0.8001750111579895\n",
      "step: 1230\n",
      "train: loss: 6460771.0 acc: 0.810108482837677  val: loss: 11669407.0 acc: 0.8059514760971069\n",
      "step: 1235\n",
      "train: loss: 5614684.0 acc: 0.7771059274673462  val: loss: 18660188.0 acc: 0.7598909139633179\n",
      "step: 1240\n",
      "train: loss: 11514824.0 acc: 0.8143825531005859  val: loss: 25350800.0 acc: 0.5762876272201538\n",
      "step: 1245\n",
      "train: loss: 13021148.0 acc: 0.6275546550750732  val: loss: 19458610.0 acc: 0.6292601227760315\n",
      "step: 1250\n",
      "train: loss: 4568997.0 acc: 0.7957023978233337  val: loss: 1602165.375 acc: 0.8045876026153564\n",
      "step: 1255\n",
      "train: loss: 2147197.5 acc: 0.8808021545410156  val: loss: 4050550.5 acc: 0.8216965198516846\n",
      "step: 1260\n",
      "train: loss: 6421116.5 acc: 0.7342899441719055  val: loss: 22961912.0 acc: 0.7136847376823425\n",
      "step: 1265\n",
      "train: loss: 6580903.0 acc: 0.7606246471405029  val: loss: 9706267.0 acc: 0.8274635076522827\n",
      "step: 1270\n",
      "train: loss: 7762691.0 acc: 0.7093086242675781  val: loss: 10350525.0 acc: 0.7413433790206909\n",
      "step: 1275\n",
      "train: loss: 5596464.0 acc: 0.672217071056366  val: loss: 26661478.0 acc: 0.5671228766441345\n",
      "step: 1280\n",
      "train: loss: 1627683.75 acc: 0.8281236886978149  val: loss: 28967748.0 acc: 0.631827175617218\n",
      "step: 1285\n",
      "train: loss: 3153901.25 acc: 0.7396754622459412  val: loss: 7844099.5 acc: 0.7877051830291748\n",
      "step: 1290\n",
      "train: loss: 1998227.0 acc: 0.849362850189209  val: loss: 32953768.0 acc: 0.5103240609169006\n",
      "step: 1295\n",
      "train: loss: 6128616.5 acc: 0.7566495537757874  val: loss: 5006851.5 acc: 0.7432231903076172\n",
      "step: 1300\n",
      "train: loss: 11755744.0 acc: 0.751934826374054  val: loss: 6044899.0 acc: 0.8375529050827026\n",
      "step: 1305\n",
      "train: loss: 5862780.0 acc: 0.7925411462783813  val: loss: 10502870.0 acc: 0.7653342485427856\n",
      "step: 1310\n",
      "train: loss: 10529973.0 acc: 0.8669263124465942  val: loss: 10812578.0 acc: 0.8423231244087219\n",
      "step: 1315\n",
      "train: loss: 7316705.5 acc: 0.9051117897033691  val: loss: 9372010.0 acc: 0.5362270474433899\n",
      "step: 1320\n",
      "train: loss: 6367414.5 acc: 0.8612850904464722  val: loss: 13229074.0 acc: 0.482511043548584\n",
      "step: 1325\n",
      "train: loss: 11157814.0 acc: 0.7881242632865906  val: loss: 27758150.0 acc: 0.7182266712188721\n",
      "step: 1330\n",
      "train: loss: 3713588.5 acc: 0.8943867087364197  val: loss: 8009159.0 acc: 0.8783988952636719\n",
      "step: 1335\n",
      "train: loss: 2905169.5 acc: 0.953062891960144  val: loss: 17934630.0 acc: -0.06421768665313721\n",
      "step: 1340\n",
      "train: loss: 3216800.25 acc: 0.9286690950393677  val: loss: 5894052.5 acc: 0.8000872731208801\n",
      "step: 1345\n",
      "train: loss: 7880194.5 acc: 0.902632474899292  val: loss: 9868129.0 acc: 0.5631545782089233\n",
      "step: 1350\n",
      "train: loss: 2882303.75 acc: 0.970678985118866  val: loss: 8856132.0 acc: 0.9039384722709656\n",
      "step: 1355\n",
      "train: loss: 3030589.0 acc: 0.9608255624771118  val: loss: 13010586.0 acc: 0.8223998546600342\n",
      "step: 1360\n",
      "train: loss: 6860190.5 acc: 0.9209917187690735  val: loss: 7984882.0 acc: 0.8742029070854187\n",
      "step: 1365\n",
      "train: loss: 2778743.25 acc: 0.9603032469749451  val: loss: 9396906.0 acc: 0.7995102405548096\n",
      "step: 1370\n",
      "train: loss: 3026076.5 acc: 0.9450085759162903  val: loss: 14121445.0 acc: 0.572353720664978\n",
      "step: 1375\n",
      "train: loss: 1738858.125 acc: 0.9721551537513733  val: loss: 15248584.0 acc: 0.7270709276199341\n",
      "step: 1380\n",
      "train: loss: 2572315.75 acc: 0.9504362344741821  val: loss: 14129004.0 acc: -0.0346226692199707\n",
      "step: 1385\n",
      "train: loss: 2087917.625 acc: 0.954637348651886  val: loss: 9218407.0 acc: 0.8083788156509399\n",
      "step: 1390\n",
      "train: loss: 1499286.5 acc: 0.9494122862815857  val: loss: 4148348.25 acc: 0.918606698513031\n",
      "step: 1395\n",
      "train: loss: 1231662.25 acc: 0.9579479098320007  val: loss: 12327976.0 acc: 0.749167799949646\n",
      "step: 1400\n",
      "train: loss: 1113696.75 acc: 0.9292654395103455  val: loss: 4425092.5 acc: 0.9067202210426331\n",
      "step: 1405\n",
      "train: loss: 1939804.75 acc: 0.9518246650695801  val: loss: 14274958.0 acc: 0.24136412143707275\n",
      "step: 1410\n",
      "train: loss: 2243537.75 acc: 0.8787360787391663  val: loss: 5114849.0 acc: 0.9066773653030396\n",
      "step: 1415\n",
      "train: loss: 1730707.25 acc: 0.6561750173568726  val: loss: 15612784.0 acc: 0.7968567609786987\n",
      "step: 1420\n",
      "train: loss: 614108.0 acc: 0.8609116077423096  val: loss: 3466422.0 acc: 0.7923370599746704\n",
      "step: 1425\n",
      "train: loss: 475238.5625 acc: 0.963614821434021  val: loss: 10313846.0 acc: 0.7139010429382324\n",
      "step: 1430\n",
      "train: loss: 589253.375 acc: 0.9402905106544495  val: loss: 9006198.0 acc: 0.6607484221458435\n",
      "step: 1435\n",
      "train: loss: 950620.0 acc: 0.9299052953720093  val: loss: 4226333.0 acc: 0.8801864385604858\n",
      "step: 1440\n",
      "train: loss: 943040.4375 acc: 0.9459998607635498  val: loss: 6693075.5 acc: 0.8831250667572021\n",
      "step: 1445\n",
      "train: loss: 170607.890625 acc: 0.8879651427268982  val: loss: 19090182.0 acc: 0.7419021725654602\n",
      "step: 1450\n",
      "train: loss: 409735.96875 acc: 0.9076907634735107  val: loss: 12872496.0 acc: 0.8303061723709106\n",
      "step: 1455\n",
      "train: loss: 705275.6875 acc: 0.9683012366294861  val: loss: 14178232.0 acc: 0.7388671636581421\n",
      "step: 1460\n",
      "train: loss: 2235745.75 acc: 0.827385663986206  val: loss: 14626610.0 acc: 0.783676028251648\n",
      "step: 1465\n",
      "train: loss: 1129536.875 acc: 0.9301560521125793  val: loss: 24994992.0 acc: 0.7365503311157227\n",
      "step: 1470\n",
      "train: loss: 695964.875 acc: 0.9276401996612549  val: loss: 18964834.0 acc: 0.7101917862892151\n",
      "step: 1475\n",
      "train: loss: 1448147.75 acc: 0.9122125506401062  val: loss: 16007636.0 acc: 0.7652775049209595\n",
      "step: 1480\n",
      "train: loss: 1181033.75 acc: 0.9315096735954285  val: loss: 13150636.0 acc: 0.7997364401817322\n",
      "step: 1485\n",
      "train: loss: 1646306.875 acc: 0.8716044425964355  val: loss: 12710953.0 acc: 0.6796225905418396\n",
      "step: 1490\n",
      "train: loss: 387546.0625 acc: 0.9507548809051514  val: loss: 10989028.0 acc: 0.7906230092048645\n",
      "step: 1495\n",
      "train: loss: 975135.4375 acc: 0.8932766318321228  val: loss: 37766392.0 acc: 0.7340352535247803\n",
      "step: 1500\n",
      "train: loss: 681820.4375 acc: 0.9100337624549866  val: loss: 17368976.0 acc: 0.6751034259796143\n",
      "step: 1505\n",
      "train: loss: 777942.875 acc: 0.8982680439949036  val: loss: 11312750.0 acc: 0.7652254700660706\n",
      "step: 1510\n",
      "train: loss: 1506676.5 acc: 0.8667033910751343  val: loss: 9574480.0 acc: 0.530376136302948\n",
      "step: 1515\n",
      "train: loss: 848703.25 acc: 0.9521995186805725  val: loss: 22141048.0 acc: 0.6904780864715576\n",
      "step: 1520\n",
      "train: loss: 3825791.25 acc: 0.8984668254852295  val: loss: 13221509.0 acc: 0.8343805074691772\n",
      "step: 1525\n",
      "train: loss: 1340960.0 acc: 0.8934769630432129  val: loss: 30096058.0 acc: 0.6756458282470703\n",
      "step: 1530\n",
      "train: loss: 1770727.875 acc: 0.9320517778396606  val: loss: 10391321.0 acc: 0.731351912021637\n",
      "step: 1535\n",
      "train: loss: 1323359.625 acc: 0.9344017505645752  val: loss: 11344546.0 acc: 0.7383718490600586\n",
      "step: 1540\n",
      "train: loss: 884000.125 acc: 0.9700560569763184  val: loss: 10307348.0 acc: 0.7367421388626099\n",
      "step: 1545\n",
      "train: loss: 4124150.75 acc: 0.824916422367096  val: loss: 13944906.0 acc: 0.7607635855674744\n",
      "step: 1550\n",
      "train: loss: 2058553.5 acc: 0.8922778367996216  val: loss: 7722701.0 acc: 0.8654858469963074\n",
      "step: 1555\n",
      "train: loss: 1473224.25 acc: 0.9389218091964722  val: loss: 16164892.0 acc: 0.6598453521728516\n",
      "step: 1560\n",
      "train: loss: 2030554.875 acc: 0.8943349123001099  val: loss: 11690724.0 acc: 0.2555392384529114\n",
      "step: 1565\n",
      "train: loss: 5338456.5 acc: 0.8139104843139648  val: loss: 8803853.0 acc: 0.6367513537406921\n",
      "step: 1570\n",
      "train: loss: 3515565.25 acc: 0.9241663217544556  val: loss: 13462176.0 acc: 0.7947959303855896\n",
      "step: 1575\n",
      "train: loss: 2102788.75 acc: 0.9707096815109253  val: loss: 19379472.0 acc: 0.7187986969947815\n",
      "step: 1580\n",
      "train: loss: 2747908.5 acc: 0.9600330591201782  val: loss: 6994264.5 acc: 0.7442319393157959\n",
      "step: 1585\n",
      "train: loss: 1128591.875 acc: 0.9833908677101135  val: loss: 7071283.5 acc: 0.6935475468635559\n",
      "step: 1590\n",
      "train: loss: 1795283.0 acc: 0.962013840675354  val: loss: 2169795.5 acc: 0.9394374489784241\n",
      "step: 1595\n",
      "train: loss: 7523739.0 acc: 0.8644001483917236  val: loss: 5817039.0 acc: 0.9005339741706848\n",
      "step: 1600\n",
      "train: loss: 5676181.5 acc: 0.9018913507461548  val: loss: 15224569.0 acc: 0.8206659555435181\n",
      "step: 1605\n",
      "train: loss: 14768016.0 acc: 0.8228713274002075  val: loss: 9044310.0 acc: 0.6856763362884521\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 1610\n",
      "train: loss: 11514569.0 acc: 0.8695435523986816  val: loss: 2095472.25 acc: 0.9496012330055237\n",
      "step: 1615\n",
      "train: loss: 6511639.5 acc: 0.8888038396835327  val: loss: 26234098.0 acc: 0.7486158609390259\n",
      "step: 1620\n",
      "train: loss: 6459760.5 acc: 0.8782281875610352  val: loss: 13265274.0 acc: 0.7496618628501892\n",
      "step: 1625\n",
      "train: loss: 16337252.0 acc: 0.840154230594635  val: loss: 6569499.0 acc: 0.901332437992096\n",
      "step: 1630\n",
      "train: loss: 23932536.0 acc: 0.8677400350570679  val: loss: 20631230.0 acc: 0.6378713846206665\n",
      "step: 1635\n",
      "train: loss: 35218724.0 acc: 0.8470367193222046  val: loss: 3970798.5 acc: 0.8660362362861633\n",
      "step: 1640\n",
      "train: loss: 14267904.0 acc: 0.8738330602645874  val: loss: 8903256.0 acc: 0.8505712747573853\n",
      "step: 1645\n",
      "train: loss: 15855118.0 acc: 0.904881477355957  val: loss: 23428814.0 acc: -0.31775856018066406\n",
      "step: 1650\n",
      "train: loss: 15681212.0 acc: 0.9094858169555664  val: loss: 11743638.0 acc: 0.8699479699134827\n",
      "step: 1655\n",
      "train: loss: 5444466.0 acc: 0.8584902286529541  val: loss: 16021362.0 acc: 0.8520791530609131\n",
      "step: 1660\n",
      "train: loss: 6971781.0 acc: 0.8628445863723755  val: loss: 15079384.0 acc: 0.15623462200164795\n",
      "step: 1665\n",
      "train: loss: 7935984.5 acc: 0.8801575899124146  val: loss: 1395726.25 acc: 0.857244074344635\n",
      "step: 1670\n",
      "train: loss: 9810234.0 acc: 0.7341358661651611  val: loss: 17126526.0 acc: 0.8056474924087524\n",
      "step: 1675\n",
      "train: loss: 18320070.0 acc: 0.7986406087875366  val: loss: 2261674.75 acc: 0.8895778059959412\n",
      "step: 1680\n",
      "train: loss: 9309093.0 acc: 0.9034666419029236  val: loss: 3958445.5 acc: 0.9009693264961243\n",
      "step: 1685\n",
      "train: loss: 5009714.0 acc: 0.8948792815208435  val: loss: 11913432.0 acc: 0.6390275955200195\n",
      "step: 1690\n",
      "train: loss: 4983952.0 acc: 0.8522686958312988  val: loss: 15873625.0 acc: 0.7804631590843201\n",
      "step: 1695\n",
      "train: loss: 10364423.0 acc: 0.859061598777771  val: loss: 5153601.5 acc: 0.9135442972183228\n",
      "step: 1700\n",
      "train: loss: 14770438.0 acc: 0.817561149597168  val: loss: 5818091.5 acc: 0.8358697891235352\n",
      "step: 1705\n",
      "train: loss: 3868627.0 acc: 0.714372456073761  val: loss: 16903168.0 acc: 0.8057576417922974\n",
      "step: 1710\n",
      "train: loss: 3330811.25 acc: 0.8537681698799133  val: loss: 10219211.0 acc: 0.8319154381752014\n",
      "step: 1715\n",
      "train: loss: 8175329.0 acc: 0.6840773820877075  val: loss: 3148706.0 acc: 0.7004314064979553\n",
      "step: 1720\n",
      "train: loss: 2403253.75 acc: 0.8982782959938049  val: loss: 26542780.0 acc: 0.7354345321655273\n",
      "step: 1725\n",
      "train: loss: 3186868.0 acc: 0.8439447283744812  val: loss: 11164404.0 acc: 0.7242443561553955\n",
      "step: 1730\n",
      "train: loss: 2423913.5 acc: 0.8775948882102966  val: loss: 30124860.0 acc: 0.7128424644470215\n",
      "step: 1735\n",
      "train: loss: 2837925.5 acc: 0.8639536499977112  val: loss: 16094159.0 acc: 0.7112506628036499\n",
      "step: 1740\n",
      "train: loss: 2909337.75 acc: 0.8626538515090942  val: loss: 3047006.5 acc: 0.8451007008552551\n",
      "step: 1745\n",
      "train: loss: 3863368.5 acc: 0.8381879329681396  val: loss: 21532616.0 acc: 0.7526320219039917\n",
      "step: 1750\n",
      "train: loss: 4445782.0 acc: 0.8125119805335999  val: loss: 11378088.0 acc: 0.6680780053138733\n",
      "step: 1755\n",
      "train: loss: 8159671.0 acc: 0.7964034676551819  val: loss: 14912048.0 acc: 0.678416907787323\n",
      "step: 1760\n",
      "train: loss: 7159556.0 acc: 0.8767200708389282  val: loss: 10183464.0 acc: 0.7672231793403625\n",
      "step: 1765\n",
      "train: loss: 1959705.0 acc: 0.8758783340454102  val: loss: 18439218.0 acc: 0.8024104833602905\n",
      "step: 1770\n",
      "train: loss: 2139105.75 acc: 0.811933159828186  val: loss: 6842429.5 acc: 0.8687168955802917\n",
      "step: 1775\n",
      "train: loss: 3056905.75 acc: 0.8172236680984497  val: loss: 13435175.0 acc: 0.7033072710037231\n",
      "step: 1780\n",
      "train: loss: 2421798.25 acc: 0.8136739730834961  val: loss: 12869997.0 acc: 0.6822813749313354\n",
      "step: 1785\n",
      "train: loss: 4537870.5 acc: 0.732330322265625  val: loss: 1731626.25 acc: 0.8678969144821167\n",
      "step: 1790\n",
      "train: loss: 6552008.5 acc: 0.6709325313568115  val: loss: 7719198.5 acc: 0.7193582653999329\n",
      "step: 1795\n",
      "train: loss: 6490910.0 acc: 0.7128996849060059  val: loss: 20454740.0 acc: 0.5168017148971558\n",
      "step: 1800\n",
      "train: loss: 7012806.0 acc: 0.6991125345230103  val: loss: 11580108.0 acc: 0.7134705185890198\n",
      "step: 1805\n",
      "train: loss: 7265961.5 acc: 0.7236163020133972  val: loss: 15578730.0 acc: 0.8532134890556335\n",
      "step: 1810\n",
      "train: loss: 3631824.0 acc: 0.7240655422210693  val: loss: 30836812.0 acc: 0.5266855955123901\n",
      "step: 1815\n",
      "train: loss: 7856411.0 acc: 0.7809863090515137  val: loss: 29567240.0 acc: 0.680414617061615\n",
      "step: 1820\n",
      "train: loss: 8330104.5 acc: 0.7411969900131226  val: loss: 6946203.0 acc: 0.8615845441818237\n",
      "step: 1825\n",
      "train: loss: 7375961.5 acc: 0.8197503685951233  val: loss: 12330911.0 acc: 0.7988501787185669\n",
      "step: 1830\n",
      "train: loss: 8130358.5 acc: 0.8632860779762268  val: loss: 4412463.0 acc: 0.849526584148407\n",
      "step: 1835\n",
      "train: loss: 9962640.0 acc: 0.900728166103363  val: loss: 7926117.0 acc: 0.8047842383384705\n",
      "step: 1840\n",
      "train: loss: 7301741.0 acc: 0.8385723233222961  val: loss: 4713998.5 acc: 0.7663710117340088\n",
      "step: 1845\n",
      "train: loss: 8817897.0 acc: 0.8686671853065491  val: loss: 8388892.0 acc: 0.7498086094856262\n",
      "step: 1850\n",
      "train: loss: 3824703.5 acc: 0.9002470374107361  val: loss: 8827817.0 acc: 0.7797667980194092\n",
      "step: 1855\n",
      "train: loss: 8572309.0 acc: 0.8818730711936951  val: loss: 18402954.0 acc: 0.7568250894546509\n",
      "step: 1860\n",
      "train: loss: 5933180.5 acc: 0.9001637697219849  val: loss: 10896209.0 acc: 0.8706080913543701\n",
      "step: 1865\n",
      "train: loss: 3613780.0 acc: 0.9433763027191162  val: loss: 5280629.0 acc: 0.8816638588905334\n",
      "step: 1870\n",
      "train: loss: 2931414.25 acc: 0.9647212028503418  val: loss: 11908641.0 acc: 0.825450599193573\n",
      "step: 1875\n",
      "train: loss: 5711969.5 acc: 0.9333822131156921  val: loss: 18970644.0 acc: 0.7980902791023254\n",
      "step: 1880\n",
      "train: loss: 1836634.125 acc: 0.9769726395606995  val: loss: 7697991.5 acc: 0.756791353225708\n",
      "step: 1885\n",
      "train: loss: 1402449.625 acc: 0.9853322505950928  val: loss: 10370186.0 acc: 0.7942026853561401\n",
      "step: 1890\n",
      "train: loss: 2416703.75 acc: 0.9598978757858276  val: loss: 4265945.0 acc: 0.8312207460403442\n",
      "step: 1895\n",
      "train: loss: 3516880.75 acc: 0.9251232147216797  val: loss: 7530909.0 acc: 0.9027539491653442\n",
      "step: 1900\n",
      "train: loss: 2073926.0 acc: 0.9668578505516052  val: loss: 10710940.0 acc: 0.5982190370559692\n",
      "step: 1905\n",
      "train: loss: 974296.5625 acc: 0.9799435138702393  val: loss: 8337888.0 acc: 0.6366897821426392\n",
      "step: 1910\n",
      "train: loss: 574290.8125 acc: 0.9851765632629395  val: loss: 6932595.5 acc: 0.7850543856620789\n",
      "step: 1915\n",
      "train: loss: 1012489.125 acc: 0.9669116139411926  val: loss: 8850932.0 acc: 0.5193434953689575\n",
      "step: 1920\n",
      "train: loss: 2735590.75 acc: 0.8559832572937012  val: loss: 9020043.0 acc: 0.8305659294128418\n",
      "step: 1925\n",
      "train: loss: 940775.0 acc: 0.9665580987930298  val: loss: 3454054.5 acc: 0.8642308115959167\n",
      "step: 1930\n",
      "train: loss: 953327.0 acc: 0.9536728262901306  val: loss: 11380855.0 acc: 0.5529136657714844\n",
      "step: 1935\n",
      "train: loss: 1475410.375 acc: 0.9536320567131042  val: loss: 18266344.0 acc: 0.405085027217865\n",
      "step: 1940\n",
      "train: loss: 783965.75 acc: 0.8018373250961304  val: loss: 19009802.0 acc: 0.7224621772766113\n",
      "step: 1945\n",
      "train: loss: 2197598.25 acc: 0.7670566439628601  val: loss: 7306758.5 acc: 0.9102350473403931\n",
      "step: 1950\n",
      "train: loss: 747118.1875 acc: 0.7041390538215637  val: loss: 5970111.5 acc: 0.8640007972717285\n",
      "step: 1955\n",
      "train: loss: 2457081.5 acc: 0.6407303810119629  val: loss: 11083409.0 acc: 0.48742175102233887\n",
      "step: 1960\n",
      "train: loss: 832826.4375 acc: 0.9413751363754272  val: loss: 18261870.0 acc: 0.48289573192596436\n",
      "step: 1965\n",
      "train: loss: 305734.90625 acc: 0.9792401790618896  val: loss: 8037924.5 acc: 0.8598544597625732\n",
      "step: 1970\n",
      "train: loss: 454819.65625 acc: 0.9740114212036133  val: loss: 3708048.5 acc: 0.8238821029663086\n",
      "step: 1975\n",
      "train: loss: 1462976.75 acc: 0.8740541934967041  val: loss: 6604178.5 acc: 0.7466124296188354\n",
      "step: 1980\n",
      "train: loss: 1223989.625 acc: 0.9050830006599426  val: loss: 23414980.0 acc: 0.7410770654678345\n",
      "step: 1985\n",
      "train: loss: 2322610.0 acc: 0.4979814291000366  val: loss: 12036637.0 acc: 0.8127110600471497\n",
      "step: 1990\n",
      "train: loss: 946118.9375 acc: 0.8965026140213013  val: loss: 18133172.0 acc: 0.8079865574836731\n",
      "step: 1995\n",
      "train: loss: 1090864.0 acc: 0.8962373733520508  val: loss: 20612620.0 acc: 0.700360119342804\n",
      "step: 2000\n",
      "train: loss: 866385.6875 acc: 0.9117109775543213  val: loss: 3987441.5 acc: 0.9204807877540588\n",
      "step: 2005\n",
      "train: loss: 465545.34375 acc: 0.8666677474975586  val: loss: 34718628.0 acc: 0.6337316632270813\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 2010\n",
      "train: loss: 558002.5 acc: 0.952362596988678  val: loss: 11654470.0 acc: 0.5697673559188843\n",
      "step: 2015\n",
      "train: loss: 361223.53125 acc: 0.9583234786987305  val: loss: 5915285.0 acc: 0.8311645984649658\n",
      "step: 2020\n",
      "train: loss: 1465314.0 acc: 0.8070938587188721  val: loss: 7950736.0 acc: 0.23962318897247314\n",
      "step: 2025\n",
      "train: loss: 738411.5625 acc: 0.8980919718742371  val: loss: 23721492.0 acc: 0.6681287884712219\n",
      "step: 2030\n",
      "train: loss: 619750.3125 acc: 0.9174423813819885  val: loss: 14278634.0 acc: 0.7799606323242188\n",
      "step: 2035\n",
      "train: loss: 1957334.5 acc: 0.8805405497550964  val: loss: 11980916.0 acc: 0.7076334953308105\n",
      "step: 2040\n",
      "train: loss: 1074306.75 acc: 0.9227246046066284  val: loss: 13299659.0 acc: 0.8996239900588989\n",
      "step: 2045\n",
      "train: loss: 1980125.375 acc: 0.9410470724105835  val: loss: 20227952.0 acc: 0.7582212090492249\n",
      "step: 2050\n",
      "train: loss: 4274843.5 acc: 0.839044451713562  val: loss: 21342612.0 acc: 0.5074850916862488\n",
      "step: 2055\n",
      "train: loss: 1740744.75 acc: 0.8798449039459229  val: loss: 9207001.0 acc: 0.8202876448631287\n",
      "step: 2060\n",
      "train: loss: 1771549.125 acc: 0.9216346144676208  val: loss: 4394179.0 acc: 0.938107967376709\n",
      "step: 2065\n",
      "train: loss: 1171503.5 acc: 0.8908998966217041  val: loss: 4859609.0 acc: 0.8867001533508301\n",
      "step: 2070\n",
      "train: loss: 3257965.75 acc: 0.8296128511428833  val: loss: 15757317.0 acc: -0.3559253215789795\n",
      "step: 2075\n",
      "train: loss: 3443799.5 acc: 0.848170280456543  val: loss: 7143010.5 acc: 0.8574532270431519\n",
      "step: 2080\n",
      "train: loss: 1422097.75 acc: 0.9460394382476807  val: loss: 3650472.75 acc: 0.827704906463623\n",
      "step: 2085\n",
      "train: loss: 2770722.75 acc: 0.9132847189903259  val: loss: 5334439.0 acc: 0.6666616797447205\n",
      "step: 2090\n",
      "train: loss: 2255773.25 acc: 0.9632257223129272  val: loss: 8767172.0 acc: 0.902787446975708\n",
      "step: 2095\n",
      "train: loss: 2761487.75 acc: 0.9179967641830444  val: loss: 1987595.25 acc: 0.9236248731613159\n",
      "step: 2100\n",
      "train: loss: 1564222.75 acc: 0.9736561179161072  val: loss: 5690868.0 acc: 0.913375973701477\n",
      "step: 2105\n",
      "train: loss: 2430201.25 acc: 0.9129650592803955  val: loss: 7523775.5 acc: 0.9045886397361755\n",
      "step: 2110\n",
      "train: loss: 1903704.125 acc: 0.9537640810012817  val: loss: 17473724.0 acc: 0.5532650351524353\n",
      "step: 2115\n",
      "train: loss: 2824980.25 acc: 0.9487574696540833  val: loss: 15543536.0 acc: 0.796297013759613\n",
      "step: 2120\n",
      "train: loss: 3884855.75 acc: 0.8701791763305664  val: loss: 4489121.5 acc: 0.8842970728874207\n",
      "step: 2125\n",
      "train: loss: 14852519.0 acc: 0.8661273717880249  val: loss: 4719473.5 acc: 0.8445608615875244\n",
      "step: 2130\n",
      "train: loss: 7463934.5 acc: 0.8792252540588379  val: loss: 4101121.5 acc: 0.8370893001556396\n",
      "step: 2135\n",
      "train: loss: 6103599.0 acc: 0.844822108745575  val: loss: 11999265.0 acc: 0.8650956153869629\n",
      "step: 2140\n",
      "train: loss: 8649184.0 acc: 0.8891528248786926  val: loss: 5718054.0 acc: 0.9130849838256836\n",
      "step: 2145\n",
      "train: loss: 12795336.0 acc: 0.8622899651527405  val: loss: 1849493.5 acc: 0.9547314047813416\n",
      "step: 2150\n",
      "train: loss: 17406786.0 acc: 0.9194130301475525  val: loss: 5274612.0 acc: 0.9401147365570068\n",
      "step: 2155\n",
      "train: loss: 13542574.0 acc: 0.93193519115448  val: loss: 31061150.0 acc: 0.5433698892593384\n",
      "step: 2160\n",
      "train: loss: 19558278.0 acc: 0.893811821937561  val: loss: 9254562.0 acc: 0.881804347038269\n",
      "step: 2165\n",
      "train: loss: 11630265.0 acc: 0.9226030707359314  val: loss: 16013527.0 acc: 0.7095119953155518\n",
      "step: 2170\n",
      "train: loss: 9557037.0 acc: 0.8940040469169617  val: loss: 13323035.0 acc: 0.8111248016357422\n",
      "step: 2175\n",
      "train: loss: 7047680.5 acc: 0.9390488266944885  val: loss: 8672337.0 acc: 0.9390649199485779\n",
      "step: 2180\n",
      "train: loss: 8891213.0 acc: 0.8997907638549805  val: loss: 3541832.5 acc: 0.8721758127212524\n",
      "step: 2185\n",
      "train: loss: 10870696.0 acc: 0.9276601672172546  val: loss: 5730170.5 acc: 0.9243378639221191\n",
      "step: 2190\n",
      "train: loss: 7816324.0 acc: 0.8963887691497803  val: loss: 12719247.0 acc: 0.8838621973991394\n",
      "step: 2195\n",
      "train: loss: 10273293.0 acc: 0.9108285903930664  val: loss: 8661635.0 acc: 0.8788057565689087\n",
      "step: 2200\n",
      "train: loss: 1950157.0 acc: 0.9174385666847229  val: loss: 10621460.0 acc: 0.7832493782043457\n",
      "step: 2205\n",
      "train: loss: 7528544.0 acc: 0.7266464233398438  val: loss: 5812001.0 acc: 0.8687657117843628\n",
      "step: 2210\n",
      "train: loss: 6626145.0 acc: 0.9218719005584717  val: loss: 9867596.0 acc: 0.8458040952682495\n",
      "step: 2215\n",
      "train: loss: 5642187.0 acc: 0.811619758605957  val: loss: 14942819.0 acc: 0.8369615077972412\n",
      "step: 2220\n",
      "train: loss: 8808101.0 acc: 0.6908953785896301  val: loss: 10975569.0 acc: 0.7856886982917786\n",
      "step: 2225\n",
      "train: loss: 5792144.5 acc: 0.8011926412582397  val: loss: 3435544.25 acc: 0.6858800649642944\n",
      "step: 2230\n",
      "train: loss: 8063539.0 acc: 0.8363964557647705  val: loss: 6578960.5 acc: 0.8988462686538696\n",
      "step: 2235\n",
      "train: loss: 6540833.5 acc: 0.7042475342750549  val: loss: 3389351.25 acc: 0.936251163482666\n",
      "step: 2240\n",
      "train: loss: 2366107.75 acc: 0.8972241282463074  val: loss: 5934184.5 acc: 0.8184308409690857\n",
      "step: 2245\n",
      "train: loss: 2553272.5 acc: 0.8755788803100586  val: loss: 12506093.0 acc: 0.8070921301841736\n",
      "step: 2250\n",
      "train: loss: 4220848.5 acc: 0.8224018216133118  val: loss: 14985445.0 acc: 0.7154144644737244\n",
      "step: 2255\n",
      "train: loss: 1791964.25 acc: 0.8875364661216736  val: loss: 11749154.0 acc: 0.8431013822555542\n",
      "step: 2260\n",
      "train: loss: 4932307.0 acc: 0.7187206745147705  val: loss: 17457126.0 acc: 0.6966413855552673\n",
      "step: 2265\n",
      "train: loss: 4412580.5 acc: 0.8286736011505127  val: loss: 13368901.0 acc: 0.7798891663551331\n",
      "step: 2270\n",
      "train: loss: 5907569.0 acc: 0.8164432048797607  val: loss: 1994028.75 acc: 0.8547914624214172\n",
      "step: 2275\n",
      "train: loss: 4079884.0 acc: 0.8320597410202026  val: loss: 8634504.0 acc: 0.6968051195144653\n",
      "step: 2280\n",
      "train: loss: 949613.1875 acc: 0.9254539608955383  val: loss: 16581751.0 acc: 0.8270446062088013\n",
      "step: 2285\n",
      "train: loss: 3081072.5 acc: 0.8479934334754944  val: loss: 11554077.0 acc: 0.7725701332092285\n",
      "step: 2290\n",
      "train: loss: 1665666.375 acc: 0.8210117816925049  val: loss: 8435903.0 acc: 0.7448923587799072\n",
      "step: 2295\n",
      "train: loss: 4797951.0 acc: 0.7875125408172607  val: loss: 8832461.0 acc: 0.7235922813415527\n",
      "step: 2300\n",
      "train: loss: 6586649.5 acc: 0.7886054515838623  val: loss: 9768787.0 acc: 0.8504666090011597\n",
      "step: 2305\n",
      "train: loss: 9145353.0 acc: 0.8600264191627502  val: loss: 10477836.0 acc: 0.7505666017532349\n",
      "step: 2310\n",
      "train: loss: 13940829.0 acc: 0.791553258895874  val: loss: 22375344.0 acc: 0.6980230808258057\n",
      "step: 2315\n",
      "train: loss: 2626374.5 acc: 0.8307157158851624  val: loss: 8443138.0 acc: 0.8665016889572144\n",
      "step: 2320\n",
      "train: loss: 4772253.5 acc: 0.7658105492591858  val: loss: 10616861.0 acc: 0.6935928463935852\n",
      "step: 2325\n",
      "train: loss: 3992892.5 acc: 0.7822144031524658  val: loss: 2521997.0 acc: 0.8589839935302734\n",
      "step: 2330\n",
      "train: loss: 4805990.0 acc: 0.7944068908691406  val: loss: 32250546.0 acc: 0.43256866931915283\n",
      "step: 2335\n",
      "train: loss: 4775924.0 acc: 0.7180172801017761  val: loss: 14798695.0 acc: 0.7987889051437378\n",
      "step: 2340\n",
      "train: loss: 10382155.0 acc: 0.759139895439148  val: loss: 25287214.0 acc: 0.5302094221115112\n",
      "step: 2345\n",
      "train: loss: 6332493.0 acc: 0.8413360714912415  val: loss: 4936378.0 acc: 0.7294834852218628\n",
      "step: 2350\n",
      "train: loss: 11531834.0 acc: 0.8657936453819275  val: loss: 21226156.0 acc: 0.7121471166610718\n",
      "step: 2355\n",
      "train: loss: 3777490.25 acc: 0.9466009140014648  val: loss: 36779220.0 acc: 0.6934794187545776\n",
      "step: 2360\n",
      "train: loss: 7014082.0 acc: 0.8470024466514587  val: loss: 6442710.0 acc: 0.8377912640571594\n",
      "step: 2365\n",
      "train: loss: 3507722.25 acc: 0.9303053617477417  val: loss: 3204126.5 acc: 0.8817896246910095\n",
      "step: 2370\n",
      "train: loss: 3362547.0 acc: 0.9287592172622681  val: loss: 9145364.0 acc: 0.8415393233299255\n",
      "step: 2375\n",
      "train: loss: 5584791.5 acc: 0.9246115684509277  val: loss: 17701014.0 acc: 0.7502409219741821\n",
      "step: 2380\n",
      "train: loss: 2011700.125 acc: 0.969119668006897  val: loss: 15115531.0 acc: 0.7515230178833008\n",
      "step: 2385\n",
      "train: loss: 2597991.0 acc: 0.9543682336807251  val: loss: 7657377.5 acc: 0.793448805809021\n",
      "step: 2390\n",
      "train: loss: 1472475.0 acc: 0.9850014448165894  val: loss: 1084778.0 acc: 0.7974088191986084\n",
      "step: 2395\n",
      "train: loss: 2858701.0 acc: 0.9737846851348877  val: loss: 1899644.625 acc: 0.95686274766922\n",
      "step: 2400\n",
      "train: loss: 3816229.25 acc: 0.9568440914154053  val: loss: 10849116.0 acc: 0.8237873315811157\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 2405\n",
      "train: loss: 2670938.5 acc: 0.9385720491409302  val: loss: 4561331.5 acc: 0.8545083999633789\n",
      "step: 2410\n",
      "train: loss: 3390284.75 acc: 0.9454492926597595  val: loss: 28398802.0 acc: 0.7549266219139099\n",
      "step: 2415\n",
      "train: loss: 1665201.5 acc: 0.9422102570533752  val: loss: 4496563.0 acc: 0.5529290437698364\n",
      "step: 2420\n",
      "train: loss: 234307.59375 acc: 0.9933032989501953  val: loss: 23292584.0 acc: 0.738365650177002\n",
      "step: 2425\n",
      "train: loss: 759087.4375 acc: 0.9818817377090454  val: loss: 14709611.0 acc: 0.770060122013092\n",
      "step: 2430\n",
      "train: loss: 809975.9375 acc: 0.9692575931549072  val: loss: 9046148.0 acc: 0.7800673246383667\n",
      "step: 2435\n",
      "train: loss: 578857.5625 acc: 0.9612813591957092  val: loss: 25118914.0 acc: 0.5719882845878601\n",
      "step: 2440\n",
      "train: loss: 1872387.625 acc: 0.8702291250228882  val: loss: 10011218.0 acc: 0.8472325801849365\n",
      "step: 2445\n",
      "train: loss: 1408045.125 acc: 0.9265396595001221  val: loss: 2742013.5 acc: 0.9181919097900391\n",
      "step: 2450\n",
      "train: loss: 609962.625 acc: 0.9859791994094849  val: loss: 8922172.0 acc: 0.569024384021759\n",
      "step: 2455\n",
      "train: loss: 401420.1875 acc: 0.9336680173873901  val: loss: 5273629.5 acc: 0.9120885133743286\n",
      "step: 2460\n",
      "train: loss: 353561.46875 acc: 0.9926144480705261  val: loss: 14851990.0 acc: 0.8606152534484863\n",
      "step: 2465\n",
      "train: loss: 815260.375 acc: 0.9667432308197021  val: loss: 14229258.0 acc: 0.674720823764801\n",
      "step: 2470\n",
      "train: loss: 1163027.0 acc: 0.8084140419960022  val: loss: 10156327.0 acc: 0.7903658747673035\n",
      "step: 2475\n",
      "train: loss: 479384.6875 acc: 0.9653369784355164  val: loss: 20073372.0 acc: 0.7960498929023743\n",
      "step: 2480\n",
      "train: loss: 476946.6875 acc: 0.9613044857978821  val: loss: 4929987.0 acc: 0.8739359378814697\n",
      "step: 2485\n",
      "train: loss: 1102073.375 acc: 0.7513542175292969  val: loss: 15338719.0 acc: 0.6723450422286987\n",
      "step: 2490\n",
      "train: loss: 254416.328125 acc: 0.9828096032142639  val: loss: 5355150.5 acc: 0.8885855078697205\n",
      "step: 2495\n",
      "train: loss: 1792187.0 acc: 0.8534785509109497  val: loss: 13410253.0 acc: 0.8514585494995117\n",
      "step: 2500\n",
      "train: loss: 1176847.5 acc: 0.8967645764350891  val: loss: 3824905.5 acc: 0.9025992155075073\n",
      "step: 2505\n",
      "train: loss: 1134657.0 acc: 0.9488152265548706  val: loss: 6699700.0 acc: 0.5454853177070618\n",
      "step: 2510\n",
      "train: loss: 857072.125 acc: 0.9251415729522705  val: loss: 3460397.0 acc: 0.9261435270309448\n",
      "step: 2515\n",
      "train: loss: 901137.1875 acc: 0.9052919149398804  val: loss: 4532769.5 acc: 0.4246075749397278\n",
      "step: 2520\n",
      "train: loss: 1082423.5 acc: 0.9316524267196655  val: loss: 18789468.0 acc: 0.8380700945854187\n",
      "step: 2525\n",
      "train: loss: 1312576.0 acc: 0.794269859790802  val: loss: 10453361.0 acc: 0.8641467094421387\n",
      "step: 2530\n",
      "train: loss: 602281.625 acc: 0.9059045314788818  val: loss: 7797650.0 acc: 0.6431858539581299\n",
      "step: 2535\n",
      "train: loss: 287107.3125 acc: 0.864004373550415  val: loss: 12629971.0 acc: 0.6853705644607544\n",
      "step: 2540\n",
      "train: loss: 1604844.625 acc: 0.8808164000511169  val: loss: 14397247.0 acc: 0.8906136751174927\n",
      "step: 2545\n",
      "train: loss: 723124.875 acc: 0.8780437707901001  val: loss: 9521973.0 acc: 0.7576718330383301\n",
      "step: 2550\n",
      "train: loss: 1341843.875 acc: 0.9107636213302612  val: loss: 13546376.0 acc: 0.8243238925933838\n",
      "step: 2555\n",
      "train: loss: 2611349.25 acc: 0.8836351633071899  val: loss: 2220137.5 acc: 0.9488219022750854\n",
      "step: 2560\n",
      "train: loss: 1708861.125 acc: 0.9396956562995911  val: loss: 12210269.0 acc: 0.7930306792259216\n",
      "step: 2565\n",
      "train: loss: 1606165.5 acc: 0.947544276714325  val: loss: 8022156.5 acc: 0.8724880218505859\n",
      "step: 2570\n",
      "train: loss: 868061.0 acc: 0.9423322677612305  val: loss: 24267690.0 acc: 0.7203468680381775\n",
      "step: 2575\n",
      "train: loss: 2068794.125 acc: 0.8475137948989868  val: loss: 15065563.0 acc: 0.8225797414779663\n",
      "step: 2580\n",
      "train: loss: 1656541.875 acc: 0.9537762403488159  val: loss: 5497580.5 acc: 0.8083986043930054\n",
      "step: 2585\n",
      "train: loss: 2677703.75 acc: 0.922386109828949  val: loss: 8596333.0 acc: 0.864449143409729\n",
      "step: 2590\n",
      "train: loss: 2345969.0 acc: 0.8525331020355225  val: loss: 7477427.5 acc: 0.6828305721282959\n",
      "step: 2595\n",
      "train: loss: 1481742.875 acc: 0.9299079775810242  val: loss: 11256499.0 acc: 0.8660761117935181\n",
      "step: 2600\n",
      "train: loss: 2660178.25 acc: 0.648646891117096  val: loss: 3984311.5 acc: 0.2522197365760803\n",
      "step: 2605\n",
      "train: loss: 2478875.75 acc: 0.891192615032196  val: loss: 14943034.0 acc: 0.7279576659202576\n",
      "step: 2610\n",
      "train: loss: 2753874.5 acc: 0.9532873630523682  val: loss: 13003287.0 acc: 0.699592649936676\n",
      "step: 2615\n",
      "train: loss: 3400836.25 acc: 0.9066630005836487  val: loss: 16089049.0 acc: 0.44061481952667236\n",
      "step: 2620\n",
      "train: loss: 1217657.625 acc: 0.9823812246322632  val: loss: 18144406.0 acc: 0.8360805511474609\n",
      "step: 2625\n",
      "train: loss: 1652700.875 acc: 0.9592899084091187  val: loss: 14230480.0 acc: 0.6664028763771057\n",
      "step: 2630\n",
      "train: loss: 8537879.0 acc: 0.8557084798812866  val: loss: 14339593.0 acc: 0.7365370392799377\n",
      "step: 2635\n",
      "train: loss: 8687768.0 acc: 0.8890594244003296  val: loss: 22902302.0 acc: 0.6210708022117615\n",
      "step: 2640\n",
      "train: loss: 12706770.0 acc: 0.797418475151062  val: loss: 10899438.0 acc: 0.8894850611686707\n",
      "step: 2645\n",
      "train: loss: 8867636.0 acc: 0.9188964366912842  val: loss: 8952471.0 acc: 0.8899979591369629\n",
      "step: 2650\n",
      "train: loss: 8377209.0 acc: 0.9394416213035583  val: loss: 3724398.75 acc: 0.8587781190872192\n",
      "step: 2655\n",
      "train: loss: 5284208.0 acc: 0.8963038325309753  val: loss: 32211702.0 acc: 0.1672419309616089\n",
      "step: 2660\n",
      "train: loss: 9670729.0 acc: 0.9321966171264648  val: loss: 10149666.0 acc: 0.5975345373153687\n",
      "step: 2665\n",
      "train: loss: 10901561.0 acc: 0.8093147277832031  val: loss: 7386422.0 acc: 0.6914029717445374\n",
      "step: 2670\n",
      "train: loss: 11994166.0 acc: 0.9381985664367676  val: loss: 4170005.75 acc: 0.8941150307655334\n",
      "step: 2675\n",
      "train: loss: 8204157.0 acc: 0.9341681003570557  val: loss: 5082597.0 acc: 0.7865250706672668\n",
      "step: 2680\n",
      "train: loss: 10537229.0 acc: 0.9146146178245544  val: loss: 10633742.0 acc: 0.776156485080719\n",
      "step: 2685\n",
      "train: loss: 10076987.0 acc: 0.9378312826156616  val: loss: 11428225.0 acc: 0.814810574054718\n",
      "step: 2690\n",
      "train: loss: 5992895.5 acc: 0.9128406643867493  val: loss: 8459615.0 acc: 0.8558427095413208\n",
      "step: 2695\n",
      "train: loss: 5301935.0 acc: 0.9646720886230469  val: loss: 6494368.5 acc: 0.8782394528388977\n",
      "step: 2700\n",
      "train: loss: 3618236.5 acc: 0.9463232755661011  val: loss: 5849088.0 acc: 0.8128980398178101\n",
      "step: 2705\n",
      "train: loss: 3695182.0 acc: 0.8983538746833801  val: loss: 4566747.0 acc: 0.8958228826522827\n",
      "step: 2710\n",
      "train: loss: 15137657.0 acc: 0.8395264148712158  val: loss: 6488039.5 acc: 0.9374197125434875\n",
      "step: 2715\n",
      "train: loss: 9723125.0 acc: 0.8688798546791077  val: loss: 4316838.5 acc: 0.9061391353607178\n",
      "step: 2720\n",
      "train: loss: 12998024.0 acc: 0.8767791986465454  val: loss: 11363136.0 acc: 0.8554729223251343\n",
      "step: 2725\n",
      "train: loss: 12641925.0 acc: 0.8895103931427002  val: loss: 4537366.5 acc: 0.925169050693512\n",
      "step: 2730\n",
      "train: loss: 3210254.75 acc: 0.9457620978355408  val: loss: 6808767.0 acc: 0.7890723943710327\n",
      "step: 2735\n",
      "train: loss: 8225697.0 acc: 0.8532457947731018  val: loss: 7471184.0 acc: 0.8904246687889099\n",
      "step: 2740\n",
      "train: loss: 9823166.0 acc: 0.8223636150360107  val: loss: 2251326.0 acc: 0.8467017412185669\n",
      "step: 2745\n",
      "train: loss: 7784397.0 acc: 0.8587148785591125  val: loss: 6582102.0 acc: 0.8150381445884705\n",
      "step: 2750\n",
      "train: loss: 8389227.0 acc: 0.7755656838417053  val: loss: 1820217.375 acc: 0.866500198841095\n",
      "step: 2755\n",
      "train: loss: 912855.0625 acc: 0.9207686185836792  val: loss: 16572222.0 acc: 0.6587716341018677\n",
      "step: 2760\n",
      "train: loss: 1407215.625 acc: 0.9057039022445679  val: loss: 8957046.0 acc: 0.7181814908981323\n",
      "step: 2765\n",
      "train: loss: 3451629.75 acc: 0.8623090982437134  val: loss: 15390057.0 acc: 0.6772676110267639\n",
      "step: 2770\n",
      "train: loss: 2626040.5 acc: 0.8833192586898804  val: loss: 9351194.0 acc: 0.8941923975944519\n",
      "step: 2775\n",
      "train: loss: 1748674.75 acc: 0.8953197002410889  val: loss: 14413372.0 acc: 0.7258729338645935\n",
      "step: 2780\n",
      "train: loss: 4065156.75 acc: 0.8036712408065796  val: loss: 16969000.0 acc: 0.7092546224594116\n",
      "step: 2785\n",
      "train: loss: 6398419.0 acc: 0.7943313717842102  val: loss: 18604356.0 acc: 0.7462413311004639\n",
      "step: 2790\n",
      "train: loss: 5822117.5 acc: 0.8432514667510986  val: loss: 5881158.5 acc: 0.888774037361145\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 2795\n",
      "train: loss: 3461874.25 acc: 0.8149014711380005  val: loss: 14126585.0 acc: 0.7192764282226562\n",
      "step: 2800\n",
      "train: loss: 2761804.25 acc: 0.8257400989532471  val: loss: 2255998.75 acc: 0.8454347848892212\n",
      "step: 2805\n",
      "train: loss: 2414359.5 acc: 0.8627235889434814  val: loss: 13158078.0 acc: 0.7331951260566711\n",
      "step: 2810\n",
      "train: loss: 1061023.25 acc: 0.8913687467575073  val: loss: 9041339.0 acc: 0.8912028074264526\n",
      "step: 2815\n",
      "train: loss: 9532350.0 acc: 0.7818730473518372  val: loss: 6663192.5 acc: 0.8312559723854065\n",
      "step: 2820\n",
      "train: loss: 4655987.5 acc: 0.7953976392745972  val: loss: 9445580.0 acc: 0.8464092016220093\n",
      "step: 2825\n",
      "train: loss: 10628969.0 acc: 0.7246588468551636  val: loss: 12738511.0 acc: 0.8567732572555542\n",
      "step: 2830\n",
      "train: loss: 5908790.0 acc: 0.7744009494781494  val: loss: 9001709.0 acc: 0.7367932200431824\n",
      "step: 2835\n",
      "train: loss: 7224218.5 acc: 0.6882843971252441  val: loss: 3621260.75 acc: 0.8124866485595703\n",
      "step: 2840\n",
      "train: loss: 6254580.5 acc: 0.6936049461364746  val: loss: 6424377.5 acc: 0.7545484900474548\n",
      "step: 2845\n",
      "train: loss: 7599888.0 acc: 0.7210683226585388  val: loss: 4914623.0 acc: 0.9003624320030212\n",
      "step: 2850\n",
      "train: loss: 4795917.5 acc: 0.7764785289764404  val: loss: 1553099.0 acc: 0.8401517868041992\n",
      "step: 2855\n",
      "train: loss: 6068994.0 acc: 0.7703460454940796  val: loss: 16384157.0 acc: 0.7414602041244507\n",
      "step: 2860\n",
      "train: loss: 12158410.0 acc: 0.7570322751998901  val: loss: 8840784.0 acc: 0.8789358139038086\n",
      "step: 2865\n",
      "train: loss: 8462050.0 acc: 0.808983325958252  val: loss: 17239400.0 acc: 0.7911248207092285\n",
      "step: 2870\n",
      "train: loss: 11767387.0 acc: 0.8458751440048218  val: loss: 12774290.0 acc: 0.853428840637207\n",
      "step: 2875\n",
      "train: loss: 8829643.0 acc: 0.8791196346282959  val: loss: 13076724.0 acc: 0.8966408371925354\n",
      "step: 2880\n",
      "train: loss: 6699221.0 acc: 0.8855384588241577  val: loss: 7331157.0 acc: 0.7122724056243896\n",
      "step: 2885\n",
      "train: loss: 2625660.75 acc: 0.9159740209579468  val: loss: 2342164.25 acc: 0.9210805892944336\n",
      "step: 2890\n",
      "train: loss: 4715607.5 acc: 0.9211459159851074  val: loss: 22150278.0 acc: 0.6941624283790588\n",
      "step: 2895\n",
      "train: loss: 3343838.0 acc: 0.9146287441253662  val: loss: 8280721.0 acc: 0.6441203355789185\n",
      "step: 2900\n",
      "train: loss: 4993386.0 acc: 0.8971757292747498  val: loss: 7759824.0 acc: 0.8068568706512451\n",
      "step: 2905\n",
      "train: loss: 4291830.5 acc: 0.9551926255226135  val: loss: 9731756.0 acc: 0.8189923763275146\n",
      "step: 2910\n",
      "train: loss: 1925428.625 acc: 0.9818582534790039  val: loss: 9992856.0 acc: 0.7651122808456421\n",
      "step: 2915\n",
      "train: loss: 3486729.75 acc: 0.9402024745941162  val: loss: 7228801.5 acc: 0.8250846862792969\n",
      "step: 2920\n",
      "train: loss: 3709758.25 acc: 0.9506767988204956  val: loss: 7925307.0 acc: 0.8686437606811523\n",
      "step: 2925\n",
      "train: loss: 1473929.0 acc: 0.9843664765357971  val: loss: 10700172.0 acc: 0.7111826539039612\n",
      "step: 2930\n",
      "train: loss: 3646302.0 acc: 0.93932044506073  val: loss: 21433252.0 acc: 0.7758861184120178\n",
      "step: 2935\n",
      "train: loss: 4607714.0 acc: 0.7842910885810852  val: loss: 13409593.0 acc: 0.8125253915786743\n",
      "step: 2940\n",
      "train: loss: 1251867.875 acc: 0.9705214500427246  val: loss: 12734852.0 acc: 0.7857208251953125\n",
      "step: 2945\n",
      "train: loss: 3453601.0 acc: 0.9115682244300842  val: loss: 7583851.0 acc: 0.8681042790412903\n",
      "step: 2950\n",
      "train: loss: 862723.0 acc: 0.9475865364074707  val: loss: 24853942.0 acc: 0.7532585263252258\n",
      "step: 2955\n",
      "train: loss: 421274.03125 acc: 0.8280942440032959  val: loss: 13087458.0 acc: 0.7934942841529846\n",
      "step: 2960\n",
      "train: loss: 425408.1875 acc: 0.9702162742614746  val: loss: 7610859.0 acc: 0.865469217300415\n",
      "step: 2965\n",
      "train: loss: 1117143.125 acc: 0.9014018774032593  val: loss: 8146191.0 acc: 0.6025403738021851\n",
      "step: 2970\n",
      "train: loss: 5181357.5 acc: 0.7547336220741272  val: loss: 22849722.0 acc: 0.6916277408599854\n",
      "step: 2975\n",
      "train: loss: 593529.125 acc: 0.9770127534866333  val: loss: 3401293.25 acc: 0.9232732653617859\n",
      "step: 2980\n",
      "train: loss: 422873.0 acc: 0.9700830578804016  val: loss: 10636095.0 acc: 0.8506450653076172\n",
      "step: 2985\n",
      "train: loss: 1124572.75 acc: 0.733462393283844  val: loss: 10945884.0 acc: 0.8756000399589539\n",
      "step: 2990\n",
      "train: loss: 421131.75 acc: 0.8586941361427307  val: loss: 12440363.0 acc: 0.5095072984695435\n",
      "step: 2995\n",
      "train: loss: 2023204.125 acc: 0.8817788362503052  val: loss: 2848250.0 acc: 0.8844510316848755\n",
      "step: 3000\n",
      "train: loss: 514232.96875 acc: 0.8250905275344849  val: loss: 6284520.0 acc: 0.8111424446105957\n",
      "step: 3005\n",
      "train: loss: 1201858.875 acc: 0.9537650346755981  val: loss: 21015858.0 acc: 0.8212438821792603\n",
      "step: 3010\n",
      "train: loss: 1030307.4375 acc: 0.8981950879096985  val: loss: 7361501.0 acc: 0.8545051217079163\n",
      "step: 3015\n",
      "train: loss: 735538.5625 acc: 0.8583360910415649  val: loss: 10572156.0 acc: 0.7248097658157349\n",
      "step: 3020\n",
      "train: loss: 2101535.25 acc: 0.7859727740287781  val: loss: 13699425.0 acc: 0.7704311609268188\n",
      "step: 3025\n",
      "train: loss: 898233.625 acc: 0.9016774892807007  val: loss: 2430190.0 acc: 0.9560346603393555\n",
      "step: 3030\n",
      "train: loss: 1948980.125 acc: 0.8931368589401245  val: loss: 12793254.0 acc: 0.2685059905052185\n",
      "step: 3035\n",
      "train: loss: 1441657.375 acc: 0.932774543762207  val: loss: 11154871.0 acc: 0.784138023853302\n",
      "step: 3040\n",
      "train: loss: 470574.0 acc: 0.9151827096939087  val: loss: 7823375.5 acc: 0.812482476234436\n",
      "step: 3045\n",
      "train: loss: 480154.0625 acc: 0.9681054949760437  val: loss: 6426547.5 acc: 0.7996976375579834\n",
      "step: 3050\n",
      "train: loss: 365077.0625 acc: 0.9775813817977905  val: loss: 18531586.0 acc: 0.5923041105270386\n",
      "step: 3055\n",
      "train: loss: 319055.46875 acc: 0.9547598958015442  val: loss: 19060190.0 acc: 0.7791295647621155\n",
      "step: 3060\n",
      "train: loss: 787081.5625 acc: 0.9195688962936401  val: loss: 6112126.0 acc: 0.6106035709381104\n",
      "step: 3065\n",
      "train: loss: 538602.5 acc: 0.9330374598503113  val: loss: 5840043.5 acc: 0.7290847897529602\n",
      "step: 3070\n",
      "train: loss: 1092976.625 acc: 0.9204126000404358  val: loss: 10528889.0 acc: 0.7327067852020264\n",
      "step: 3075\n",
      "train: loss: 1920075.375 acc: 0.8804145455360413  val: loss: 17276366.0 acc: 0.7856086492538452\n",
      "step: 3080\n",
      "train: loss: 644725.5 acc: 0.9426149129867554  val: loss: 17447040.0 acc: 0.02318096160888672\n",
      "step: 3085\n",
      "train: loss: 3294622.75 acc: 0.8470149040222168  val: loss: 5446016.0 acc: 0.832144021987915\n",
      "step: 3090\n",
      "train: loss: 718962.9375 acc: 0.9547718167304993  val: loss: 17036280.0 acc: 0.7606592774391174\n",
      "step: 3095\n",
      "train: loss: 2455686.25 acc: 0.927847146987915  val: loss: 13737674.0 acc: 0.6843734979629517\n",
      "step: 3100\n",
      "train: loss: 3913191.25 acc: 0.8220475316047668  val: loss: 6505740.5 acc: 0.8456038236618042\n",
      "step: 3105\n",
      "train: loss: 2986779.25 acc: 0.8864758014678955  val: loss: 15516331.0 acc: 0.7823276519775391\n",
      "step: 3110\n",
      "train: loss: 2656210.0 acc: 0.9006085991859436  val: loss: 10267534.0 acc: 0.65898197889328\n",
      "step: 3115\n",
      "train: loss: 1952426.875 acc: 0.8077604174613953  val: loss: 1592455.875 acc: 0.9383389949798584\n",
      "step: 3120\n",
      "train: loss: 3867097.0 acc: 0.9168973565101624  val: loss: 4531264.5 acc: 0.8942350745201111\n",
      "step: 3125\n",
      "train: loss: 1255623.375 acc: 0.9639695882797241  val: loss: 5222498.5 acc: 0.768112301826477\n",
      "step: 3130\n",
      "train: loss: 3842996.5 acc: 0.9316033124923706  val: loss: 17871894.0 acc: 0.6384317874908447\n",
      "step: 3135\n",
      "train: loss: 4435059.5 acc: 0.9120732545852661  val: loss: 3036019.75 acc: 0.8996744155883789\n",
      "step: 3140\n",
      "train: loss: 5665592.0 acc: 0.9110925197601318  val: loss: 11462744.0 acc: 0.8042412996292114\n",
      "step: 3145\n",
      "train: loss: 1245401.875 acc: 0.9609949588775635  val: loss: 16132607.0 acc: 0.7313997745513916\n",
      "step: 3150\n",
      "train: loss: 11723622.0 acc: 0.7640510201454163  val: loss: 13079381.0 acc: 0.8580167293548584\n",
      "step: 3155\n",
      "train: loss: 8463004.0 acc: 0.8774499297142029  val: loss: 2150565.5 acc: 0.816022515296936\n",
      "step: 3160\n",
      "train: loss: 5911764.5 acc: 0.9021994471549988  val: loss: 3900320.5 acc: 0.8728533983230591\n",
      "step: 3165\n",
      "train: loss: 13282847.0 acc: 0.8789302706718445  val: loss: 5669978.0 acc: 0.9229088425636292\n",
      "step: 3170\n",
      "train: loss: 9894715.0 acc: 0.9052829146385193  val: loss: 13073419.0 acc: 0.8069227337837219\n",
      "step: 3175\n",
      "train: loss: 11473777.0 acc: 0.8748502731323242  val: loss: 11242868.0 acc: 0.7853956818580627\n",
      "step: 3180\n",
      "train: loss: 3331474.0 acc: 0.9369869232177734  val: loss: 13629037.0 acc: 0.5356507301330566\n",
      "step: 3185\n",
      "train: loss: 11997466.0 acc: 0.916946530342102  val: loss: 12741484.0 acc: 0.7690812349319458\n",
      "step: 3190\n",
      "train: loss: 5372513.5 acc: 0.9736955761909485  val: loss: 13414922.0 acc: -0.1701803207397461\n",
      "step: 3195\n",
      "train: loss: 9890108.0 acc: 0.9315077066421509  val: loss: 7187853.0 acc: 0.6642080545425415\n",
      "step: 3200\n",
      "train: loss: 9781810.0 acc: 0.963708221912384  val: loss: 11416543.0 acc: 0.8330609798431396\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 3205\n",
      "train: loss: 4698363.0 acc: 0.9313212633132935  val: loss: 10141653.0 acc: 0.7602114081382751\n",
      "step: 3210\n",
      "train: loss: 6636143.0 acc: 0.966755211353302  val: loss: 8088014.0 acc: 0.8736542463302612\n",
      "step: 3215\n",
      "train: loss: 4190348.5 acc: 0.9695689678192139  val: loss: 2689919.25 acc: 0.8496110439300537\n",
      "step: 3220\n",
      "train: loss: 3666388.75 acc: 0.9651086926460266  val: loss: 7001269.0 acc: 0.808777928352356\n",
      "step: 3225\n",
      "train: loss: 15114126.0 acc: 0.8394451141357422  val: loss: 5885447.5 acc: 0.8265138864517212\n",
      "step: 3230\n",
      "train: loss: 16222777.0 acc: 0.835595428943634  val: loss: 3303066.75 acc: 0.8629764914512634\n",
      "step: 3235\n",
      "train: loss: 8741834.0 acc: 0.8973101377487183  val: loss: 3579695.5 acc: 0.9480416178703308\n",
      "step: 3240\n",
      "train: loss: 11843128.0 acc: 0.6956376433372498  val: loss: 4013400.0 acc: 0.9397082328796387\n",
      "step: 3245\n",
      "train: loss: 5490033.0 acc: 0.9135240316390991  val: loss: 9643617.0 acc: 0.921601414680481\n",
      "step: 3250\n",
      "train: loss: 5499135.5 acc: 0.8729137182235718  val: loss: 8772852.0 acc: 0.902387261390686\n",
      "step: 3255\n",
      "train: loss: 12290428.0 acc: 0.4712185263633728  val: loss: 4773763.5 acc: 0.8574464917182922\n",
      "step: 3260\n",
      "train: loss: 7934124.0 acc: 0.7535275220870972  val: loss: 8927862.0 acc: 0.7387288808822632\n",
      "step: 3265\n",
      "train: loss: 11522533.0 acc: 0.7415665984153748  val: loss: 7666545.0 acc: 0.8867570757865906\n",
      "step: 3270\n",
      "train: loss: 10955417.0 acc: 0.29785144329071045  val: loss: 10224094.0 acc: 0.9024023413658142\n",
      "step: 3275\n",
      "train: loss: 7751356.0 acc: 0.8415299654006958  val: loss: 9192662.0 acc: 0.8017483949661255\n",
      "step: 3280\n",
      "train: loss: 2562022.75 acc: 0.8370885848999023  val: loss: 3762291.25 acc: 0.9160378575325012\n",
      "step: 3285\n",
      "train: loss: 4617665.5 acc: 0.9205185174942017  val: loss: 13454951.0 acc: 0.6817234754562378\n",
      "step: 3290\n",
      "train: loss: 3266863.5 acc: 0.8695701360702515  val: loss: 16426281.0 acc: 0.8049358129501343\n",
      "step: 3295\n",
      "train: loss: 4779163.0 acc: 0.8170461654663086  val: loss: 24690030.0 acc: 0.7525253891944885\n",
      "step: 3300\n",
      "train: loss: 2085502.5 acc: 0.8867998123168945  val: loss: 21390172.0 acc: 0.6883096694946289\n",
      "step: 3305\n",
      "train: loss: 7089863.0 acc: 0.6631353497505188  val: loss: 5742289.0 acc: 0.9273856282234192\n",
      "step: 3310\n",
      "train: loss: 4374547.5 acc: 0.82517409324646  val: loss: 21341696.0 acc: 0.8343796730041504\n",
      "step: 3315\n",
      "train: loss: 5005689.5 acc: 0.8337987065315247  val: loss: 3624405.0 acc: 0.843029260635376\n",
      "step: 3320\n",
      "train: loss: 2234145.5 acc: 0.8665456175804138  val: loss: 6895885.5 acc: 0.8854882121086121\n",
      "step: 3325\n",
      "train: loss: 2477512.75 acc: 0.8480669260025024  val: loss: 5995649.0 acc: 0.6987954378128052\n",
      "step: 3330\n",
      "train: loss: 5402829.5 acc: 0.7851783633232117  val: loss: 10912048.0 acc: 0.8597906231880188\n",
      "step: 3335\n",
      "train: loss: 3495299.0 acc: 0.6664003133773804  val: loss: 14560962.0 acc: 0.7206528782844543\n",
      "step: 3340\n",
      "train: loss: 4888878.5 acc: 0.8016680479049683  val: loss: 3278568.75 acc: 0.847554087638855\n",
      "step: 3345\n",
      "train: loss: 5859785.0 acc: 0.7811737656593323  val: loss: 6711691.0 acc: 0.7224107980728149\n",
      "step: 3350\n",
      "train: loss: 7038772.0 acc: 0.7530978322029114  val: loss: 10898253.0 acc: 0.8497050404548645\n",
      "step: 3355\n",
      "train: loss: 5277472.0 acc: 0.7607235908508301  val: loss: 6426343.5 acc: 0.7941615581512451\n",
      "step: 3360\n",
      "train: loss: 4256131.0 acc: 0.7683568000793457  val: loss: 25860894.0 acc: 0.6058787107467651\n",
      "step: 3365\n",
      "train: loss: 13603186.0 acc: 0.6718936562538147  val: loss: 18952114.0 acc: 0.78373122215271\n",
      "step: 3370\n",
      "train: loss: 6479107.5 acc: 0.6813487410545349  val: loss: 20197800.0 acc: 0.6274983286857605\n",
      "step: 3375\n",
      "train: loss: 6760361.0 acc: 0.7845654487609863  val: loss: 15850886.0 acc: 0.7442436814308167\n",
      "step: 3380\n",
      "train: loss: 7379287.0 acc: 0.753380298614502  val: loss: 5295002.5 acc: 0.8623014688491821\n",
      "step: 3385\n",
      "train: loss: 7520712.0 acc: 0.7850582599639893  val: loss: 4626676.0 acc: 0.9079931974411011\n",
      "step: 3390\n",
      "train: loss: 8852498.0 acc: 0.8955923318862915  val: loss: 8770522.0 acc: 0.6293225884437561\n",
      "step: 3395\n",
      "train: loss: 7538583.5 acc: 0.8916391730308533  val: loss: 4512979.0 acc: 0.7447486519813538\n",
      "step: 3400\n",
      "train: loss: 5737373.0 acc: 0.8465619087219238  val: loss: 23581876.0 acc: 0.7762016654014587\n",
      "step: 3405\n",
      "train: loss: 7926433.0 acc: 0.7942717671394348  val: loss: 11245057.0 acc: 0.857420802116394\n",
      "step: 3410\n",
      "train: loss: 5550301.0 acc: 0.8979278206825256  val: loss: 2222137.25 acc: 0.805976152420044\n",
      "step: 3415\n",
      "train: loss: 8344677.0 acc: 0.8461993932723999  val: loss: 14107018.0 acc: 0.710162878036499\n",
      "step: 3420\n",
      "train: loss: 6282492.0 acc: 0.8996878266334534  val: loss: 14560118.0 acc: 0.8292300701141357\n",
      "step: 3425\n",
      "train: loss: 3061024.5 acc: 0.9312089681625366  val: loss: 9443435.0 acc: 0.778730034828186\n",
      "step: 3430\n",
      "train: loss: 1528697.75 acc: 0.9827325940132141  val: loss: 1944901.5 acc: 0.944510817527771\n",
      "step: 3435\n",
      "train: loss: 1182605.25 acc: 0.9824106693267822  val: loss: 28023008.0 acc: 0.7809006571769714\n",
      "step: 3440\n",
      "train: loss: 1885193.75 acc: 0.9334567785263062  val: loss: 14236527.0 acc: 0.76730877161026\n",
      "step: 3445\n",
      "train: loss: 3401684.25 acc: 0.9223255515098572  val: loss: 13136026.0 acc: 0.31874698400497437\n",
      "step: 3450\n",
      "train: loss: 1141014.25 acc: 0.9747412204742432  val: loss: 3376688.25 acc: 0.9280338287353516\n",
      "step: 3455\n",
      "train: loss: 1602782.25 acc: 0.9825870990753174  val: loss: 1736419.5 acc: 0.691271960735321\n",
      "step: 3460\n",
      "train: loss: 682333.6875 acc: 0.9868448972702026  val: loss: 15560046.0 acc: 0.7550146579742432\n",
      "step: 3465\n",
      "train: loss: 2574618.5 acc: 0.937293291091919  val: loss: 19911258.0 acc: 0.7988697290420532\n",
      "step: 3470\n",
      "train: loss: 969137.6875 acc: 0.9414072036743164  val: loss: 3239537.0 acc: 0.8459497094154358\n",
      "step: 3475\n",
      "train: loss: 1814455.625 acc: 0.9241343140602112  val: loss: 3732065.5 acc: 0.9286070466041565\n",
      "step: 3480\n",
      "train: loss: 719484.6875 acc: 0.8668103218078613  val: loss: 2946001.25 acc: 0.875328540802002\n",
      "step: 3485\n",
      "train: loss: 1094855.5 acc: 0.748340368270874  val: loss: 15589457.0 acc: 0.736845850944519\n",
      "step: 3490\n",
      "train: loss: 777429.625 acc: 0.9341296553611755  val: loss: 14310176.0 acc: 0.5961177349090576\n",
      "step: 3495\n",
      "train: loss: 673434.25 acc: 0.9625521302223206  val: loss: 5086853.0 acc: 0.818852424621582\n",
      "step: 3500\n",
      "train: loss: 324862.0625 acc: 0.9311981797218323  val: loss: 5995952.0 acc: 0.7991285920143127\n",
      "step: 3505\n",
      "train: loss: 810253.8125 acc: 0.9495622515678406  val: loss: 21246014.0 acc: 0.7656000256538391\n",
      "step: 3510\n",
      "train: loss: 924623.8125 acc: 0.9385325312614441  val: loss: 20327678.0 acc: 0.7698246836662292\n",
      "step: 3515\n",
      "train: loss: 315449.4375 acc: 0.8743754625320435  val: loss: 14936215.0 acc: 0.8602458238601685\n",
      "step: 3520\n",
      "train: loss: 376986.1875 acc: 0.9726916551589966  val: loss: 14164913.0 acc: 0.7806627154350281\n",
      "step: 3525\n",
      "train: loss: 279831.625 acc: 0.9532574415206909  val: loss: 20879798.0 acc: 0.7706313133239746\n",
      "step: 3530\n",
      "train: loss: 578349.125 acc: 0.9318509101867676  val: loss: 16396906.0 acc: 0.6749234199523926\n",
      "step: 3535\n",
      "train: loss: 901099.0 acc: 0.8537449240684509  val: loss: 9868849.0 acc: 0.8740410804748535\n",
      "step: 3540\n",
      "train: loss: 1730281.0 acc: 0.9289767742156982  val: loss: 2739595.0 acc: 0.8826194405555725\n",
      "step: 3545\n",
      "train: loss: 1451133.25 acc: 0.8956218957901001  val: loss: 8993537.0 acc: 0.7067313194274902\n",
      "step: 3550\n",
      "train: loss: 1221020.125 acc: 0.9316567182540894  val: loss: 8421074.0 acc: 0.8677465915679932\n",
      "step: 3555\n",
      "train: loss: 1504037.25 acc: 0.8447827100753784  val: loss: 25013654.0 acc: 0.6817606687545776\n",
      "step: 3560\n",
      "train: loss: 183331.28125 acc: 0.9430447816848755  val: loss: 6916752.5 acc: 0.5961505174636841\n",
      "step: 3565\n",
      "train: loss: 1179386.5 acc: 0.8476064205169678  val: loss: 3655296.0 acc: 0.8674424886703491\n",
      "step: 3570\n",
      "train: loss: 1442598.875 acc: 0.896491527557373  val: loss: 10396228.0 acc: 0.8410225510597229\n",
      "step: 3575\n",
      "train: loss: 360076.9375 acc: 0.9291281700134277  val: loss: 5970544.5 acc: 0.8995320796966553\n",
      "step: 3580\n",
      "train: loss: 530939.75 acc: 0.9298635721206665  val: loss: 11090919.0 acc: 0.7079094648361206\n",
      "step: 3585\n",
      "train: loss: 1468133.125 acc: 0.8590216040611267  val: loss: 14498389.0 acc: 0.8173477053642273\n",
      "step: 3590\n",
      "train: loss: 1130915.75 acc: 0.941881537437439  val: loss: 6141820.5 acc: 0.333357572555542\n",
      "step: 3595\n",
      "train: loss: 933433.0 acc: 0.9243694543838501  val: loss: 16226053.0 acc: 0.7362533807754517\n",
      "step: 3600\n",
      "train: loss: 2804000.75 acc: 0.9126197099685669  val: loss: 11860575.0 acc: 0.7256613373756409\n",
      "step: 3605\n",
      "train: loss: 1286889.75 acc: 0.9446207284927368  val: loss: 7001005.5 acc: 0.8738521337509155\n",
      "step: 3610\n",
      "train: loss: 2021945.5 acc: 0.940812885761261  val: loss: 8132274.5 acc: 0.8733500838279724\n",
      "step: 3615\n",
      "train: loss: 2748929.25 acc: 0.9359554052352905  val: loss: 14032082.0 acc: 0.5740410685539246\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 3620\n",
      "train: loss: 1233391.75 acc: 0.9334254264831543  val: loss: 12682762.0 acc: 0.8312700986862183\n",
      "step: 3625\n",
      "train: loss: 1786406.25 acc: 0.9309819340705872  val: loss: 11516124.0 acc: 0.8074442148208618\n",
      "step: 3630\n",
      "train: loss: 1379519.375 acc: 0.9241980314254761  val: loss: 11298771.0 acc: 0.43925702571868896\n",
      "step: 3635\n",
      "train: loss: 813425.9375 acc: 0.9185535907745361  val: loss: 17962606.0 acc: 0.5943396687507629\n",
      "step: 3640\n",
      "train: loss: 2365170.75 acc: 0.9353418350219727  val: loss: 24646148.0 acc: 0.6295648813247681\n",
      "step: 3645\n",
      "train: loss: 1806149.5 acc: 0.9738537669181824  val: loss: 13076580.0 acc: 0.5474921464920044\n",
      "step: 3650\n",
      "train: loss: 2506180.0 acc: 0.9616434574127197  val: loss: 17495570.0 acc: -1.6315922737121582\n",
      "step: 3655\n",
      "train: loss: 2364033.0 acc: 0.9369858503341675  val: loss: 7676840.0 acc: 0.5553139448165894\n",
      "step: 3660\n",
      "train: loss: 3172499.75 acc: 0.9288394451141357  val: loss: 9013983.0 acc: 0.8874325752258301\n",
      "step: 3665\n",
      "train: loss: 2473863.75 acc: 0.9589766263961792  val: loss: 14786591.0 acc: 0.589370846748352\n",
      "step: 3670\n",
      "train: loss: 3086381.5 acc: 0.9114037156105042  val: loss: 11067204.0 acc: 0.9093284606933594\n",
      "step: 3675\n",
      "train: loss: 6545336.5 acc: 0.922082781791687  val: loss: 5678583.5 acc: 0.944496750831604\n",
      "step: 3680\n",
      "train: loss: 15546185.0 acc: 0.8181860446929932  val: loss: 1699549.375 acc: 0.968600869178772\n",
      "step: 3685\n",
      "train: loss: 5893783.0 acc: 0.9105767011642456  val: loss: 6530781.5 acc: 0.8565804958343506\n",
      "step: 3690\n",
      "train: loss: 6019363.5 acc: 0.8185268640518188  val: loss: 10012380.0 acc: 0.8676354289054871\n",
      "step: 3695\n",
      "train: loss: 2393192.75 acc: 0.9261254072189331  val: loss: 7048440.5 acc: 0.892963707447052\n",
      "step: 3700\n",
      "train: loss: 6146976.5 acc: 0.9624019265174866  val: loss: 16324160.0 acc: 0.5510386228561401\n",
      "step: 3705\n",
      "train: loss: 8690472.0 acc: 0.9339694380760193  val: loss: 5632122.5 acc: 0.8655776977539062\n",
      "step: 3710\n",
      "train: loss: 10910709.0 acc: 0.9106451869010925  val: loss: 31294768.0 acc: 0.14494287967681885\n",
      "step: 3715\n",
      "train: loss: 14198804.0 acc: 0.9067566990852356  val: loss: 6682255.0 acc: 0.7657104730606079\n",
      "step: 3720\n",
      "train: loss: 5205250.0 acc: 0.9542141556739807  val: loss: 4812381.5 acc: 0.7964639067649841\n",
      "step: 3725\n",
      "train: loss: 5393436.5 acc: 0.9470195770263672  val: loss: 2780391.0 acc: 0.9388284683227539\n",
      "step: 3730\n",
      "train: loss: 5509398.0 acc: 0.9393863677978516  val: loss: 2829797.25 acc: 0.9394111633300781\n",
      "step: 3735\n",
      "train: loss: 11343370.0 acc: 0.9312456846237183  val: loss: 6437195.0 acc: 0.9017173647880554\n",
      "step: 3740\n",
      "train: loss: 6202229.0 acc: 0.946707546710968  val: loss: 6597733.0 acc: 0.8501416444778442\n",
      "step: 3745\n",
      "train: loss: 15956844.0 acc: 0.7207291722297668  val: loss: 9929366.0 acc: 0.8894712328910828\n",
      "step: 3750\n",
      "train: loss: 12744347.0 acc: 0.8651995658874512  val: loss: 2364878.5 acc: 0.8423739671707153\n",
      "step: 3755\n",
      "train: loss: 14308271.0 acc: 0.8065180778503418  val: loss: 8270269.0 acc: 0.808078944683075\n",
      "step: 3760\n",
      "train: loss: 3974800.25 acc: 0.8259320259094238  val: loss: 7592658.5 acc: 0.880333662033081\n",
      "step: 3765\n",
      "train: loss: 15457337.0 acc: 0.7732683420181274  val: loss: 4345020.0 acc: 0.9158596992492676\n",
      "step: 3770\n",
      "train: loss: 6503372.5 acc: 0.8960562944412231  val: loss: 8500764.0 acc: 0.8789672255516052\n",
      "step: 3775\n",
      "train: loss: 14191221.0 acc: 0.3599308729171753  val: loss: 10326226.0 acc: 0.869500458240509\n",
      "step: 3780\n",
      "train: loss: 4364303.5 acc: 0.9173537492752075  val: loss: 10243266.0 acc: 0.8933478593826294\n",
      "step: 3785\n",
      "train: loss: 5340426.5 acc: 0.7970828413963318  val: loss: 4010048.5 acc: 0.9079610705375671\n",
      "step: 3790\n",
      "train: loss: 6547640.5 acc: 0.8455687761306763  val: loss: 9897915.0 acc: 0.7176175117492676\n",
      "step: 3795\n",
      "train: loss: 17220636.0 acc: 0.7855780720710754  val: loss: 26440792.0 acc: 0.6749430894851685\n",
      "step: 3800\n",
      "train: loss: 5361597.0 acc: 0.7802560329437256  val: loss: 2236011.5 acc: 0.9254136085510254\n",
      "step: 3805\n",
      "train: loss: 7519461.0 acc: 0.7594955563545227  val: loss: 9785342.0 acc: 0.6566835641860962\n",
      "step: 3810\n",
      "train: loss: 1206309.25 acc: 0.9169760942459106  val: loss: 10970668.0 acc: 0.663364052772522\n",
      "step: 3815\n",
      "train: loss: 5291139.0 acc: 0.8218487501144409  val: loss: 9623015.0 acc: 0.7251075506210327\n",
      "step: 3820\n",
      "train: loss: 3277906.0 acc: 0.8221306800842285  val: loss: 7747724.0 acc: 0.873637318611145\n",
      "step: 3825\n",
      "train: loss: 3770742.5 acc: 0.8566226363182068  val: loss: 6921389.0 acc: 0.8305960893630981\n",
      "step: 3830\n",
      "train: loss: 3028287.75 acc: 0.8532640933990479  val: loss: 13183600.0 acc: 0.9021264314651489\n",
      "step: 3835\n",
      "train: loss: 1891056.875 acc: 0.8767631649971008  val: loss: 6801353.5 acc: 0.886692225933075\n",
      "step: 3840\n",
      "train: loss: 3330983.25 acc: 0.848038911819458  val: loss: 1744028.25 acc: 0.8287100791931152\n",
      "step: 3845\n",
      "train: loss: 1796831.375 acc: 0.8815851211547852  val: loss: 9087529.0 acc: 0.7327123880386353\n",
      "step: 3850\n",
      "train: loss: 6144505.0 acc: 0.6136806011199951  val: loss: 7197166.5 acc: 0.6833267211914062\n",
      "step: 3855\n",
      "train: loss: 3439526.5 acc: 0.8045077323913574  val: loss: 17640878.0 acc: 0.8151854276657104\n",
      "step: 3860\n",
      "train: loss: 3895711.0 acc: 0.8018049001693726  val: loss: 1684831.25 acc: 0.8583682775497437\n",
      "step: 3865\n",
      "train: loss: 7131316.5 acc: 0.6987120509147644  val: loss: 10034639.0 acc: 0.8794086575508118\n",
      "step: 3870\n",
      "train: loss: 3551308.75 acc: 0.807399570941925  val: loss: 12282259.0 acc: 0.8418234586715698\n",
      "step: 3875\n",
      "train: loss: 4244675.5 acc: 0.7807058095932007  val: loss: 5067403.5 acc: 0.7417615056037903\n",
      "step: 3880\n",
      "train: loss: 2528469.75 acc: 0.8465667963027954  val: loss: 15344901.0 acc: 0.8082297444343567\n",
      "step: 3885\n",
      "train: loss: 6475184.5 acc: 0.7668231725692749  val: loss: 30641458.0 acc: 0.6130259037017822\n",
      "step: 3890\n",
      "train: loss: 6385045.0 acc: 0.7681055665016174  val: loss: 11025530.0 acc: 0.7533248662948608\n",
      "step: 3895\n",
      "train: loss: 8560144.0 acc: 0.7625207304954529  val: loss: 2107955.75 acc: 0.8102083802223206\n",
      "step: 3900\n",
      "train: loss: 13109924.0 acc: 0.7721484303474426  val: loss: 8487808.0 acc: 0.7631109952926636\n",
      "step: 3905\n",
      "train: loss: 6388918.5 acc: 0.9123656749725342  val: loss: 3200060.0 acc: 0.9128334522247314\n",
      "step: 3910\n",
      "train: loss: 6408693.0 acc: 0.9139049649238586  val: loss: 10054143.0 acc: 0.8127734661102295\n",
      "step: 3915\n",
      "train: loss: 5803214.5 acc: 0.9220556020736694  val: loss: 8024203.5 acc: 0.7913991212844849\n",
      "step: 3920\n",
      "train: loss: 6541253.0 acc: 0.8906081914901733  val: loss: 7096948.0 acc: 0.6527881622314453\n",
      "step: 3925\n",
      "train: loss: 6740525.5 acc: 0.7745577692985535  val: loss: 2236775.5 acc: 0.924826979637146\n",
      "step: 3930\n",
      "train: loss: 4862131.5 acc: 0.8784024715423584  val: loss: 10319050.0 acc: 0.5758566856384277\n",
      "step: 3935\n",
      "train: loss: 4636476.5 acc: 0.9496929049491882  val: loss: 6370868.0 acc: 0.8943312764167786\n",
      "step: 3940\n",
      "train: loss: 4586430.5 acc: 0.8989287614822388  val: loss: 11141408.0 acc: 0.5840242505073547\n",
      "step: 3945\n",
      "train: loss: 5787178.0 acc: 0.9465383887290955  val: loss: 11647964.0 acc: 0.8490760922431946\n",
      "step: 3950\n",
      "train: loss: 4953879.5 acc: 0.9445128440856934  val: loss: 8137188.0 acc: 0.9321058392524719\n",
      "step: 3955\n",
      "train: loss: 3865493.0 acc: 0.9650394916534424  val: loss: 10079485.0 acc: 0.8700529336929321\n",
      "step: 3960\n",
      "train: loss: 3396057.0 acc: 0.9349685907363892  val: loss: 14565613.0 acc: 0.3756122589111328\n",
      "step: 3965\n",
      "train: loss: 2548903.0 acc: 0.9552507996559143  val: loss: 12151636.0 acc: 0.22241508960723877\n",
      "step: 3970\n",
      "train: loss: 4636191.5 acc: 0.9322392344474792  val: loss: 8395410.0 acc: 0.8777521252632141\n",
      "step: 3975\n",
      "train: loss: 866584.6875 acc: 0.9871380925178528  val: loss: 18661850.0 acc: 0.7232094407081604\n",
      "step: 3980\n",
      "train: loss: 1663616.25 acc: 0.9457280039787292  val: loss: 3093566.75 acc: 0.7754998207092285\n",
      "step: 3985\n",
      "train: loss: 1909234.875 acc: 0.9645926356315613  val: loss: 11510884.0 acc: 0.7686524391174316\n",
      "step: 3990\n",
      "train: loss: 911827.6875 acc: 0.9394559264183044  val: loss: 16305072.0 acc: 0.578117847442627\n",
      "step: 3995\n",
      "train: loss: 464876.875 acc: 0.9872646331787109  val: loss: 4547375.5 acc: 0.8524121046066284\n",
      "step: 4000\n",
      "train: loss: 1309923.25 acc: 0.9519317150115967  val: loss: 6308702.5 acc: 0.8211453557014465\n",
      "step: 4005\n",
      "train: loss: 1586996.25 acc: 0.9040743708610535  val: loss: 14184340.0 acc: -0.3375110626220703\n",
      "step: 4010\n",
      "train: loss: 193854.046875 acc: 0.8979673385620117  val: loss: 17010760.0 acc: 0.7179037928581238\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 4015\n",
      "train: loss: 1386365.5 acc: 0.9074664115905762  val: loss: 17622362.0 acc: 0.7436202168464661\n",
      "step: 4020\n",
      "train: loss: 298768.5 acc: 0.8203214406967163  val: loss: 10877875.0 acc: 0.5804343223571777\n",
      "step: 4025\n",
      "train: loss: 1562340.0 acc: 0.9063711166381836  val: loss: 10962276.0 acc: 0.8341965675354004\n",
      "step: 4030\n",
      "train: loss: 506781.15625 acc: 0.8684377670288086  val: loss: 8079219.5 acc: 0.8326818943023682\n",
      "step: 4035\n",
      "train: loss: 941566.6875 acc: 0.7839783430099487  val: loss: 21712694.0 acc: 0.8345227241516113\n",
      "step: 4040\n",
      "train: loss: 1264329.875 acc: 0.6370716691017151  val: loss: 6078172.5 acc: 0.8792058229446411\n",
      "step: 4045\n",
      "train: loss: 1242010.75 acc: 0.9637704491615295  val: loss: 4951370.5 acc: 0.7884348630905151\n",
      "step: 4050\n",
      "train: loss: 817569.625 acc: 0.9136821031570435  val: loss: 11533119.0 acc: 0.8031392693519592\n",
      "step: 4055\n",
      "train: loss: 1039718.625 acc: 0.8695383071899414  val: loss: 14502054.0 acc: 0.5637681484222412\n",
      "step: 4060\n",
      "train: loss: 1029232.8125 acc: 0.8922387957572937  val: loss: 14196339.0 acc: 0.3833397626876831\n",
      "step: 4065\n",
      "train: loss: 676896.8125 acc: 0.9245758056640625  val: loss: 7573726.0 acc: 0.906764030456543\n",
      "step: 4070\n",
      "train: loss: 803929.5 acc: 0.8825656771659851  val: loss: 25446766.0 acc: 0.2636728882789612\n",
      "step: 4075\n",
      "train: loss: 1303302.0 acc: 0.9338854551315308  val: loss: 14976809.0 acc: 0.5323197841644287\n",
      "step: 4080\n",
      "train: loss: 1786338.0 acc: 0.8713901042938232  val: loss: 16657476.0 acc: 0.8482009172439575\n",
      "step: 4085\n",
      "train: loss: 347535.5625 acc: 0.9216019511222839  val: loss: 15455319.0 acc: 0.7439089417457581\n",
      "step: 4090\n",
      "train: loss: 764162.75 acc: 0.9240995049476624  val: loss: 11344002.0 acc: 0.657030463218689\n",
      "step: 4095\n",
      "train: loss: 706678.75 acc: 0.944037914276123  val: loss: 9502039.0 acc: 0.7497810125350952\n",
      "step: 4100\n",
      "train: loss: 504815.46875 acc: 0.8886525630950928  val: loss: 12067276.0 acc: 0.8049638271331787\n",
      "step: 4105\n",
      "train: loss: 263447.90625 acc: 0.9421133995056152  val: loss: 18301926.0 acc: 0.8007991313934326\n",
      "step: 4110\n",
      "train: loss: 637152.375 acc: 0.9559773802757263  val: loss: 2053415.625 acc: 0.94896000623703\n",
      "step: 4115\n",
      "train: loss: 482509.71875 acc: 0.9616174101829529  val: loss: 4670315.5 acc: 0.8911635279655457\n",
      "step: 4120\n",
      "train: loss: 1175326.375 acc: 0.9461002349853516  val: loss: 19679946.0 acc: 0.7179108262062073\n",
      "step: 4125\n",
      "train: loss: 1349288.625 acc: 0.9046722054481506  val: loss: 15129626.0 acc: 0.04328775405883789\n",
      "step: 4130\n",
      "train: loss: 936159.8125 acc: 0.9466654658317566  val: loss: 10348759.0 acc: 0.7428478002548218\n",
      "step: 4135\n",
      "train: loss: 1849955.625 acc: 0.9190492033958435  val: loss: 5708498.0 acc: 0.7396930456161499\n",
      "step: 4140\n",
      "train: loss: 2613999.0 acc: 0.8831295967102051  val: loss: 11820824.0 acc: -0.1536039113998413\n",
      "step: 4145\n",
      "train: loss: 2309095.5 acc: 0.9023200273513794  val: loss: 9850956.0 acc: 0.7895510792732239\n",
      "step: 4150\n",
      "train: loss: 2447348.0 acc: 0.9024729132652283  val: loss: 9733568.0 acc: 0.73904949426651\n",
      "step: 4155\n",
      "train: loss: 1740300.75 acc: 0.8501886129379272  val: loss: 6983234.5 acc: 0.9279802441596985\n",
      "step: 4160\n",
      "train: loss: 1454615.625 acc: 0.9535584449768066  val: loss: 13552707.0 acc: 0.8089330196380615\n",
      "step: 4165\n",
      "train: loss: 6497160.5 acc: 0.805457353591919  val: loss: 9586015.0 acc: 0.8740324974060059\n",
      "step: 4170\n",
      "train: loss: 3385552.25 acc: 0.9553680419921875  val: loss: 6910945.5 acc: 0.8454906940460205\n",
      "step: 4175\n",
      "train: loss: 1755001.625 acc: 0.9119656085968018  val: loss: 6291177.0 acc: 0.9318192601203918\n",
      "step: 4180\n",
      "train: loss: 3214164.25 acc: 0.9451777338981628  val: loss: 9548473.0 acc: 0.6605010032653809\n",
      "step: 4185\n",
      "train: loss: 3266874.5 acc: 0.9190020561218262  val: loss: 6925778.0 acc: 0.8746174573898315\n",
      "step: 4190\n",
      "train: loss: 8585171.0 acc: 0.8611122965812683  val: loss: 14400040.0 acc: 0.8596124053001404\n",
      "step: 4195\n",
      "train: loss: 7148381.0 acc: 0.8717641234397888  val: loss: 8784413.0 acc: 0.7159718871116638\n",
      "step: 4200\n",
      "train: loss: 7810887.0 acc: 0.8160659074783325  val: loss: 6830845.5 acc: 0.8845587372779846\n",
      "step: 4205\n",
      "train: loss: 12335149.0 acc: 0.8726518750190735  val: loss: 9090414.0 acc: 0.8466476202011108\n",
      "step: 4210\n",
      "train: loss: 10272272.0 acc: 0.8662543296813965  val: loss: 2697048.25 acc: 0.8760050535202026\n",
      "step: 4215\n",
      "train: loss: 14512063.0 acc: 0.920932948589325  val: loss: 3389175.25 acc: 0.809606671333313\n",
      "step: 4220\n",
      "train: loss: 5676045.5 acc: 0.9593177437782288  val: loss: 4718973.5 acc: 0.904983401298523\n",
      "step: 4225\n",
      "train: loss: 8044686.5 acc: 0.9602622985839844  val: loss: 23629124.0 acc: -0.08607292175292969\n",
      "step: 4230\n",
      "train: loss: 9186705.0 acc: 0.9526156187057495  val: loss: 24501688.0 acc: 0.7653453350067139\n",
      "step: 4235\n",
      "train: loss: 7897814.5 acc: 0.9381750822067261  val: loss: 5089341.5 acc: 0.8927678465843201\n",
      "step: 4240\n",
      "train: loss: 6309378.0 acc: 0.9495491981506348  val: loss: 29702386.0 acc: 0.022770583629608154\n",
      "step: 4245\n",
      "train: loss: 8446288.0 acc: 0.9102578163146973  val: loss: 2312741.75 acc: 0.9176474809646606\n",
      "step: 4250\n",
      "train: loss: 7786550.5 acc: 0.8973654508590698  val: loss: 3701841.0 acc: 0.8766288757324219\n",
      "step: 4255\n",
      "train: loss: 11866830.0 acc: 0.9327875971794128  val: loss: 13850407.0 acc: 0.8747930526733398\n",
      "step: 4260\n",
      "train: loss: 6589414.0 acc: 0.9501239657402039  val: loss: 13369334.0 acc: 0.48686325550079346\n",
      "step: 4265\n",
      "train: loss: 10320115.0 acc: 0.8526926040649414  val: loss: 7425572.0 acc: 0.9488440155982971\n",
      "step: 4270\n",
      "train: loss: 10883945.0 acc: 0.8696331977844238  val: loss: 5617171.0 acc: 0.8040655851364136\n",
      "step: 4275\n",
      "train: loss: 8840827.0 acc: 0.6133813858032227  val: loss: 4449018.5 acc: 0.8271799683570862\n",
      "step: 4280\n",
      "train: loss: 2572943.75 acc: 0.9712290167808533  val: loss: 6584738.0 acc: 0.8814800977706909\n",
      "step: 4285\n",
      "train: loss: 10809545.0 acc: 0.8248018622398376  val: loss: 2033698.625 acc: 0.9787254333496094\n",
      "step: 4290\n",
      "train: loss: 5995946.0 acc: 0.8919144868850708  val: loss: 4475327.5 acc: 0.9388813972473145\n",
      "step: 4295\n",
      "train: loss: 7313725.0 acc: 0.8800397515296936  val: loss: 7186051.0 acc: 0.8783329725265503\n",
      "step: 4300\n",
      "train: loss: 7339266.5 acc: 0.7244091033935547  val: loss: 13057040.0 acc: 0.8118307590484619\n",
      "step: 4305\n",
      "train: loss: 9425538.0 acc: 0.7618285417556763  val: loss: 3021570.0 acc: 0.8428798913955688\n",
      "step: 4310\n",
      "train: loss: 4643051.0 acc: 0.8070704936981201  val: loss: 10210706.0 acc: 0.8580827116966248\n",
      "step: 4315\n",
      "train: loss: 2532483.0 acc: 0.8369655013084412  val: loss: 7396238.0 acc: 0.7452103495597839\n",
      "step: 4320\n",
      "train: loss: 2435659.75 acc: 0.8315553069114685  val: loss: 10739908.0 acc: 0.731149435043335\n",
      "step: 4325\n",
      "train: loss: 1480899.25 acc: 0.883266031742096  val: loss: 9089710.0 acc: 0.7064881324768066\n",
      "step: 4330\n",
      "train: loss: 3980820.0 acc: 0.7606392502784729  val: loss: 15516739.0 acc: 0.879355788230896\n",
      "step: 4335\n",
      "train: loss: 2875467.25 acc: 0.8365631103515625  val: loss: 11053310.0 acc: 0.7594654560089111\n",
      "step: 4340\n",
      "train: loss: 2733186.0 acc: 0.9432891607284546  val: loss: 10927545.0 acc: 0.901459276676178\n",
      "step: 4345\n",
      "train: loss: 2188061.25 acc: 0.8866442441940308  val: loss: 6200517.0 acc: 0.931159496307373\n",
      "step: 4350\n",
      "train: loss: 2040917.125 acc: 0.8821262717247009  val: loss: 7222646.5 acc: 0.6944199800491333\n",
      "step: 4355\n",
      "train: loss: 2532801.75 acc: 0.8374147415161133  val: loss: 6529197.5 acc: 0.6907809376716614\n",
      "step: 4360\n",
      "train: loss: 2708733.75 acc: 0.8619662523269653  val: loss: 9533897.0 acc: 0.8278460502624512\n",
      "step: 4365\n",
      "train: loss: 2547767.25 acc: 0.9433549642562866  val: loss: 6202875.0 acc: 0.7260486483573914\n",
      "step: 4370\n",
      "train: loss: 3176747.75 acc: 0.8298991322517395  val: loss: 1271588.875 acc: 0.8726767301559448\n",
      "step: 4375\n",
      "train: loss: 4309515.5 acc: 0.8068808317184448  val: loss: 12740482.0 acc: 0.7182539701461792\n",
      "step: 4380\n",
      "train: loss: 4222308.5 acc: 0.814832866191864  val: loss: 7205034.0 acc: 0.7731958627700806\n",
      "step: 4385\n",
      "train: loss: 3361022.25 acc: 0.774666965007782  val: loss: 4254924.5 acc: 0.7836366295814514\n",
      "step: 4390\n",
      "train: loss: 6047471.0 acc: 0.7558947801589966  val: loss: 52868276.0 acc: 0.44699227809906006\n",
      "step: 4395\n",
      "train: loss: 4941359.0 acc: 0.7529810667037964  val: loss: 10571613.0 acc: 0.682117223739624\n",
      "step: 4400\n",
      "train: loss: 5968777.0 acc: 0.6966850757598877  val: loss: 6073081.0 acc: 0.7561209797859192\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 4405\n",
      "train: loss: 6970140.0 acc: 0.7241066098213196  val: loss: 19631666.0 acc: 0.7577245831489563\n",
      "step: 4410\n",
      "train: loss: 6265061.5 acc: 0.7418356537818909  val: loss: 23977826.0 acc: 0.5606749653816223\n",
      "step: 4415\n",
      "train: loss: 10391012.0 acc: 0.7541388273239136  val: loss: 10070488.0 acc: 0.8727511763572693\n",
      "step: 4420\n",
      "train: loss: 6126608.5 acc: 0.7954229116439819  val: loss: 9304618.0 acc: 0.7633348703384399\n",
      "step: 4425\n",
      "train: loss: 15623426.0 acc: 0.6493233442306519  val: loss: 8630291.0 acc: 0.518370509147644\n",
      "step: 4430\n",
      "train: loss: 10829378.0 acc: 0.778300404548645  val: loss: 7666867.5 acc: 0.8517820239067078\n",
      "step: 4435\n",
      "train: loss: 5887128.5 acc: 0.7996524572372437  val: loss: 4081358.0 acc: 0.8938767910003662\n",
      "step: 4440\n",
      "train: loss: 8745691.0 acc: 0.8264673352241516  val: loss: 4313535.0 acc: 0.8895807266235352\n",
      "step: 4445\n",
      "train: loss: 4205784.0 acc: 0.9244014024734497  val: loss: 10044246.0 acc: 0.7811788320541382\n",
      "step: 4450\n",
      "train: loss: 4298779.0 acc: 0.8993057608604431  val: loss: 21945702.0 acc: 0.6382611989974976\n",
      "step: 4455\n",
      "train: loss: 2473259.25 acc: 0.9506215453147888  val: loss: 19652624.0 acc: 0.7204295992851257\n",
      "step: 4460\n",
      "train: loss: 5030141.0 acc: 0.9285616278648376  val: loss: 19538060.0 acc: 0.6238100528717041\n",
      "step: 4465\n",
      "train: loss: 5265193.5 acc: 0.9305248260498047  val: loss: 14852930.0 acc: 0.7691344618797302\n",
      "step: 4470\n",
      "train: loss: 2547970.25 acc: 0.9736101031303406  val: loss: 21370728.0 acc: 0.7393089532852173\n",
      "step: 4475\n",
      "train: loss: 1628276.375 acc: 0.9752740263938904  val: loss: 15772421.0 acc: 0.5422829985618591\n",
      "step: 4480\n",
      "train: loss: 2173034.0 acc: 0.9653164744377136  val: loss: 7156829.0 acc: 0.811652660369873\n",
      "step: 4485\n",
      "train: loss: 2901253.75 acc: 0.9534263610839844  val: loss: 13564139.0 acc: 0.8171770572662354\n",
      "step: 4490\n",
      "train: loss: 1310712.375 acc: 0.9765140414237976  val: loss: 5878734.0 acc: 0.8643506765365601\n",
      "step: 4495\n",
      "train: loss: 6666062.0 acc: 0.8786482810974121  val: loss: 7575682.5 acc: 0.8340211510658264\n",
      "step: 4500\n",
      "train: loss: 3626034.5 acc: 0.9161632061004639  val: loss: 23938890.0 acc: 0.5130088329315186\n",
      "step: 4505\n",
      "train: loss: 3568045.5 acc: 0.8718282580375671  val: loss: 17376128.0 acc: 0.8634878396987915\n",
      "step: 4510\n",
      "train: loss: 531548.25 acc: 0.8097734451293945  val: loss: 10477987.0 acc: 0.8530898094177246\n",
      "step: 4515\n",
      "train: loss: 1971930.5 acc: 0.8089190125465393  val: loss: 9283864.0 acc: 0.7736153602600098\n",
      "step: 4520\n",
      "train: loss: 346877.0625 acc: 0.9865531325340271  val: loss: 3584853.75 acc: 0.9195961356163025\n",
      "step: 4525\n",
      "train: loss: 3449836.75 acc: 0.8066631555557251  val: loss: 5908865.5 acc: 0.898311972618103\n",
      "step: 4530\n",
      "train: loss: 3382896.75 acc: 0.8284800052642822  val: loss: 10580751.0 acc: 0.1965395212173462\n",
      "step: 4535\n",
      "train: loss: 308630.9375 acc: 0.9782339930534363  val: loss: 7028601.0 acc: 0.9150417447090149\n",
      "step: 4540\n",
      "train: loss: 673175.25 acc: 0.8988486528396606  val: loss: 7147906.5 acc: 0.6646901965141296\n",
      "step: 4545\n",
      "train: loss: 1523395.625 acc: 0.7917339205741882  val: loss: 4331242.5 acc: 0.9278912544250488\n",
      "step: 4550\n",
      "train: loss: 972603.1875 acc: 0.964769721031189  val: loss: 10876519.0 acc: 0.829393744468689\n",
      "step: 4555\n",
      "train: loss: 338935.0625 acc: 0.9759979248046875  val: loss: 8777989.0 acc: 0.6613470911979675\n",
      "step: 4560\n",
      "train: loss: 557251.25 acc: 0.9268136024475098  val: loss: 8302109.5 acc: 0.7884498238563538\n",
      "step: 4565\n",
      "train: loss: 377480.40625 acc: 0.9742680191993713  val: loss: 18453918.0 acc: 0.709672749042511\n",
      "step: 4570\n",
      "train: loss: 1542633.0 acc: 0.9211709499359131  val: loss: 25741214.0 acc: 0.5858604311943054\n",
      "step: 4575\n",
      "train: loss: 846284.25 acc: 0.8654783368110657  val: loss: 2710632.5 acc: 0.9384865760803223\n",
      "step: 4580\n",
      "train: loss: 1199013.125 acc: 0.9044453501701355  val: loss: 15828105.0 acc: 0.5021763443946838\n",
      "step: 4585\n",
      "train: loss: 1172155.5 acc: 0.9608018398284912  val: loss: 5282896.5 acc: 0.8538632392883301\n",
      "step: 4590\n",
      "train: loss: 791392.6875 acc: 0.9366371631622314  val: loss: 14385707.0 acc: 0.8284691572189331\n",
      "step: 4595\n",
      "train: loss: 1650963.625 acc: 0.8865166902542114  val: loss: 4443706.5 acc: 0.9350221753120422\n",
      "step: 4600\n",
      "train: loss: 487048.0625 acc: 0.9186890125274658  val: loss: 18376470.0 acc: 0.4808226227760315\n",
      "step: 4605\n",
      "train: loss: 865356.1875 acc: 0.8796381950378418  val: loss: 6685696.5 acc: 0.8558210134506226\n",
      "step: 4610\n",
      "train: loss: 548239.0625 acc: 0.9647386074066162  val: loss: 8111187.0 acc: 0.8655878305435181\n",
      "step: 4615\n",
      "train: loss: 394398.21875 acc: 0.7748326063156128  val: loss: 10968308.0 acc: 0.7982515692710876\n",
      "step: 4620\n",
      "train: loss: 1123501.125 acc: 0.7640032768249512  val: loss: 4561273.0 acc: 0.8440043330192566\n",
      "step: 4625\n",
      "train: loss: 1014729.4375 acc: 0.9213824272155762  val: loss: 5316163.0 acc: 0.7816122770309448\n",
      "step: 4630\n",
      "train: loss: 638408.875 acc: 0.9377294182777405  val: loss: 10221772.0 acc: 0.8604674339294434\n",
      "step: 4635\n",
      "train: loss: 952026.125 acc: 0.9362861514091492  val: loss: 21607576.0 acc: 0.8217532634735107\n",
      "step: 4640\n",
      "train: loss: 1283719.875 acc: 0.9161328077316284  val: loss: 10047215.0 acc: 0.7366833686828613\n",
      "step: 4645\n",
      "train: loss: 1325804.125 acc: 0.8951630592346191  val: loss: 7789708.5 acc: 0.736345648765564\n",
      "step: 4650\n",
      "train: loss: 2061341.625 acc: 0.9045657515525818  val: loss: 18623334.0 acc: 0.4779587984085083\n",
      "step: 4655\n",
      "train: loss: 2907682.75 acc: 0.9221795797348022  val: loss: 8613359.0 acc: 0.7783023715019226\n",
      "step: 4660\n",
      "train: loss: 2120355.0 acc: 0.9163590669631958  val: loss: 7240294.0 acc: 0.8938685059547424\n",
      "step: 4665\n",
      "train: loss: 3083348.75 acc: 0.8894846439361572  val: loss: 8830140.0 acc: 0.7917732000350952\n",
      "step: 4670\n",
      "train: loss: 2174270.5 acc: 0.8314943313598633  val: loss: 17856158.0 acc: 0.2769712209701538\n",
      "step: 4675\n",
      "train: loss: 1452544.5 acc: 0.9199892282485962  val: loss: 20296010.0 acc: 0.6610442399978638\n",
      "step: 4680\n",
      "train: loss: 2871291.25 acc: 0.934401273727417  val: loss: 1932925.875 acc: 0.9228403568267822\n",
      "step: 4685\n",
      "train: loss: 1329006.0 acc: 0.9740315675735474  val: loss: 9909244.0 acc: 0.882454514503479\n",
      "step: 4690\n",
      "train: loss: 1982507.75 acc: 0.9517956972122192  val: loss: 7387921.0 acc: 0.9113364219665527\n",
      "step: 4695\n",
      "train: loss: 686030.5 acc: 0.9791039228439331  val: loss: 4183297.25 acc: 0.9145563244819641\n",
      "step: 4700\n",
      "train: loss: 2684895.75 acc: 0.919353723526001  val: loss: 7952417.0 acc: 0.9189617037773132\n",
      "step: 4705\n",
      "train: loss: 3496060.75 acc: 0.928736686706543  val: loss: 1538093.625 acc: 0.9296086430549622\n",
      "step: 4710\n",
      "train: loss: 2072862.875 acc: 0.9248491525650024  val: loss: 19117202.0 acc: 0.36861908435821533\n",
      "step: 4715\n",
      "train: loss: 21641216.0 acc: 0.7571312785148621  val: loss: 6761328.5 acc: 0.883671760559082\n",
      "step: 4720\n",
      "train: loss: 10469215.0 acc: 0.940049409866333  val: loss: 6732761.0 acc: 0.7894287109375\n",
      "step: 4725\n",
      "train: loss: 6124958.0 acc: 0.8995190858840942  val: loss: 3331701.25 acc: 0.7757362723350525\n",
      "step: 4730\n",
      "train: loss: 9422891.0 acc: 0.9259825348854065  val: loss: 5321582.0 acc: 0.6591769456863403\n",
      "step: 4735\n",
      "train: loss: 9414409.0 acc: 0.9268457293510437  val: loss: 6383797.0 acc: 0.6528285145759583\n",
      "step: 4740\n",
      "train: loss: 5686196.0 acc: 0.9696775078773499  val: loss: 7607343.0 acc: 0.8483010530471802\n",
      "step: 4745\n",
      "train: loss: 8226242.0 acc: 0.8879755139350891  val: loss: 8696822.0 acc: 0.9267328977584839\n",
      "step: 4750\n",
      "train: loss: 12725473.0 acc: 0.9453456401824951  val: loss: 5197918.5 acc: 0.936913251876831\n",
      "step: 4755\n",
      "train: loss: 12290443.0 acc: 0.9311563968658447  val: loss: 15131431.0 acc: 0.03361856937408447\n",
      "step: 4760\n",
      "train: loss: 9686023.0 acc: 0.9362320899963379  val: loss: 4769166.5 acc: 0.9263873100280762\n",
      "step: 4765\n",
      "train: loss: 7919922.0 acc: 0.9544473886489868  val: loss: 4122792.0 acc: 0.9440405368804932\n",
      "step: 4770\n",
      "train: loss: 8418102.0 acc: 0.9016124606132507  val: loss: 8264611.0 acc: 0.8839678764343262\n",
      "step: 4775\n",
      "train: loss: 4732874.0 acc: 0.9658045172691345  val: loss: 13598218.0 acc: 0.6270100474357605\n",
      "step: 4780\n",
      "train: loss: 6953678.5 acc: 0.90842205286026  val: loss: 6512926.0 acc: 0.41668474674224854\n",
      "step: 4785\n",
      "train: loss: 9187590.0 acc: 0.7661957740783691  val: loss: 3472794.75 acc: 0.7669814229011536\n",
      "step: 4790\n",
      "train: loss: 7382334.5 acc: 0.849254310131073  val: loss: 3752784.25 acc: 0.9376910328865051\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 4795\n",
      "train: loss: 10762356.0 acc: 0.88045734167099  val: loss: 8819568.0 acc: 0.8515945076942444\n",
      "step: 4800\n",
      "train: loss: 11756607.0 acc: 0.77275550365448  val: loss: 4332509.5 acc: 0.8784478902816772\n",
      "step: 4805\n",
      "train: loss: 10753162.0 acc: 0.8461582064628601  val: loss: 8136503.0 acc: 0.8662805557250977\n",
      "step: 4810\n",
      "train: loss: 5884628.0 acc: 0.9070915579795837  val: loss: 5604787.5 acc: 0.9192947149276733\n",
      "step: 4815\n",
      "train: loss: 15594327.0 acc: 0.73763507604599  val: loss: 7709555.5 acc: 0.7969530820846558\n",
      "step: 4820\n",
      "train: loss: 8084166.5 acc: 0.7915276288986206  val: loss: 12607013.0 acc: 0.8305550813674927\n",
      "step: 4825\n",
      "train: loss: 3127315.25 acc: 0.9337822794914246  val: loss: 5916686.5 acc: 0.8070382475852966\n",
      "step: 4830\n",
      "train: loss: 3172108.75 acc: 0.8243997097015381  val: loss: 7997304.0 acc: 0.8583496809005737\n",
      "step: 4835\n",
      "train: loss: 4481537.0 acc: 0.7873361110687256  val: loss: 21641784.0 acc: 0.7883682250976562\n",
      "step: 4840\n",
      "train: loss: 5542644.0 acc: 0.742157518863678  val: loss: 3003361.75 acc: 0.942814826965332\n",
      "step: 4845\n",
      "train: loss: 5623804.5 acc: 0.8656874299049377  val: loss: 9254662.0 acc: 0.7757425904273987\n",
      "step: 4850\n",
      "train: loss: 3961187.5 acc: 0.8283601999282837  val: loss: 13818013.0 acc: 0.7187871336936951\n",
      "step: 4855\n",
      "train: loss: 4297909.0 acc: 0.7902579307556152  val: loss: 22057886.0 acc: 0.7654297947883606\n",
      "step: 4860\n",
      "train: loss: 2336804.75 acc: 0.8662850856781006  val: loss: 4068135.75 acc: 0.8028069138526917\n",
      "step: 4865\n",
      "train: loss: 3896106.5 acc: 0.9346683621406555  val: loss: 3741943.25 acc: 0.9252747297286987\n",
      "step: 4870\n",
      "train: loss: 3755802.75 acc: 0.8217940330505371  val: loss: 11370972.0 acc: 0.7301464080810547\n",
      "step: 4875\n",
      "train: loss: 4497060.5 acc: 0.8727054595947266  val: loss: 9556765.0 acc: 0.855990469455719\n",
      "step: 4880\n",
      "train: loss: 3339566.5 acc: 0.8359953761100769  val: loss: 13348594.0 acc: 0.73727947473526\n",
      "step: 4885\n",
      "train: loss: 2669056.5 acc: 0.7863531112670898  val: loss: 4379470.0 acc: 0.9212168455123901\n",
      "step: 4890\n",
      "train: loss: 2667160.0 acc: 0.8508747816085815  val: loss: 22062808.0 acc: 0.8278152942657471\n",
      "step: 4895\n",
      "train: loss: 5623134.5 acc: 0.7433401346206665  val: loss: 15556934.0 acc: 0.7202013731002808\n",
      "step: 4900\n",
      "train: loss: 8035746.5 acc: 0.7341824173927307  val: loss: 5374754.5 acc: 0.7735819816589355\n",
      "step: 4905\n",
      "train: loss: 5976775.0 acc: 0.7739783525466919  val: loss: 5659054.5 acc: 0.855290949344635\n",
      "step: 4910\n",
      "train: loss: 4917931.5 acc: 0.7321457266807556  val: loss: 14038233.0 acc: 0.7610607147216797\n",
      "step: 4915\n",
      "train: loss: 7253540.0 acc: 0.7055680751800537  val: loss: 10874612.0 acc: 0.8512853384017944\n",
      "step: 4920\n",
      "train: loss: 3405638.5 acc: 0.8010639548301697  val: loss: 6597469.0 acc: 0.9028328657150269\n",
      "step: 4925\n",
      "train: loss: 5294108.5 acc: 0.7321413159370422  val: loss: 7751788.0 acc: 0.7379853129386902\n",
      "step: 4930\n",
      "train: loss: 5533602.5 acc: 0.6823511719703674  val: loss: 18382252.0 acc: 0.7720845937728882\n",
      "step: 4935\n",
      "train: loss: 8103831.5 acc: 0.822708785533905  val: loss: 24248848.0 acc: 0.6601523756980896\n",
      "step: 4940\n",
      "train: loss: 7722083.5 acc: 0.798590898513794  val: loss: 27244004.0 acc: 0.6569886803627014\n",
      "step: 4945\n",
      "train: loss: 8141728.0 acc: 0.8981528282165527  val: loss: 7865417.0 acc: 0.8574210405349731\n",
      "step: 4950\n",
      "train: loss: 11115097.0 acc: 0.7771010398864746  val: loss: 9858709.0 acc: 0.8002034425735474\n",
      "step: 4955\n",
      "train: loss: 10422456.0 acc: 0.7293617725372314  val: loss: 12085685.0 acc: 0.6681830883026123\n",
      "step: 4960\n",
      "train: loss: 2589468.75 acc: 0.8910713791847229  val: loss: 8859783.0 acc: 0.8119568824768066\n",
      "step: 4965\n",
      "train: loss: 6612575.5 acc: 0.8686076402664185  val: loss: 14200975.0 acc: 0.7774181365966797\n",
      "step: 4970\n",
      "train: loss: 3804295.75 acc: 0.9255820512771606  val: loss: 21205098.0 acc: 0.8406951427459717\n",
      "step: 4975\n",
      "train: loss: 11786083.0 acc: 0.5651933550834656  val: loss: 4076457.0 acc: 0.8661428093910217\n",
      "step: 4980\n",
      "train: loss: 3423588.0 acc: 0.9595439434051514  val: loss: 10886824.0 acc: 0.6992003917694092\n",
      "step: 4985\n",
      "train: loss: 1872362.5 acc: 0.9741278886795044  val: loss: 10844925.0 acc: 0.8060359358787537\n",
      "step: 4990\n",
      "train: loss: 5669845.0 acc: 0.9308448433876038  val: loss: 12072938.0 acc: 0.8200101852416992\n",
      "step: 4995\n",
      "train: loss: 2155201.25 acc: 0.9608842134475708  val: loss: 8466406.0 acc: 0.6889015436172485\n",
      "step: 5000\n",
      "train: loss: 8701065.0 acc: 0.8983851075172424  val: loss: 1485211.25 acc: 0.9252648949623108\n",
      "step: 5005\n",
      "train: loss: 1369858.125 acc: 0.980902373790741  val: loss: 13091813.0 acc: 0.5389881134033203\n",
      "step: 5010\n",
      "train: loss: 2248015.75 acc: 0.969229519367218  val: loss: 7462560.0 acc: 0.7783036828041077\n",
      "step: 5015\n",
      "train: loss: 3861191.25 acc: 0.8791244029998779  val: loss: 8352732.5 acc: 0.7112009525299072\n",
      "step: 5020\n",
      "train: loss: 757645.4375 acc: 0.9858860373497009  val: loss: 10610802.0 acc: 0.8333542346954346\n",
      "step: 5025\n",
      "train: loss: 1476426.0 acc: 0.9269634485244751  val: loss: 15522005.0 acc: 0.7183293104171753\n",
      "step: 5030\n",
      "train: loss: 605805.6875 acc: 0.981544554233551  val: loss: 2735513.5 acc: 0.941064715385437\n",
      "step: 5035\n",
      "train: loss: 1173365.875 acc: 0.8768976926803589  val: loss: 10310921.0 acc: 0.09964704513549805\n",
      "step: 5040\n",
      "train: loss: 572601.25 acc: 0.9650272130966187  val: loss: 9475306.0 acc: 0.7692395448684692\n",
      "step: 5045\n",
      "train: loss: 1585254.875 acc: 0.9162750244140625  val: loss: 11598198.0 acc: 0.8433584570884705\n",
      "step: 5050\n",
      "train: loss: 1289601.75 acc: 0.9448410868644714  val: loss: 6872451.5 acc: 0.8964471817016602\n",
      "step: 5055\n",
      "train: loss: 2187301.75 acc: 0.9038380980491638  val: loss: 22506652.0 acc: 0.8320348858833313\n",
      "step: 5060\n",
      "train: loss: 1197137.125 acc: 0.7380648851394653  val: loss: 11600012.0 acc: 0.7607665657997131\n",
      "step: 5065\n",
      "train: loss: 488653.96875 acc: 0.914486825466156  val: loss: 5037439.0 acc: 0.7664671540260315\n",
      "step: 5070\n",
      "train: loss: 1568098.625 acc: 0.907833993434906  val: loss: 5981040.5 acc: 0.8894996047019958\n",
      "step: 5075\n",
      "train: loss: 868777.9375 acc: 0.9477390050888062  val: loss: 10774654.0 acc: 0.8921588659286499\n",
      "step: 5080\n",
      "train: loss: 1031917.375 acc: 0.8089749217033386  val: loss: 16459740.0 acc: 0.7855486869812012\n",
      "step: 5085\n",
      "train: loss: 538147.375 acc: 0.9019411206245422  val: loss: 1997738.375 acc: 0.9590585231781006\n",
      "step: 5090\n",
      "train: loss: 1035875.0625 acc: 0.7907229065895081  val: loss: 8795487.0 acc: 0.6779024600982666\n",
      "step: 5095\n",
      "train: loss: 585709.8125 acc: 0.9220971465110779  val: loss: 10985563.0 acc: 0.8795986175537109\n",
      "step: 5100\n",
      "train: loss: 1875420.125 acc: 0.8681497573852539  val: loss: 5658035.0 acc: 0.8598214387893677\n",
      "step: 5105\n",
      "train: loss: 1323758.5 acc: 0.8815961480140686  val: loss: 15633750.0 acc: 0.8388664126396179\n",
      "step: 5110\n",
      "train: loss: 1252743.5 acc: 0.9501748085021973  val: loss: 2296898.75 acc: 0.9494529366493225\n",
      "step: 5115\n",
      "train: loss: 1560423.375 acc: 0.9028990864753723  val: loss: 7976638.5 acc: 0.7403023838996887\n",
      "step: 5120\n",
      "train: loss: 756447.0 acc: 0.9332013130187988  val: loss: 3131555.75 acc: 0.7503303289413452\n",
      "step: 5125\n",
      "train: loss: 704696.8125 acc: 0.9239003658294678  val: loss: 14354622.0 acc: 0.5498616695404053\n",
      "step: 5130\n",
      "train: loss: 469825.90625 acc: 0.8423283100128174  val: loss: 6514096.0 acc: 0.8692160248756409\n",
      "step: 5135\n",
      "train: loss: 375854.9375 acc: 0.9155935049057007  val: loss: 16305863.0 acc: 0.3818546533584595\n",
      "step: 5140\n",
      "train: loss: 755737.3125 acc: 0.9621118903160095  val: loss: 19044130.0 acc: 0.7805027961730957\n",
      "step: 5145\n",
      "train: loss: 523786.59375 acc: 0.9507860541343689  val: loss: 15938382.0 acc: 0.4808219075202942\n",
      "step: 5150\n",
      "train: loss: 1360033.625 acc: 0.9490959644317627  val: loss: 24452748.0 acc: 0.7718167304992676\n",
      "step: 5155\n",
      "train: loss: 660025.6875 acc: 0.9450265169143677  val: loss: 6348080.0 acc: 0.7251509428024292\n",
      "step: 5160\n",
      "train: loss: 1493259.0 acc: 0.9579468369483948  val: loss: 12071962.0 acc: 0.3786555528640747\n",
      "step: 5165\n",
      "train: loss: 2785091.5 acc: 0.855393648147583  val: loss: 8278611.0 acc: 0.7394062876701355\n",
      "step: 5170\n",
      "train: loss: 1198467.25 acc: 0.9414767622947693  val: loss: 16664962.0 acc: 0.7932220697402954\n",
      "step: 5175\n",
      "train: loss: 2179075.0 acc: 0.938473105430603  val: loss: 3549157.5 acc: 0.9341109991073608\n",
      "step: 5180\n",
      "train: loss: 2861069.5 acc: 0.8419228792190552  val: loss: 9562050.0 acc: 0.8707380294799805\n",
      "step: 5185\n",
      "train: loss: 2910328.5 acc: 0.7984282970428467  val: loss: 11040593.0 acc: 0.7225130796432495\n",
      "step: 5190\n",
      "train: loss: 753125.0625 acc: 0.9285145401954651  val: loss: 24583596.0 acc: 0.7562586069107056\n",
      "step: 5195\n",
      "train: loss: 2991724.75 acc: 0.865393340587616  val: loss: 17706544.0 acc: 0.8212266564369202\n",
      "step: 5200\n",
      "train: loss: 2135936.25 acc: 0.938883900642395  val: loss: 4949952.5 acc: 0.919073224067688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 5205\n",
      "train: loss: 1909785.625 acc: 0.9636494517326355  val: loss: 8158498.5 acc: 0.7383548021316528\n",
      "step: 5210\n",
      "train: loss: 3798774.0 acc: 0.913666307926178  val: loss: 19292214.0 acc: 0.7195254564285278\n",
      "step: 5215\n",
      "train: loss: 3539431.0 acc: 0.929375171661377  val: loss: 9344137.0 acc: -0.07948732376098633\n",
      "step: 5220\n",
      "train: loss: 2940970.0 acc: 0.9384984374046326  val: loss: 13624492.0 acc: 0.8350880146026611\n",
      "step: 5225\n",
      "train: loss: 6369950.5 acc: 0.9249259233474731  val: loss: 5687603.0 acc: 0.9167941212654114\n",
      "step: 5230\n",
      "train: loss: 5380838.5 acc: 0.8470621109008789  val: loss: 2960216.5 acc: 0.9038816690444946\n",
      "step: 5235\n",
      "train: loss: 11311075.0 acc: 0.9330731630325317  val: loss: 10292524.0 acc: 0.7808322906494141\n",
      "step: 5240\n",
      "train: loss: 7050694.0 acc: 0.9016189575195312  val: loss: 14630956.0 acc: 0.7024585008621216\n",
      "step: 5245\n",
      "train: loss: 12701026.0 acc: 0.8463894128799438  val: loss: 18310006.0 acc: 0.6219865083694458\n",
      "step: 5250\n",
      "train: loss: 17121448.0 acc: 0.9031541347503662  val: loss: 1918792.0 acc: 0.8775404691696167\n",
      "step: 5255\n",
      "train: loss: 8263231.5 acc: 0.914456307888031  val: loss: 3559515.75 acc: 0.8901519179344177\n",
      "step: 5260\n",
      "train: loss: 7244778.5 acc: 0.9284297823905945  val: loss: 3309941.75 acc: 0.9680004715919495\n",
      "step: 5265\n",
      "train: loss: 4502160.0 acc: 0.9806725382804871  val: loss: 2107402.5 acc: 0.9752188324928284\n",
      "step: 5270\n",
      "train: loss: 14814497.0 acc: 0.8860787153244019  val: loss: 16842788.0 acc: 0.684228241443634\n",
      "step: 5275\n",
      "train: loss: 7981267.5 acc: 0.9545570015907288  val: loss: 4597075.0 acc: 0.9314397573471069\n",
      "step: 5280\n",
      "train: loss: 6184339.5 acc: 0.9343443512916565  val: loss: 4060604.75 acc: 0.8078099489212036\n",
      "step: 5285\n",
      "train: loss: 6160676.5 acc: 0.9444003701210022  val: loss: 16171974.0 acc: 0.7663388252258301\n",
      "step: 5290\n",
      "train: loss: 5373169.5 acc: 0.9455438852310181  val: loss: 9611194.0 acc: 0.08809071779251099\n",
      "step: 5295\n",
      "train: loss: 2906468.25 acc: 0.9224480390548706  val: loss: 4728149.5 acc: 0.8895175457000732\n",
      "step: 5300\n",
      "train: loss: 6308718.5 acc: 0.9518051743507385  val: loss: 4755884.5 acc: 0.9391991496086121\n",
      "step: 5305\n",
      "train: loss: 16732279.0 acc: 0.8131128549575806  val: loss: 4784819.5 acc: 0.8931323289871216\n",
      "step: 5310\n",
      "train: loss: 4414112.5 acc: 0.9099828004837036  val: loss: 4245369.0 acc: 0.9161277413368225\n",
      "step: 5315\n",
      "train: loss: 3743365.5 acc: 0.9420816898345947  val: loss: 7890389.5 acc: 0.9065325260162354\n",
      "step: 5320\n",
      "train: loss: 6926191.5 acc: 0.9058973789215088  val: loss: 6557911.5 acc: 0.85466468334198\n",
      "step: 5325\n",
      "train: loss: 6463240.0 acc: 0.832268238067627  val: loss: 4376806.5 acc: 0.8371137380599976\n",
      "step: 5330\n",
      "train: loss: 5743912.0 acc: 0.9102084636688232  val: loss: 5912334.5 acc: 0.919191837310791\n",
      "step: 5335\n",
      "train: loss: 4405821.0 acc: 0.8505823612213135  val: loss: 11672380.0 acc: 0.7177528142929077\n",
      "step: 5340\n",
      "train: loss: 3531521.25 acc: 0.7357913851737976  val: loss: 4772916.0 acc: 0.940299928188324\n",
      "step: 5345\n",
      "train: loss: 5319953.5 acc: 0.7219444513320923  val: loss: 8832242.0 acc: 0.8646999597549438\n",
      "step: 5350\n",
      "train: loss: 4509015.5 acc: 0.9089812636375427  val: loss: 10080513.0 acc: 0.7852641344070435\n",
      "step: 5355\n",
      "train: loss: 2732205.25 acc: 0.9618403315544128  val: loss: 9373447.0 acc: 0.7939642667770386\n",
      "step: 5360\n",
      "train: loss: 4634381.5 acc: 0.8175106048583984  val: loss: 4393292.0 acc: 0.8147069215774536\n",
      "step: 5365\n",
      "train: loss: 5471426.0 acc: 0.9162783622741699  val: loss: 8833761.0 acc: 0.8733218312263489\n",
      "step: 5370\n",
      "train: loss: 2485440.25 acc: 0.937268078327179  val: loss: 3385938.25 acc: 0.8256974220275879\n",
      "step: 5375\n",
      "train: loss: 4721867.0 acc: 0.8509420156478882  val: loss: 6615776.0 acc: 0.8474710583686829\n",
      "step: 5380\n",
      "train: loss: 1421987.0 acc: 0.898952066898346  val: loss: 4692614.0 acc: 0.9225727319717407\n",
      "step: 5385\n",
      "train: loss: 5315539.5 acc: 0.8084548711776733  val: loss: 10909397.0 acc: 0.9134504199028015\n",
      "step: 5390\n",
      "train: loss: 2836376.75 acc: 0.8496179580688477  val: loss: 6898727.0 acc: 0.7736793756484985\n",
      "step: 5395\n",
      "train: loss: 4578464.0 acc: 0.8147642612457275  val: loss: 6646936.5 acc: 0.7598846554756165\n",
      "step: 5400\n",
      "train: loss: 2366692.0 acc: 0.8360942006111145  val: loss: 9630820.0 acc: 0.7644758820533752\n",
      "step: 5405\n",
      "train: loss: 3388128.5 acc: 0.7842791676521301  val: loss: 7010819.5 acc: 0.8358117341995239\n",
      "step: 5410\n",
      "train: loss: 3010236.25 acc: 0.7827836871147156  val: loss: 4254777.0 acc: 0.9338343143463135\n",
      "step: 5415\n",
      "train: loss: 4341873.5 acc: 0.7923197150230408  val: loss: 7602401.0 acc: 0.7948341369628906\n",
      "step: 5420\n",
      "train: loss: 5663542.5 acc: 0.7570446729660034  val: loss: 6290123.5 acc: 0.767017662525177\n",
      "step: 5425\n",
      "train: loss: 5126634.5 acc: 0.7501096129417419  val: loss: 20307214.0 acc: 0.7638685703277588\n",
      "step: 5430\n",
      "train: loss: 9785334.0 acc: 0.6735814809799194  val: loss: 2970857.25 acc: 0.9388878345489502\n",
      "step: 5435\n",
      "train: loss: 5158228.0 acc: 0.7916207909584045  val: loss: 9362964.0 acc: 0.8747009634971619\n",
      "step: 5440\n",
      "train: loss: 11170964.0 acc: 0.036502718925476074  val: loss: 13526031.0 acc: 0.669278621673584\n",
      "step: 5445\n",
      "train: loss: 4389251.5 acc: 0.7302061915397644  val: loss: 3639890.75 acc: 0.781906247138977\n",
      "step: 5450\n",
      "train: loss: 12163600.0 acc: 0.7436000108718872  val: loss: 27244156.0 acc: 0.5226066708564758\n",
      "step: 5455\n",
      "train: loss: 10664195.0 acc: 0.7923445105552673  val: loss: 3857590.25 acc: 0.8880700469017029\n",
      "step: 5460\n",
      "train: loss: 11282261.0 acc: 0.7269799113273621  val: loss: 4568670.5 acc: 0.7069042921066284\n",
      "step: 5465\n",
      "train: loss: 7962428.0 acc: 0.8639453649520874  val: loss: 7307732.5 acc: 0.7350094318389893\n",
      "step: 5470\n",
      "train: loss: 10456852.0 acc: 0.591690719127655  val: loss: 12557099.0 acc: 0.6177525520324707\n",
      "step: 5475\n",
      "train: loss: 6598656.0 acc: 0.8437674045562744  val: loss: 3729034.25 acc: 0.9447361826896667\n",
      "step: 5480\n",
      "train: loss: 2493722.5 acc: 0.9652537703514099  val: loss: 15536818.0 acc: 0.8654375076293945\n",
      "step: 5485\n",
      "train: loss: 5092489.0 acc: 0.9142646789550781  val: loss: 7448389.5 acc: 0.9049777984619141\n",
      "step: 5490\n",
      "train: loss: 3103746.0 acc: 0.9090074300765991  val: loss: 19816558.0 acc: 0.6641874313354492\n",
      "step: 5495\n",
      "train: loss: 9225655.0 acc: 0.8561220169067383  val: loss: 8572183.0 acc: 0.8342180252075195\n",
      "step: 5500\n",
      "train: loss: 2459703.5 acc: 0.9752001762390137  val: loss: 10515591.0 acc: 0.800309956073761\n",
      "step: 5505\n",
      "train: loss: 5925763.5 acc: 0.9131479859352112  val: loss: 9356197.0 acc: 0.8278258442878723\n",
      "step: 5510\n",
      "train: loss: 4708167.0 acc: 0.9068852663040161  val: loss: 7809852.0 acc: 0.7790123820304871\n",
      "step: 5515\n",
      "train: loss: 7504591.0 acc: 0.9075115323066711  val: loss: 2303068.0 acc: 0.9100935459136963\n",
      "step: 5520\n",
      "train: loss: 4965342.5 acc: 0.8863049149513245  val: loss: 2338787.25 acc: 0.9464682340621948\n",
      "step: 5525\n",
      "train: loss: 1328384.125 acc: 0.9724271893501282  val: loss: 1451450.0 acc: 0.9321480989456177\n",
      "step: 5530\n",
      "train: loss: 724288.375 acc: 0.9900160431861877  val: loss: 9660981.0 acc: 0.5731397867202759\n",
      "step: 5535\n",
      "train: loss: 517175.46875 acc: 0.9797608852386475  val: loss: 15963342.0 acc: 0.6421006321907043\n",
      "step: 5540\n",
      "train: loss: 193742.625 acc: 0.9904180765151978  val: loss: 7042561.0 acc: 0.6409943103790283\n",
      "step: 5545\n",
      "train: loss: 2234924.0 acc: 0.8837209343910217  val: loss: 7375539.0 acc: 0.723949670791626\n",
      "step: 5550\n",
      "train: loss: 872897.3125 acc: 0.9330964684486389  val: loss: 4952886.0 acc: 0.9441919326782227\n",
      "step: 5555\n",
      "train: loss: 970293.0 acc: 0.9679115414619446  val: loss: 14305744.0 acc: 0.7898246049880981\n",
      "step: 5560\n",
      "train: loss: 1037540.625 acc: 0.9645463824272156  val: loss: 10117936.0 acc: 0.6194921731948853\n",
      "step: 5565\n",
      "train: loss: 625584.9375 acc: 0.8759874701499939  val: loss: 10953026.0 acc: 0.6247545480728149\n",
      "step: 5570\n",
      "train: loss: 534003.3125 acc: 0.9637237787246704  val: loss: 20110846.0 acc: 0.7604813575744629\n",
      "step: 5575\n",
      "train: loss: 806188.6875 acc: 0.9713664650917053  val: loss: 2779356.25 acc: 0.9217379689216614\n",
      "step: 5580\n",
      "train: loss: 315402.0625 acc: 0.8666727542877197  val: loss: 16311743.0 acc: 0.6990798711776733\n",
      "step: 5585\n",
      "train: loss: 311452.1875 acc: 0.8006024360656738  val: loss: 4808206.5 acc: 0.9442058205604553\n",
      "step: 5590\n",
      "train: loss: 705439.0625 acc: 0.9588475227355957  val: loss: 15733376.0 acc: 0.7258610725402832\n",
      "step: 5595\n",
      "train: loss: 636779.75 acc: 0.8485846519470215  val: loss: 1833543.625 acc: 0.8957995176315308\n",
      "step: 5600\n",
      "train: loss: 741742.9375 acc: 0.8572553396224976  val: loss: 4346918.5 acc: 0.924217939376831\n",
      "step: 5605\n",
      "train: loss: 787113.5625 acc: 0.8913377523422241  val: loss: 12836890.0 acc: -0.24870121479034424\n",
      "step: 5610\n",
      "train: loss: 1045730.5 acc: 0.8434467911720276  val: loss: 12615210.0 acc: 0.8101300001144409\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 5615\n",
      "train: loss: 915580.375 acc: 0.9592421650886536  val: loss: 12118554.0 acc: 0.8744749426841736\n",
      "step: 5620\n",
      "train: loss: 1155042.0 acc: 0.9502256512641907  val: loss: 7635360.0 acc: 0.8349745273590088\n",
      "step: 5625\n",
      "train: loss: 860538.0 acc: 0.9078525900840759  val: loss: 9137047.0 acc: 0.8979025483131409\n",
      "step: 5630\n",
      "train: loss: 762783.9375 acc: 0.9086992740631104  val: loss: 11194679.0 acc: 0.9233772158622742\n",
      "step: 5635\n",
      "train: loss: 661434.375 acc: 0.8960102200508118  val: loss: 5872868.5 acc: 0.9081462621688843\n",
      "step: 5640\n",
      "train: loss: 520224.34375 acc: 0.87042635679245  val: loss: 11162283.0 acc: 0.8918070793151855\n",
      "step: 5645\n",
      "train: loss: 459378.71875 acc: 0.8390116095542908  val: loss: 22835432.0 acc: 0.6477699875831604\n",
      "step: 5650\n",
      "train: loss: 731490.0 acc: 0.9172818064689636  val: loss: 15396972.0 acc: 0.7039711475372314\n",
      "step: 5655\n",
      "train: loss: 521099.90625 acc: 0.9097011685371399  val: loss: 29058186.0 acc: 0.770354151725769\n",
      "step: 5660\n",
      "train: loss: 395795.46875 acc: 0.9416286945343018  val: loss: 4838935.0 acc: 0.7643300890922546\n",
      "step: 5665\n",
      "train: loss: 1108445.875 acc: 0.9341046214103699  val: loss: 12649268.0 acc: 0.7466124296188354\n",
      "step: 5670\n",
      "train: loss: 1888196.375 acc: 0.9094382524490356  val: loss: 6742684.0 acc: 0.6007186770439148\n",
      "step: 5675\n",
      "train: loss: 1822229.25 acc: 0.9464160203933716  val: loss: 8193594.0 acc: 0.8685210943222046\n",
      "step: 5680\n",
      "train: loss: 1892180.375 acc: 0.8676615953445435  val: loss: 27735450.0 acc: 0.7470458745956421\n",
      "step: 5685\n",
      "train: loss: 2183768.75 acc: 0.9401887059211731  val: loss: 8061791.0 acc: 0.7403581142425537\n",
      "step: 5690\n",
      "train: loss: 1859059.125 acc: 0.9242660999298096  val: loss: 10652521.0 acc: 0.7970709204673767\n",
      "step: 5695\n",
      "train: loss: 1971485.875 acc: 0.9464840888977051  val: loss: 3446087.25 acc: 0.8924932479858398\n",
      "step: 5700\n",
      "train: loss: 2623279.5 acc: 0.8844704031944275  val: loss: 7212612.5 acc: 0.8268795013427734\n",
      "step: 5705\n",
      "train: loss: 1571649.375 acc: 0.8228068947792053  val: loss: 4508113.5 acc: 0.8716486692428589\n",
      "step: 5710\n",
      "train: loss: 1365781.125 acc: 0.9441024661064148  val: loss: 2222168.5 acc: 0.9456485509872437\n",
      "step: 5715\n",
      "train: loss: 1730831.375 acc: 0.963018000125885  val: loss: 3056329.25 acc: 0.9666346311569214\n",
      "step: 5720\n",
      "train: loss: 1280538.25 acc: 0.9577646851539612  val: loss: 3891591.75 acc: 0.8808205127716064\n",
      "step: 5725\n",
      "train: loss: 1256363.75 acc: 0.9814521670341492  val: loss: 7941807.5 acc: 0.7086292505264282\n",
      "step: 5730\n",
      "train: loss: 1877802.125 acc: 0.9688185453414917  val: loss: 42162148.0 acc: 0.2927306890487671\n",
      "step: 5735\n",
      "train: loss: 2598601.0 acc: 0.9123092889785767  val: loss: 1659674.25 acc: 0.9667712450027466\n",
      "step: 5740\n",
      "train: loss: 2319236.25 acc: 0.9462957978248596  val: loss: 6995012.0 acc: 0.7743762731552124\n",
      "step: 5745\n",
      "train: loss: 8297051.5 acc: 0.888071596622467  val: loss: 5465920.5 acc: 0.8270828127861023\n",
      "step: 5750\n",
      "train: loss: 6637441.0 acc: 0.9165158271789551  val: loss: 8020761.0 acc: 0.8727942705154419\n",
      "step: 5755\n",
      "train: loss: 3889692.75 acc: 0.8926653265953064  val: loss: 19977628.0 acc: -0.12189841270446777\n",
      "step: 5760\n",
      "train: loss: 11766131.0 acc: 0.8774310946464539  val: loss: 7591265.0 acc: 0.845270037651062\n",
      "step: 5765\n",
      "train: loss: 6684995.0 acc: 0.9356777667999268  val: loss: 12397281.0 acc: 0.9227330684661865\n",
      "step: 5770\n",
      "train: loss: 3600352.75 acc: 0.927094578742981  val: loss: 8804552.0 acc: 0.7744020819664001\n",
      "step: 5775\n",
      "train: loss: 10422352.0 acc: 0.903664767742157  val: loss: 3041484.5 acc: 0.9734140634536743\n",
      "step: 5780\n",
      "train: loss: 5513878.5 acc: 0.9642183780670166  val: loss: 7584384.0 acc: 0.8369377851486206\n",
      "step: 5785\n",
      "train: loss: 9434165.0 acc: 0.9474254846572876  val: loss: 9084389.0 acc: 0.8256316184997559\n",
      "step: 5790\n",
      "train: loss: 9441688.0 acc: 0.9445362687110901  val: loss: 2501981.5 acc: 0.9514719843864441\n",
      "step: 5795\n",
      "train: loss: 5328834.0 acc: 0.9692516922950745  val: loss: 6528332.0 acc: 0.7047958970069885\n",
      "step: 5800\n",
      "train: loss: 8394334.0 acc: 0.9411284923553467  val: loss: 6446390.0 acc: 0.9232808947563171\n",
      "step: 5805\n",
      "train: loss: 7510259.0 acc: 0.9576483964920044  val: loss: 25488558.0 acc: 0.5725175142288208\n",
      "step: 5810\n",
      "train: loss: 8926332.0 acc: 0.9306036233901978  val: loss: 4764320.5 acc: 0.8610285520553589\n",
      "step: 5815\n",
      "train: loss: 2766843.75 acc: 0.9194513559341431  val: loss: 3203408.0 acc: 0.9247934818267822\n",
      "step: 5820\n",
      "train: loss: 23417484.0 acc: 0.733401894569397  val: loss: 11780684.0 acc: 0.8555313348770142\n",
      "step: 5825\n",
      "train: loss: 19888044.0 acc: 0.8231559991836548  val: loss: 3770393.5 acc: 0.8587228059768677\n",
      "step: 5830\n",
      "train: loss: 7802564.0 acc: 0.9220798015594482  val: loss: 13280973.0 acc: 0.8275104761123657\n",
      "step: 5835\n",
      "train: loss: 11820035.0 acc: 0.8461670875549316  val: loss: 4713354.0 acc: 0.8570832014083862\n",
      "step: 5840\n",
      "train: loss: 4120797.5 acc: 0.8852360844612122  val: loss: 6246025.0 acc: 0.8707013130187988\n",
      "step: 5845\n",
      "train: loss: 11911841.0 acc: 0.7986040711402893  val: loss: 17557354.0 acc: 0.8280128240585327\n",
      "step: 5850\n",
      "train: loss: 7617622.5 acc: 0.6923638582229614  val: loss: 5408959.5 acc: 0.7988405227661133\n",
      "step: 5855\n",
      "train: loss: 9314941.0 acc: 0.8796676993370056  val: loss: 10810518.0 acc: 0.7422751784324646\n",
      "step: 5860\n",
      "train: loss: 6545135.0 acc: 0.7620798349380493  val: loss: 6717899.0 acc: 0.885330080986023\n",
      "step: 5865\n",
      "train: loss: 6059324.5 acc: 0.8387680649757385  val: loss: 12625803.0 acc: 0.7823629379272461\n",
      "step: 5870\n",
      "train: loss: 3328662.0 acc: 0.8515008687973022  val: loss: 10089519.0 acc: 0.8717256188392639\n",
      "step: 5875\n",
      "train: loss: 3484659.75 acc: 0.9320633411407471  val: loss: 6426364.5 acc: 0.8594433069229126\n",
      "step: 5880\n",
      "train: loss: 6727810.5 acc: 0.8310583829879761  val: loss: 4320011.0 acc: 0.907415509223938\n",
      "step: 5885\n",
      "train: loss: 737340.4375 acc: 0.9392491579055786  val: loss: 19331270.0 acc: 0.7047504782676697\n",
      "step: 5890\n",
      "train: loss: 4634551.0 acc: 0.8547353744506836  val: loss: 6157973.5 acc: 0.8486101031303406\n",
      "step: 5895\n",
      "train: loss: 7677995.5 acc: 0.47264569997787476  val: loss: 17311780.0 acc: 0.6217339038848877\n",
      "step: 5900\n",
      "train: loss: 2902961.25 acc: 0.9109225869178772  val: loss: 4091009.5 acc: 0.9136590361595154\n",
      "step: 5905\n",
      "train: loss: 2950626.75 acc: 0.8773108720779419  val: loss: 7704781.5 acc: 0.7450979948043823\n",
      "step: 5910\n",
      "train: loss: 3078593.0 acc: 0.8454576730728149  val: loss: 10371041.0 acc: 0.7435704469680786\n",
      "step: 5915\n",
      "train: loss: 4891086.0 acc: 0.7940475344657898  val: loss: 5160976.5 acc: 0.8644727468490601\n",
      "step: 5920\n",
      "train: loss: 2590261.25 acc: 0.8523578643798828  val: loss: 2756014.5 acc: 0.8483759760856628\n",
      "step: 5925\n",
      "train: loss: 1580250.0 acc: 0.8732821941375732  val: loss: 4649450.0 acc: 0.9135687351226807\n",
      "step: 5930\n",
      "train: loss: 3841698.0 acc: 0.8392347097396851  val: loss: 4161858.0 acc: 0.7804155349731445\n",
      "step: 5935\n",
      "train: loss: 8070612.0 acc: 0.7381072640419006  val: loss: 8031221.0 acc: 0.717012882232666\n",
      "step: 5940\n",
      "train: loss: 3661792.25 acc: 0.8085910081863403  val: loss: 11312011.0 acc: 0.8277400732040405\n",
      "step: 5945\n",
      "train: loss: 5424172.0 acc: 0.6659795641899109  val: loss: 10791846.0 acc: 0.8458546996116638\n",
      "step: 5950\n",
      "train: loss: 9489912.0 acc: 0.622821033000946  val: loss: 17557254.0 acc: 0.6641289591789246\n",
      "step: 5955\n",
      "train: loss: 4124600.5 acc: 0.7573980093002319  val: loss: 9939618.0 acc: 0.6798641085624695\n",
      "step: 5960\n",
      "train: loss: 2141170.0 acc: 0.834454357624054  val: loss: 20425656.0 acc: 0.6348008513450623\n",
      "step: 5965\n",
      "train: loss: 5064009.0 acc: 0.7787008285522461  val: loss: 9505712.0 acc: 0.7020462155342102\n",
      "step: 5970\n",
      "train: loss: 9369364.0 acc: 0.7742259502410889  val: loss: 4938803.0 acc: 0.7790036797523499\n",
      "step: 5975\n",
      "train: loss: 6731032.0 acc: 0.7720975279808044  val: loss: 26084896.0 acc: 0.6791197657585144\n",
      "step: 5980\n",
      "train: loss: 8327909.0 acc: 0.8921201825141907  val: loss: 6283416.0 acc: 0.7584131956100464\n",
      "step: 5985\n",
      "train: loss: 9674348.0 acc: 0.8757219314575195  val: loss: 6233439.0 acc: 0.8311463594436646\n",
      "step: 5990\n",
      "train: loss: 4177206.5 acc: 0.9328210353851318  val: loss: 28623292.0 acc: 0.7303926944732666\n",
      "step: 5995\n",
      "train: loss: 7088480.5 acc: 0.9143391847610474  val: loss: 18225496.0 acc: 0.7832579612731934\n",
      "step: 6000\n",
      "train: loss: 2455221.0 acc: 0.9501885175704956  val: loss: 9548303.0 acc: 0.6222094297409058\n",
      "step: 6005\n",
      "train: loss: 4617277.0 acc: 0.9127629995346069  val: loss: 8672305.0 acc: 0.7335116267204285\n",
      "step: 6010\n",
      "train: loss: 7492700.5 acc: 0.9000292420387268  val: loss: 11952238.0 acc: 0.5295425057411194\n",
      "step: 6015\n",
      "train: loss: 3992137.25 acc: 0.9234910011291504  val: loss: 4332265.0 acc: 0.869387686252594\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 6020\n",
      "train: loss: 9605481.0 acc: 0.8932979702949524  val: loss: 4009083.75 acc: 0.9072868227958679\n",
      "step: 6025\n",
      "train: loss: 2384784.5 acc: 0.9753333330154419  val: loss: 13564546.0 acc: 0.7547575831413269\n",
      "step: 6030\n",
      "train: loss: 8234923.0 acc: 0.8250411748886108  val: loss: 20577792.0 acc: 0.6985393762588501\n",
      "step: 6035\n",
      "train: loss: 1175709.0 acc: 0.9852148294448853  val: loss: 14726207.0 acc: 0.7247032523155212\n",
      "step: 6040\n",
      "train: loss: 1534759.375 acc: 0.9839689135551453  val: loss: 7534758.5 acc: 0.8863674402236938\n",
      "step: 6045\n",
      "train: loss: 1855182.375 acc: 0.9779819250106812  val: loss: 9225380.0 acc: 0.6587396264076233\n",
      "step: 6050\n",
      "train: loss: 1040550.875 acc: 0.9870163798332214  val: loss: 11443535.0 acc: 0.8243756294250488\n",
      "step: 6055\n",
      "train: loss: 702958.0 acc: 0.9850667715072632  val: loss: 11386750.0 acc: 0.7933704853057861\n",
      "step: 6060\n",
      "train: loss: 1455504.25 acc: 0.9629014134407043  val: loss: 16583302.0 acc: 0.8276205062866211\n",
      "step: 6065\n",
      "train: loss: 1230240.625 acc: 0.9261572957038879  val: loss: 2580346.25 acc: 0.8922064304351807\n",
      "step: 6070\n",
      "train: loss: 498788.3125 acc: 0.982111394405365  val: loss: 8026880.0 acc: 0.8042572736740112\n",
      "step: 6075\n",
      "train: loss: 1195542.125 acc: 0.9126571416854858  val: loss: 3219763.75 acc: 0.9419147968292236\n",
      "step: 6080\n",
      "train: loss: 762206.75 acc: 0.9680650234222412  val: loss: 14842567.0 acc: 0.6804958581924438\n",
      "step: 6085\n",
      "train: loss: 323774.03125 acc: 0.9798784852027893  val: loss: 12382847.0 acc: 0.8236851096153259\n",
      "step: 6090\n",
      "train: loss: 1078786.125 acc: 0.9002127647399902  val: loss: 6910616.5 acc: 0.6518996357917786\n",
      "step: 6095\n",
      "train: loss: 853327.9375 acc: 0.971015989780426  val: loss: 4234073.5 acc: 0.9123432040214539\n",
      "step: 6100\n",
      "train: loss: 662791.4375 acc: 0.7798307538032532  val: loss: 10222789.0 acc: 0.9030720591545105\n",
      "step: 6105\n",
      "train: loss: 308514.3125 acc: 0.9375371932983398  val: loss: 2812791.5 acc: 0.8900936245918274\n",
      "step: 6110\n",
      "train: loss: 295112.6875 acc: 0.9778371453285217  val: loss: 5662014.5 acc: 0.8065013289451599\n",
      "step: 6115\n",
      "train: loss: 419446.625 acc: 0.901381254196167  val: loss: 5129119.0 acc: 0.876158595085144\n",
      "step: 6120\n",
      "train: loss: 517769.3125 acc: 0.7092434763908386  val: loss: 31595036.0 acc: 0.7738357186317444\n",
      "step: 6125\n",
      "train: loss: 1651201.25 acc: 0.8556113243103027  val: loss: 7869057.5 acc: 0.6756494045257568\n",
      "step: 6130\n",
      "train: loss: 2034208.25 acc: 0.829585611820221  val: loss: 17641850.0 acc: 0.6429254412651062\n",
      "step: 6135\n",
      "train: loss: 903558.6875 acc: 0.8984222412109375  val: loss: 8725992.0 acc: 0.8220902681350708\n",
      "step: 6140\n",
      "train: loss: 3189433.75 acc: 0.8025040626525879  val: loss: 11825386.0 acc: 0.8332588076591492\n",
      "step: 6145\n",
      "train: loss: 826423.375 acc: 0.9563562870025635  val: loss: 27747058.0 acc: 0.7222440838813782\n",
      "step: 6150\n",
      "train: loss: 883329.125 acc: 0.9216029047966003  val: loss: 17478166.0 acc: 0.7878978252410889\n",
      "step: 6155\n",
      "train: loss: 721159.875 acc: 0.9192036390304565  val: loss: 15011593.0 acc: 0.8814708590507507\n",
      "step: 6160\n",
      "train: loss: 864325.4375 acc: 0.9609007835388184  val: loss: 7354384.0 acc: 0.88746577501297\n",
      "step: 6165\n",
      "train: loss: 859345.9375 acc: 0.856953501701355  val: loss: 15481962.0 acc: 0.6191566586494446\n",
      "step: 6170\n",
      "train: loss: 266295.40625 acc: 0.8845112919807434  val: loss: 13249753.0 acc: 0.8398163914680481\n",
      "step: 6175\n",
      "train: loss: 667515.5625 acc: 0.8363851308822632  val: loss: 14659632.0 acc: 0.7684842348098755\n",
      "step: 6180\n",
      "train: loss: 404937.90625 acc: 0.9561470746994019  val: loss: 7812291.5 acc: 0.7820770740509033\n",
      "step: 6185\n",
      "train: loss: 2661990.25 acc: 0.9103386998176575  val: loss: 9360019.0 acc: 0.5397862792015076\n",
      "step: 6190\n",
      "train: loss: 1039444.6875 acc: 0.9320439696311951  val: loss: 10616169.0 acc: 0.7433929443359375\n",
      "step: 6195\n",
      "train: loss: 1672212.75 acc: 0.9351401925086975  val: loss: 8961886.0 acc: 0.6410391926765442\n",
      "step: 6200\n",
      "train: loss: 1772457.25 acc: 0.9305073618888855  val: loss: 29745210.0 acc: 0.694767415523529\n",
      "step: 6205\n",
      "train: loss: 1540326.125 acc: 0.9542365074157715  val: loss: 17462932.0 acc: 0.36281466484069824\n",
      "step: 6210\n",
      "train: loss: 1571886.5 acc: 0.952620804309845  val: loss: 19091924.0 acc: 0.5505871772766113\n",
      "step: 6215\n",
      "train: loss: 1931222.625 acc: 0.930795431137085  val: loss: 13612480.0 acc: 0.6700455546379089\n",
      "step: 6220\n",
      "train: loss: 1271816.375 acc: 0.9505836963653564  val: loss: 5130240.5 acc: 0.5574604272842407\n",
      "step: 6225\n",
      "train: loss: 2077104.25 acc: 0.8875972032546997  val: loss: 11748737.0 acc: 0.6347885727882385\n",
      "step: 6230\n",
      "train: loss: 1491084.125 acc: 0.9321885108947754  val: loss: 6043984.5 acc: 0.9220059514045715\n",
      "step: 6235\n",
      "train: loss: 3734141.25 acc: 0.8857849836349487  val: loss: 4302113.0 acc: 0.8109887838363647\n",
      "step: 6240\n",
      "train: loss: 5151110.0 acc: 0.9008215069770813  val: loss: 12937874.0 acc: 0.311132550239563\n",
      "step: 6245\n",
      "train: loss: 1291933.125 acc: 0.9738672375679016  val: loss: 4209716.0 acc: 0.7912176847457886\n",
      "step: 6250\n",
      "train: loss: 1093797.875 acc: 0.9342474937438965  val: loss: 7912301.0 acc: 0.8779166340827942\n",
      "step: 6255\n",
      "train: loss: 1830097.875 acc: 0.9589973092079163  val: loss: 8976321.0 acc: 0.9199488162994385\n",
      "step: 6260\n",
      "train: loss: 6731262.5 acc: 0.8825143575668335  val: loss: 7328484.5 acc: 0.9059803485870361\n",
      "step: 6265\n",
      "train: loss: 3625032.0 acc: 0.9105528593063354  val: loss: 3757236.25 acc: 0.9484564661979675\n",
      "step: 6270\n",
      "train: loss: 4526488.0 acc: 0.920457124710083  val: loss: 11635020.0 acc: 0.7855319976806641\n",
      "step: 6275\n",
      "train: loss: 8484513.0 acc: 0.861842930316925  val: loss: 9357916.0 acc: 0.9139463901519775\n",
      "step: 6280\n",
      "train: loss: 8048071.5 acc: 0.8585667610168457  val: loss: 2754201.25 acc: 0.9576379060745239\n",
      "step: 6285\n",
      "train: loss: 8332784.5 acc: 0.9088895916938782  val: loss: 12231497.0 acc: 0.5458556413650513\n",
      "step: 6290\n",
      "train: loss: 3899778.25 acc: 0.8953333497047424  val: loss: 5232134.0 acc: 0.8361126184463501\n",
      "step: 6295\n",
      "train: loss: 3474783.5 acc: 0.9545185565948486  val: loss: 2219459.0 acc: 0.925936758518219\n",
      "step: 6300\n",
      "train: loss: 5838183.5 acc: 0.9619836211204529  val: loss: 2533957.25 acc: 0.8521348237991333\n",
      "step: 6305\n",
      "train: loss: 12277336.0 acc: 0.9397857189178467  val: loss: 5078764.0 acc: 0.9165451526641846\n",
      "step: 6310\n",
      "train: loss: 11130081.0 acc: 0.9530680179595947  val: loss: 25314162.0 acc: -0.2164473533630371\n",
      "step: 6315\n",
      "train: loss: 10602008.0 acc: 0.955875813961029  val: loss: 7340151.0 acc: 0.8864235281944275\n",
      "step: 6320\n",
      "train: loss: 4131409.75 acc: 0.9478668570518494  val: loss: 3042725.25 acc: 0.9558124542236328\n",
      "step: 6325\n",
      "train: loss: 9427190.0 acc: 0.9514049291610718  val: loss: 7383501.0 acc: 0.89485102891922\n",
      "step: 6330\n",
      "train: loss: 6779116.0 acc: 0.912329912185669  val: loss: 2619998.5 acc: 0.915732741355896\n",
      "step: 6335\n",
      "train: loss: 3886927.75 acc: 0.8831424117088318  val: loss: 10998160.0 acc: 0.5324302911758423\n",
      "step: 6340\n",
      "train: loss: 14912941.0 acc: 0.8847193121910095  val: loss: 11953035.0 acc: 0.7696492075920105\n",
      "step: 6345\n",
      "train: loss: 8627814.0 acc: 0.9371956586837769  val: loss: 6755308.0 acc: 0.9286183714866638\n",
      "step: 6350\n",
      "train: loss: 11301162.0 acc: 0.8431853652000427  val: loss: 4180013.75 acc: 0.9463398456573486\n",
      "step: 6355\n",
      "train: loss: 3424796.5 acc: 0.9636614322662354  val: loss: 3284987.25 acc: 0.9369851350784302\n",
      "step: 6360\n",
      "train: loss: 4194648.5 acc: 0.8720890879631042  val: loss: 6304307.0 acc: 0.8651435375213623\n",
      "step: 6365\n",
      "train: loss: 6265497.5 acc: 0.9001907706260681  val: loss: 7971894.0 acc: 0.9146824479103088\n",
      "step: 6370\n",
      "train: loss: 4366886.5 acc: 0.8356513977050781  val: loss: 10888140.0 acc: 0.8062299489974976\n",
      "step: 6375\n",
      "train: loss: 17163154.0 acc: 0.6988636255264282  val: loss: 9814410.0 acc: 0.8621319532394409\n",
      "step: 6380\n",
      "train: loss: 4531476.5 acc: 0.8318564295768738  val: loss: 2342357.75 acc: 0.8474845886230469\n",
      "step: 6385\n",
      "train: loss: 2844591.0 acc: 0.820883572101593  val: loss: 9348308.0 acc: 0.8466715812683105\n",
      "step: 6390\n",
      "train: loss: 15668795.0 acc: 0.6866478323936462  val: loss: 5569529.5 acc: 0.9047725796699524\n",
      "step: 6395\n",
      "train: loss: 3256514.0 acc: 0.8379097580909729  val: loss: 5892229.5 acc: 0.9314476847648621\n",
      "step: 6400\n",
      "train: loss: 1567545.375 acc: 0.9582353830337524  val: loss: 9823930.0 acc: 0.8559048175811768\n",
      "step: 6405\n",
      "train: loss: 1370419.875 acc: 0.8919426202774048  val: loss: 11046529.0 acc: 0.8859221339225769\n",
      "step: 6410\n",
      "train: loss: 4409689.0 acc: 0.8035358786582947  val: loss: 1827667.0 acc: 0.965571403503418\n",
      "step: 6415\n",
      "train: loss: 2564141.75 acc: 0.8567740321159363  val: loss: 5449122.5 acc: 0.8489778637886047\n",
      "step: 6420\n",
      "train: loss: 3569602.25 acc: 0.8527480363845825  val: loss: 7207410.0 acc: 0.9207852482795715\n",
      "step: 6425\n",
      "train: loss: 4904968.5 acc: 0.9016443490982056  val: loss: 7341437.0 acc: 0.9111398458480835\n",
      "step: 6430\n",
      "train: loss: 6415199.5 acc: 0.6439306735992432  val: loss: 8843932.0 acc: 0.7732688188552856\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 6435\n",
      "train: loss: 6918967.0 acc: 0.8120609521865845  val: loss: 9179123.0 acc: 0.8546245098114014\n",
      "step: 6440\n",
      "train: loss: 1528833.875 acc: 0.8831387758255005  val: loss: 10812325.0 acc: 0.8111090660095215\n",
      "step: 6445\n",
      "train: loss: 2528189.25 acc: 0.860558271408081  val: loss: 7185568.0 acc: 0.8906651735305786\n",
      "step: 6450\n",
      "train: loss: 5324575.0 acc: 0.7559018135070801  val: loss: 5856128.5 acc: 0.894184947013855\n",
      "step: 6455\n",
      "train: loss: 3738796.5 acc: 0.8341794013977051  val: loss: 13627700.0 acc: 0.8393884897232056\n",
      "step: 6460\n",
      "train: loss: 5530144.5 acc: 0.7024906277656555  val: loss: 12018142.0 acc: 0.6860060691833496\n",
      "step: 6465\n",
      "train: loss: 8193760.0 acc: 0.7180559635162354  val: loss: 6446013.0 acc: 0.8757930397987366\n",
      "step: 6470\n",
      "train: loss: 8240174.0 acc: 0.6561610698699951  val: loss: 11088398.0 acc: 0.7432671189308167\n",
      "step: 6475\n",
      "train: loss: 4509760.0 acc: 0.7580910325050354  val: loss: 13370467.0 acc: 0.7070577144622803\n",
      "step: 6480\n",
      "train: loss: 3972854.0 acc: 0.8236646056175232  val: loss: 4297927.5 acc: 0.926815927028656\n",
      "step: 6485\n",
      "train: loss: 3245525.25 acc: 0.8009674549102783  val: loss: 10473958.0 acc: 0.8465515375137329\n",
      "step: 6490\n",
      "train: loss: 10412147.0 acc: 0.7074131965637207  val: loss: 1614126.875 acc: 0.8587298393249512\n",
      "step: 6495\n",
      "train: loss: 13443662.0 acc: 0.7195868492126465  val: loss: 17454192.0 acc: 0.6775572896003723\n",
      "step: 6500\n",
      "train: loss: 8447390.0 acc: 0.905011773109436  val: loss: 10592518.0 acc: 0.857169508934021\n",
      "step: 6505\n",
      "train: loss: 6557454.5 acc: 0.8951870799064636  val: loss: 4814837.5 acc: 0.9084545373916626\n",
      "step: 6510\n",
      "train: loss: 10843912.0 acc: 0.8204861879348755  val: loss: 19494876.0 acc: 0.7319454550743103\n",
      "step: 6515\n",
      "train: loss: 5479985.5 acc: 0.8895736932754517  val: loss: 3819993.0 acc: 0.9113636612892151\n",
      "step: 6520\n",
      "train: loss: 6877971.0 acc: 0.7712857127189636  val: loss: 6236773.0 acc: 0.1248694658279419\n",
      "step: 6525\n",
      "train: loss: 9129874.0 acc: 0.7941672205924988  val: loss: 7946439.0 acc: 0.5908769369125366\n",
      "step: 6530\n",
      "train: loss: 6353966.5 acc: 0.8476420640945435  val: loss: 10170361.0 acc: 0.6267339587211609\n",
      "step: 6535\n",
      "train: loss: 2692549.0 acc: 0.9708760380744934  val: loss: 18785992.0 acc: 0.371451735496521\n",
      "step: 6540\n",
      "train: loss: 1413645.125 acc: 0.9825237989425659  val: loss: 20529272.0 acc: 0.867760419845581\n",
      "step: 6545\n",
      "train: loss: 4125194.0 acc: 0.9463418126106262  val: loss: 16848278.0 acc: 0.8415266275405884\n",
      "step: 6550\n",
      "train: loss: 1868047.125 acc: 0.9819927215576172  val: loss: 13588250.0 acc: 0.7650851011276245\n",
      "step: 6555\n",
      "train: loss: 2108651.0 acc: 0.9509433507919312  val: loss: 3029166.0 acc: 0.9529454112052917\n",
      "step: 6560\n",
      "train: loss: 2505726.75 acc: 0.9411155581474304  val: loss: 6091578.5 acc: 0.8804633021354675\n",
      "step: 6565\n",
      "train: loss: 4362510.5 acc: 0.9147089719772339  val: loss: 4208969.5 acc: 0.885114312171936\n",
      "step: 6570\n",
      "train: loss: 1341999.375 acc: 0.9704954028129578  val: loss: 16067388.0 acc: 0.8377113938331604\n",
      "step: 6575\n",
      "train: loss: 393579.0 acc: 0.990888774394989  val: loss: 10333979.0 acc: 0.5312134027481079\n",
      "step: 6580\n",
      "train: loss: 945412.3125 acc: 0.6740421652793884  val: loss: 8482711.0 acc: 0.854844868183136\n",
      "step: 6585\n",
      "train: loss: 443488.25 acc: 0.9754323363304138  val: loss: 18406876.0 acc: 0.7583258152008057\n",
      "step: 6590\n",
      "train: loss: 2391191.25 acc: 0.9150272607803345  val: loss: 5127013.5 acc: 0.9000749588012695\n",
      "step: 6595\n",
      "train: loss: 542030.1875 acc: 0.9468054175376892  val: loss: 5412478.0 acc: 0.9080311059951782\n",
      "step: 6600\n",
      "train: loss: 1339784.375 acc: 0.9087724089622498  val: loss: 17088000.0 acc: 0.7474929094314575\n",
      "step: 6605\n",
      "train: loss: 279143.4375 acc: 0.9091584086418152  val: loss: 13800059.0 acc: 0.629226803779602\n",
      "step: 6610\n",
      "train: loss: 732886.0 acc: 0.9692339897155762  val: loss: 18701002.0 acc: 0.5832052230834961\n",
      "step: 6615\n",
      "train: loss: 241041.796875 acc: 0.8765907287597656  val: loss: 8384341.0 acc: 0.7721704840660095\n",
      "step: 6620\n",
      "train: loss: 1331532.25 acc: 0.9172716736793518  val: loss: 10307785.0 acc: 0.7936992645263672\n",
      "step: 6625\n",
      "train: loss: 315605.90625 acc: 0.8556299209594727  val: loss: 16780372.0 acc: 0.7105703949928284\n",
      "step: 6630\n",
      "train: loss: 323081.90625 acc: 0.8734837770462036  val: loss: 9803790.0 acc: 0.8579162359237671\n",
      "step: 6635\n",
      "train: loss: 400263.1875 acc: 0.9322829246520996  val: loss: 18084680.0 acc: 0.5385562181472778\n",
      "step: 6640\n",
      "train: loss: 541826.1875 acc: 0.8282347321510315  val: loss: 6905424.5 acc: 0.8054701089859009\n",
      "step: 6645\n",
      "train: loss: 594655.875 acc: 0.8399907350540161  val: loss: 12992554.0 acc: 0.4783669710159302\n",
      "step: 6650\n",
      "train: loss: 1767849.25 acc: 0.8660367727279663  val: loss: 12616732.0 acc: 0.668394148349762\n",
      "step: 6655\n",
      "train: loss: 632853.0 acc: 0.9626960754394531  val: loss: 6667210.0 acc: 0.8976068496704102\n",
      "step: 6660\n",
      "train: loss: 1598030.375 acc: 0.8482087254524231  val: loss: 5764777.0 acc: 0.8291831016540527\n",
      "step: 6665\n",
      "train: loss: 780066.375 acc: 0.9362010359764099  val: loss: 5530075.0 acc: 0.7313339114189148\n",
      "step: 6670\n",
      "train: loss: 653597.4375 acc: 0.9691689014434814  val: loss: 7671035.5 acc: 0.8161763548851013\n",
      "step: 6675\n",
      "train: loss: 607358.5 acc: 0.9172834753990173  val: loss: 13249558.0 acc: 0.4017707109451294\n",
      "step: 6680\n",
      "train: loss: 1039097.4375 acc: 0.896274209022522  val: loss: 4220846.5 acc: 0.8909415006637573\n",
      "step: 6685\n",
      "train: loss: 616640.125 acc: 0.931259274482727  val: loss: 18266724.0 acc: 0.7923054695129395\n",
      "step: 6690\n",
      "train: loss: 412848.71875 acc: 0.8983196020126343  val: loss: 2991140.5 acc: 0.9021270275115967\n",
      "step: 6695\n",
      "train: loss: 282384.40625 acc: 0.9067838788032532  val: loss: 9742663.0 acc: 0.8046438097953796\n",
      "step: 6700\n",
      "train: loss: 605381.25 acc: 0.9282143115997314  val: loss: 5442718.0 acc: 0.9366246461868286\n",
      "step: 6705\n",
      "train: loss: 1465289.0 acc: 0.9384364485740662  val: loss: 19810072.0 acc: 0.05400407314300537\n",
      "step: 6710\n",
      "train: loss: 1272588.125 acc: 0.9186641573905945  val: loss: 2074617.75 acc: 0.9362590312957764\n",
      "step: 6715\n",
      "train: loss: 1289927.0 acc: 0.9317890405654907  val: loss: 5635822.5 acc: 0.8547310829162598\n",
      "step: 6720\n",
      "train: loss: 824426.625 acc: 0.935198962688446  val: loss: 22706926.0 acc: 0.46771180629730225\n",
      "step: 6725\n",
      "train: loss: 975319.0 acc: 0.9552242755889893  val: loss: 11493993.0 acc: 0.7294138669967651\n",
      "step: 6730\n",
      "train: loss: 2061623.125 acc: 0.9385402202606201  val: loss: 7423970.0 acc: 0.8974895477294922\n",
      "step: 6735\n",
      "train: loss: 2129474.75 acc: 0.945978581905365  val: loss: 14145483.0 acc: 0.7654379606246948\n",
      "step: 6740\n",
      "train: loss: 1556654.5 acc: 0.9445182085037231  val: loss: 19853638.0 acc: 0.6431360840797424\n",
      "step: 6745\n",
      "train: loss: 2727593.25 acc: 0.8420464992523193  val: loss: 9090482.0 acc: 0.8307968378067017\n",
      "step: 6750\n",
      "train: loss: 2635708.75 acc: 0.8980273008346558  val: loss: 12066187.0 acc: 0.529077410697937\n",
      "step: 6755\n",
      "train: loss: 4080378.5 acc: 0.9090040326118469  val: loss: 26646596.0 acc: 0.6865477561950684\n",
      "step: 6760\n",
      "train: loss: 1062501.625 acc: 0.9802902936935425  val: loss: 28854632.0 acc: 0.1751265525817871\n",
      "step: 6765\n",
      "train: loss: 2420226.75 acc: 0.9692316055297852  val: loss: 2505861.75 acc: 0.9476630687713623\n",
      "step: 6770\n",
      "train: loss: 3372438.0 acc: 0.9434084892272949  val: loss: 7105303.5 acc: 0.40021049976348877\n",
      "step: 6775\n",
      "train: loss: 587380.875 acc: 0.9863981604576111  val: loss: 7291636.0 acc: 0.8630428314208984\n",
      "step: 6780\n",
      "train: loss: 2120301.75 acc: 0.9536604881286621  val: loss: 21827658.0 acc: 0.5161367058753967\n",
      "step: 6785\n",
      "train: loss: 2534875.5 acc: 0.9088878631591797  val: loss: 13464985.0 acc: 0.7666141390800476\n",
      "step: 6790\n",
      "train: loss: 5330701.5 acc: 0.8790169954299927  val: loss: 2480518.25 acc: 0.9575748443603516\n",
      "step: 6795\n",
      "train: loss: 7373866.5 acc: 0.8894145488739014  val: loss: 5264118.5 acc: 0.8302634954452515\n",
      "step: 6800\n",
      "train: loss: 5733717.5 acc: 0.8949005007743835  val: loss: 2974877.5 acc: 0.8277494311332703\n",
      "step: 6805\n",
      "train: loss: 11750352.0 acc: 0.8200116753578186  val: loss: 4752932.5 acc: 0.9020252227783203\n",
      "step: 6810\n",
      "train: loss: 5621795.0 acc: 0.9006587862968445  val: loss: 4653347.0 acc: 0.8440132737159729\n",
      "step: 6815\n",
      "train: loss: 11411148.0 acc: 0.8829860687255859  val: loss: 5908118.5 acc: 0.9308505058288574\n",
      "step: 6820\n",
      "train: loss: 6697141.0 acc: 0.9665066003799438  val: loss: 5596312.5 acc: 0.92110276222229\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 6825\n",
      "train: loss: 15687141.0 acc: 0.9338832497596741  val: loss: 7621086.5 acc: 0.8911041617393494\n",
      "step: 6830\n",
      "train: loss: 8253907.5 acc: 0.9602391123771667  val: loss: 12841391.0 acc: 0.7536427974700928\n",
      "step: 6835\n",
      "train: loss: 7867224.5 acc: 0.931713342666626  val: loss: 6665250.0 acc: 0.9276617169380188\n",
      "step: 6840\n",
      "train: loss: 3229636.0 acc: 0.9709907174110413  val: loss: 4271252.5 acc: 0.8662428855895996\n",
      "step: 6845\n",
      "train: loss: 3820040.5 acc: 0.9623333215713501  val: loss: 7736927.5 acc: 0.7701573371887207\n",
      "step: 6850\n",
      "train: loss: 4382380.5 acc: 0.96840500831604  val: loss: 4047877.5 acc: 0.7438144087791443\n",
      "step: 6855\n",
      "train: loss: 2390416.0 acc: 0.9256863594055176  val: loss: 5324106.5 acc: 0.8995338678359985\n",
      "step: 6860\n",
      "train: loss: 15131120.0 acc: 0.4547160863876343  val: loss: 6253542.5 acc: 0.8198787569999695\n",
      "step: 6865\n",
      "train: loss: 8744507.0 acc: 0.6976174712181091  val: loss: 11048099.0 acc: 0.8714687824249268\n",
      "step: 6870\n",
      "train: loss: 7738026.5 acc: 0.37472498416900635  val: loss: 9436360.0 acc: 0.9122281670570374\n",
      "step: 6875\n",
      "train: loss: 5874558.0 acc: 0.8346982002258301  val: loss: 14610082.0 acc: 0.8191106915473938\n",
      "step: 6880\n",
      "train: loss: 4661064.5 acc: 0.6593513488769531  val: loss: 4745416.0 acc: 0.861525297164917\n",
      "step: 6885\n",
      "train: loss: 3619391.25 acc: 0.9308659434318542  val: loss: 2690141.25 acc: 0.8835350275039673\n",
      "step: 6890\n",
      "train: loss: 12936874.0 acc: 0.6405491828918457  val: loss: 8172854.5 acc: 0.7719259858131409\n",
      "step: 6895\n",
      "train: loss: 4268486.5 acc: 0.9236724972724915  val: loss: 4326095.0 acc: 0.8086363077163696\n",
      "step: 6900\n",
      "train: loss: 8003677.5 acc: 0.781904935836792  val: loss: 5607157.5 acc: 0.8787475824356079\n",
      "step: 6905\n",
      "train: loss: 2358222.75 acc: 0.8177618384361267  val: loss: 4489687.5 acc: 0.8018498420715332\n",
      "step: 6910\n",
      "train: loss: 4899294.0 acc: 0.9035791754722595  val: loss: 13428189.0 acc: 0.8797078132629395\n",
      "step: 6915\n",
      "train: loss: 3109843.75 acc: 0.8476109504699707  val: loss: 6645460.0 acc: 0.7473479509353638\n",
      "step: 6920\n",
      "train: loss: 4270865.5 acc: 0.9026460647583008  val: loss: 29336984.0 acc: 0.6956786513328552\n",
      "step: 6925\n",
      "train: loss: 3249938.75 acc: 0.8336440920829773  val: loss: 12016903.0 acc: 0.7121803760528564\n",
      "step: 6930\n",
      "train: loss: 2007267.625 acc: 0.8999425172805786  val: loss: 4524165.5 acc: 0.7830762267112732\n",
      "step: 6935\n",
      "train: loss: 2640192.0 acc: 0.8709005117416382  val: loss: 15300003.0 acc: 0.8110767602920532\n",
      "step: 6940\n",
      "train: loss: 5346761.5 acc: 0.8148869276046753  val: loss: 5016198.5 acc: 0.9080755114555359\n",
      "step: 6945\n",
      "train: loss: 2190087.5 acc: 0.8782989382743835  val: loss: 4188428.5 acc: 0.9583722352981567\n",
      "step: 6950\n",
      "train: loss: 2285623.5 acc: 0.9562034606933594  val: loss: 13733560.0 acc: 0.7357689142227173\n",
      "step: 6955\n",
      "train: loss: 1495762.875 acc: 0.9511797428131104  val: loss: 2301151.75 acc: 0.9218061566352844\n",
      "step: 6960\n",
      "train: loss: 3552734.25 acc: 0.7666556239128113  val: loss: 2251570.25 acc: 0.8289675712585449\n",
      "step: 6965\n",
      "train: loss: 5196146.0 acc: 0.8291537761688232  val: loss: 7052205.5 acc: 0.8597313761711121\n",
      "step: 6970\n",
      "train: loss: 8411745.0 acc: 0.7526562809944153  val: loss: 3640434.25 acc: 0.8323174715042114\n",
      "step: 6975\n",
      "train: loss: 6578670.0 acc: 0.7849681377410889  val: loss: 4846350.0 acc: 0.9314936995506287\n",
      "step: 6980\n",
      "train: loss: 4969824.5 acc: 0.774781346321106  val: loss: 6932028.5 acc: 0.8128308057785034\n",
      "step: 6985\n",
      "train: loss: 12146952.0 acc: 0.016125380992889404  val: loss: 7622941.0 acc: 0.7147654891014099\n",
      "step: 6990\n",
      "train: loss: 6014824.5 acc: 0.733214259147644  val: loss: 8272954.0 acc: 0.9210727214813232\n",
      "step: 6995\n",
      "train: loss: 5163334.0 acc: 0.7552378177642822  val: loss: 11370712.0 acc: 0.8310591578483582\n",
      "step: 7000\n",
      "train: loss: 6285656.5 acc: 0.6813493967056274  val: loss: 3308543.75 acc: 0.8951901197433472\n",
      "step: 7005\n",
      "train: loss: 5436212.0 acc: 0.5196120142936707  val: loss: 8637229.0 acc: 0.7554621696472168\n",
      "step: 7010\n",
      "train: loss: 10837624.0 acc: 0.7855479717254639  val: loss: 8056839.5 acc: 0.8295222520828247\n",
      "step: 7015\n",
      "train: loss: 10639785.0 acc: 0.431640088558197  val: loss: 3438216.25 acc: 0.9391606450080872\n",
      "step: 7020\n",
      "train: loss: 10672518.0 acc: 0.8696072101593018  val: loss: 6646366.0 acc: 0.8997732996940613\n",
      "step: 7025\n",
      "train: loss: 6030954.0 acc: 0.9274318814277649  val: loss: 13821419.0 acc: 0.7120133638381958\n",
      "step: 7030\n",
      "train: loss: 12294570.0 acc: 0.667453408241272  val: loss: 7972369.5 acc: 0.8146865963935852\n",
      "step: 7035\n",
      "train: loss: 7661013.5 acc: 0.8735374212265015  val: loss: 15317906.0 acc: 0.838599681854248\n",
      "step: 7040\n",
      "train: loss: 7398358.5 acc: 0.8314008116722107  val: loss: 18321614.0 acc: 0.6679092049598694\n",
      "step: 7045\n",
      "train: loss: 2111694.5 acc: 0.9442430138587952  val: loss: 12900347.0 acc: 0.8491566181182861\n",
      "step: 7050\n",
      "train: loss: 5037191.5 acc: 0.9138277769088745  val: loss: 5277866.5 acc: 0.663490355014801\n",
      "step: 7055\n",
      "train: loss: 2836618.0 acc: 0.9615761041641235  val: loss: 13309041.0 acc: 0.8458787202835083\n",
      "step: 7060\n",
      "train: loss: 1237042.5 acc: 0.9881070852279663  val: loss: 8201961.5 acc: 0.6039160490036011\n",
      "step: 7065\n",
      "train: loss: 2088960.0 acc: 0.9680251479148865  val: loss: 19043536.0 acc: 0.7249795198440552\n",
      "step: 7070\n",
      "train: loss: 5157514.0 acc: 0.9187285900115967  val: loss: 6521911.0 acc: 0.8796180486679077\n",
      "step: 7075\n",
      "train: loss: 1240872.125 acc: 0.9760473370552063  val: loss: 1708311.75 acc: 0.955023467540741\n",
      "step: 7080\n",
      "train: loss: 1960119.875 acc: 0.9563336968421936  val: loss: 21881682.0 acc: 0.7748333215713501\n",
      "step: 7085\n",
      "train: loss: 1909284.25 acc: 0.9499773979187012  val: loss: 3234752.25 acc: 0.9057132601737976\n",
      "step: 7090\n",
      "train: loss: 277782.1875 acc: 0.9895092844963074  val: loss: 3345383.75 acc: 0.8627070784568787\n",
      "step: 7095\n",
      "train: loss: 4347400.5 acc: 0.932961106300354  val: loss: 15088894.0 acc: 0.8698604106903076\n",
      "step: 7100\n",
      "train: loss: 624963.375 acc: 0.8956118226051331  val: loss: 9941015.0 acc: 0.7203192710876465\n",
      "step: 7105\n",
      "train: loss: 240927.53125 acc: 0.9911229014396667  val: loss: 1278126.75 acc: 0.9510100483894348\n",
      "step: 7110\n",
      "train: loss: 664234.1875 acc: 0.983171820640564  val: loss: 6764635.0 acc: 0.903204619884491\n",
      "step: 7115\n",
      "train: loss: 996926.1875 acc: 0.9551171064376831  val: loss: 7575247.0 acc: 0.9194635152816772\n",
      "step: 7120\n",
      "train: loss: 512588.40625 acc: 0.9694490432739258  val: loss: 5725997.0 acc: 0.7668704986572266\n",
      "step: 7125\n",
      "train: loss: 653559.625 acc: 0.9751710295677185  val: loss: 20378476.0 acc: 0.8299967646598816\n",
      "step: 7130\n",
      "train: loss: 920840.125 acc: 0.8717296719551086  val: loss: 9053149.0 acc: 0.750294029712677\n",
      "step: 7135\n",
      "train: loss: 572149.125 acc: 0.8591846823692322  val: loss: 1958300.0 acc: 0.9363464117050171\n",
      "step: 7140\n",
      "train: loss: 396612.90625 acc: 0.9748557806015015  val: loss: 6867602.0 acc: 0.8793711066246033\n",
      "step: 7145\n",
      "train: loss: 1226001.75 acc: 0.9275907874107361  val: loss: 6604025.5 acc: 0.938027560710907\n",
      "step: 7150\n",
      "train: loss: 272618.71875 acc: 0.9805759787559509  val: loss: 13972779.0 acc: 0.7625275254249573\n",
      "step: 7155\n",
      "train: loss: 339831.1875 acc: 0.8978812098503113  val: loss: 5680891.0 acc: 0.9098182916641235\n",
      "step: 7160\n",
      "train: loss: 301620.375 acc: 0.9388126134872437  val: loss: 6114695.0 acc: 0.7353137135505676\n",
      "step: 7165\n",
      "train: loss: 394355.4375 acc: 0.9684793949127197  val: loss: 10387825.0 acc: 0.8679959774017334\n",
      "step: 7170\n",
      "train: loss: 1093767.75 acc: 0.9345636963844299  val: loss: 8577512.0 acc: 0.7661526203155518\n",
      "step: 7175\n",
      "train: loss: 1033047.375 acc: 0.9322167634963989  val: loss: 18956768.0 acc: 0.6158958077430725\n",
      "step: 7180\n",
      "train: loss: 657259.5 acc: 0.9809053540229797  val: loss: 14624723.0 acc: 0.39307355880737305\n",
      "step: 7185\n",
      "train: loss: 791832.4375 acc: 0.9515934586524963  val: loss: 21260120.0 acc: 0.633821427822113\n",
      "step: 7190\n",
      "train: loss: 776898.5625 acc: 0.9444676637649536  val: loss: 12368440.0 acc: 0.6634731888771057\n",
      "step: 7195\n",
      "train: loss: 732271.875 acc: 0.958490252494812  val: loss: 14762747.0 acc: 0.4174644947052002\n",
      "step: 7200\n",
      "train: loss: 817772.5 acc: 0.9202765226364136  val: loss: 20380590.0 acc: 0.6697165369987488\n",
      "step: 7205\n",
      "train: loss: 419339.3125 acc: 0.8580813407897949  val: loss: 9035060.0 acc: 0.7571365833282471\n",
      "step: 7210\n",
      "train: loss: 362291.65625 acc: 0.873935878276825  val: loss: 16362099.0 acc: 0.7346748113632202\n",
      "step: 7215\n",
      "train: loss: 177425.21875 acc: 0.9191981554031372  val: loss: 16827598.0 acc: 0.7513304352760315\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 7220\n",
      "train: loss: 1022052.0625 acc: 0.9370733499526978  val: loss: 12913601.0 acc: 0.4476943016052246\n",
      "step: 7225\n",
      "train: loss: 1008515.5 acc: 0.9566592574119568  val: loss: 1819868.125 acc: 0.8995663523674011\n",
      "step: 7230\n",
      "train: loss: 865234.6875 acc: 0.9602705240249634  val: loss: 10869647.0 acc: 0.8095850944519043\n",
      "step: 7235\n",
      "train: loss: 1450065.125 acc: 0.9479435682296753  val: loss: 10831492.0 acc: 0.5797466039657593\n",
      "step: 7240\n",
      "train: loss: 1196297.75 acc: 0.9134348034858704  val: loss: 12828449.0 acc: -0.19391846656799316\n",
      "step: 7245\n",
      "train: loss: 862505.3125 acc: 0.9505547881126404  val: loss: 18855534.0 acc: 0.7813237905502319\n",
      "step: 7250\n",
      "train: loss: 986635.875 acc: 0.9615681171417236  val: loss: 23852136.0 acc: 0.5909209251403809\n",
      "step: 7255\n",
      "train: loss: 1207283.25 acc: 0.9483794569969177  val: loss: 6944344.5 acc: 0.8843712210655212\n",
      "step: 7260\n",
      "train: loss: 789037.3125 acc: 0.9752949476242065  val: loss: 9773797.0 acc: 0.7971609830856323\n",
      "step: 7265\n",
      "train: loss: 1101365.75 acc: 0.8924360275268555  val: loss: 3804424.0 acc: 0.9604675769805908\n",
      "step: 7270\n",
      "train: loss: 1493083.25 acc: 0.9479899406433105  val: loss: 7734057.5 acc: 0.7367740869522095\n",
      "step: 7275\n",
      "train: loss: 1424701.125 acc: 0.9751867651939392  val: loss: 4730905.5 acc: 0.6778073310852051\n",
      "step: 7280\n",
      "train: loss: 2025971.75 acc: 0.9656651020050049  val: loss: 13708324.0 acc: 0.7460635900497437\n",
      "step: 7285\n",
      "train: loss: 1251598.875 acc: 0.9769508838653564  val: loss: 20109052.0 acc: 0.2648683190345764\n",
      "step: 7290\n",
      "train: loss: 2903421.75 acc: 0.9428597092628479  val: loss: 5450813.0 acc: 0.9166619181632996\n",
      "step: 7295\n",
      "train: loss: 851141.6875 acc: 0.9788523316383362  val: loss: 4695455.0 acc: 0.8341314792633057\n",
      "step: 7300\n",
      "train: loss: 4482673.5 acc: 0.9321057796478271  val: loss: 1754459.875 acc: 0.9571782350540161\n",
      "step: 7305\n",
      "train: loss: 5803877.5 acc: 0.8762480020523071  val: loss: 8764777.0 acc: 0.8625907897949219\n",
      "step: 7310\n",
      "train: loss: 11558723.0 acc: 0.8439891338348389  val: loss: 39585228.0 acc: -0.5888049602508545\n",
      "step: 7315\n",
      "train: loss: 6586975.5 acc: 0.7587462067604065  val: loss: 6721205.5 acc: 0.8915417790412903\n",
      "step: 7320\n",
      "train: loss: 6721571.5 acc: 0.892405092716217  val: loss: 7053179.0 acc: 0.9337611198425293\n",
      "step: 7325\n",
      "train: loss: 6082929.5 acc: 0.8508235812187195  val: loss: 10709012.0 acc: 0.8788368701934814\n",
      "step: 7330\n",
      "train: loss: 4787040.5 acc: 0.9082133173942566  val: loss: 4724818.0 acc: 0.9458371996879578\n",
      "step: 7335\n",
      "train: loss: 6777049.5 acc: 0.9467291831970215  val: loss: 18621672.0 acc: 0.7431343197822571\n",
      "step: 7340\n",
      "train: loss: 9472879.0 acc: 0.9551436901092529  val: loss: 2602141.25 acc: 0.8729244470596313\n",
      "step: 7345\n",
      "train: loss: 7900465.0 acc: 0.9635742902755737  val: loss: 14777628.0 acc: 0.4919038414955139\n",
      "step: 7350\n",
      "train: loss: 8266464.0 acc: 0.9527666568756104  val: loss: 14824575.0 acc: 0.2727808356285095\n",
      "step: 7355\n",
      "train: loss: 5650115.0 acc: 0.9564605355262756  val: loss: 14109129.0 acc: 0.7731417417526245\n",
      "step: 7360\n",
      "train: loss: 5519834.0 acc: 0.9506543874740601  val: loss: 2009215.375 acc: 0.9691064953804016\n",
      "step: 7365\n",
      "train: loss: 5817368.0 acc: 0.9242536425590515  val: loss: 10289691.0 acc: 0.45396798849105835\n",
      "step: 7370\n",
      "train: loss: 11050751.0 acc: 0.9044623374938965  val: loss: 5438796.0 acc: 0.8215286731719971\n",
      "step: 7375\n",
      "train: loss: 4694171.0 acc: 0.9398654699325562  val: loss: 6641933.5 acc: 0.8241167664527893\n",
      "step: 7380\n",
      "train: loss: 3476527.75 acc: 0.9483113884925842  val: loss: 9599645.0 acc: 0.8316452503204346\n",
      "step: 7385\n",
      "train: loss: 5576560.5 acc: 0.9297667145729065  val: loss: 4382531.0 acc: 0.8640027046203613\n",
      "step: 7390\n",
      "train: loss: 8260167.5 acc: 0.9064029455184937  val: loss: 1640601.75 acc: 0.9781336188316345\n",
      "step: 7395\n",
      "train: loss: 3976141.25 acc: 0.9679756164550781  val: loss: 7314926.5 acc: 0.9031834602355957\n",
      "step: 7400\n",
      "train: loss: 2121821.5 acc: 0.8957725763320923  val: loss: 9463055.0 acc: 0.8844436407089233\n",
      "step: 7405\n",
      "train: loss: 6922579.5 acc: 0.846432089805603  val: loss: 8369250.0 acc: 0.8989992737770081\n",
      "step: 7410\n",
      "train: loss: 4526480.0 acc: 0.8461296558380127  val: loss: 6367151.0 acc: 0.9357588291168213\n",
      "step: 7415\n",
      "train: loss: 3974039.25 acc: 0.7547776103019714  val: loss: 7191995.0 acc: 0.9113718271255493\n",
      "step: 7420\n",
      "train: loss: 21045596.0 acc: 0.5450710654258728  val: loss: 1905135.25 acc: 0.9197467565536499\n",
      "step: 7425\n",
      "train: loss: 7413330.0 acc: 0.7298463582992554  val: loss: 7432172.5 acc: 0.7672109007835388\n",
      "step: 7430\n",
      "train: loss: 2971161.25 acc: 0.8637925386428833  val: loss: 23216518.0 acc: 0.7751671671867371\n",
      "step: 7435\n",
      "train: loss: 4046988.5 acc: 0.831533670425415  val: loss: 7165375.5 acc: 0.8761491179466248\n",
      "step: 7440\n",
      "train: loss: 1562818.5 acc: 0.8992642760276794  val: loss: 10510383.0 acc: 0.7937635183334351\n",
      "step: 7445\n",
      "train: loss: 2555874.0 acc: 0.8614387512207031  val: loss: 1684680.625 acc: 0.8964961171150208\n",
      "step: 7450\n",
      "train: loss: 2991185.5 acc: 0.8514577746391296  val: loss: 3983931.5 acc: 0.8461023569107056\n",
      "step: 7455\n",
      "train: loss: 3074056.5 acc: 0.8580886721611023  val: loss: 10077127.0 acc: 0.8951241374015808\n",
      "step: 7460\n",
      "train: loss: 4233799.0 acc: 0.7873508930206299  val: loss: 7828524.5 acc: 0.6604267358779907\n",
      "step: 7465\n",
      "train: loss: 2426460.25 acc: 0.8678942918777466  val: loss: 3716503.75 acc: 0.8079292178153992\n",
      "step: 7470\n",
      "train: loss: 3141033.75 acc: 0.8013619184494019  val: loss: 2129432.25 acc: 0.8567320108413696\n",
      "step: 7475\n",
      "train: loss: 4855034.5 acc: 0.8194259405136108  val: loss: 2677015.25 acc: 0.831733226776123\n",
      "step: 7480\n",
      "train: loss: 3393108.25 acc: 0.9167260527610779  val: loss: 22864728.0 acc: 0.8303435444831848\n",
      "step: 7485\n",
      "train: loss: 2847069.25 acc: 0.8151452541351318  val: loss: 7776351.0 acc: 0.9027642011642456\n",
      "step: 7490\n",
      "train: loss: 2752429.25 acc: 0.835905909538269  val: loss: 5895083.0 acc: 0.8422946929931641\n",
      "step: 7495\n",
      "train: loss: 3167123.5 acc: 0.802298903465271  val: loss: 21777760.0 acc: 0.7008189558982849\n",
      "step: 7500\n",
      "train: loss: 4772561.5 acc: 0.7773868441581726  val: loss: 12061185.0 acc: 0.8187594413757324\n",
      "step: 7505\n",
      "train: loss: 2858920.5 acc: 0.8134765028953552  val: loss: 43480136.0 acc: 0.6963115930557251\n",
      "step: 7510\n",
      "train: loss: 7301565.0 acc: 0.7674944400787354  val: loss: 14325735.0 acc: 0.7814138531684875\n",
      "step: 7515\n",
      "train: loss: 7651119.0 acc: 0.7105367183685303  val: loss: 21473098.0 acc: 0.5875869989395142\n",
      "step: 7520\n",
      "train: loss: 5447117.0 acc: 0.7292084097862244  val: loss: 11597660.0 acc: 0.6984763741493225\n",
      "step: 7525\n",
      "train: loss: 5895824.5 acc: 0.7732499241828918  val: loss: 29773922.0 acc: 0.7145238518714905\n",
      "step: 7530\n",
      "train: loss: 8754418.0 acc: 0.7305962443351746  val: loss: 26850832.0 acc: 0.6214509606361389\n",
      "step: 7535\n",
      "train: loss: 6780332.5 acc: 0.6601178646087646  val: loss: 2826602.75 acc: 0.765925943851471\n",
      "step: 7540\n",
      "train: loss: 7279951.0 acc: 0.9400246143341064  val: loss: 6039681.0 acc: 0.8884646892547607\n",
      "step: 7545\n",
      "train: loss: 8426526.0 acc: 0.8470326662063599  val: loss: 3657702.0 acc: 0.7849035859107971\n",
      "step: 7550\n",
      "train: loss: 4536182.5 acc: 0.937367856502533  val: loss: 15596016.0 acc: 0.7522308230400085\n",
      "step: 7555\n",
      "train: loss: 2189400.5 acc: 0.9503581523895264  val: loss: 7882940.0 acc: 0.8292102813720703\n",
      "step: 7560\n",
      "train: loss: 7744331.5 acc: 0.8607769012451172  val: loss: 16757110.0 acc: -0.39200854301452637\n",
      "step: 7565\n",
      "train: loss: 7584986.0 acc: 0.8969299793243408  val: loss: 9295522.0 acc: 0.7280609607696533\n",
      "step: 7570\n",
      "train: loss: 2840712.25 acc: 0.9468645453453064  val: loss: 10772479.0 acc: 0.4715590476989746\n",
      "step: 7575\n",
      "train: loss: 2030903.125 acc: 0.9760968685150146  val: loss: 5465248.0 acc: 0.8185680508613586\n",
      "step: 7580\n",
      "train: loss: 1502704.375 acc: 0.9726194143295288  val: loss: 16983552.0 acc: 0.6970057487487793\n",
      "step: 7585\n",
      "train: loss: 7294823.0 acc: 0.9115864038467407  val: loss: 7684518.5 acc: 0.8056135177612305\n",
      "step: 7590\n",
      "train: loss: 2044369.75 acc: 0.9767206907272339  val: loss: 5494727.5 acc: 0.7978164553642273\n",
      "step: 7595\n",
      "train: loss: 1353434.125 acc: 0.9575618505477905  val: loss: 8490933.0 acc: 0.06466400623321533\n",
      "step: 7600\n",
      "train: loss: 691352.1875 acc: 0.9905123710632324  val: loss: 9391919.0 acc: 0.842166543006897\n",
      "step: 7605\n",
      "train: loss: 1955145.75 acc: 0.9715808033943176  val: loss: 7692591.5 acc: 0.7536486983299255\n",
      "step: 7610\n",
      "train: loss: 4834666.0 acc: 0.7623564600944519  val: loss: 3373202.25 acc: 0.871180534362793\n",
      "step: 7615\n",
      "train: loss: 392288.25 acc: 0.9736089706420898  val: loss: 17296470.0 acc: 0.7959713935852051\n",
      "step: 7620\n",
      "train: loss: 1765975.5 acc: 0.8309339284896851  val: loss: 19156304.0 acc: 0.782780647277832\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 7625\n",
      "train: loss: 587461.0625 acc: 0.9640254974365234  val: loss: 13691552.0 acc: 0.8098335266113281\n",
      "step: 7630\n",
      "train: loss: 333252.0625 acc: 0.9904924631118774  val: loss: 12411986.0 acc: 0.4554618000984192\n",
      "step: 7635\n",
      "train: loss: 1174444.25 acc: 0.8717213869094849  val: loss: 12507086.0 acc: 0.842382550239563\n",
      "step: 7640\n",
      "train: loss: 1033322.375 acc: 0.955780029296875  val: loss: 3064816.25 acc: 0.9516039490699768\n",
      "step: 7645\n",
      "train: loss: 520273.9375 acc: 0.7835711240768433  val: loss: 11726173.0 acc: 0.827340841293335\n",
      "step: 7650\n",
      "train: loss: 634575.375 acc: 0.9660956859588623  val: loss: 16871142.0 acc: 0.795998752117157\n",
      "step: 7655\n",
      "train: loss: 306856.9375 acc: 0.9793908596038818  val: loss: 1570557.875 acc: 0.9608135223388672\n",
      "step: 7660\n",
      "train: loss: 316302.71875 acc: 0.8926234245300293  val: loss: 15701211.0 acc: 0.7007398009300232\n",
      "step: 7665\n",
      "train: loss: 1111280.75 acc: 0.8651160597801208  val: loss: 8966477.0 acc: 0.9019734263420105\n",
      "step: 7670\n",
      "train: loss: 393514.5 acc: 0.8947038650512695  val: loss: 29792816.0 acc: 0.8063676357269287\n",
      "step: 7675\n",
      "train: loss: 975943.5 acc: 0.8977665305137634  val: loss: 2601274.75 acc: 0.9061997532844543\n",
      "step: 7680\n",
      "train: loss: 411815.875 acc: 0.8955612182617188  val: loss: 18455196.0 acc: 0.7877681255340576\n",
      "step: 7685\n",
      "train: loss: 980084.5625 acc: 0.9127933979034424  val: loss: 9117779.0 acc: 0.8549685478210449\n",
      "step: 7690\n",
      "train: loss: 686194.8125 acc: 0.9428393244743347  val: loss: 7936178.5 acc: 0.5966001749038696\n",
      "step: 7695\n",
      "train: loss: 934959.3125 acc: 0.9571843147277832  val: loss: 10471857.0 acc: 0.8268643617630005\n",
      "step: 7700\n",
      "train: loss: 835992.0625 acc: 0.9064485430717468  val: loss: 10649565.0 acc: 0.7249971032142639\n",
      "step: 7705\n",
      "train: loss: 823996.625 acc: 0.9563287496566772  val: loss: 6951930.0 acc: 0.8529764413833618\n",
      "step: 7710\n",
      "train: loss: 665926.9375 acc: 0.9455041885375977  val: loss: 8088555.0 acc: 0.34484273195266724\n",
      "step: 7715\n",
      "train: loss: 465594.90625 acc: 0.9313017129898071  val: loss: 4844312.5 acc: 0.9277232885360718\n",
      "step: 7720\n",
      "train: loss: 314121.21875 acc: 0.9495074152946472  val: loss: 14622172.0 acc: 0.5018996596336365\n",
      "step: 7725\n",
      "train: loss: 318473.09375 acc: 0.95473313331604  val: loss: 10084497.0 acc: 0.4004725217819214\n",
      "step: 7730\n",
      "train: loss: 531477.1875 acc: 0.9669654369354248  val: loss: 11127844.0 acc: 0.5613871812820435\n",
      "step: 7735\n",
      "train: loss: 627286.25 acc: 0.919517993927002  val: loss: 11782963.0 acc: 0.6969349980354309\n",
      "step: 7740\n",
      "train: loss: 366803.40625 acc: 0.9446837902069092  val: loss: 17164984.0 acc: 0.5247689485549927\n",
      "step: 7745\n",
      "train: loss: 1640258.5 acc: 0.9324185252189636  val: loss: 10452420.0 acc: 0.6399973630905151\n",
      "step: 7750\n",
      "train: loss: 1593344.75 acc: 0.9150035381317139  val: loss: 21336490.0 acc: 0.6058329343795776\n",
      "step: 7755\n",
      "train: loss: 733928.75 acc: 0.9591464400291443  val: loss: 7099422.0 acc: 0.8802236914634705\n",
      "step: 7760\n",
      "train: loss: 1144203.875 acc: 0.9572234153747559  val: loss: 19841510.0 acc: 0.20728737115859985\n",
      "step: 7765\n",
      "train: loss: 972039.625 acc: 0.9682821035385132  val: loss: 9920106.0 acc: 0.7178388237953186\n",
      "step: 7770\n",
      "train: loss: 1178993.0 acc: 0.9661626815795898  val: loss: 18218424.0 acc: 0.7423291206359863\n",
      "step: 7775\n",
      "train: loss: 1532256.25 acc: 0.945964515209198  val: loss: 17937080.0 acc: 0.7643861770629883\n",
      "step: 7780\n",
      "train: loss: 1399242.25 acc: 0.9343509078025818  val: loss: 13128737.0 acc: 0.6475909352302551\n",
      "step: 7785\n",
      "train: loss: 474926.96875 acc: 0.9321216344833374  val: loss: 13052798.0 acc: 0.8094688057899475\n",
      "step: 7790\n",
      "train: loss: 1202805.0 acc: 0.9327664971351624  val: loss: 1647078.75 acc: 0.9523272514343262\n",
      "step: 7795\n",
      "train: loss: 1203667.5 acc: 0.9722121357917786  val: loss: 2686720.5 acc: 0.9775807857513428\n",
      "step: 7800\n",
      "train: loss: 1008217.125 acc: 0.9875918030738831  val: loss: 1796938.875 acc: 0.9230868220329285\n",
      "step: 7805\n",
      "train: loss: 2934989.75 acc: 0.9556791186332703  val: loss: 7923687.0 acc: 0.7591335773468018\n",
      "step: 7810\n",
      "train: loss: 1381019.5 acc: 0.9783714413642883  val: loss: 9715356.0 acc: 0.014613747596740723\n",
      "step: 7815\n",
      "train: loss: 792824.4375 acc: 0.9754331707954407  val: loss: 5181772.0 acc: 0.7618242502212524\n",
      "step: 7820\n",
      "train: loss: 2680851.5 acc: 0.9509932398796082  val: loss: 15374335.0 acc: 0.6779875159263611\n",
      "step: 7825\n",
      "train: loss: 3873702.0 acc: 0.9352253675460815  val: loss: 3542856.75 acc: 0.9421136975288391\n",
      "step: 7830\n",
      "train: loss: 7940510.0 acc: 0.9172049760818481  val: loss: 6364021.0 acc: 0.8179536461830139\n",
      "step: 7835\n",
      "train: loss: 6284555.5 acc: 0.9019359946250916  val: loss: 11366243.0 acc: 0.8363572359085083\n",
      "step: 7840\n",
      "train: loss: 5281617.0 acc: 0.8884070515632629  val: loss: 15239793.0 acc: 0.6437743902206421\n",
      "step: 7845\n",
      "train: loss: 4709367.5 acc: 0.9152161478996277  val: loss: 3579010.75 acc: 0.8762776255607605\n",
      "step: 7850\n",
      "train: loss: 7509653.5 acc: 0.9114692807197571  val: loss: 4540117.0 acc: 0.9327760934829712\n",
      "step: 7855\n",
      "train: loss: 9802589.0 acc: 0.9538784623146057  val: loss: 6165566.5 acc: 0.8749674558639526\n",
      "step: 7860\n",
      "train: loss: 8690800.0 acc: 0.9531208276748657  val: loss: 8323567.5 acc: 0.81535804271698\n",
      "step: 7865\n",
      "train: loss: 11989740.0 acc: 0.9160829186439514  val: loss: 3790641.0 acc: 0.9349144101142883\n",
      "step: 7870\n",
      "train: loss: 4769660.5 acc: 0.9658458232879639  val: loss: 7317589.5 acc: 0.9078700542449951\n",
      "step: 7875\n",
      "train: loss: 5430563.5 acc: 0.9584334492683411  val: loss: 15882165.0 acc: 0.717287540435791\n",
      "step: 7880\n",
      "train: loss: 3503613.0 acc: 0.975463330745697  val: loss: 8767081.0 acc: 0.6535957455635071\n",
      "step: 7885\n",
      "train: loss: 8025519.5 acc: 0.8833264708518982  val: loss: 15256448.0 acc: 0.7793886661529541\n",
      "step: 7890\n",
      "train: loss: 4754732.0 acc: 0.9342873692512512  val: loss: 8105489.5 acc: 0.6897932887077332\n",
      "step: 7895\n",
      "train: loss: 17316904.0 acc: 0.8287159204483032  val: loss: 6651547.5 acc: 0.8744461536407471\n",
      "step: 7900\n",
      "train: loss: 10440752.0 acc: 0.7453836798667908  val: loss: 5718231.0 acc: 0.9290233850479126\n",
      "step: 7905\n",
      "train: loss: 3848055.25 acc: 0.9407122731208801  val: loss: 3172147.5 acc: 0.8874183893203735\n",
      "step: 7910\n",
      "train: loss: 5676618.5 acc: 0.915736198425293  val: loss: 4705205.5 acc: 0.844286322593689\n",
      "step: 7915\n",
      "train: loss: 2298839.5 acc: 0.9487636685371399  val: loss: 10677762.0 acc: 0.8381507396697998\n",
      "step: 7920\n",
      "train: loss: 3485184.5 acc: 0.8765482306480408  val: loss: 4174766.5 acc: 0.8839616179466248\n",
      "step: 7925\n",
      "train: loss: 4786595.5 acc: 0.9360269904136658  val: loss: 7666941.5 acc: 0.9067919254302979\n",
      "step: 7930\n",
      "train: loss: 6018802.0 acc: 0.7559494972229004  val: loss: 7514180.0 acc: 0.913796067237854\n",
      "step: 7935\n",
      "train: loss: 13268532.0 acc: 0.792315661907196  val: loss: 2510478.75 acc: 0.8641818761825562\n",
      "step: 7940\n",
      "train: loss: 2235895.75 acc: 0.8778706789016724  val: loss: 1930334.625 acc: 0.9720646142959595\n",
      "step: 7945\n",
      "train: loss: 1793782.125 acc: 0.8966304659843445  val: loss: 13894537.0 acc: 0.8198367357254028\n",
      "step: 7950\n",
      "train: loss: 3429494.5 acc: 0.8686243295669556  val: loss: 3215671.25 acc: 0.9634935259819031\n",
      "step: 7955\n",
      "train: loss: 4881707.0 acc: 0.9167671203613281  val: loss: 8646837.0 acc: 0.6903401613235474\n",
      "step: 7960\n",
      "train: loss: 825040.6875 acc: 0.9214961528778076  val: loss: 4228280.0 acc: 0.9510314464569092\n",
      "step: 7965\n",
      "train: loss: 1671433.0 acc: 0.9017952084541321  val: loss: 9336155.0 acc: 0.7965856194496155\n",
      "step: 7970\n",
      "train: loss: 4825663.0 acc: 0.7837334871292114  val: loss: 12025795.0 acc: 0.8213208317756653\n",
      "step: 7975\n",
      "train: loss: 4487260.5 acc: 0.7890922427177429  val: loss: 3617044.5 acc: 0.8458489179611206\n",
      "step: 7980\n",
      "train: loss: 3846754.25 acc: 0.7930207848548889  val: loss: 8258747.5 acc: 0.8978070020675659\n",
      "step: 7985\n",
      "train: loss: 2177864.75 acc: 0.8819413185119629  val: loss: 6084879.5 acc: 0.7639102935791016\n",
      "step: 7990\n",
      "train: loss: 3123337.5 acc: 0.8348042964935303  val: loss: 12349495.0 acc: 0.7612069845199585\n",
      "step: 7995\n",
      "train: loss: 4903646.0 acc: 0.7901513576507568  val: loss: 7391640.5 acc: 0.76078200340271\n",
      "step: 8000\n",
      "train: loss: 7468162.0 acc: 0.7423723340034485  val: loss: 5832474.5 acc: 0.8805913329124451\n",
      "step: 8005\n",
      "train: loss: 3330169.25 acc: 0.8088539838790894  val: loss: 12247484.0 acc: 0.8497703671455383\n",
      "step: 8010\n",
      "train: loss: 2815460.75 acc: 0.8172181844711304  val: loss: 11938390.0 acc: 0.7461740374565125\n",
      "step: 8015\n",
      "train: loss: 7100538.0 acc: 0.7735602259635925  val: loss: 10795620.0 acc: 0.8585188388824463\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 8020\n",
      "train: loss: 11028339.0 acc: 0.6933443546295166  val: loss: 2881141.0 acc: 0.965437650680542\n",
      "step: 8025\n",
      "train: loss: 9790639.0 acc: 0.704851508140564  val: loss: 12991031.0 acc: 0.6582731008529663\n",
      "step: 8030\n",
      "train: loss: 3175706.5 acc: 0.817805826663971  val: loss: 24113550.0 acc: 0.669499933719635\n",
      "step: 8035\n",
      "train: loss: 3532993.75 acc: 0.809001088142395  val: loss: 1537754.875 acc: 0.9022629261016846\n",
      "step: 8040\n",
      "train: loss: 3771790.75 acc: 0.7639550566673279  val: loss: 7543195.5 acc: 0.7125356197357178\n",
      "step: 8045\n",
      "train: loss: 8525349.0 acc: 0.7534374594688416  val: loss: 6956544.0 acc: 0.7961491346359253\n",
      "step: 8050\n",
      "train: loss: 8252529.5 acc: 0.8147412538528442  val: loss: 4536484.0 acc: 0.5598137378692627\n",
      "step: 8055\n",
      "train: loss: 8957078.0 acc: 0.852240800857544  val: loss: 24533224.0 acc: 0.65425705909729\n",
      "step: 8060\n",
      "train: loss: 4210082.0 acc: 0.9293858408927917  val: loss: 19944304.0 acc: 0.431765079498291\n",
      "step: 8065\n",
      "train: loss: 3383224.5 acc: 0.9414645433425903  val: loss: 3908200.0 acc: 0.9482584595680237\n",
      "step: 8070\n",
      "train: loss: 7820977.5 acc: 0.838061511516571  val: loss: 5274650.0 acc: 0.9538379311561584\n",
      "step: 8075\n",
      "train: loss: 1899840.75 acc: 0.9405826330184937  val: loss: 3812324.0 acc: 0.8414076566696167\n",
      "step: 8080\n",
      "train: loss: 2105767.5 acc: 0.9611212015151978  val: loss: 8438183.0 acc: 0.8763658404350281\n",
      "step: 8085\n",
      "train: loss: 3403469.25 acc: 0.9368911385536194  val: loss: 13973886.0 acc: 0.7811697721481323\n",
      "step: 8090\n",
      "train: loss: 5233763.0 acc: 0.9190314412117004  val: loss: 1511749.125 acc: 0.9478027820587158\n",
      "step: 8095\n",
      "train: loss: 3441819.75 acc: 0.9726125001907349  val: loss: 4118234.0 acc: 0.8174513578414917\n",
      "step: 8100\n",
      "train: loss: 10216713.0 acc: 0.8848690390586853  val: loss: 2586951.25 acc: 0.9532985091209412\n",
      "step: 8105\n",
      "train: loss: 2204105.75 acc: 0.971837043762207  val: loss: 9411310.0 acc: 0.8405702114105225\n",
      "step: 8110\n",
      "train: loss: 2020564.375 acc: 0.9597548246383667  val: loss: 6624787.5 acc: 0.8786007761955261\n",
      "step: 8115\n",
      "train: loss: 3937646.75 acc: 0.9465005397796631  val: loss: 13265617.0 acc: 0.7349255084991455\n",
      "step: 8120\n",
      "train: loss: 629497.125 acc: 0.9899123311042786  val: loss: 5297574.5 acc: 0.9348406791687012\n",
      "step: 8125\n",
      "train: loss: 1405216.125 acc: 0.9799405932426453  val: loss: 23946626.0 acc: 0.8310116529464722\n",
      "step: 8130\n",
      "train: loss: 1739281.125 acc: 0.9737194180488586  val: loss: 16234573.0 acc: 0.7169427871704102\n",
      "step: 8135\n",
      "train: loss: 329883.90625 acc: 0.9948981404304504  val: loss: 4870528.5 acc: 0.9201277494430542\n",
      "step: 8140\n",
      "train: loss: 382505.4375 acc: 0.8734034299850464  val: loss: 9419487.0 acc: 0.8261831402778625\n",
      "step: 8145\n",
      "train: loss: 692419.1875 acc: 0.9814199209213257  val: loss: 6782129.0 acc: 0.9069214463233948\n",
      "step: 8150\n",
      "train: loss: 913758.1875 acc: 0.9610838890075684  val: loss: 14572073.0 acc: 0.5381399393081665\n",
      "step: 8155\n",
      "train: loss: 1041051.0 acc: 0.9698323607444763  val: loss: 19291238.0 acc: 0.6538491249084473\n",
      "step: 8160\n",
      "train: loss: 381074.21875 acc: 0.9730468988418579  val: loss: 13909806.0 acc: 0.8285974264144897\n",
      "step: 8165\n",
      "train: loss: 2558624.5 acc: 0.7362098693847656  val: loss: 8516897.0 acc: 0.7591665387153625\n",
      "step: 8170\n",
      "train: loss: 409290.0625 acc: 0.8627843260765076  val: loss: 5439493.0 acc: 0.8152883052825928\n",
      "step: 8175\n",
      "train: loss: 812814.8125 acc: 0.9154195785522461  val: loss: 13418738.0 acc: 0.5940004587173462\n",
      "step: 8180\n",
      "train: loss: 907246.5625 acc: 0.9504750967025757  val: loss: 8473839.0 acc: 0.8759434223175049\n",
      "step: 8185\n",
      "train: loss: 338381.5 acc: 0.9738063216209412  val: loss: 7947549.5 acc: 0.7827212810516357\n",
      "step: 8190\n",
      "train: loss: 168648.34375 acc: 0.9883719682693481  val: loss: 24417266.0 acc: 0.3173023462295532\n",
      "step: 8195\n",
      "train: loss: 217474.25 acc: 0.9512243270874023  val: loss: 4003944.5 acc: 0.8540878295898438\n",
      "step: 8200\n",
      "train: loss: 105764.6484375 acc: 0.9196066856384277  val: loss: 23988414.0 acc: 0.8144601583480835\n",
      "step: 8205\n",
      "train: loss: 938917.1875 acc: 0.9124706983566284  val: loss: 10544926.0 acc: 0.8624483942985535\n",
      "step: 8210\n",
      "train: loss: 717077.6875 acc: 0.9338635802268982  val: loss: 6832231.5 acc: 0.6980238556861877\n",
      "step: 8215\n",
      "train: loss: 1253568.75 acc: 0.8837727904319763  val: loss: 2218941.0 acc: 0.9574149250984192\n",
      "step: 8220\n",
      "train: loss: 735304.875 acc: 0.9178613424301147  val: loss: 8762733.0 acc: 0.8815661072731018\n",
      "step: 8225\n",
      "train: loss: 620916.5 acc: 0.9644914865493774  val: loss: 6206569.5 acc: 0.7670446038246155\n",
      "step: 8230\n",
      "train: loss: 769961.4375 acc: 0.9240989089012146  val: loss: 3596694.25 acc: 0.9018373489379883\n",
      "step: 8235\n",
      "train: loss: 561473.625 acc: 0.9525340795516968  val: loss: 12768495.0 acc: 0.402377188205719\n",
      "step: 8240\n",
      "train: loss: 911980.5625 acc: 0.9247689247131348  val: loss: 8211096.5 acc: 0.631483256816864\n",
      "step: 8245\n",
      "train: loss: 566025.625 acc: 0.8952592611312866  val: loss: 9866582.0 acc: -0.13400399684906006\n",
      "step: 8250\n",
      "train: loss: 337392.21875 acc: 0.9553076028823853  val: loss: 15930036.0 acc: 0.8569148778915405\n",
      "step: 8255\n",
      "train: loss: 242730.28125 acc: 0.9779562950134277  val: loss: 27907388.0 acc: 0.03702431917190552\n",
      "step: 8260\n",
      "train: loss: 1701709.875 acc: 0.9261941313743591  val: loss: 23982326.0 acc: 0.22532939910888672\n",
      "step: 8265\n",
      "train: loss: 784997.5625 acc: 0.9343972206115723  val: loss: 9073197.0 acc: 0.6968975067138672\n",
      "step: 8270\n",
      "train: loss: 733907.9375 acc: 0.9544159173965454  val: loss: 10330016.0 acc: 0.8688498139381409\n",
      "step: 8275\n",
      "train: loss: 897991.0625 acc: 0.9707136750221252  val: loss: 12857914.0 acc: 0.5264166593551636\n",
      "step: 8280\n",
      "train: loss: 955225.6875 acc: 0.9177680015563965  val: loss: 10670772.0 acc: 0.761042594909668\n",
      "step: 8285\n",
      "train: loss: 1975191.125 acc: 0.9090288281440735  val: loss: 8064848.0 acc: 0.8370063304901123\n",
      "step: 8290\n",
      "train: loss: 1139980.125 acc: 0.9745566248893738  val: loss: 19019658.0 acc: 0.5668805837631226\n",
      "step: 8295\n",
      "train: loss: 2700840.75 acc: 0.8692722320556641  val: loss: 11998997.0 acc: 0.9124331474304199\n",
      "step: 8300\n",
      "train: loss: 1537811.75 acc: 0.9218557476997375  val: loss: 8296558.0 acc: 0.8258664608001709\n",
      "step: 8305\n",
      "train: loss: 847161.9375 acc: 0.9446931481361389  val: loss: 2638791.75 acc: 0.9716285467147827\n",
      "step: 8310\n",
      "train: loss: 6152076.5 acc: 0.8329214453697205  val: loss: 24554822.0 acc: 0.6795260906219482\n",
      "step: 8315\n",
      "train: loss: 1428084.125 acc: 0.9646940231323242  val: loss: 7340069.5 acc: 0.8356562852859497\n",
      "step: 8320\n",
      "train: loss: 4596227.0 acc: 0.94234299659729  val: loss: 12933395.0 acc: 0.7359105348587036\n",
      "step: 8325\n",
      "train: loss: 2934107.25 acc: 0.9377605319023132  val: loss: 6500522.5 acc: 0.9038436412811279\n",
      "step: 8330\n",
      "train: loss: 1520203.625 acc: 0.9803977608680725  val: loss: 8194649.5 acc: 0.8728440999984741\n",
      "step: 8335\n",
      "train: loss: 1734543.5 acc: 0.9262953996658325  val: loss: 1940830.875 acc: 0.8769596219062805\n",
      "step: 8340\n",
      "train: loss: 1869241.375 acc: 0.9462884664535522  val: loss: 2545157.75 acc: 0.8791636824607849\n",
      "step: 8345\n",
      "train: loss: 3852498.25 acc: 0.9367076754570007  val: loss: 13379278.0 acc: 0.7767382860183716\n",
      "step: 8350\n",
      "train: loss: 7234760.0 acc: 0.8540173768997192  val: loss: 14397456.0 acc: 0.607427716255188\n",
      "step: 8355\n",
      "train: loss: 3097657.75 acc: 0.8909061551094055  val: loss: 3502725.25 acc: 0.9599043726921082\n",
      "step: 8360\n",
      "train: loss: 3992393.75 acc: 0.9270673990249634  val: loss: 8111591.5 acc: 0.9176087379455566\n",
      "step: 8365\n",
      "train: loss: 2015577.5 acc: 0.9599985480308533  val: loss: 13275060.0 acc: 0.0736004114151001\n",
      "step: 8370\n",
      "train: loss: 6710102.0 acc: 0.958638608455658  val: loss: 22639610.0 acc: 0.7211175560951233\n",
      "step: 8375\n",
      "train: loss: 3922214.0 acc: 0.9746812582015991  val: loss: 4736148.0 acc: 0.9012489318847656\n",
      "step: 8380\n",
      "train: loss: 7253417.0 acc: 0.9506238102912903  val: loss: 4546349.5 acc: 0.9092378616333008\n",
      "step: 8385\n",
      "train: loss: 9076014.0 acc: 0.9629383683204651  val: loss: 20437530.0 acc: 0.4409816265106201\n",
      "step: 8390\n",
      "train: loss: 5929900.5 acc: 0.9434417486190796  val: loss: 16014052.0 acc: 0.7510128021240234\n",
      "step: 8395\n",
      "train: loss: 6459730.5 acc: 0.957057535648346  val: loss: 5153603.5 acc: 0.9467328190803528\n",
      "step: 8400\n",
      "train: loss: 4559823.5 acc: 0.9492115378379822  val: loss: 3876151.25 acc: 0.9308611750602722\n",
      "step: 8405\n",
      "train: loss: 7161924.0 acc: 0.9275548458099365  val: loss: 21982210.0 acc: 0.2545853853225708\n",
      "step: 8410\n",
      "train: loss: 4237032.5 acc: 0.8395981788635254  val: loss: 23552646.0 acc: 0.4471496343612671\n",
      "step: 8415\n",
      "train: loss: 19906798.0 acc: 0.7920037508010864  val: loss: 7953495.5 acc: 0.7985358238220215\n",
      "step: 8420\n",
      "train: loss: 7227906.5 acc: 0.9531527161598206  val: loss: 4929962.5 acc: 0.8351194858551025\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 8425\n",
      "train: loss: 6018990.5 acc: 0.9438116550445557  val: loss: 7103960.0 acc: 0.6208750009536743\n",
      "step: 8430\n",
      "train: loss: 7229030.5 acc: 0.9138950705528259  val: loss: 8807675.0 acc: 0.6995559930801392\n",
      "step: 8435\n",
      "train: loss: 3008793.0 acc: 0.893964946269989  val: loss: 3665100.0 acc: 0.9218642115592957\n",
      "step: 8440\n",
      "train: loss: 2651222.25 acc: 0.8542944192886353  val: loss: 1813748.375 acc: 0.8493852615356445\n",
      "step: 8445\n",
      "train: loss: 6488796.5 acc: 0.8313081860542297  val: loss: 7653338.5 acc: 0.8879546523094177\n",
      "step: 8450\n",
      "train: loss: 6822611.0 acc: 0.8225452899932861  val: loss: 4286071.5 acc: 0.8270743489265442\n",
      "step: 8455\n",
      "train: loss: 2836849.0 acc: 0.8863772749900818  val: loss: 2822071.75 acc: 0.8217534422874451\n",
      "step: 8460\n",
      "train: loss: 4182358.75 acc: 0.8597816228866577  val: loss: 14140464.0 acc: 0.6468740105628967\n",
      "step: 8465\n",
      "train: loss: 21530066.0 acc: -0.7115612030029297  val: loss: 6038980.0 acc: 0.6937140226364136\n",
      "step: 8470\n",
      "train: loss: 4632042.0 acc: 0.792204737663269  val: loss: 1009364.625 acc: 0.8546552062034607\n",
      "step: 8475\n",
      "train: loss: 2949522.25 acc: 0.8629189133644104  val: loss: 3818781.25 acc: 0.9063807129859924\n",
      "step: 8480\n",
      "train: loss: 1728891.0 acc: 0.9577979445457458  val: loss: 9267067.0 acc: 0.735336184501648\n",
      "step: 8485\n",
      "train: loss: 1003972.125 acc: 0.9774383306503296  val: loss: 2325099.75 acc: 0.8585797548294067\n",
      "step: 8490\n",
      "train: loss: 4560823.5 acc: 0.8095909357070923  val: loss: 8365214.5 acc: 0.897799015045166\n",
      "step: 8495\n",
      "train: loss: 4907986.5 acc: 0.8504464030265808  val: loss: 7638549.0 acc: 0.6792702674865723\n",
      "step: 8500\n",
      "train: loss: 3095370.0 acc: 0.8379620313644409  val: loss: 6829950.0 acc: 0.905099630355835\n",
      "step: 8505\n",
      "train: loss: 2497437.75 acc: 0.8669362664222717  val: loss: 8074125.0 acc: 0.8570448756217957\n",
      "step: 8510\n",
      "train: loss: 1997123.125 acc: 0.8661981821060181  val: loss: 7609713.0 acc: 0.7557046413421631\n",
      "step: 8515\n",
      "train: loss: 1370668.0 acc: 0.8908792734146118  val: loss: 4970499.5 acc: 0.9434298872947693\n",
      "step: 8520\n",
      "train: loss: 5842233.0 acc: 0.7015284299850464  val: loss: 3832136.5 acc: 0.8885924816131592\n",
      "step: 8525\n",
      "train: loss: 3075290.25 acc: 0.8410440683364868  val: loss: 3503637.75 acc: 0.9045843482017517\n",
      "step: 8530\n",
      "train: loss: 5106150.5 acc: 0.7638229727745056  val: loss: 2993995.75 acc: 0.9372722506523132\n",
      "step: 8535\n",
      "train: loss: 3972559.25 acc: 0.8153125643730164  val: loss: 7529493.5 acc: 0.8018935918807983\n",
      "step: 8540\n",
      "train: loss: 5022109.0 acc: 0.7507287263870239  val: loss: 5114237.5 acc: 0.8549887537956238\n",
      "step: 8545\n",
      "train: loss: 3358019.25 acc: 0.7525376677513123  val: loss: 8718709.0 acc: 0.7682511806488037\n",
      "step: 8550\n",
      "train: loss: 9869576.0 acc: 0.7762777805328369  val: loss: 4509036.5 acc: 0.9227816462516785\n",
      "step: 8555\n",
      "train: loss: 7389231.0 acc: 0.6902894973754883  val: loss: 13599484.0 acc: 0.8083128929138184\n",
      "step: 8560\n",
      "train: loss: 4358372.5 acc: 0.8006694912910461  val: loss: 22447300.0 acc: 0.8494526147842407\n",
      "step: 8565\n",
      "train: loss: 14141915.0 acc: 0.698928952217102  val: loss: 26974728.0 acc: 0.803496241569519\n",
      "step: 8570\n",
      "train: loss: 8232468.5 acc: 0.826763927936554  val: loss: 3055434.25 acc: 0.9421852827072144\n",
      "step: 8575\n",
      "train: loss: 7106544.0 acc: 0.8939282298088074  val: loss: 6945679.0 acc: 0.9006875157356262\n",
      "step: 8580\n",
      "train: loss: 5130898.0 acc: 0.8777959942817688  val: loss: 4673118.0 acc: 0.8881893157958984\n",
      "step: 8585\n",
      "train: loss: 5213896.0 acc: 0.9127676486968994  val: loss: 9575839.0 acc: 0.5985373854637146\n",
      "step: 8590\n",
      "train: loss: 6531797.0 acc: 0.8737093210220337  val: loss: 15352211.0 acc: 0.8305286765098572\n",
      "step: 8595\n",
      "train: loss: 4443739.5 acc: 0.8994818329811096  val: loss: 5598293.5 acc: 0.868468165397644\n",
      "step: 8600\n",
      "train: loss: 4867373.0 acc: 0.8950610160827637  val: loss: 4096996.75 acc: 0.8644176125526428\n",
      "step: 8605\n",
      "train: loss: 4686093.0 acc: 0.9318904876708984  val: loss: 35056900.0 acc: 0.759989321231842\n",
      "step: 8610\n",
      "train: loss: 12795054.0 acc: 0.8903557658195496  val: loss: 10635786.0 acc: 0.830377995967865\n",
      "step: 8615\n",
      "train: loss: 4437255.0 acc: 0.9128007888793945  val: loss: 21576814.0 acc: 0.7060328722000122\n",
      "step: 8620\n",
      "train: loss: 1395921.125 acc: 0.9882602095603943  val: loss: 2818729.0 acc: 0.9149209856987\n",
      "step: 8625\n",
      "train: loss: 1041697.0625 acc: 0.9881011247634888  val: loss: 7473246.0 acc: 0.8897814154624939\n",
      "step: 8630\n",
      "train: loss: 1142293.75 acc: 0.979840099811554  val: loss: 19493060.0 acc: 0.7832093238830566\n",
      "step: 8635\n",
      "train: loss: 3286405.75 acc: 0.9590514898300171  val: loss: 675408.6875 acc: 0.9789555072784424\n",
      "step: 8640\n",
      "train: loss: 1500808.375 acc: 0.9711351990699768  val: loss: 8691814.0 acc: 0.7921465635299683\n",
      "step: 8645\n",
      "train: loss: 1881793.75 acc: 0.9691036343574524  val: loss: 2759572.0 acc: 0.9013113379478455\n",
      "step: 8650\n",
      "train: loss: 542243.3125 acc: 0.8814911842346191  val: loss: 9526932.0 acc: 0.71196448802948\n",
      "step: 8655\n",
      "train: loss: 1048995.5 acc: 0.9290513396263123  val: loss: 16368580.0 acc: 0.8297851085662842\n",
      "step: 8660\n",
      "train: loss: 380353.90625 acc: 0.9899817109107971  val: loss: 6907717.5 acc: 0.6627416014671326\n",
      "step: 8665\n",
      "train: loss: 2174714.25 acc: 0.9478446841239929  val: loss: 9237218.0 acc: 0.8141456842422485\n",
      "step: 8670\n",
      "train: loss: 1272946.5 acc: 0.9121010303497314  val: loss: 16596892.0 acc: 0.872250497341156\n",
      "step: 8675\n",
      "train: loss: 976309.375 acc: 0.9075968861579895  val: loss: 7285788.0 acc: 0.7015510201454163\n",
      "step: 8680\n",
      "train: loss: 269416.5 acc: 0.9871850609779358  val: loss: 21614564.0 acc: 0.8203326463699341\n",
      "step: 8685\n",
      "train: loss: 520982.71875 acc: 0.905942976474762  val: loss: 16569988.0 acc: -0.05005478858947754\n",
      "step: 8690\n",
      "train: loss: 572671.5 acc: 0.9512641429901123  val: loss: 7354666.5 acc: 0.7940652966499329\n",
      "step: 8695\n",
      "train: loss: 214412.75 acc: 0.9840495586395264  val: loss: 21282862.0 acc: 0.738501250743866\n",
      "step: 8700\n",
      "train: loss: 264746.9375 acc: 0.9081464409828186  val: loss: 2359024.25 acc: 0.9329311847686768\n",
      "step: 8705\n",
      "train: loss: 321989.84375 acc: 0.9785385727882385  val: loss: 9041011.0 acc: 0.8052030801773071\n",
      "step: 8710\n",
      "train: loss: 844529.625 acc: 0.9509971737861633  val: loss: 9631764.0 acc: 0.7856723666191101\n",
      "step: 8715\n",
      "train: loss: 302489.53125 acc: 0.97928386926651  val: loss: 6177363.0 acc: 0.7543131709098816\n",
      "step: 8720\n",
      "train: loss: 2619746.75 acc: 0.8809103965759277  val: loss: 15152695.0 acc: 0.5525370240211487\n",
      "step: 8725\n",
      "train: loss: 1132202.375 acc: 0.815214991569519  val: loss: 9262338.0 acc: 0.9078008532524109\n",
      "step: 8730\n",
      "train: loss: 1281311.75 acc: 0.8947895765304565  val: loss: 6271387.0 acc: 0.9030566215515137\n",
      "step: 8735\n",
      "train: loss: 737922.875 acc: 0.9458385705947876  val: loss: 5035461.5 acc: 0.8532813787460327\n",
      "step: 8740\n",
      "train: loss: 1216960.125 acc: 0.9172381162643433  val: loss: 5491236.0 acc: 0.8145751953125\n",
      "step: 8745\n",
      "train: loss: 1053078.625 acc: 0.9097937941551208  val: loss: 8974828.0 acc: 0.7159833908081055\n",
      "step: 8750\n",
      "train: loss: 862986.375 acc: 0.9274757504463196  val: loss: 14815265.0 acc: 0.8043053150177002\n",
      "step: 8755\n",
      "train: loss: 371381.875 acc: 0.9087403416633606  val: loss: 10605528.0 acc: 0.6404749155044556\n",
      "step: 8760\n",
      "train: loss: 425879.78125 acc: 0.9001139402389526  val: loss: 14823130.0 acc: 0.5459808111190796\n",
      "step: 8765\n",
      "train: loss: 160696.390625 acc: 0.889860212802887  val: loss: 26283254.0 acc: 0.4810895323753357\n",
      "step: 8770\n",
      "train: loss: 479353.0 acc: 0.8996425867080688  val: loss: 5427324.0 acc: 0.9002878665924072\n",
      "step: 8775\n",
      "train: loss: 272009.90625 acc: 0.9556025266647339  val: loss: 11534718.0 acc: 0.8327240943908691\n",
      "step: 8780\n",
      "train: loss: 613222.875 acc: 0.9611287713050842  val: loss: 20845804.0 acc: 0.7387790679931641\n",
      "step: 8785\n",
      "train: loss: 715202.625 acc: 0.9313753247261047  val: loss: 8286511.0 acc: 0.34653371572494507\n",
      "step: 8790\n",
      "train: loss: 1383394.125 acc: 0.9581314921379089  val: loss: 21464166.0 acc: 0.6379041075706482\n",
      "step: 8795\n",
      "train: loss: 1414887.375 acc: 0.9601587057113647  val: loss: 16166086.0 acc: 0.808856725692749\n",
      "step: 8800\n",
      "train: loss: 1016295.5625 acc: 0.9681133031845093  val: loss: 9585410.0 acc: 0.6668368577957153\n",
      "step: 8805\n",
      "train: loss: 1536759.625 acc: 0.945580244064331  val: loss: 12206834.0 acc: 0.7736468315124512\n",
      "step: 8810\n",
      "train: loss: 2344547.0 acc: 0.8676458597183228  val: loss: 7745275.5 acc: 0.8223650455474854\n",
      "step: 8815\n",
      "train: loss: 1463405.125 acc: 0.9070442914962769  val: loss: 2870891.5 acc: 0.962121307849884\n",
      "step: 8820\n",
      "train: loss: 1258346.875 acc: 0.9430724382400513  val: loss: 5757753.5 acc: 0.7439852952957153\n",
      "step: 8825\n",
      "train: loss: 1178099.125 acc: 0.890744149684906  val: loss: 7433554.0 acc: 0.927383303642273\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 8830\n",
      "train: loss: 3116348.0 acc: 0.9356578588485718  val: loss: 10955187.0 acc: 0.8614906072616577\n",
      "step: 8835\n",
      "train: loss: 3310558.0 acc: 0.9262933731079102  val: loss: 3985973.5 acc: 0.9344712495803833\n",
      "step: 8840\n",
      "train: loss: 3595083.5 acc: 0.9236022233963013  val: loss: 10438408.0 acc: 0.874589741230011\n",
      "step: 8845\n",
      "train: loss: 3059674.5 acc: 0.9525442719459534  val: loss: 20920036.0 acc: 0.244958758354187\n",
      "step: 8850\n",
      "train: loss: 1453890.875 acc: 0.97849440574646  val: loss: 841599.5625 acc: 0.9809091687202454\n",
      "step: 8855\n",
      "train: loss: 1857622.5 acc: 0.9628384709358215  val: loss: 8603032.0 acc: 0.5720899105072021\n",
      "step: 8860\n",
      "train: loss: 7242504.5 acc: 0.8544045090675354  val: loss: 11757528.0 acc: 0.85066819190979\n",
      "step: 8865\n",
      "train: loss: 5829299.0 acc: 0.90827476978302  val: loss: 8369265.0 acc: 0.8014752864837646\n",
      "step: 8870\n",
      "train: loss: 6417270.0 acc: 0.8343323469161987  val: loss: 17002362.0 acc: 0.7256829738616943\n",
      "step: 8875\n",
      "train: loss: 5986767.0 acc: 0.9136295318603516  val: loss: 30578098.0 acc: 0.64342200756073\n",
      "step: 8880\n",
      "train: loss: 5951522.5 acc: 0.945479154586792  val: loss: 8171438.0 acc: 0.5640220642089844\n",
      "step: 8885\n",
      "train: loss: 6557874.0 acc: 0.9501630663871765  val: loss: 6974410.5 acc: 0.8971046805381775\n",
      "step: 8890\n",
      "train: loss: 4721489.5 acc: 0.965800404548645  val: loss: 7603788.5 acc: 0.824619472026825\n",
      "step: 8895\n",
      "train: loss: 11016710.0 acc: 0.9336560964584351  val: loss: 3643622.0 acc: 0.9182537198066711\n",
      "step: 8900\n",
      "train: loss: 5994552.0 acc: 0.9802224040031433  val: loss: 12338066.0 acc: 0.510413408279419\n",
      "step: 8905\n",
      "train: loss: 4934831.0 acc: 0.9627551436424255  val: loss: 7049490.5 acc: 0.9198741316795349\n",
      "step: 8910\n",
      "train: loss: 7903554.0 acc: 0.9449461698532104  val: loss: 14975261.0 acc: 0.729295551776886\n",
      "step: 8915\n",
      "train: loss: 7401738.0 acc: 0.9528217315673828  val: loss: 2240990.0 acc: 0.9520375728607178\n",
      "step: 8920\n",
      "train: loss: 13538860.0 acc: 0.9088302254676819  val: loss: 3835015.75 acc: 0.9687778949737549\n",
      "step: 8925\n",
      "train: loss: 5050865.5 acc: 0.9449183344841003  val: loss: 7845177.5 acc: 0.3377584218978882\n",
      "step: 8930\n",
      "train: loss: 4790664.0 acc: 0.9446740746498108  val: loss: 2934584.25 acc: 0.8515216112136841\n",
      "step: 8935\n",
      "train: loss: 45530632.0 acc: 0.5114688873291016  val: loss: 3174173.5 acc: 0.9445805549621582\n",
      "step: 8940\n",
      "train: loss: 8117404.0 acc: 0.9241609573364258  val: loss: 5227693.5 acc: 0.8566765785217285\n",
      "step: 8945\n",
      "train: loss: 4380851.5 acc: 0.9520549178123474  val: loss: 4399556.0 acc: 0.5436028242111206\n",
      "step: 8950\n",
      "train: loss: 7545242.0 acc: 0.8825559020042419  val: loss: 11461133.0 acc: 0.809002161026001\n",
      "step: 8955\n",
      "train: loss: 5076967.0 acc: 0.7843273878097534  val: loss: 8478534.0 acc: 0.8204020261764526\n",
      "step: 8960\n",
      "train: loss: 5938509.5 acc: 0.7839642763137817  val: loss: 3945389.25 acc: 0.9442108273506165\n",
      "step: 8965\n",
      "train: loss: 5058057.0 acc: 0.9239591956138611  val: loss: 4583971.0 acc: 0.8956267237663269\n",
      "step: 8970\n",
      "train: loss: 6729139.0 acc: 0.7642015814781189  val: loss: 4950570.0 acc: 0.903495192527771\n",
      "step: 8975\n",
      "train: loss: 1348173.875 acc: 0.9191534519195557  val: loss: 5538002.5 acc: 0.9141384363174438\n",
      "step: 8980\n",
      "train: loss: 7108618.5 acc: 0.7797291278839111  val: loss: 4147477.0 acc: 0.7934144139289856\n",
      "step: 8985\n",
      "train: loss: 2334901.5 acc: 0.853134274482727  val: loss: 6275779.0 acc: 0.9108186960220337\n",
      "step: 8990\n",
      "train: loss: 1358838.5 acc: 0.8983384370803833  val: loss: 4177097.0 acc: 0.8860445618629456\n",
      "step: 8995\n",
      "train: loss: 803791.6875 acc: 0.9321231245994568  val: loss: 10759017.0 acc: 0.7352415323257446\n",
      "step: 9000\n",
      "train: loss: 1979046.25 acc: 0.8854976892471313  val: loss: 6707169.0 acc: 0.88747239112854\n",
      "step: 9005\n",
      "train: loss: 4378443.0 acc: 0.9047084450721741  val: loss: 3313309.0 acc: 0.9334144592285156\n",
      "step: 9010\n",
      "train: loss: 3840641.75 acc: 0.8582124710083008  val: loss: 6090434.0 acc: 0.743076503276825\n",
      "step: 9015\n",
      "train: loss: 2097525.25 acc: 0.8799153566360474  val: loss: 4216972.0 acc: 0.8171307444572449\n",
      "step: 9020\n",
      "train: loss: 5487463.5 acc: 0.8053139448165894  val: loss: 5132322.5 acc: 0.7906918525695801\n",
      "step: 9025\n",
      "train: loss: 4711117.5 acc: 0.8264126777648926  val: loss: 11956482.0 acc: 0.8423978090286255\n",
      "step: 9030\n",
      "train: loss: 6004765.0 acc: 0.7988579273223877  val: loss: 4383113.5 acc: 0.8390448093414307\n",
      "step: 9035\n",
      "train: loss: 2104567.75 acc: 0.8271769881248474  val: loss: 2243650.5 acc: 0.8615096211433411\n",
      "step: 9040\n",
      "train: loss: 1799962.875 acc: 0.8690377473831177  val: loss: 6614230.0 acc: 0.8864881992340088\n",
      "step: 9045\n",
      "train: loss: 4366429.0 acc: 0.8676040768623352  val: loss: 11366616.0 acc: 0.8987374305725098\n",
      "step: 9050\n",
      "train: loss: 7646327.5 acc: 0.7505667209625244  val: loss: 7371408.0 acc: 0.8702636361122131\n",
      "step: 9055\n",
      "train: loss: 5231665.0 acc: 0.7998514175415039  val: loss: 2815810.25 acc: 0.8498548865318298\n",
      "step: 9060\n",
      "train: loss: 4794711.0 acc: 0.7350723147392273  val: loss: 7465374.5 acc: 0.8840457797050476\n",
      "step: 9065\n",
      "train: loss: 5289480.5 acc: 0.7626132369041443  val: loss: 10204967.0 acc: 0.8647130131721497\n",
      "step: 9070\n",
      "train: loss: 2399164.0 acc: 0.7838093042373657  val: loss: 12519581.0 acc: 0.8430176377296448\n",
      "step: 9075\n",
      "train: loss: 6394693.5 acc: 0.7610801458358765  val: loss: 14175512.0 acc: 0.8248623609542847\n",
      "step: 9080\n",
      "train: loss: 4293882.5 acc: 0.7836611270904541  val: loss: 10683717.0 acc: 0.6373151540756226\n",
      "step: 9085\n",
      "train: loss: 13594072.0 acc: 0.7176275253295898  val: loss: 11676756.0 acc: 0.7420357465744019\n",
      "step: 9090\n",
      "train: loss: 4797020.5 acc: 0.7727010846138  val: loss: 4438207.0 acc: 0.78678297996521\n",
      "step: 9095\n",
      "train: loss: 6627448.0 acc: 0.8746514320373535  val: loss: 8660371.0 acc: 0.769220769405365\n",
      "step: 9100\n",
      "train: loss: 10519724.0 acc: 0.7890571355819702  val: loss: 13446050.0 acc: 0.766031801700592\n",
      "step: 9105\n",
      "train: loss: 7839825.0 acc: 0.8763431906700134  val: loss: 7834960.0 acc: 0.7592689394950867\n",
      "step: 9110\n",
      "train: loss: 9053090.0 acc: 0.8743573427200317  val: loss: 5832362.0 acc: 0.8926096558570862\n",
      "step: 9115\n",
      "train: loss: 2714879.25 acc: 0.9419589638710022  val: loss: 5132715.0 acc: 0.9207403659820557\n",
      "step: 9120\n",
      "train: loss: 3080102.0 acc: 0.9255338907241821  val: loss: 11718191.0 acc: -0.05851864814758301\n",
      "step: 9125\n",
      "train: loss: 3073395.75 acc: 0.9594798684120178  val: loss: 12759936.0 acc: 0.8590673208236694\n",
      "step: 9130\n",
      "train: loss: 2428282.0 acc: 0.970026969909668  val: loss: 4169534.75 acc: 0.8934189081192017\n",
      "step: 9135\n",
      "train: loss: 3797128.75 acc: 0.9604305028915405  val: loss: 1823696.875 acc: 0.9627007246017456\n",
      "step: 9140\n",
      "train: loss: 2777865.5 acc: 0.9739575386047363  val: loss: 6880303.0 acc: 0.917229175567627\n",
      "step: 9145\n",
      "train: loss: 964318.9375 acc: 0.9872913360595703  val: loss: 9297993.0 acc: 0.9048004746437073\n",
      "step: 9150\n",
      "train: loss: 4816959.5 acc: 0.9114351868629456  val: loss: 7622224.0 acc: 0.7659060955047607\n",
      "step: 9155\n",
      "train: loss: 3474474.25 acc: 0.9204388856887817  val: loss: 12893233.0 acc: 0.8130954504013062\n",
      "step: 9160\n",
      "train: loss: 1519371.75 acc: 0.9768248796463013  val: loss: 2653385.75 acc: 0.8557165861129761\n",
      "step: 9165\n",
      "train: loss: 1471258.5 acc: 0.9553008079528809  val: loss: 10014542.0 acc: 0.7442630529403687\n",
      "step: 9170\n",
      "train: loss: 695676.9375 acc: 0.9744017124176025  val: loss: 24479372.0 acc: 0.8067562580108643\n",
      "step: 9175\n",
      "train: loss: 1040537.4375 acc: 0.9682307243347168  val: loss: 13685270.0 acc: 0.3335258364677429\n",
      "step: 9180\n",
      "train: loss: 1663854.75 acc: 0.9342114329338074  val: loss: 7967072.0 acc: 0.7051398754119873\n",
      "step: 9185\n",
      "train: loss: 1200072.25 acc: 0.9701430797576904  val: loss: 10872587.0 acc: 0.873252272605896\n",
      "step: 9190\n",
      "train: loss: 545324.125 acc: 0.811805248260498  val: loss: 16410492.0 acc: 0.7614425420761108\n",
      "step: 9195\n",
      "train: loss: 1412263.875 acc: 0.8745434284210205  val: loss: 10578593.0 acc: 0.6297198534011841\n",
      "step: 9200\n",
      "train: loss: 359141.25 acc: 0.9855173230171204  val: loss: 8476339.0 acc: 0.789172351360321\n",
      "step: 9205\n",
      "train: loss: 463269.875 acc: 0.9860867857933044  val: loss: 23350058.0 acc: 0.5258216261863708\n",
      "step: 9210\n",
      "train: loss: 274980.8125 acc: 0.9881529808044434  val: loss: 13229344.0 acc: 0.7138488292694092\n",
      "step: 9215\n",
      "train: loss: 375668.15625 acc: 0.8886376023292542  val: loss: 7698225.5 acc: 0.8408998847007751\n",
      "step: 9220\n",
      "train: loss: 451476.40625 acc: 0.9170039892196655  val: loss: 2773927.75 acc: 0.8640267848968506\n",
      "step: 9225\n",
      "train: loss: 162338.140625 acc: 0.9235319495201111  val: loss: 896079.3125 acc: 0.9570387601852417\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 9230\n",
      "train: loss: 422032.90625 acc: 0.9316386580467224  val: loss: 18543618.0 acc: 0.5148207545280457\n",
      "step: 9235\n",
      "train: loss: 500749.6875 acc: 0.9837260246276855  val: loss: 17966130.0 acc: 0.717068076133728\n",
      "step: 9240\n",
      "train: loss: 1872165.5 acc: 0.9083181619644165  val: loss: 20584876.0 acc: 0.7317209839820862\n",
      "step: 9245\n",
      "train: loss: 970947.5 acc: 0.919842004776001  val: loss: 11348968.0 acc: 0.7752582430839539\n",
      "step: 9250\n",
      "train: loss: 1226921.625 acc: 0.9433889389038086  val: loss: 11158287.0 acc: 0.7354850769042969\n",
      "step: 9255\n",
      "train: loss: 673363.625 acc: 0.9703154563903809  val: loss: 14032943.0 acc: 0.3993566632270813\n",
      "step: 9260\n",
      "train: loss: 667385.5625 acc: 0.9451054930686951  val: loss: 14551610.0 acc: 0.7145801782608032\n",
      "step: 9265\n",
      "train: loss: 838833.3125 acc: 0.9080603718757629  val: loss: 30941542.0 acc: 0.7121970653533936\n",
      "step: 9270\n",
      "train: loss: 834694.25 acc: 0.9383665919303894  val: loss: 24486620.0 acc: 0.696147084236145\n",
      "step: 9275\n",
      "train: loss: 662819.625 acc: 0.9427028894424438  val: loss: 15089234.0 acc: 0.7087706923484802\n",
      "step: 9280\n",
      "train: loss: 342917.84375 acc: 0.9201051592826843  val: loss: 1286057.375 acc: 0.9345801472663879\n",
      "step: 9285\n",
      "train: loss: 808637.5 acc: 0.9137176871299744  val: loss: 7335567.0 acc: 0.7800304889678955\n",
      "step: 9290\n",
      "train: loss: 242104.921875 acc: 0.9607911109924316  val: loss: 3508171.75 acc: 0.8694654703140259\n",
      "step: 9295\n",
      "train: loss: 587686.625 acc: 0.9599709510803223  val: loss: 10420638.0 acc: 0.823459267616272\n",
      "step: 9300\n",
      "train: loss: 1086965.0 acc: 0.9495810866355896  val: loss: 15579705.0 acc: 0.5893176198005676\n",
      "step: 9305\n",
      "train: loss: 979866.875 acc: 0.9405696392059326  val: loss: 11730357.0 acc: 0.7803251147270203\n",
      "step: 9310\n",
      "train: loss: 519710.25 acc: 0.9591689109802246  val: loss: 12831771.0 acc: 0.7328893542289734\n",
      "step: 9315\n",
      "train: loss: 794673.375 acc: 0.9552884101867676  val: loss: 14272636.0 acc: 0.39219170808792114\n",
      "step: 9320\n",
      "train: loss: 1549813.125 acc: 0.9457895159721375  val: loss: 13560082.0 acc: 0.8401654958724976\n",
      "step: 9325\n",
      "train: loss: 2066905.375 acc: 0.8396627902984619  val: loss: 12348096.0 acc: 0.7498489618301392\n",
      "step: 9330\n",
      "train: loss: 2220452.75 acc: 0.9018889665603638  val: loss: 4151006.25 acc: 0.8883458375930786\n",
      "step: 9335\n",
      "train: loss: 1473942.0 acc: 0.9391767382621765  val: loss: 3468361.0 acc: 0.9767164587974548\n",
      "step: 9340\n",
      "train: loss: 866591.5 acc: 0.9641323089599609  val: loss: 6678288.0 acc: 0.8711181879043579\n",
      "step: 9345\n",
      "train: loss: 947145.6875 acc: 0.9624136686325073  val: loss: 7751627.0 acc: 0.9029566049575806\n",
      "step: 9350\n",
      "train: loss: 2417553.0 acc: 0.9109383225440979  val: loss: 5857234.0 acc: 0.9115113615989685\n",
      "step: 9355\n",
      "train: loss: 3487455.75 acc: 0.9262719750404358  val: loss: 11371678.0 acc: 0.5440479516983032\n",
      "step: 9360\n",
      "train: loss: 1620417.0 acc: 0.9649857878684998  val: loss: 6463262.0 acc: 0.8767638206481934\n",
      "step: 9365\n",
      "train: loss: 1722244.0 acc: 0.9460350275039673  val: loss: 13363336.0 acc: 0.7836718559265137\n",
      "step: 9370\n",
      "train: loss: 1068098.375 acc: 0.9784446954727173  val: loss: 6609660.5 acc: 0.8381478786468506\n",
      "step: 9375\n",
      "train: loss: 2706794.75 acc: 0.9185134172439575  val: loss: 7842776.0 acc: 0.887988805770874\n",
      "step: 9380\n",
      "train: loss: 4026647.25 acc: 0.9034671783447266  val: loss: 7355188.5 acc: 0.46576762199401855\n",
      "step: 9385\n",
      "train: loss: 5465055.0 acc: 0.9230038523674011  val: loss: 2422968.75 acc: 0.9038926959037781\n",
      "step: 9390\n",
      "train: loss: 9630528.0 acc: 0.9060859680175781  val: loss: 6998464.5 acc: 0.8461658358573914\n",
      "step: 9395\n",
      "train: loss: 10020859.0 acc: 0.881087064743042  val: loss: 5159958.0 acc: 0.9385687708854675\n",
      "step: 9400\n",
      "train: loss: 8501736.0 acc: 0.8495296835899353  val: loss: 5760772.5 acc: 0.911064088344574\n",
      "step: 9405\n",
      "train: loss: 6539503.5 acc: 0.9243131279945374  val: loss: 10375454.0 acc: 0.8337481021881104\n",
      "step: 9410\n",
      "train: loss: 5071244.5 acc: 0.9359954595565796  val: loss: 29118092.0 acc: 0.5439651012420654\n",
      "step: 9415\n",
      "train: loss: 8394579.0 acc: 0.9608253836631775  val: loss: 14970968.0 acc: 0.777947187423706\n",
      "step: 9420\n",
      "train: loss: 11246878.0 acc: 0.9622228145599365  val: loss: 6552397.5 acc: 0.8367593288421631\n",
      "step: 9425\n",
      "train: loss: 6461455.5 acc: 0.9364640712738037  val: loss: 21176028.0 acc: 0.7747779488563538\n",
      "step: 9430\n",
      "train: loss: 4268251.0 acc: 0.9693729877471924  val: loss: 4125519.25 acc: 0.865111768245697\n",
      "step: 9435\n",
      "train: loss: 4341431.0 acc: 0.9240472316741943  val: loss: 4871022.0 acc: 0.41252756118774414\n",
      "step: 9440\n",
      "train: loss: 7359739.5 acc: 0.9468830823898315  val: loss: 7529336.5 acc: 0.8896341919898987\n",
      "step: 9445\n",
      "train: loss: 6569088.5 acc: 0.938687801361084  val: loss: 11625267.0 acc: 0.6936022043228149\n",
      "step: 9450\n",
      "train: loss: 5994530.5 acc: 0.9050697684288025  val: loss: 7578667.5 acc: 0.7286285161972046\n",
      "step: 9455\n",
      "train: loss: 11609849.0 acc: 0.9040115475654602  val: loss: 7946518.5 acc: 0.8233965635299683\n",
      "step: 9460\n",
      "train: loss: 3308287.0 acc: 0.9386103749275208  val: loss: 5550586.0 acc: 0.8697879314422607\n",
      "step: 9465\n",
      "train: loss: 6380504.5 acc: 0.8900538682937622  val: loss: 13767275.0 acc: 0.8453187942504883\n",
      "step: 9470\n",
      "train: loss: 3137863.75 acc: 0.9588186144828796  val: loss: 3587900.5 acc: 0.9498067498207092\n",
      "step: 9475\n",
      "train: loss: 2048694.875 acc: 0.9488534331321716  val: loss: 4167497.25 acc: 0.790930986404419\n",
      "step: 9480\n",
      "train: loss: 5195834.5 acc: 0.8517531156539917  val: loss: 2940247.5 acc: 0.8596943616867065\n",
      "step: 9485\n",
      "train: loss: 6416962.0 acc: 0.7935526371002197  val: loss: 6404259.5 acc: 0.8490505218505859\n",
      "step: 9490\n",
      "train: loss: 5246551.5 acc: 0.9220641255378723  val: loss: 4659435.5 acc: 0.9228162169456482\n",
      "step: 9495\n",
      "train: loss: 4543597.5 acc: 0.7954367995262146  val: loss: 8632867.0 acc: 0.6957851052284241\n",
      "step: 9500\n",
      "train: loss: 3278776.5 acc: 0.7972933053970337  val: loss: 9196720.0 acc: 0.8720038533210754\n",
      "step: 9505\n",
      "train: loss: 3018679.5 acc: 0.8684077262878418  val: loss: 6915206.0 acc: 0.8799077272415161\n",
      "step: 9510\n",
      "train: loss: 2821233.25 acc: 0.8779686689376831  val: loss: 7783052.0 acc: 0.7529723644256592\n",
      "step: 9515\n",
      "train: loss: 2760068.25 acc: 0.9453774690628052  val: loss: 15775603.0 acc: 0.6941306591033936\n",
      "step: 9520\n",
      "train: loss: 7629668.5 acc: 0.7618736028671265  val: loss: 6547477.5 acc: 0.9007280468940735\n",
      "step: 9525\n",
      "train: loss: 7457532.5 acc: 0.8002508878707886  val: loss: 6218154.5 acc: 0.7635769248008728\n",
      "step: 9530\n",
      "train: loss: 2065954.875 acc: 0.8663482069969177  val: loss: 19862730.0 acc: 0.6768174767494202\n",
      "step: 9535\n",
      "train: loss: 6232593.5 acc: 0.797132134437561  val: loss: 9263320.0 acc: 0.8786181211471558\n",
      "step: 9540\n",
      "train: loss: 2525376.0 acc: 0.8476430773735046  val: loss: 4830295.5 acc: 0.7765606045722961\n",
      "step: 9545\n",
      "train: loss: 3605288.0 acc: 0.8352559804916382  val: loss: 14806259.0 acc: 0.8104598522186279\n",
      "step: 9550\n",
      "train: loss: 1885039.625 acc: 0.8796350359916687  val: loss: 15687931.0 acc: 0.8645457625389099\n",
      "step: 9555\n",
      "train: loss: 1464889.0 acc: 0.8616997003555298  val: loss: 12518322.0 acc: 0.6258295774459839\n",
      "step: 9560\n",
      "train: loss: 1030021.5 acc: 0.9203081130981445  val: loss: 4795340.5 acc: 0.9067763090133667\n",
      "step: 9565\n",
      "train: loss: 4757267.0 acc: 0.7796662449836731  val: loss: 10031171.0 acc: 0.7662523984909058\n",
      "step: 9570\n",
      "train: loss: 6725479.5 acc: 0.7591405510902405  val: loss: 10593340.0 acc: 0.859805703163147\n",
      "step: 9575\n",
      "train: loss: 6987674.0 acc: 0.7053294777870178  val: loss: 5371573.0 acc: 0.704399585723877\n",
      "step: 9580\n",
      "train: loss: 2297561.5 acc: 0.8205767869949341  val: loss: 17890172.0 acc: 0.831464409828186\n",
      "step: 9585\n",
      "train: loss: 8531080.0 acc: 0.6950055956840515  val: loss: 13021591.0 acc: 0.8452438712120056\n",
      "step: 9590\n",
      "train: loss: 5288113.5 acc: 0.7689470052719116  val: loss: 5609585.0 acc: 0.7548229694366455\n",
      "step: 9595\n",
      "train: loss: 9280410.0 acc: 0.7087946534156799  val: loss: 12562052.0 acc: 0.8897548317909241\n",
      "step: 9600\n",
      "train: loss: 10859426.0 acc: 0.7168876528739929  val: loss: 15171036.0 acc: 0.7125704884529114\n",
      "step: 9605\n",
      "train: loss: 11055711.0 acc: 0.7868896126747131  val: loss: 2977903.75 acc: 0.7178134322166443\n",
      "step: 9610\n",
      "train: loss: 7217464.0 acc: 0.8449832797050476  val: loss: 4697767.0 acc: 0.8779301047325134\n",
      "step: 9615\n",
      "train: loss: 8220888.0 acc: 0.6940069198608398  val: loss: 6971591.5 acc: 0.9159502387046814\n",
      "step: 9620\n",
      "train: loss: 5734272.5 acc: 0.9235135316848755  val: loss: 7632837.5 acc: 0.857083797454834\n",
      "step: 9625\n",
      "train: loss: 8164688.5 acc: 0.8725341558456421  val: loss: 19604190.0 acc: 0.13685864210128784\n",
      "step: 9630\n",
      "train: loss: 4661669.5 acc: 0.8506177663803101  val: loss: 14674954.0 acc: 0.8143185377120972\n",
      "step: 9635\n",
      "train: loss: 5099004.0 acc: 0.9172621965408325  val: loss: 11943450.0 acc: 0.8133413791656494\n",
      "step: 9640\n",
      "train: loss: 5191578.5 acc: 0.8927668333053589  val: loss: 7701805.0 acc: 0.9152133464813232\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 9645\n",
      "train: loss: 4817126.5 acc: 0.9357168674468994  val: loss: 7352888.5 acc: 0.761263906955719\n",
      "step: 9650\n",
      "train: loss: 2418766.5 acc: 0.9652509689331055  val: loss: 25263948.0 acc: 0.6991649866104126\n",
      "step: 9655\n",
      "train: loss: 1619022.25 acc: 0.9846481084823608  val: loss: 5910029.5 acc: 0.9224178194999695\n",
      "step: 9660\n",
      "train: loss: 6856208.0 acc: 0.9361594319343567  val: loss: 7272671.0 acc: 0.7815593481063843\n",
      "step: 9665\n",
      "train: loss: 2311577.0 acc: 0.9608935713768005  val: loss: 8638782.0 acc: 0.8631575703620911\n",
      "step: 9670\n",
      "train: loss: 3694923.0 acc: 0.9445474743843079  val: loss: 8728517.0 acc: 0.8404074907302856\n",
      "step: 9675\n",
      "train: loss: 465078.875 acc: 0.9941495060920715  val: loss: 10386895.0 acc: 0.8094267249107361\n",
      "step: 9680\n",
      "train: loss: 1505646.125 acc: 0.970809817314148  val: loss: 11564161.0 acc: 0.8161998391151428\n",
      "step: 9685\n",
      "train: loss: 5604896.0 acc: 0.7877157926559448  val: loss: 8990937.0 acc: 0.8577907085418701\n",
      "step: 9690\n",
      "train: loss: 386757.375 acc: 0.9819884300231934  val: loss: 7480036.5 acc: 0.7345119714736938\n",
      "step: 9695\n",
      "train: loss: 358802.71875 acc: 0.9851421117782593  val: loss: 4832660.5 acc: 0.8843896985054016\n",
      "step: 9700\n",
      "train: loss: 390050.03125 acc: 0.9929686784744263  val: loss: 6962199.5 acc: 0.7778103947639465\n",
      "step: 9705\n",
      "train: loss: 621713.4375 acc: 0.9840160608291626  val: loss: 7608755.0 acc: 0.7105483412742615\n",
      "step: 9710\n",
      "train: loss: 516510.53125 acc: 0.9664021730422974  val: loss: 8221576.0 acc: 0.8620944023132324\n",
      "step: 9715\n",
      "train: loss: 1969919.375 acc: 0.9300353527069092  val: loss: 13735128.0 acc: 0.8188523054122925\n",
      "step: 9720\n",
      "train: loss: 629455.0625 acc: 0.9764053821563721  val: loss: 12374051.0 acc: 0.8567313551902771\n",
      "step: 9725\n",
      "train: loss: 377197.4375 acc: 0.9534982442855835  val: loss: 2282552.0 acc: 0.9659638404846191\n",
      "step: 9730\n",
      "train: loss: 179982.265625 acc: 0.9053779244422913  val: loss: 4241458.5 acc: 0.9371233582496643\n",
      "step: 9735\n",
      "train: loss: 910351.0 acc: 0.9238368272781372  val: loss: 15702479.0 acc: 0.3227142095565796\n",
      "step: 9740\n",
      "train: loss: 524361.9375 acc: 0.8680573105812073  val: loss: 11926635.0 acc: 0.8569372296333313\n",
      "step: 9745\n",
      "train: loss: 3016736.5 acc: 0.3775569796562195  val: loss: 3973193.5 acc: 0.7402623295783997\n",
      "step: 9750\n",
      "train: loss: 357744.71875 acc: 0.9765128493309021  val: loss: 7713354.5 acc: 0.7877190709114075\n",
      "step: 9755\n",
      "train: loss: 493120.84375 acc: 0.9696930050849915  val: loss: 12823342.0 acc: 0.6368426084518433\n",
      "step: 9760\n",
      "train: loss: 502669.71875 acc: 0.9132735133171082  val: loss: 19224764.0 acc: 0.7581956386566162\n",
      "step: 9765\n",
      "train: loss: 875387.9375 acc: 0.8493744134902954  val: loss: 17615236.0 acc: 0.4848552942276001\n",
      "step: 9770\n",
      "train: loss: 976947.875 acc: 0.9313340187072754  val: loss: 3320636.0 acc: 0.9069697856903076\n",
      "step: 9775\n",
      "train: loss: 1143338.375 acc: 0.926926851272583  val: loss: 16660142.0 acc: 0.8140653967857361\n",
      "step: 9780\n",
      "train: loss: 901324.4375 acc: 0.9163861274719238  val: loss: 8239686.5 acc: 0.8304792642593384\n",
      "step: 9785\n",
      "train: loss: 496052.0 acc: 0.97663813829422  val: loss: 25596348.0 acc: 0.532609224319458\n",
      "step: 9790\n",
      "train: loss: 339115.90625 acc: 0.9564535021781921  val: loss: 7742106.5 acc: 0.8323106169700623\n",
      "step: 9795\n",
      "train: loss: 713668.1875 acc: 0.9109338521957397  val: loss: 19361780.0 acc: 0.7181318998336792\n",
      "step: 9800\n",
      "train: loss: 917677.125 acc: 0.9369789361953735  val: loss: 4204546.5 acc: 0.9293531179428101\n",
      "step: 9805\n",
      "train: loss: 345436.375 acc: 0.9725929498672485  val: loss: 3632994.5 acc: 0.9122583866119385\n",
      "step: 9810\n",
      "train: loss: 295225.375 acc: 0.9589875936508179  val: loss: 10796113.0 acc: 0.8121330738067627\n",
      "step: 9815\n",
      "train: loss: 1413682.25 acc: 0.949686586856842  val: loss: 20381974.0 acc: 0.19129908084869385\n",
      "step: 9820\n",
      "train: loss: 1665940.625 acc: 0.9111635684967041  val: loss: 5874926.0 acc: 0.8581932783126831\n",
      "step: 9825\n",
      "train: loss: 627165.125 acc: 0.9556898474693298  val: loss: 16267239.0 acc: -0.07643234729766846\n",
      "step: 9830\n",
      "train: loss: 523402.15625 acc: 0.924453616142273  val: loss: 13218397.0 acc: 0.7609792947769165\n",
      "step: 9835\n",
      "train: loss: 742755.1875 acc: 0.958271324634552  val: loss: 7211681.0 acc: 0.870570182800293\n",
      "step: 9840\n",
      "train: loss: 941202.4375 acc: 0.9614378213882446  val: loss: 10759165.0 acc: 0.784234881401062\n",
      "step: 9845\n",
      "train: loss: 2847851.0 acc: 0.8636742830276489  val: loss: 9741271.0 acc: 0.800045371055603\n",
      "step: 9850\n",
      "train: loss: 2940400.0 acc: 0.902111291885376  val: loss: 17503294.0 acc: 0.4038754105567932\n",
      "step: 9855\n",
      "train: loss: 2142336.5 acc: 0.908989429473877  val: loss: 9043153.0 acc: 0.7467244267463684\n",
      "step: 9860\n",
      "train: loss: 1859854.125 acc: 0.9439314007759094  val: loss: 8299176.5 acc: 0.9246598482131958\n",
      "step: 9865\n",
      "train: loss: 682913.5 acc: 0.9720595479011536  val: loss: 5440664.5 acc: 0.9080775380134583\n",
      "step: 9870\n",
      "train: loss: 1980389.125 acc: 0.9634501338005066  val: loss: 10571791.0 acc: 0.8610472083091736\n",
      "step: 9875\n",
      "train: loss: 2283944.75 acc: 0.9474809169769287  val: loss: 5075072.5 acc: 0.9024416208267212\n",
      "step: 9880\n",
      "train: loss: 2265916.75 acc: 0.9641639590263367  val: loss: 6548854.5 acc: 0.8045837879180908\n",
      "step: 9885\n",
      "train: loss: 1244889.125 acc: 0.9733306765556335  val: loss: 15544446.0 acc: -0.9823287725448608\n",
      "step: 9890\n",
      "train: loss: 1416374.625 acc: 0.9775335788726807  val: loss: 10602447.0 acc: 0.7647765278816223\n",
      "step: 9895\n",
      "train: loss: 6472635.5 acc: 0.8837252855300903  val: loss: 15788932.0 acc: 0.772945761680603\n",
      "step: 9900\n",
      "train: loss: 6090144.5 acc: 0.8815661072731018  val: loss: 6387115.5 acc: 0.8896828293800354\n",
      "step: 9905\n",
      "train: loss: 11739820.0 acc: 0.8802715539932251  val: loss: 10832451.0 acc: 0.7683759927749634\n",
      "step: 9910\n",
      "train: loss: 9613909.0 acc: 0.8742978572845459  val: loss: 2699489.25 acc: 0.8806037902832031\n",
      "step: 9915\n",
      "train: loss: 6812883.0 acc: 0.8649784326553345  val: loss: 2792769.25 acc: 0.9028451442718506\n",
      "step: 9920\n",
      "train: loss: 12335961.0 acc: 0.8338906168937683  val: loss: 4919478.5 acc: 0.9189397692680359\n",
      "step: 9925\n",
      "train: loss: 10653766.0 acc: 0.9463659524917603  val: loss: 2530157.25 acc: 0.976806104183197\n",
      "step: 9930\n",
      "train: loss: 5642406.0 acc: 0.970149040222168  val: loss: 8994171.0 acc: 0.5243849754333496\n",
      "step: 9935\n",
      "train: loss: 9345842.0 acc: 0.9620603322982788  val: loss: 13722113.0 acc: 0.5212981104850769\n",
      "step: 9940\n",
      "train: loss: 12578541.0 acc: 0.901375412940979  val: loss: 58067092.0 acc: 0.4495915174484253\n",
      "step: 9945\n",
      "train: loss: 6288260.5 acc: 0.9440815448760986  val: loss: 32357362.0 acc: 0.38502371311187744\n",
      "step: 9950\n",
      "train: loss: 11334043.0 acc: 0.9210265874862671  val: loss: 3897363.25 acc: 0.9171890020370483\n",
      "step: 9955\n",
      "train: loss: 10633467.0 acc: 0.9445815682411194  val: loss: 3381820.5 acc: 0.9253112077713013\n",
      "step: 9960\n",
      "train: loss: 3065137.25 acc: 0.9791271090507507  val: loss: 10380982.0 acc: 0.7211727499961853\n",
      "step: 9965\n",
      "train: loss: 5689158.5 acc: 0.9528989791870117  val: loss: 2085807.125 acc: 0.9571176171302795\n",
      "step: 9970\n",
      "train: loss: 7065988.0 acc: 0.9196776151657104  val: loss: 10389735.0 acc: 0.837894856929779\n",
      "step: 9975\n",
      "train: loss: 13876240.0 acc: 0.8926461935043335  val: loss: 3605445.0 acc: 0.8794665336608887\n",
      "step: 9980\n",
      "train: loss: 19669052.0 acc: 0.6276204586029053  val: loss: 1770850.125 acc: 0.8873622417449951\n",
      "step: 9985\n",
      "train: loss: 7107105.0 acc: 0.9121900796890259  val: loss: 11334029.0 acc: 0.7935223579406738\n",
      "step: 9990\n",
      "train: loss: 4691959.5 acc: 0.9503334164619446  val: loss: 6210501.5 acc: 0.8619476556777954\n",
      "step: 9995\n",
      "train: loss: 6420470.5 acc: 0.8198400139808655  val: loss: 2726463.5 acc: 0.9679564237594604\n"
     ]
    }
   ],
   "source": [
    "min_loss=10000\n",
    "sess=tf.Session()\n",
    "train_writer=tf.summary.FileWriter('D:/graph/twomonthes/wind1/train/',sess.graph)\n",
    "test_writer = tf.summary.FileWriter('D:/graph/twomonthes/wind1/test/', sess.graph)\n",
    "saver=tf.train.Saver(max_to_keep=3)\n",
    "\n",
    "init_op = tf.global_variables_initializer()\n",
    "sess.run(init_op)\n",
    "coord = tf.train.Coordinator()\n",
    "threads = tf.train.start_queue_runners(sess, coord)\n",
    "\n",
    "for training_itr in range(10000):\n",
    "    x1, y1 = sess.run([batch_xt,batch_yt])\n",
    "    feed_dict1 = {enc_inp[t]: x1[:,t,:] for t in range(len(enc_inp))}\n",
    "    feed_dict1.update({expected_sparse_output[k]: y1[:,k,:] for k in range(len(expected_sparse_output))})\n",
    "    _, loss1,acc1,summaries1 = sess.run([train_op, loss,acc,merged_summary], feed_dict1)\n",
    "\n",
    "    train_writer.add_summary(summaries1, training_itr)\n",
    "    if training_itr %5==0:\n",
    "#             saver.save(sess=sess, save_path='model/hand_landmark_v6.1_model/model.ckpt',global_step=(global_step + 1))\n",
    "        mean_val_loss = 0\n",
    "\n",
    "        x2,y2=sess.run([batch_xv,batch_yv])\n",
    "        feed_dict2 = {enc_inp[t]: x2[:,t,:] for t in range(len(enc_inp))}\n",
    "        feed_dict2.update({expected_sparse_output[t]: y2[:,t,:] for t in range(len(expected_sparse_output))})\n",
    "        loss2,acc2,summaries2 = sess.run([loss,acc,merged_summary], feed_dict2)\n",
    "\n",
    "        print('step: {}'.format(training_itr))\n",
    "        print('train: loss: {} acc: {}  val: loss: {} acc: {}'.format(loss1,acc1,loss2,acc2))\n",
    "        test_writer.add_summary(summaries2, training_itr)\n",
    "        if loss1 < min_loss:\n",
    "            min_loss=loss1\n",
    "            saver.save(sess=sess, save_path='D:/model//twomonthes/wind1/model.ckpt',global_step=(training_itr + 1))\n",
    "sess.close()\n",
    "coord.request_stop()\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
