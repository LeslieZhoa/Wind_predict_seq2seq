{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import h5py\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#读取数据\n",
    "f=h5py.File('data/tdata1.h5','r')\n",
    "data_x=f['x'][:]\n",
    "data_y=f['y'][:]\n",
    "#求取均值方差，归一化\n",
    "x_=np.reshape(data_x,(-1,3))\n",
    "y_=np.reshape(data_y,(-1,1))\n",
    "x_mean=np.mean(x_,axis=0)\n",
    "x_std=np.std(x_,axis=0)\n",
    "\n",
    "y_mean=np.mean(y_,axis=0)\n",
    "y_std=np.std(y_,axis=0)\n",
    "\n",
    "data_x=(data_x-x_mean)/x_std\n",
    "data_y=(data_y-y_mean)/y_std\n",
    "#分配训练测试集\n",
    "num=list(range(data_x.shape[0]))\n",
    "num1=random.sample(num,200)\n",
    "num2=set(num)-set(num1)\n",
    "num2=list(num2)\n",
    "num1.sort()\n",
    "\n",
    "\n",
    "x_train=data_x[num2]\n",
    "y_train=data_y[num2]\n",
    "\n",
    "x_test=data_x[num1]\n",
    "y_test=data_y[num1]\n",
    "#生成批次\n",
    "train_queue = tf.train.slice_input_producer([x_train,y_train],shuffle=None)\n",
    "val_queue = tf.train.slice_input_producer([x_test,y_test],shuffle=None)\n",
    "batch_xt,batch_yt=tf.train.shuffle_batch(train_queue,batch_size=16,capacity=500,min_after_dequeue=150)\n",
    "batch_xv,batch_yv=tf.train.shuffle_batch(val_queue,batch_size=16,capacity=500,min_after_dequeue=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 搭建模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 超参数设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = batch_xt.shape[1]  # Time series will have the same past and future (to be predicted) lenght. \n",
    "batch_size = batch_xt.shape[0]  # Low value used for live demo purposes - 100 and 1000 would be possible too, crank that up!\n",
    "\n",
    "output_dim = 1\n",
    "input_dim = batch_xt.shape[-1]  # Output dimension (e.g.: multiple signals at once, tied in time)\n",
    "hidden_dim = 12 # Count of hidden neurons in the recurrent units. \n",
    "layers_stacked_count = 4  # Number of stacked recurrent cells, on the neural depth axis. \n",
    "\n",
    "# Optmizer: \n",
    "learning_rate = 0.007  # Small lr helps not to diverge during training. \n",
    "nb_iters = 10000  # How many times we perform a training step (therefore how many times we show a batch). \n",
    "lr_decay = 0.92  # default: 0.9 . Simulated annealing.\n",
    "momentum = 0.5  # default: 0.0 . Momentum technique in weights update\n",
    "lambda_l2_reg = 0.003  # L2 regularization of weights - avoids overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 搭建s2s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('encoder'):\n",
    "\n",
    "    # Encoder: inputs\n",
    "    enc_inp = tf.placeholder(tf.float32, shape=(None, 10,3), name=\"inp\")\n",
    "\n",
    "    # Decoder: expected outputs\n",
    "    expected_sparse_output = tf.placeholder(tf.float32, shape=(None, 10,1), name=\"expected_sparse_output\")\n",
    "    pharse=tf.placeholder(tf.bool,name='if_train')\n",
    "    \n",
    "   \n",
    "\n",
    "    # encoder cell \n",
    "    cells = []\n",
    "    for i in range(layers_stacked_count):\n",
    "        with tf.variable_scope('RNN_{}'.format(i)):\n",
    "            cells.append(tf.contrib.rnn.GRUCell(hidden_dim))\n",
    "            # cells.append(tf.nn.rnn_cell.BasicLSTMCell(...))\n",
    "    cell = tf.contrib.rnn.MultiRNNCell(cells)\n",
    "    \n",
    "    \n",
    "    enc_outputs, enc_memory = tf.nn.dynamic_rnn(cell,enc_inp,dtype=tf.float32)\n",
    "\n",
    "with tf.variable_scope('decoder'):  \n",
    "    decells=[]\n",
    "    for i in range(layers_stacked_count):\n",
    "        with tf.variable_scope('RNN_{}'.format(i)):\n",
    "            decells.append(tf.contrib.rnn.GRUCell(hidden_dim))\n",
    "            \n",
    "    decell = tf.contrib.rnn.MultiRNNCell(decells)\n",
    "    dec_state=enc_memory\n",
    "    dec_inp=enc_outputs[:,-1,:]\n",
    "    dec_outputs=[]\n",
    "    #reshape expected_sparse_output\n",
    "    w_h = tf.Variable(tf.random_normal([output_dim,hidden_dim]))\n",
    "    b_h = tf.Variable(tf.random_normal([hidden_dim]))\n",
    "    dec_inputs=[tf.matmul(expected_sparse_output[:,i,:],w_h)+b_h for i in range(seq_length)]\n",
    "    \n",
    "    for time_step in range(seq_length):\n",
    "        if time_step>0:tf.get_variable_scope().reuse_variables()\n",
    "        (dec_output,dec_state)=decell(dec_inp,dec_state)\n",
    "        dec_inp = tf.where(tf.equal(pharse,True),dec_inputs[time_step],dec_output)\n",
    "        dec_outputs.append(dec_output)\n",
    "    \n",
    "    # reshape seq2seq输出 \n",
    "    w_out = tf.Variable(tf.random_normal([hidden_dim, output_dim]))\n",
    "    b_out = tf.Variable(tf.random_normal([output_dim]))\n",
    "    \n",
    "    # 最终输出\n",
    "    output_scale_factor = tf.Variable(1.0, name=\"Output_ScaleFactor\")\n",
    "    \n",
    "    reshaped_outputs = [output_scale_factor*(tf.matmul(i, w_out) + b_out) for i in dec_outputs]\n",
    "    reshaped_outputs=tf.convert_to_tensor(reshaped_outputs)\n",
    "    reshaped_outputs=tf.transpose(reshaped_outputs,(1,0,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 搭建损失函数 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Training loss and optimizer\n",
    "\n",
    "with tf.variable_scope('Loss'):\n",
    "    # L2 loss\n",
    "    output_loss = 0\n",
    "    \n",
    "    output_loss = tf.reduce_sum(tf.square(reshaped_outputs - expected_sparse_output))\n",
    "        \n",
    "    # L2 regularization (to avoid overfitting and to have a  better generalization capacity)\n",
    "    reg_loss = 0\n",
    "    for tf_var in tf.trainable_variables():\n",
    "        if not (\"Bias\" in tf_var.name or \"Output_\" in tf_var.name):\n",
    "            reg_loss += tf.reduce_mean(tf.nn.l2_loss(tf_var))\n",
    "            \n",
    "#     loss = output_loss + lambda_l2_reg * reg_loss\n",
    "    loss = output_loss\n",
    "    tf.summary.scalar('loss',loss)\n",
    "\n",
    "with tf.variable_scope('acc'):\n",
    "    acc=1-tf.reduce_mean(tf.sqrt(tf.square((reshaped_outputs*y_std+y_mean) - (expected_sparse_output*y_std+y_mean)))/tf.sqrt(tf.square(expected_sparse_output*y_std+y_mean)),axis=(0,1))[0]\n",
    "    tf.summary.scalar('acc',acc)\n",
    "\n",
    "with tf.variable_scope('Optimizer'):\n",
    "    optimizer = tf.train.RMSPropOptimizer(learning_rate, decay=lr_decay, momentum=momentum)\n",
    "    train_op = optimizer.minimize(loss)\n",
    "    merged_summary = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0 time:0.14749741554260254\n",
      "train: loss: 391.54681396484375 acc: -1.6460590362548828  val: loss: 117.156005859375 acc: -0.6064255237579346\n",
      "step: 5 time:0.0\n",
      "train: loss: 58.532989501953125 acc: -1.241443157196045  val: loss: 28.24054718017578 acc: 0.16594505310058594\n",
      "step: 10 time:0.0\n",
      "train: loss: 22.179357528686523 acc: 0.2650367021560669  val: loss: 82.01847839355469 acc: 0.35285913944244385\n",
      "step: 15 time:0.0030019283294677734\n",
      "train: loss: 39.82048797607422 acc: 0.29268866777420044  val: loss: 105.89549255371094 acc: 0.3920673131942749\n",
      "step: 20 time:0.0\n",
      "train: loss: 11.080018997192383 acc: 0.6318703889846802  val: loss: 85.80674743652344 acc: 0.39071810245513916\n",
      "step: 25 time:0.0\n",
      "train: loss: 13.042516708374023 acc: 0.628836452960968  val: loss: 67.88426208496094 acc: 0.28744345903396606\n",
      "step: 30 time:0.0\n",
      "train: loss: 15.853347778320312 acc: 0.5683863162994385  val: loss: 13.08775806427002 acc: 0.6504001021385193\n",
      "step: 35 time:0.0\n",
      "train: loss: 17.28387451171875 acc: 0.6273790597915649  val: loss: 46.63752746582031 acc: 0.011380016803741455\n",
      "step: 40 time:0.0\n",
      "train: loss: 36.97858810424805 acc: 0.20648109912872314  val: loss: 30.96044158935547 acc: 0.6647663116455078\n",
      "step: 45 time:0.003002166748046875\n",
      "train: loss: 38.559043884277344 acc: 0.22533440589904785  val: loss: 22.627235412597656 acc: 0.5102695822715759\n",
      "step: 50 time:0.0\n",
      "train: loss: 145.06787109375 acc: 0.06333720684051514  val: loss: 35.21394729614258 acc: 0.3599514365196228\n",
      "step: 55 time:0.0\n",
      "train: loss: 61.43414306640625 acc: 0.01713693141937256  val: loss: 48.995296478271484 acc: 0.6241917610168457\n",
      "step: 60 time:0.0\n",
      "train: loss: 31.902862548828125 acc: -0.24235165119171143  val: loss: 14.923332214355469 acc: 0.6880322694778442\n",
      "step: 65 time:0.0030019283294677734\n",
      "train: loss: 42.531532287597656 acc: -0.22060096263885498  val: loss: 8.293709754943848 acc: 0.5083516240119934\n",
      "step: 70 time:0.0\n",
      "train: loss: 42.20897674560547 acc: 0.6303091049194336  val: loss: 35.31157684326172 acc: 0.3269529342651367\n",
      "step: 75 time:0.003002166748046875\n",
      "train: loss: 75.74580383300781 acc: 0.6566150784492493  val: loss: 33.71057891845703 acc: 0.6458647847175598\n",
      "step: 80 time:0.0156252384185791\n",
      "train: loss: 47.137474060058594 acc: -0.02641117572784424  val: loss: 26.342552185058594 acc: 0.4783175587654114\n",
      "step: 85 time:0.003002166748046875\n",
      "train: loss: 39.60371398925781 acc: -0.3509174585342407  val: loss: 35.90855407714844 acc: 0.33767664432525635\n",
      "step: 90 time:0.0\n",
      "train: loss: 35.877647399902344 acc: 0.4387695789337158  val: loss: 13.888855934143066 acc: 0.2849980592727661\n",
      "step: 95 time:0.0\n",
      "train: loss: 24.529708862304688 acc: 0.14330387115478516  val: loss: 38.850669860839844 acc: 0.07107943296432495\n",
      "step: 100 time:0.003001689910888672\n",
      "train: loss: 38.55394744873047 acc: 0.4866992235183716  val: loss: 22.407155990600586 acc: 0.5292778015136719\n",
      "step: 105 time:0.0\n",
      "train: loss: 14.590343475341797 acc: 0.3797224164009094  val: loss: 30.917903900146484 acc: 0.31397122144699097\n",
      "step: 110 time:0.001955747604370117\n",
      "train: loss: 25.825984954833984 acc: 0.6395566463470459  val: loss: 17.87383460998535 acc: 0.6907662153244019\n",
      "step: 115 time:0.0030019283294677734\n",
      "train: loss: 26.62937355041504 acc: 0.09654629230499268  val: loss: 12.959064483642578 acc: 0.38363152742385864\n",
      "step: 120 time:0.0030019283294677734\n",
      "train: loss: 13.170541763305664 acc: 0.738452672958374  val: loss: 19.224380493164062 acc: 0.6719594597816467\n",
      "step: 125 time:0.0\n",
      "train: loss: 27.522632598876953 acc: 0.3884016275405884  val: loss: 14.162776947021484 acc: -0.06395483016967773\n",
      "step: 130 time:0.0\n",
      "train: loss: 28.339868545532227 acc: -0.05839109420776367  val: loss: 16.3136043548584 acc: 0.266960084438324\n",
      "step: 135 time:0.0\n",
      "train: loss: 18.339805603027344 acc: 0.2583147883415222  val: loss: 27.506389617919922 acc: -0.48708128929138184\n",
      "step: 140 time:0.0030019283294677734\n",
      "train: loss: 15.249195098876953 acc: -0.10633587837219238  val: loss: 10.749730110168457 acc: 0.6262977123260498\n",
      "step: 145 time:0.0\n",
      "train: loss: 9.493738174438477 acc: 0.5128662586212158  val: loss: 17.61756134033203 acc: 0.5955492258071899\n",
      "step: 150 time:0.0\n",
      "train: loss: 13.544939041137695 acc: 0.7602571845054626  val: loss: 14.84195327758789 acc: 0.046034157276153564\n",
      "step: 155 time:0.0\n",
      "train: loss: 20.41750717163086 acc: 0.020664215087890625  val: loss: 9.991436004638672 acc: 0.4578777551651001\n",
      "step: 160 time:0.0\n",
      "train: loss: 7.800774097442627 acc: 0.8152810335159302  val: loss: 19.596576690673828 acc: 0.7176500558853149\n",
      "step: 165 time:0.0030002593994140625\n",
      "train: loss: 9.233535766601562 acc: 0.7463580965995789  val: loss: 18.525131225585938 acc: 0.47699564695358276\n",
      "step: 170 time:0.003001689910888672\n",
      "train: loss: 4.857666015625 acc: 0.6234503984451294  val: loss: 43.078155517578125 acc: 0.6369520425796509\n",
      "step: 175 time:0.003001689910888672\n",
      "train: loss: 10.599796295166016 acc: 0.3473951816558838  val: loss: 8.565099716186523 acc: 0.6975831985473633\n",
      "step: 180 time:0.0030014514923095703\n",
      "train: loss: 15.431496620178223 acc: 0.793146014213562  val: loss: 19.137802124023438 acc: 0.531599760055542\n",
      "step: 185 time:0.0030019283294677734\n",
      "train: loss: 18.251636505126953 acc: 0.6636160612106323  val: loss: 18.083627700805664 acc: 0.6198567748069763\n",
      "step: 190 time:0.0\n",
      "train: loss: 18.572267532348633 acc: 0.48215943574905396  val: loss: 14.941728591918945 acc: 0.5077159404754639\n",
      "step: 195 time:0.0\n",
      "train: loss: 7.866093635559082 acc: 0.8146205544471741  val: loss: 13.386127471923828 acc: 0.4522961378097534\n",
      "step: 200 time:0.003001689910888672\n",
      "train: loss: 4.340627193450928 acc: 0.7600547075271606  val: loss: 30.86678123474121 acc: 0.6376209259033203\n",
      "step: 205 time:0.0\n",
      "train: loss: 12.81386947631836 acc: 0.5218876004219055  val: loss: 8.792383193969727 acc: 0.524209201335907\n",
      "step: 210 time:0.0\n",
      "train: loss: 10.390607833862305 acc: 0.5506489872932434  val: loss: 11.466398239135742 acc: 0.46407973766326904\n",
      "step: 215 time:0.0\n",
      "train: loss: 5.081287860870361 acc: 0.5324994921684265  val: loss: 3.9396920204162598 acc: 0.7433706521987915\n",
      "step: 220 time:0.0\n",
      "train: loss: 10.234060287475586 acc: 0.34316766262054443  val: loss: 4.359104633331299 acc: 0.7567972540855408\n",
      "step: 225 time:0.003002166748046875\n",
      "train: loss: 23.357078552246094 acc: 0.5662809610366821  val: loss: 5.568791389465332 acc: 0.6073155999183655\n",
      "step: 230 time:0.004002094268798828\n",
      "train: loss: 35.17308807373047 acc: 0.4238981008529663  val: loss: 12.040471076965332 acc: 0.7018276453018188\n",
      "step: 235 time:0.00400233268737793\n",
      "train: loss: 15.878193855285645 acc: 0.5780465006828308  val: loss: 8.889446258544922 acc: 0.6749252080917358\n",
      "step: 240 time:0.003002643585205078\n",
      "train: loss: 21.090465545654297 acc: -0.3676340579986572  val: loss: 28.11773681640625 acc: 0.6814566254615784\n",
      "step: 245 time:0.0032532215118408203\n",
      "train: loss: 23.690502166748047 acc: -0.0298614501953125  val: loss: 12.719846725463867 acc: 0.6487610340118408\n",
      "step: 250 time:0.00400233268737793\n",
      "train: loss: 13.922418594360352 acc: 0.6498469114303589  val: loss: 12.535062789916992 acc: 0.6390799283981323\n",
      "step: 255 time:0.003001689910888672\n",
      "train: loss: 16.69594955444336 acc: 0.3044928312301636  val: loss: 8.17107105255127 acc: 0.5433176755905151\n",
      "step: 260 time:0.0030014514923095703\n",
      "train: loss: 13.422184944152832 acc: 0.5916804075241089  val: loss: 11.115131378173828 acc: 0.6128746271133423\n",
      "step: 265 time:0.0030019283294677734\n",
      "train: loss: 18.123065948486328 acc: 0.5127382278442383  val: loss: 11.067583084106445 acc: 0.6625674962997437\n",
      "step: 270 time:0.003001689910888672\n",
      "train: loss: 17.11117935180664 acc: 0.6114595532417297  val: loss: 43.126792907714844 acc: 0.6591964364051819\n",
      "step: 275 time:0.003001689910888672\n",
      "train: loss: 8.894471168518066 acc: 0.8128167390823364  val: loss: 24.903919219970703 acc: 0.6872560381889343\n",
      "step: 280 time:0.004002571105957031\n",
      "train: loss: 8.526979446411133 acc: 0.49472445249557495  val: loss: 26.804744720458984 acc: 0.7431025505065918\n",
      "step: 285 time:0.0030019283294677734\n",
      "train: loss: 24.327360153198242 acc: 0.5109710693359375  val: loss: 8.515517234802246 acc: 0.5456157326698303\n",
      "step: 290 time:0.004002809524536133\n",
      "train: loss: 8.076183319091797 acc: 0.6999984979629517  val: loss: 5.519229888916016 acc: 0.699439287185669\n",
      "step: 295 time:0.0030019283294677734\n",
      "train: loss: 14.588874816894531 acc: 0.3935396075248718  val: loss: 4.1743268966674805 acc: 0.5073672533035278\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 300 time:0.003001689910888672\n",
      "train: loss: 13.885286331176758 acc: 0.5151483416557312  val: loss: 12.67512321472168 acc: 0.7978612184524536\n",
      "step: 305 time:0.003001689910888672\n",
      "train: loss: 11.182228088378906 acc: 0.6703612208366394  val: loss: 5.882467269897461 acc: 0.7095698714256287\n",
      "step: 310 time:0.0\n",
      "train: loss: 6.918181419372559 acc: 0.7136713862419128  val: loss: 18.05196762084961 acc: 0.6773146986961365\n",
      "step: 315 time:0.0\n",
      "train: loss: 15.275524139404297 acc: 0.7930005192756653  val: loss: 9.205545425415039 acc: 0.6292689442634583\n",
      "step: 320 time:0.0030014514923095703\n",
      "train: loss: 5.421459197998047 acc: 0.8629103302955627  val: loss: 20.344423294067383 acc: 0.7204447388648987\n",
      "step: 325 time:0.0030012130737304688\n",
      "train: loss: 3.626352071762085 acc: 0.7479928135871887  val: loss: 4.352879524230957 acc: 0.6913034319877625\n",
      "step: 330 time:0.017093896865844727\n",
      "train: loss: 5.75339412689209 acc: 0.3928700089454651  val: loss: 11.812187194824219 acc: 0.2098260521888733\n",
      "step: 335 time:0.0040018558502197266\n",
      "train: loss: 4.272380828857422 acc: 0.5497652292251587  val: loss: 22.34516143798828 acc: 0.6713961362838745\n",
      "step: 340 time:0.0030024051666259766\n",
      "train: loss: 4.18226432800293 acc: 0.06097698211669922  val: loss: 13.447704315185547 acc: 0.2870897054672241\n",
      "step: 345 time:0.0030035972595214844\n",
      "train: loss: 2.9282240867614746 acc: 0.39084547758102417  val: loss: 8.297313690185547 acc: 0.7903463840484619\n",
      "step: 350 time:0.00400233268737793\n",
      "train: loss: 4.851192474365234 acc: 0.6624675989151001  val: loss: 23.819923400878906 acc: 0.6231162548065186\n",
      "step: 355 time:0.0030024051666259766\n",
      "train: loss: 2.0680837631225586 acc: 0.7763187885284424  val: loss: 9.470804214477539 acc: 0.37860339879989624\n",
      "step: 360 time:0.004008293151855469\n",
      "train: loss: 5.35857629776001 acc: 0.36340588331222534  val: loss: 35.61742401123047 acc: 0.3499191403388977\n",
      "step: 365 time:0.0\n",
      "train: loss: 3.1807034015655518 acc: 0.6122733354568481  val: loss: 8.605380058288574 acc: 0.684131920337677\n",
      "step: 370 time:0.01562643051147461\n",
      "train: loss: 5.975137233734131 acc: 0.5475775003433228  val: loss: 16.177553176879883 acc: 0.7504702806472778\n",
      "step: 375 time:0.003001689910888672\n",
      "train: loss: 2.503901958465576 acc: 0.6858621835708618  val: loss: 22.571386337280273 acc: 0.6749815344810486\n",
      "step: 380 time:0.003002166748046875\n",
      "train: loss: 1.5313912630081177 acc: 0.6219303011894226  val: loss: 4.62844705581665 acc: 0.5568744540214539\n",
      "step: 385 time:0.0\n",
      "train: loss: 4.764326095581055 acc: 0.6512653827667236  val: loss: 7.340234756469727 acc: 0.4616267681121826\n",
      "step: 390 time:0.0030019283294677734\n",
      "train: loss: 1.859426498413086 acc: 0.5481113195419312  val: loss: 29.65449333190918 acc: 0.6985825896263123\n",
      "step: 395 time:0.0\n",
      "train: loss: 2.265632152557373 acc: 0.622594952583313  val: loss: 22.447046279907227 acc: 0.5423380732536316\n",
      "step: 400 time:0.0030019283294677734\n",
      "train: loss: 2.678736448287964 acc: 0.6545926332473755  val: loss: 11.624088287353516 acc: 0.6450164318084717\n",
      "step: 405 time:0.0\n",
      "train: loss: 5.380415916442871 acc: -0.20417165756225586  val: loss: 21.22876739501953 acc: 0.23439788818359375\n",
      "step: 410 time:0.003002166748046875\n",
      "train: loss: 1.103887677192688 acc: 0.7093096375465393  val: loss: 5.199882507324219 acc: 0.6105459928512573\n",
      "step: 415 time:0.0030024051666259766\n",
      "train: loss: 3.103426694869995 acc: 0.5582133531570435  val: loss: 18.888669967651367 acc: 0.5167807340621948\n",
      "step: 420 time:0.003002166748046875\n",
      "train: loss: 4.092580318450928 acc: 0.5732302665710449  val: loss: 3.8930909633636475 acc: 0.8521108627319336\n",
      "step: 425 time:0.003001689910888672\n",
      "train: loss: 3.993701219558716 acc: 0.731298565864563  val: loss: 7.056769847869873 acc: 0.7754571437835693\n",
      "step: 430 time:0.0030019283294677734\n",
      "train: loss: 7.031593322753906 acc: 0.5320721864700317  val: loss: 28.738990783691406 acc: 0.7052933573722839\n",
      "step: 435 time:0.0030019283294677734\n",
      "train: loss: 4.002288818359375 acc: 0.684931755065918  val: loss: 33.61045837402344 acc: 0.7886046767234802\n",
      "step: 440 time:0.004002809524536133\n",
      "train: loss: 2.8871688842773438 acc: 0.7858307957649231  val: loss: 20.616722106933594 acc: 0.6563919186592102\n",
      "step: 445 time:0.003001689910888672\n",
      "train: loss: 4.911354064941406 acc: 0.684090256690979  val: loss: 7.064438343048096 acc: 0.7327545881271362\n",
      "step: 450 time:0.0030019283294677734\n",
      "train: loss: 2.4109079837799072 acc: 0.7325759530067444  val: loss: 8.564634323120117 acc: 0.5977298021316528\n",
      "step: 455 time:0.002348661422729492\n",
      "train: loss: 1.3652323484420776 acc: 0.669477105140686  val: loss: 4.459900856018066 acc: 0.7809949517250061\n",
      "step: 460 time:0.0\n",
      "train: loss: 2.158855676651001 acc: 0.5729767680168152  val: loss: 5.723649501800537 acc: 0.7521176338195801\n",
      "step: 465 time:0.0030019283294677734\n",
      "train: loss: 2.7938055992126465 acc: 0.6866628527641296  val: loss: 7.6658735275268555 acc: 0.7494359612464905\n",
      "step: 470 time:0.003002166748046875\n",
      "train: loss: 4.004833221435547 acc: 0.2738802433013916  val: loss: 22.480871200561523 acc: 0.7864789962768555\n",
      "step: 475 time:0.004002571105957031\n",
      "train: loss: 5.255229949951172 acc: 0.6420143842697144  val: loss: 6.51641321182251 acc: 0.7125380039215088\n",
      "step: 480 time:0.0030019283294677734\n",
      "train: loss: 5.2404561042785645 acc: 0.6517668962478638  val: loss: 80.63711547851562 acc: 0.5974478721618652\n",
      "step: 485 time:0.003001689910888672\n",
      "train: loss: 3.7688913345336914 acc: 0.7301900386810303  val: loss: 42.597164154052734 acc: 0.65144944190979\n",
      "step: 490 time:0.0030019283294677734\n",
      "train: loss: 4.923650741577148 acc: 0.7442908883094788  val: loss: 8.484275817871094 acc: 0.7624366879463196\n",
      "step: 495 time:0.015625715255737305\n",
      "train: loss: 5.3076887130737305 acc: 0.7693763971328735  val: loss: 44.22804641723633 acc: 0.672845721244812\n",
      "step: 500 time:0.003002166748046875\n",
      "train: loss: 3.986687660217285 acc: 0.7062862515449524  val: loss: 5.8973283767700195 acc: 0.583375096321106\n",
      "step: 505 time:0.0030019283294677734\n",
      "train: loss: 5.409746170043945 acc: 0.81211256980896  val: loss: 3.824963092803955 acc: 0.702578067779541\n",
      "step: 510 time:0.003001689910888672\n",
      "train: loss: 5.412245750427246 acc: 0.6355398297309875  val: loss: 5.366629600524902 acc: 0.7847455739974976\n",
      "step: 515 time:0.0030019283294677734\n",
      "train: loss: 4.491210460662842 acc: 0.7016501426696777  val: loss: 5.5433669090271 acc: 0.6201378107070923\n",
      "step: 520 time:0.004002571105957031\n",
      "train: loss: 1.8359732627868652 acc: 0.5547060966491699  val: loss: 37.30066680908203 acc: 0.6435596346855164\n",
      "step: 525 time:0.0030019283294677734\n",
      "train: loss: 5.061192035675049 acc: 0.5727270841598511  val: loss: 11.833978652954102 acc: 0.6664871573448181\n",
      "step: 530 time:0.0030019283294677734\n",
      "train: loss: 11.64653205871582 acc: 0.7125992178916931  val: loss: 9.606084823608398 acc: 0.6666085720062256\n",
      "step: 535 time:0.0\n",
      "train: loss: 5.110056400299072 acc: 0.7196161150932312  val: loss: 30.645206451416016 acc: 0.7411199808120728\n",
      "step: 540 time:0.0\n",
      "train: loss: 7.394192695617676 acc: 0.7558847665786743  val: loss: 25.19468879699707 acc: 0.7801209688186646\n",
      "step: 545 time:0.003001689910888672\n",
      "train: loss: 9.193531036376953 acc: 0.8316202163696289  val: loss: 25.3623046875 acc: 0.7414124011993408\n",
      "step: 550 time:0.0\n",
      "train: loss: 4.967459678649902 acc: 0.7527956366539001  val: loss: 3.9368252754211426 acc: 0.69454425573349\n",
      "step: 555 time:0.0030012130737304688\n",
      "train: loss: 12.06536865234375 acc: 0.629044234752655  val: loss: 46.861244201660156 acc: 0.7415987849235535\n",
      "step: 560 time:0.0\n",
      "train: loss: 19.652324676513672 acc: 0.7573447227478027  val: loss: 17.369178771972656 acc: 0.6790971755981445\n",
      "step: 565 time:0.0030019283294677734\n",
      "train: loss: 51.24925231933594 acc: 0.4906405806541443  val: loss: 12.917221069335938 acc: 0.5967387557029724\n",
      "step: 570 time:0.0030014514923095703\n",
      "train: loss: 22.02002716064453 acc: 0.32872045040130615  val: loss: 19.844003677368164 acc: 0.8053797483444214\n",
      "step: 575 time:0.003001689910888672\n",
      "train: loss: 24.05005645751953 acc: 0.6449641585350037  val: loss: 4.268439769744873 acc: 0.6496126651763916\n",
      "step: 580 time:0.0030019283294677734\n",
      "train: loss: 17.4284725189209 acc: 0.5050775408744812  val: loss: 11.556793212890625 acc: 0.6083406209945679\n",
      "step: 585 time:0.0030012130737304688\n",
      "train: loss: 14.865352630615234 acc: 0.6742923259735107  val: loss: 7.828506946563721 acc: 0.7496747970581055\n",
      "step: 590 time:0.0030019283294677734\n",
      "train: loss: 29.315216064453125 acc: 0.4158056974411011  val: loss: 13.252123832702637 acc: 0.7916088700294495\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 595 time:0.0050029754638671875\n",
      "train: loss: 22.22275161743164 acc: 0.5921027064323425  val: loss: 24.36035919189453 acc: 0.7432017922401428\n",
      "step: 600 time:0.0030019283294677734\n",
      "train: loss: 28.721263885498047 acc: 0.6503511667251587  val: loss: 2.2713663578033447 acc: 0.7662269473075867\n",
      "step: 605 time:0.003001689910888672\n",
      "train: loss: 14.661266326904297 acc: 0.7036715745925903  val: loss: 10.329866409301758 acc: 0.6818609237670898\n",
      "step: 610 time:0.004002571105957031\n",
      "train: loss: 14.178915023803711 acc: 0.8404832482337952  val: loss: 6.643684387207031 acc: 0.7721163034439087\n",
      "step: 615 time:0.003001689910888672\n",
      "train: loss: 6.576906204223633 acc: 0.5716956853866577  val: loss: 10.062825202941895 acc: 0.73405921459198\n",
      "step: 620 time:0.0030014514923095703\n",
      "train: loss: 19.73756980895996 acc: 0.7369397878646851  val: loss: 6.879045486450195 acc: 0.7366105914115906\n",
      "step: 625 time:0.0\n",
      "train: loss: 16.140975952148438 acc: 0.7886620759963989  val: loss: 16.072662353515625 acc: 0.692887544631958\n",
      "step: 630 time:0.0030024051666259766\n",
      "train: loss: 11.029333114624023 acc: 0.8279152512550354  val: loss: 9.516403198242188 acc: 0.7448137998580933\n",
      "step: 635 time:0.003002166748046875\n",
      "train: loss: 10.389005661010742 acc: 0.6277638077735901  val: loss: 9.13496208190918 acc: 0.6117074489593506\n",
      "step: 640 time:0.0030019283294677734\n",
      "train: loss: 6.930085182189941 acc: 0.8367823958396912  val: loss: 4.1929216384887695 acc: 0.694949746131897\n",
      "step: 645 time:0.0030024051666259766\n",
      "train: loss: 9.193885803222656 acc: -0.13592183589935303  val: loss: 6.131480693817139 acc: 0.7292006015777588\n",
      "step: 650 time:0.0030019283294677734\n",
      "train: loss: 4.823549270629883 acc: 0.3874034881591797  val: loss: 14.147977828979492 acc: 0.7335982918739319\n",
      "step: 655 time:0.003001689910888672\n",
      "train: loss: 7.475893974304199 acc: 0.23521965742111206  val: loss: 10.645820617675781 acc: 0.6627066731452942\n",
      "step: 660 time:0.003002166748046875\n",
      "train: loss: 7.4748077392578125 acc: 0.5993979573249817  val: loss: 4.180382251739502 acc: 0.642145574092865\n",
      "step: 665 time:0.003002166748046875\n",
      "train: loss: 6.96985387802124 acc: 0.4392004609107971  val: loss: 4.270168781280518 acc: 0.7322791814804077\n",
      "step: 670 time:0.004002571105957031\n",
      "train: loss: 20.281299591064453 acc: 0.2198467254638672  val: loss: 26.808990478515625 acc: 0.5222082138061523\n",
      "step: 675 time:0.004003047943115234\n",
      "train: loss: 2.6930463314056396 acc: 0.8142930269241333  val: loss: 8.463634490966797 acc: 0.802488386631012\n",
      "step: 680 time:0.004002809524536133\n",
      "train: loss: 4.622190952301025 acc: 0.8571516275405884  val: loss: 4.987351417541504 acc: 0.7840275168418884\n",
      "step: 685 time:0.0030019283294677734\n",
      "train: loss: 2.6633708477020264 acc: 0.8255869746208191  val: loss: 14.38936996459961 acc: 0.6253721714019775\n",
      "step: 690 time:0.003001689910888672\n",
      "train: loss: 9.483567237854004 acc: 0.3886643052101135  val: loss: 13.195781707763672 acc: 0.6890717148780823\n",
      "step: 695 time:0.0030019283294677734\n",
      "train: loss: 10.0107421875 acc: 0.7378357648849487  val: loss: 5.4788618087768555 acc: 0.6069063544273376\n",
      "step: 700 time:0.004003286361694336\n",
      "train: loss: 5.941267490386963 acc: 0.7724342346191406  val: loss: 10.679349899291992 acc: 0.6981840133666992\n",
      "step: 705 time:0.003002166748046875\n",
      "train: loss: 10.709554672241211 acc: 0.6298574209213257  val: loss: 5.465457916259766 acc: 0.6433387994766235\n",
      "step: 710 time:0.003002166748046875\n",
      "train: loss: 12.787424087524414 acc: 0.7337566614151001  val: loss: 6.322084903717041 acc: 0.7170578241348267\n",
      "step: 715 time:0.00400233268737793\n",
      "train: loss: 4.898830413818359 acc: 0.7851489782333374  val: loss: 19.909753799438477 acc: 0.8061487674713135\n",
      "step: 720 time:0.003002166748046875\n",
      "train: loss: 4.630215644836426 acc: 0.7483528852462769  val: loss: 50.62693786621094 acc: 0.6193946599960327\n",
      "step: 725 time:0.003001689910888672\n",
      "train: loss: 6.478280067443848 acc: 0.5263860821723938  val: loss: 4.5049309730529785 acc: 0.7851356267929077\n",
      "step: 730 time:0.003001689910888672\n",
      "train: loss: 3.4202587604522705 acc: 0.6689431667327881  val: loss: 10.510406494140625 acc: 0.5922186970710754\n",
      "step: 735 time:0.003001689910888672\n",
      "train: loss: 13.177972793579102 acc: -0.27626168727874756  val: loss: 25.330326080322266 acc: 0.4681047201156616\n",
      "step: 740 time:0.0\n",
      "train: loss: 9.479106903076172 acc: 0.4842926859855652  val: loss: 11.811216354370117 acc: 0.8154529333114624\n",
      "step: 745 time:0.00400233268737793\n",
      "train: loss: 14.492280006408691 acc: 0.3706279993057251  val: loss: 9.616312026977539 acc: 0.5987496972084045\n",
      "step: 750 time:0.004002571105957031\n",
      "train: loss: 7.134274482727051 acc: 0.6383424401283264  val: loss: 8.000432968139648 acc: 0.5830016136169434\n",
      "step: 755 time:0.0030014514923095703\n",
      "train: loss: 10.487663269042969 acc: 0.4841728210449219  val: loss: 34.11262893676758 acc: 0.5630934834480286\n",
      "step: 760 time:0.0030019283294677734\n",
      "train: loss: 14.137809753417969 acc: 0.573904812335968  val: loss: 19.006019592285156 acc: 0.7771896719932556\n",
      "step: 765 time:0.00400233268737793\n",
      "train: loss: 7.2093682289123535 acc: 0.5577343106269836  val: loss: 13.362053871154785 acc: 0.7527037262916565\n",
      "step: 770 time:0.0\n",
      "train: loss: 7.386528491973877 acc: 0.5147491693496704  val: loss: 5.340387344360352 acc: 0.49216902256011963\n",
      "step: 775 time:0.017587661743164062\n",
      "train: loss: 15.789834976196289 acc: 0.5796605944633484  val: loss: 12.801065444946289 acc: 0.5210214257240295\n",
      "step: 780 time:0.003000974655151367\n",
      "train: loss: 12.425741195678711 acc: 0.7052997350692749  val: loss: 9.816529273986816 acc: 0.7529668807983398\n",
      "step: 785 time:0.0030019283294677734\n",
      "train: loss: 14.384207725524902 acc: 0.4248512387275696  val: loss: 11.425012588500977 acc: 0.8435640931129456\n",
      "step: 790 time:0.004002094268798828\n",
      "train: loss: 15.48816967010498 acc: 0.5329772233963013  val: loss: 8.018074035644531 acc: 0.6224186420440674\n",
      "step: 795 time:0.0040013790130615234\n",
      "train: loss: 7.217191696166992 acc: 0.8276016712188721  val: loss: 26.794631958007812 acc: 0.6434253454208374\n",
      "step: 800 time:0.004003047943115234\n",
      "train: loss: 7.005336284637451 acc: 0.8413407802581787  val: loss: 8.490362167358398 acc: 0.6959180235862732\n",
      "step: 805 time:0.0030019283294677734\n",
      "train: loss: 9.905359268188477 acc: 0.6126974821090698  val: loss: 11.96966552734375 acc: 0.8171558976173401\n",
      "step: 810 time:0.0\n",
      "train: loss: 6.388094902038574 acc: 0.7231373190879822  val: loss: 21.265708923339844 acc: 0.8212173581123352\n",
      "step: 815 time:0.0030019283294677734\n",
      "train: loss: 10.740907669067383 acc: 0.5184916257858276  val: loss: 7.870185852050781 acc: 0.623807430267334\n",
      "step: 820 time:0.0030014514923095703\n",
      "train: loss: 23.4569091796875 acc: 0.49837082624435425  val: loss: 13.204215049743652 acc: 0.3045744299888611\n",
      "step: 825 time:0.0030019283294677734\n",
      "train: loss: 7.837149620056152 acc: 0.7120139598846436  val: loss: 6.088686466217041 acc: 0.6286047101020813\n",
      "step: 830 time:0.0030019283294677734\n",
      "train: loss: 4.331188201904297 acc: 0.8087351322174072  val: loss: 23.494468688964844 acc: 0.7661181688308716\n",
      "step: 835 time:0.0030019283294677734\n",
      "train: loss: 9.32600212097168 acc: 0.6892743706703186  val: loss: 2.044116258621216 acc: 0.8172423839569092\n",
      "step: 840 time:0.0\n",
      "train: loss: 2.9166994094848633 acc: 0.8784562349319458  val: loss: 3.7631444931030273 acc: 0.7560397982597351\n",
      "step: 845 time:0.0030014514923095703\n",
      "train: loss: 6.621817588806152 acc: 0.8184306025505066  val: loss: 21.88409423828125 acc: 0.714913010597229\n",
      "step: 850 time:0.0030014514923095703\n",
      "train: loss: 2.4442286491394043 acc: 0.6989403963088989  val: loss: 5.725699424743652 acc: 0.5411815643310547\n",
      "step: 855 time:0.0030019283294677734\n",
      "train: loss: 4.672116279602051 acc: 0.7888354063034058  val: loss: 17.671161651611328 acc: 0.5502008199691772\n",
      "step: 860 time:0.003000497817993164\n",
      "train: loss: 6.37449836730957 acc: 0.7892953157424927  val: loss: 6.696200370788574 acc: 0.8160901069641113\n",
      "step: 865 time:0.0030014514923095703\n",
      "train: loss: 1.7263327836990356 acc: 0.7988204956054688  val: loss: 2.2004079818725586 acc: 0.8781701922416687\n",
      "step: 870 time:0.0030019283294677734\n",
      "train: loss: 3.1775155067443848 acc: 0.5704225301742554  val: loss: 15.326359748840332 acc: 0.5777338743209839\n",
      "step: 875 time:0.0\n",
      "train: loss: 1.4583196640014648 acc: 0.7244546413421631  val: loss: 7.118607521057129 acc: 0.8056198358535767\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 880 time:0.0030019283294677734\n",
      "train: loss: 2.142059326171875 acc: 0.6865249276161194  val: loss: 31.526561737060547 acc: 0.769376814365387\n",
      "step: 885 time:0.0030019283294677734\n",
      "train: loss: 3.45241641998291 acc: 0.824980616569519  val: loss: 23.206390380859375 acc: 0.7228066921234131\n",
      "step: 890 time:0.003001689910888672\n",
      "train: loss: 1.8413517475128174 acc: 0.5649936199188232  val: loss: 3.7064545154571533 acc: 0.5753799080848694\n",
      "step: 895 time:0.0030019283294677734\n",
      "train: loss: 1.1872807741165161 acc: 0.6060512661933899  val: loss: 24.191892623901367 acc: 0.7534422874450684\n",
      "step: 900 time:0.0032448768615722656\n",
      "train: loss: 4.343439102172852 acc: 0.5973016023635864  val: loss: 5.153684616088867 acc: 0.5809429883956909\n",
      "step: 905 time:0.0\n",
      "train: loss: 1.2856032848358154 acc: 0.6837601065635681  val: loss: 7.987757682800293 acc: 0.6159060001373291\n",
      "step: 910 time:0.0\n",
      "train: loss: 1.050310730934143 acc: 0.6817684769630432  val: loss: 4.072297096252441 acc: 0.8694444298744202\n",
      "step: 915 time:0.003000497817993164\n",
      "train: loss: 0.7946540117263794 acc: 0.7169348001480103  val: loss: 20.566627502441406 acc: 0.7532105445861816\n",
      "step: 920 time:0.016614913940429688\n",
      "train: loss: 2.007594108581543 acc: 0.6549457907676697  val: loss: 33.400909423828125 acc: 0.6687281131744385\n",
      "step: 925 time:0.0\n",
      "train: loss: 1.9929163455963135 acc: 0.6633567810058594  val: loss: 69.79975891113281 acc: 0.5529935359954834\n",
      "step: 930 time:0.003001689910888672\n",
      "train: loss: 1.1679649353027344 acc: 0.7486646175384521  val: loss: 32.891441345214844 acc: 0.6560402512550354\n",
      "step: 935 time:0.0030019283294677734\n",
      "train: loss: 3.074759006500244 acc: 0.33615726232528687  val: loss: 23.550731658935547 acc: 0.6471644639968872\n",
      "step: 940 time:0.0030019283294677734\n",
      "train: loss: 2.3526647090911865 acc: 0.7390620708465576  val: loss: 33.855587005615234 acc: 0.7201381921768188\n",
      "step: 945 time:0.003001689910888672\n",
      "train: loss: 3.3145253658294678 acc: 0.7989816665649414  val: loss: 9.848747253417969 acc: 0.5323868989944458\n",
      "step: 950 time:0.004002809524536133\n",
      "train: loss: 2.033548593521118 acc: 0.7710546255111694  val: loss: 6.98496675491333 acc: 0.7131222486495972\n",
      "step: 955 time:0.003002166748046875\n",
      "train: loss: 5.847628593444824 acc: 0.7530288696289062  val: loss: 40.169647216796875 acc: 0.6463710069656372\n",
      "step: 960 time:0.004003286361694336\n",
      "train: loss: 2.126863956451416 acc: 0.7485536336898804  val: loss: 28.959829330444336 acc: 0.7855030298233032\n",
      "step: 965 time:0.0030019283294677734\n",
      "train: loss: 1.9144396781921387 acc: 0.7427462339401245  val: loss: 18.109500885009766 acc: 0.7424954175949097\n",
      "step: 970 time:0.0030019283294677734\n",
      "train: loss: 2.6172728538513184 acc: 0.7415695786476135  val: loss: 6.066936492919922 acc: 0.653662919998169\n",
      "step: 975 time:0.0030019283294677734\n",
      "train: loss: 1.9175008535385132 acc: 0.6676116585731506  val: loss: 26.888607025146484 acc: 0.7498379945755005\n",
      "step: 980 time:0.003002166748046875\n",
      "train: loss: 2.291790008544922 acc: 0.4985469579696655  val: loss: 2.489779472351074 acc: 0.7603868246078491\n",
      "step: 985 time:0.004002571105957031\n",
      "train: loss: 1.8398290872573853 acc: 0.6896997094154358  val: loss: 40.444766998291016 acc: 0.7223955988883972\n",
      "step: 990 time:0.003002166748046875\n",
      "train: loss: 1.1971819400787354 acc: 0.823051929473877  val: loss: 2.279568910598755 acc: 0.8078111410140991\n",
      "step: 995 time:0.004002571105957031\n",
      "train: loss: 2.525057554244995 acc: 0.794108510017395  val: loss: 26.946147918701172 acc: 0.7095168828964233\n",
      "step: 1000 time:0.0030014514923095703\n",
      "train: loss: 4.940536022186279 acc: 0.8663332462310791  val: loss: 22.129642486572266 acc: 0.7054731249809265\n",
      "step: 1005 time:0.0040035247802734375\n",
      "train: loss: 2.943007230758667 acc: 0.7736252546310425  val: loss: 10.438618659973145 acc: 0.7043936252593994\n",
      "step: 1010 time:0.003001689910888672\n",
      "train: loss: 2.630911350250244 acc: 0.7451574802398682  val: loss: 7.962074279785156 acc: 0.7924249172210693\n",
      "step: 1015 time:0.004003286361694336\n",
      "train: loss: 4.399684906005859 acc: 0.7078763246536255  val: loss: 55.17759704589844 acc: 0.7255819439888\n",
      "step: 1020 time:0.003002166748046875\n",
      "train: loss: 3.8695144653320312 acc: 0.7885672450065613  val: loss: 21.84170913696289 acc: 0.686924934387207\n",
      "step: 1025 time:0.0022046566009521484\n",
      "train: loss: 4.313583850860596 acc: 0.7554255723953247  val: loss: 38.830841064453125 acc: 0.7343184351921082\n",
      "step: 1030 time:0.003002166748046875\n",
      "train: loss: 3.124447822570801 acc: 0.7106841802597046  val: loss: 6.529195308685303 acc: 0.7887077331542969\n",
      "step: 1035 time:0.0\n",
      "train: loss: 4.557309150695801 acc: 0.6912990212440491  val: loss: 5.137330055236816 acc: 0.4430767297744751\n",
      "step: 1040 time:0.0\n",
      "train: loss: 2.6576945781707764 acc: 0.5804811716079712  val: loss: 19.986793518066406 acc: 0.7866135239601135\n",
      "step: 1045 time:0.0\n",
      "train: loss: 7.094554901123047 acc: 0.8288739323616028  val: loss: 20.417844772338867 acc: 0.5980712175369263\n",
      "step: 1050 time:0.0030019283294677734\n",
      "train: loss: 4.161843776702881 acc: 0.8065567016601562  val: loss: 11.098733901977539 acc: 0.7401732206344604\n",
      "step: 1055 time:0.003001689910888672\n",
      "train: loss: 4.346718788146973 acc: 0.7835756540298462  val: loss: 6.283254623413086 acc: 0.7389037609100342\n",
      "step: 1060 time:0.003001689910888672\n",
      "train: loss: 6.093373775482178 acc: 0.7166644930839539  val: loss: 5.711999893188477 acc: 0.7282474040985107\n",
      "step: 1065 time:0.0030019283294677734\n",
      "train: loss: 2.862567186355591 acc: 0.8006950616836548  val: loss: 6.0620317459106445 acc: 0.8028426170349121\n",
      "step: 1070 time:0.0030024051666259766\n",
      "train: loss: 3.602733612060547 acc: 0.6005361080169678  val: loss: 14.149397850036621 acc: 0.7313384413719177\n",
      "step: 1075 time:0.0\n",
      "train: loss: 9.050568580627441 acc: 0.6714025139808655  val: loss: 19.3461856842041 acc: 0.8184922337532043\n",
      "step: 1080 time:0.003002166748046875\n",
      "train: loss: 3.220366954803467 acc: 0.7367422580718994  val: loss: 7.14163875579834 acc: 0.8063677549362183\n",
      "step: 1085 time:0.0\n",
      "train: loss: 10.832428932189941 acc: 0.7273169755935669  val: loss: 18.734018325805664 acc: 0.7480629086494446\n",
      "step: 1090 time:0.004003047943115234\n",
      "train: loss: 16.35886001586914 acc: 0.6307690143585205  val: loss: 5.9873247146606445 acc: 0.7162229418754578\n",
      "step: 1095 time:0.0030019283294677734\n",
      "train: loss: 20.787193298339844 acc: 0.7425739765167236  val: loss: 14.828307151794434 acc: 0.8257037401199341\n",
      "step: 1100 time:0.0030024051666259766\n",
      "train: loss: 25.71453285217285 acc: 0.20217567682266235  val: loss: 4.08234977722168 acc: 0.6788457632064819\n",
      "step: 1105 time:0.0030012130737304688\n",
      "train: loss: 19.850927352905273 acc: 0.5637605786323547  val: loss: 6.89794921875 acc: 0.7209051847457886\n",
      "step: 1110 time:0.004002809524536133\n",
      "train: loss: 39.833702087402344 acc: 0.7242101430892944  val: loss: 7.3321356773376465 acc: 0.7355573773384094\n",
      "step: 1115 time:0.003001689910888672\n",
      "train: loss: 27.572925567626953 acc: 0.23970216512680054  val: loss: 20.415103912353516 acc: 0.6148865818977356\n",
      "step: 1120 time:0.0030019283294677734\n",
      "train: loss: 28.80070686340332 acc: 0.8274801969528198  val: loss: 12.745087623596191 acc: 0.8349624276161194\n",
      "step: 1125 time:0.0030019283294677734\n",
      "train: loss: 28.95159339904785 acc: 0.823671817779541  val: loss: 5.531706809997559 acc: 0.7896898984909058\n",
      "step: 1130 time:0.003002166748046875\n",
      "train: loss: 31.14931869506836 acc: 0.19769859313964844  val: loss: 6.426278114318848 acc: 0.7864725589752197\n",
      "step: 1135 time:0.003002166748046875\n",
      "train: loss: 12.62733268737793 acc: 0.7964882850646973  val: loss: 7.253153324127197 acc: 0.7449550628662109\n",
      "step: 1140 time:0.003001689910888672\n",
      "train: loss: 8.078927040100098 acc: 0.7104758024215698  val: loss: 7.590096473693848 acc: 0.7652620077133179\n",
      "step: 1145 time:0.003001689910888672\n",
      "train: loss: 14.12886905670166 acc: 0.8030349016189575  val: loss: 5.0024566650390625 acc: 0.6807860732078552\n",
      "step: 1150 time:0.003002166748046875\n",
      "train: loss: 10.599547386169434 acc: 0.5196024179458618  val: loss: 5.188547134399414 acc: 0.8181231021881104\n",
      "step: 1155 time:0.0\n",
      "train: loss: 15.213249206542969 acc: 0.5566324591636658  val: loss: 6.577486991882324 acc: 0.728973388671875\n",
      "step: 1160 time:0.003001689910888672\n",
      "train: loss: 10.958137512207031 acc: 0.29658687114715576  val: loss: 2.8214001655578613 acc: 0.7001234292984009\n",
      "step: 1165 time:0.015625476837158203\n",
      "train: loss: 6.533359527587891 acc: 0.7941712737083435  val: loss: 7.526599884033203 acc: 0.6414830088615417\n",
      "step: 1170 time:0.0\n",
      "train: loss: 12.912866592407227 acc: 0.44759464263916016  val: loss: 4.435242652893066 acc: 0.648229718208313\n",
      "step: 1175 time:0.0179903507232666\n",
      "train: loss: 15.890626907348633 acc: 0.3702545762062073  val: loss: 2.6557202339172363 acc: 0.7496272921562195\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 1180 time:0.004002094268798828\n",
      "train: loss: 20.260326385498047 acc: 0.4855033755302429  val: loss: 13.251882553100586 acc: 0.7259706854820251\n",
      "step: 1185 time:0.003002166748046875\n",
      "train: loss: 4.745595932006836 acc: 0.7801249623298645  val: loss: 5.793349266052246 acc: 0.6242088079452515\n",
      "step: 1190 time:0.0\n",
      "train: loss: 14.386270523071289 acc: 0.3968240022659302  val: loss: 5.338696479797363 acc: 0.8006643056869507\n",
      "step: 1195 time:0.0\n",
      "train: loss: 7.824889659881592 acc: 0.6054330468177795  val: loss: 6.271995544433594 acc: 0.8049346208572388\n",
      "step: 1200 time:0.0030019283294677734\n",
      "train: loss: 8.0689697265625 acc: 0.6591631174087524  val: loss: 15.894071578979492 acc: 0.7405440807342529\n",
      "step: 1205 time:0.0030019283294677734\n",
      "train: loss: 3.54162335395813 acc: 0.7474013566970825  val: loss: 5.15281867980957 acc: 0.7588904500007629\n",
      "step: 1210 time:0.015625953674316406\n",
      "train: loss: 4.437909126281738 acc: 0.7957034111022949  val: loss: 2.229382276535034 acc: 0.6086629629135132\n",
      "step: 1215 time:0.0\n",
      "train: loss: 2.539696455001831 acc: 0.8791831731796265  val: loss: 7.9400506019592285 acc: 0.7946968674659729\n",
      "step: 1220 time:0.003001689910888672\n",
      "train: loss: 5.105786323547363 acc: 0.8410431146621704  val: loss: 7.762944221496582 acc: 0.7489974498748779\n",
      "step: 1225 time:0.003002166748046875\n",
      "train: loss: 5.675901412963867 acc: 0.8309069871902466  val: loss: 3.6077027320861816 acc: 0.7304627895355225\n",
      "step: 1230 time:0.0\n",
      "train: loss: 4.734755516052246 acc: 0.8259670734405518  val: loss: 4.182904243469238 acc: 0.7566267251968384\n",
      "step: 1235 time:0.0030012130737304688\n",
      "train: loss: 5.3878021240234375 acc: 0.8583015203475952  val: loss: 19.47565460205078 acc: 0.6623847484588623\n",
      "step: 1240 time:0.00538945198059082\n",
      "train: loss: 4.727616310119629 acc: 0.6171013116836548  val: loss: 9.090543746948242 acc: 0.6894040107727051\n",
      "step: 1245 time:0.0\n",
      "train: loss: 8.141693115234375 acc: 0.5755201578140259  val: loss: 22.629899978637695 acc: 0.7937983274459839\n",
      "step: 1250 time:0.003001689910888672\n",
      "train: loss: 7.723267555236816 acc: 0.8596935868263245  val: loss: 8.107690811157227 acc: 0.7822672128677368\n",
      "step: 1255 time:0.0\n",
      "train: loss: 3.8499815464019775 acc: 0.8413897752761841  val: loss: 8.2177734375 acc: 0.722044825553894\n",
      "step: 1260 time:0.0012826919555664062\n",
      "train: loss: 19.73262596130371 acc: 0.5198420286178589  val: loss: 12.967489242553711 acc: 0.5727527737617493\n",
      "step: 1265 time:0.015626907348632812\n",
      "train: loss: 7.507298469543457 acc: 0.3924206495285034  val: loss: 9.235694885253906 acc: 0.8581933975219727\n",
      "step: 1270 time:0.0\n",
      "train: loss: 11.757568359375 acc: 0.4871896505355835  val: loss: 8.128331184387207 acc: 0.7710387706756592\n",
      "step: 1275 time:0.0\n",
      "train: loss: 8.615017890930176 acc: 0.6715294718742371  val: loss: 9.17015266418457 acc: 0.5798101425170898\n",
      "step: 1280 time:0.0030012130737304688\n",
      "train: loss: 13.421761512756348 acc: 0.45840203762054443  val: loss: 6.514351844787598 acc: 0.8131027817726135\n",
      "step: 1285 time:0.0\n",
      "train: loss: 7.561800003051758 acc: 0.6647406816482544  val: loss: 6.87677526473999 acc: 0.7410011887550354\n",
      "step: 1290 time:0.0\n",
      "train: loss: 7.350614547729492 acc: 0.5266494750976562  val: loss: 7.1077985763549805 acc: 0.8042274713516235\n",
      "step: 1295 time:0.0\n",
      "train: loss: 8.613874435424805 acc: 0.686488687992096  val: loss: 8.7003173828125 acc: 0.6776030659675598\n",
      "step: 1300 time:0.0030019283294677734\n",
      "train: loss: 24.145845413208008 acc: 0.585349440574646  val: loss: 9.423532485961914 acc: 0.5964913964271545\n",
      "step: 1305 time:0.0\n",
      "train: loss: 8.271699905395508 acc: 0.5462862253189087  val: loss: 13.518037796020508 acc: 0.7496532201766968\n",
      "step: 1310 time:0.0\n",
      "train: loss: 5.657866477966309 acc: 0.8462091684341431  val: loss: 8.667581558227539 acc: 0.7406554222106934\n",
      "step: 1315 time:0.0019397735595703125\n",
      "train: loss: 9.04265022277832 acc: 0.8074824810028076  val: loss: 8.406140327453613 acc: 0.8110889792442322\n",
      "step: 1320 time:0.0030024051666259766\n",
      "train: loss: 3.6140172481536865 acc: 0.8283066749572754  val: loss: 8.902091026306152 acc: 0.774660050868988\n",
      "step: 1325 time:0.0\n",
      "train: loss: 3.8988611698150635 acc: 0.8809105753898621  val: loss: 7.267144203186035 acc: 0.5792680382728577\n",
      "step: 1330 time:0.0\n",
      "train: loss: 4.905496120452881 acc: 0.48525601625442505  val: loss: 21.422391891479492 acc: 0.6257309913635254\n",
      "step: 1335 time:0.015625953674316406\n",
      "train: loss: 10.51663589477539 acc: 0.5507141947746277  val: loss: 7.487903594970703 acc: 0.7146862745285034\n",
      "step: 1340 time:0.0030019283294677734\n",
      "train: loss: 7.502484321594238 acc: 0.636569619178772  val: loss: 5.000993728637695 acc: 0.7210299372673035\n",
      "step: 1345 time:0.0\n",
      "train: loss: 5.494387626647949 acc: 0.7590600252151489  val: loss: 2.767030715942383 acc: 0.7591379880905151\n",
      "step: 1350 time:0.0\n",
      "train: loss: 6.162656784057617 acc: 0.7609914541244507  val: loss: 7.607081413269043 acc: 0.8374840021133423\n",
      "step: 1355 time:0.0\n",
      "train: loss: 2.898409843444824 acc: 0.8999872803688049  val: loss: 4.016987323760986 acc: 0.7055667638778687\n",
      "step: 1360 time:0.003001689910888672\n",
      "train: loss: 1.2644122838974 acc: 0.8797725439071655  val: loss: 8.377693176269531 acc: 0.8412540555000305\n",
      "step: 1365 time:0.00400233268737793\n",
      "train: loss: 3.215803623199463 acc: 0.7602299451828003  val: loss: 2.6488406658172607 acc: 0.785315752029419\n",
      "step: 1370 time:0.015626192092895508\n",
      "train: loss: 4.326061248779297 acc: 0.7658466100692749  val: loss: 3.016401529312134 acc: 0.8412500023841858\n",
      "step: 1375 time:0.0\n",
      "train: loss: 0.7546265721321106 acc: 0.7702887654304504  val: loss: 24.59276008605957 acc: 0.7530697584152222\n",
      "step: 1380 time:0.004003047943115234\n",
      "train: loss: 2.3213610649108887 acc: 0.5375684499740601  val: loss: 7.233312606811523 acc: 0.7779759168624878\n",
      "step: 1385 time:0.0030024051666259766\n",
      "train: loss: 1.8347041606903076 acc: 0.6843267679214478  val: loss: 21.089176177978516 acc: 0.6813435554504395\n",
      "step: 1390 time:0.0030019283294677734\n",
      "train: loss: 1.0935624837875366 acc: 0.712277889251709  val: loss: 3.9329888820648193 acc: 0.8024721741676331\n",
      "step: 1395 time:0.0030019283294677734\n",
      "train: loss: 2.5130410194396973 acc: 0.6521768569946289  val: loss: 12.743000984191895 acc: 0.7775900959968567\n",
      "step: 1400 time:0.0\n",
      "train: loss: 2.697096109390259 acc: 0.7294573187828064  val: loss: 3.732802152633667 acc: 0.6145935654640198\n",
      "step: 1405 time:0.0\n",
      "train: loss: 0.755195677280426 acc: 0.7796595692634583  val: loss: 10.07053279876709 acc: 0.8146864175796509\n",
      "step: 1410 time:0.015625715255737305\n",
      "train: loss: 2.0207700729370117 acc: 0.5418769121170044  val: loss: 3.57930064201355 acc: 0.8739318251609802\n",
      "step: 1415 time:0.0030014514923095703\n",
      "train: loss: 4.564282417297363 acc: 0.6571379899978638  val: loss: 6.975388050079346 acc: 0.7759460210800171\n",
      "step: 1420 time:0.0030024051666259766\n",
      "train: loss: 1.5610355138778687 acc: 0.5152791738510132  val: loss: 19.863889694213867 acc: 0.4757370948791504\n",
      "step: 1425 time:0.003002166748046875\n",
      "train: loss: 1.6971148252487183 acc: 0.6658123731613159  val: loss: 8.882329940795898 acc: 0.6598760485649109\n",
      "step: 1430 time:0.003001689910888672\n",
      "train: loss: 2.0020925998687744 acc: 0.7440013885498047  val: loss: 59.59567642211914 acc: 0.6733261942863464\n",
      "step: 1435 time:0.004002571105957031\n",
      "train: loss: 1.1636648178100586 acc: 0.6334174871444702  val: loss: 10.872675895690918 acc: 0.7964264750480652\n",
      "step: 1440 time:0.0030019283294677734\n",
      "train: loss: 1.2429444789886475 acc: 0.6590470671653748  val: loss: 10.297236442565918 acc: 0.8316861987113953\n",
      "step: 1445 time:0.0\n",
      "train: loss: 0.886734127998352 acc: 0.7488002181053162  val: loss: 12.1077241897583 acc: 0.8102283477783203\n",
      "step: 1450 time:0.003001689910888672\n",
      "train: loss: 0.8132617473602295 acc: 0.7746715545654297  val: loss: 17.670120239257812 acc: 0.6140176057815552\n",
      "step: 1455 time:0.003002166748046875\n",
      "train: loss: 1.4333741664886475 acc: 0.816067636013031  val: loss: 2.5001380443573 acc: 0.7467082738876343\n",
      "step: 1460 time:0.0030024051666259766\n",
      "train: loss: 1.1353899240493774 acc: 0.7329627871513367  val: loss: 2.6033248901367188 acc: 0.6896007061004639\n",
      "step: 1465 time:0.003002166748046875\n",
      "train: loss: 4.796046257019043 acc: 0.5129483342170715  val: loss: 20.78888702392578 acc: 0.6945633888244629\n",
      "step: 1470 time:0.003001689910888672\n",
      "train: loss: 1.544936180114746 acc: 0.6321188807487488  val: loss: 10.932476043701172 acc: 0.7755995392799377\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 1475 time:0.004002571105957031\n",
      "train: loss: 2.0173261165618896 acc: 0.7742708325386047  val: loss: 21.15374755859375 acc: 0.838089108467102\n",
      "step: 1480 time:0.0030024051666259766\n",
      "train: loss: 2.3907694816589355 acc: 0.7017218470573425  val: loss: 8.891769409179688 acc: 0.8161356449127197\n",
      "step: 1485 time:0.0\n",
      "train: loss: 2.6369450092315674 acc: 0.7871863842010498  val: loss: 1.7193206548690796 acc: 0.7609091997146606\n",
      "step: 1490 time:0.0\n",
      "train: loss: 1.8359215259552002 acc: 0.7115446329116821  val: loss: 2.7443501949310303 acc: 0.7860591411590576\n",
      "step: 1495 time:0.003002166748046875\n",
      "train: loss: 1.3102614879608154 acc: 0.772623598575592  val: loss: 24.900150299072266 acc: 0.7851243019104004\n",
      "step: 1500 time:0.003001689910888672\n",
      "train: loss: 1.4070522785186768 acc: 0.7068926095962524  val: loss: 10.455228805541992 acc: 0.8541359901428223\n",
      "step: 1505 time:0.003002166748046875\n",
      "train: loss: 1.2279181480407715 acc: 0.7361587285995483  val: loss: 11.709918975830078 acc: 0.7959678173065186\n",
      "step: 1510 time:0.0030019283294677734\n",
      "train: loss: 2.0159542560577393 acc: 0.8315920829772949  val: loss: 2.9848289489746094 acc: 0.868242084980011\n",
      "step: 1515 time:0.004002571105957031\n",
      "train: loss: 1.4610021114349365 acc: 0.708763062953949  val: loss: 5.667545318603516 acc: 0.7957187294960022\n",
      "step: 1520 time:0.003002166748046875\n",
      "train: loss: 4.85360050201416 acc: 0.8014912009239197  val: loss: 4.092185974121094 acc: 0.7175934314727783\n",
      "step: 1525 time:0.0030024051666259766\n",
      "train: loss: 1.9629138708114624 acc: 0.7358032464981079  val: loss: 4.400458335876465 acc: 0.8345393538475037\n",
      "step: 1530 time:0.0030019283294677734\n",
      "train: loss: 1.899293303489685 acc: 0.6355213522911072  val: loss: 23.673564910888672 acc: 0.7308652997016907\n",
      "step: 1535 time:0.0030014514923095703\n",
      "train: loss: 3.274033784866333 acc: 0.7458559274673462  val: loss: 5.61509895324707 acc: 0.8033919930458069\n",
      "step: 1540 time:0.0030019283294677734\n",
      "train: loss: 2.9234423637390137 acc: 0.8154802918434143  val: loss: 20.92814064025879 acc: 0.817100465297699\n",
      "step: 1545 time:0.0020012855529785156\n",
      "train: loss: 2.361276149749756 acc: 0.8530553579330444  val: loss: 4.463132858276367 acc: 0.7818725109100342\n",
      "step: 1550 time:0.003001689910888672\n",
      "train: loss: 2.961905002593994 acc: 0.8714234232902527  val: loss: 31.891525268554688 acc: 0.5907326936721802\n",
      "step: 1555 time:0.003002166748046875\n",
      "train: loss: 2.757774829864502 acc: 0.5427297949790955  val: loss: 2.224076271057129 acc: 0.7783066034317017\n",
      "step: 1560 time:0.0\n",
      "train: loss: 2.757955551147461 acc: 0.7061896324157715  val: loss: 18.228744506835938 acc: 0.7839733362197876\n",
      "step: 1565 time:0.0030019283294677734\n",
      "train: loss: 3.5022969245910645 acc: 0.8094659447669983  val: loss: 5.658949851989746 acc: 0.7939103841781616\n",
      "step: 1570 time:0.003002166748046875\n",
      "train: loss: 3.723156452178955 acc: 0.8158385753631592  val: loss: 5.630588054656982 acc: 0.7897395491600037\n",
      "step: 1575 time:0.003002166748046875\n",
      "train: loss: 2.187973976135254 acc: 0.8078685402870178  val: loss: 1.2736122608184814 acc: 0.7556631565093994\n",
      "step: 1580 time:0.0030019283294677734\n",
      "train: loss: 3.0379397869110107 acc: 0.8170521855354309  val: loss: 2.8465495109558105 acc: 0.8327823877334595\n",
      "step: 1585 time:0.0030019283294677734\n",
      "train: loss: 3.854313611984253 acc: 0.8151309490203857  val: loss: 18.589963912963867 acc: 0.7799314856529236\n",
      "step: 1590 time:0.003001689910888672\n",
      "train: loss: 2.4724912643432617 acc: 0.7880414724349976  val: loss: 3.029834270477295 acc: 0.7909426689147949\n",
      "step: 1595 time:0.0030019283294677734\n",
      "train: loss: 1.8642646074295044 acc: 0.7482654452323914  val: loss: 7.562346458435059 acc: 0.8354296088218689\n",
      "step: 1600 time:0.0030012130737304688\n",
      "train: loss: 13.065446853637695 acc: 0.7099428176879883  val: loss: 7.610180854797363 acc: 0.7600401639938354\n",
      "step: 1605 time:0.003001689910888672\n",
      "train: loss: 17.975975036621094 acc: 0.6518557667732239  val: loss: 21.30886459350586 acc: 0.6526138782501221\n",
      "step: 1610 time:0.0030019283294677734\n",
      "train: loss: 19.271116256713867 acc: 0.6044395565986633  val: loss: 10.575801849365234 acc: 0.7437580227851868\n",
      "step: 1615 time:0.003002166748046875\n",
      "train: loss: 16.155088424682617 acc: 0.35945117473602295  val: loss: 4.605298042297363 acc: 0.6622430086135864\n",
      "step: 1620 time:0.004002571105957031\n",
      "train: loss: 14.05102825164795 acc: 0.6520410776138306  val: loss: 6.079668998718262 acc: 0.7658350467681885\n",
      "step: 1625 time:0.0\n",
      "train: loss: 9.078022003173828 acc: 0.7292302250862122  val: loss: 3.212385654449463 acc: 0.8399051427841187\n",
      "step: 1630 time:0.003002643585205078\n",
      "train: loss: 32.11125946044922 acc: 0.727683424949646  val: loss: 12.44621467590332 acc: 0.78706294298172\n",
      "step: 1635 time:0.003001689910888672\n",
      "train: loss: 17.803024291992188 acc: 0.6480818390846252  val: loss: 3.032961845397949 acc: 0.8255194425582886\n",
      "step: 1640 time:0.003001689910888672\n",
      "train: loss: 29.704917907714844 acc: 0.7677369117736816  val: loss: 2.7205214500427246 acc: 0.7706218361854553\n",
      "step: 1645 time:0.004002571105957031\n",
      "train: loss: 15.641971588134766 acc: 0.7394286394119263  val: loss: 1.2301299571990967 acc: 0.7829105854034424\n",
      "step: 1650 time:0.00400233268737793\n",
      "train: loss: 11.817831993103027 acc: 0.7277172803878784  val: loss: 8.796546936035156 acc: 0.7890058755874634\n",
      "step: 1655 time:0.00400233268737793\n",
      "train: loss: 12.155365943908691 acc: 0.37345921993255615  val: loss: 2.2751994132995605 acc: 0.8237000703811646\n",
      "step: 1660 time:0.0030019283294677734\n",
      "train: loss: 9.406261444091797 acc: 0.7982298731803894  val: loss: 2.567401885986328 acc: 0.880017101764679\n",
      "step: 1665 time:0.003001689910888672\n",
      "train: loss: 9.840579986572266 acc: 0.7419835329055786  val: loss: 7.089057922363281 acc: 0.8230932950973511\n",
      "step: 1670 time:0.0030014514923095703\n",
      "train: loss: 8.660810470581055 acc: 0.8502771258354187  val: loss: 4.216067314147949 acc: 0.6763712167739868\n",
      "step: 1675 time:0.003001689910888672\n",
      "train: loss: 6.7328643798828125 acc: 0.6334937810897827  val: loss: 4.468587875366211 acc: 0.7320566177368164\n",
      "step: 1680 time:0.0030019283294677734\n",
      "train: loss: 7.508845329284668 acc: 0.6704236268997192  val: loss: 7.016470909118652 acc: 0.5074862241744995\n",
      "step: 1685 time:0.0\n",
      "train: loss: 3.658958911895752 acc: 0.7402890920639038  val: loss: 5.022610664367676 acc: 0.7337660789489746\n",
      "step: 1690 time:0.0\n",
      "train: loss: 4.177064895629883 acc: 0.8182485103607178  val: loss: 12.057336807250977 acc: 0.7913509607315063\n",
      "step: 1695 time:0.0\n",
      "train: loss: 4.878628730773926 acc: 0.6689879298210144  val: loss: 9.584673881530762 acc: 0.7528187036514282\n",
      "step: 1700 time:0.003001689910888672\n",
      "train: loss: 6.476409912109375 acc: 0.7468280792236328  val: loss: 4.451662540435791 acc: 0.8294083476066589\n",
      "step: 1705 time:0.0\n",
      "train: loss: 2.2059640884399414 acc: 0.8345234394073486  val: loss: 5.628878593444824 acc: 0.7904480695724487\n",
      "step: 1710 time:0.017890453338623047\n",
      "train: loss: 7.352191925048828 acc: 0.5237703919410706  val: loss: 14.726797103881836 acc: 0.6524515748023987\n",
      "step: 1715 time:0.0\n",
      "train: loss: 1.5676136016845703 acc: 0.8631694912910461  val: loss: 10.935761451721191 acc: 0.8070738315582275\n",
      "step: 1720 time:0.003002166748046875\n",
      "train: loss: 1.63495671749115 acc: 0.8832646012306213  val: loss: 1.0950530767440796 acc: 0.857109546661377\n",
      "step: 1725 time:0.003002166748046875\n",
      "train: loss: 2.097489833831787 acc: 0.9072425365447998  val: loss: 2.3352320194244385 acc: 0.802642822265625\n",
      "step: 1730 time:0.0\n",
      "train: loss: 18.924930572509766 acc: 0.8164060115814209  val: loss: 5.3622846603393555 acc: 0.7070877552032471\n",
      "step: 1735 time:0.0\n",
      "train: loss: 3.5198681354522705 acc: 0.8898553848266602  val: loss: 6.0634050369262695 acc: 0.7170416116714478\n",
      "step: 1740 time:0.0030019283294677734\n",
      "train: loss: 3.4369070529937744 acc: 0.8159044981002808  val: loss: 8.251199722290039 acc: 0.6889379024505615\n",
      "step: 1745 time:0.003001689910888672\n",
      "train: loss: 1.2558002471923828 acc: 0.9070526361465454  val: loss: 4.064359664916992 acc: 0.8127590417861938\n",
      "step: 1750 time:0.0\n",
      "train: loss: 4.4475297927856445 acc: 0.8397449254989624  val: loss: 3.943431854248047 acc: 0.5780354142189026\n",
      "step: 1755 time:0.0030019283294677734\n",
      "train: loss: 3.279491424560547 acc: 0.8096489906311035  val: loss: 3.995112895965576 acc: 0.8652517795562744\n",
      "step: 1760 time:0.0030019283294677734\n",
      "train: loss: 6.5680952072143555 acc: 0.5771010518074036  val: loss: 6.2406768798828125 acc: 0.6853480935096741\n",
      "step: 1765 time:0.0030007362365722656\n",
      "train: loss: 3.9644627571105957 acc: 0.8103950023651123  val: loss: 5.855623245239258 acc: 0.7920176386833191\n",
      "step: 1770 time:0.0\n",
      "train: loss: 2.528921127319336 acc: 0.8680826425552368  val: loss: 3.2963480949401855 acc: 0.8040320873260498\n",
      "step: 1775 time:0.0\n",
      "train: loss: 2.206725597381592 acc: 0.8155446648597717  val: loss: 3.009006977081299 acc: 0.7680495977401733\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 1780 time:0.0030019283294677734\n",
      "train: loss: 5.095118522644043 acc: 0.8155101537704468  val: loss: 4.451663017272949 acc: 0.7304018139839172\n",
      "step: 1785 time:0.015625476837158203\n",
      "train: loss: 9.414924621582031 acc: 0.5745331645011902  val: loss: 8.467814445495605 acc: 0.826343834400177\n",
      "step: 1790 time:0.0030019283294677734\n",
      "train: loss: 5.233975410461426 acc: 0.4311586022377014  val: loss: 7.516942977905273 acc: 0.7947162389755249\n",
      "step: 1795 time:0.0\n",
      "train: loss: 7.827658653259277 acc: 0.6757131814956665  val: loss: 21.263877868652344 acc: 0.6952992677688599\n",
      "step: 1800 time:0.0030019283294677734\n",
      "train: loss: 14.310714721679688 acc: 0.575366735458374  val: loss: 5.827922821044922 acc: 0.6165485382080078\n",
      "step: 1805 time:0.015625715255737305\n",
      "train: loss: 8.407155990600586 acc: 0.6302130222320557  val: loss: 8.94308853149414 acc: 0.7656738758087158\n",
      "step: 1810 time:0.0\n",
      "train: loss: 7.696653366088867 acc: 0.7229074239730835  val: loss: 4.514029502868652 acc: 0.8851965665817261\n",
      "step: 1815 time:0.0\n",
      "train: loss: 5.375424385070801 acc: 0.7538686990737915  val: loss: 2.6706457138061523 acc: 0.6258746385574341\n",
      "step: 1820 time:0.0030014514923095703\n",
      "train: loss: 9.50925350189209 acc: 0.6598007678985596  val: loss: 4.25743293762207 acc: 0.8342812061309814\n",
      "step: 1825 time:0.0\n",
      "train: loss: 4.122696399688721 acc: 0.8170440793037415  val: loss: 27.302221298217773 acc: 0.7564823031425476\n",
      "step: 1830 time:0.0\n",
      "train: loss: 8.664251327514648 acc: 0.8173487186431885  val: loss: 8.209362030029297 acc: 0.7420982122421265\n",
      "step: 1835 time:0.003002166748046875\n",
      "train: loss: 4.3575944900512695 acc: 0.7825068235397339  val: loss: 11.271041870117188 acc: 0.8464277982711792\n",
      "step: 1840 time:0.0030024051666259766\n",
      "train: loss: 6.127559661865234 acc: 0.8495218753814697  val: loss: 4.511368274688721 acc: 0.6853196620941162\n",
      "step: 1845 time:0.0030019283294677734\n",
      "train: loss: 3.0784292221069336 acc: 0.8088177442550659  val: loss: 8.832883834838867 acc: 0.8175581693649292\n",
      "step: 1850 time:0.015625953674316406\n",
      "train: loss: 1.6165883541107178 acc: 0.8456994295120239  val: loss: 5.9575042724609375 acc: 0.8463280200958252\n",
      "step: 1855 time:0.0\n",
      "train: loss: 6.0897746086120605 acc: 0.8137500882148743  val: loss: 7.824250221252441 acc: 0.793103039264679\n",
      "step: 1860 time:0.0030019283294677734\n",
      "train: loss: 3.0533218383789062 acc: 0.7774944305419922  val: loss: 5.914419174194336 acc: 0.8547327518463135\n",
      "step: 1865 time:0.015625715255737305\n",
      "train: loss: 6.652100563049316 acc: 0.6962569952011108  val: loss: 5.016101360321045 acc: 0.7181895971298218\n",
      "step: 1870 time:0.0030014514923095703\n",
      "train: loss: 5.860198020935059 acc: 0.8754673004150391  val: loss: 1.049899697303772 acc: 0.7832568883895874\n",
      "step: 1875 time:0.0030012130737304688\n",
      "train: loss: 3.397853374481201 acc: 0.8582037687301636  val: loss: 2.9281740188598633 acc: 0.774426281452179\n",
      "step: 1880 time:0.0030014514923095703\n",
      "train: loss: 2.7261805534362793 acc: 0.905578076839447  val: loss: 1.0000547170639038 acc: 0.8373686075210571\n",
      "step: 1885 time:0.015625715255737305\n",
      "train: loss: 2.9198598861694336 acc: 0.8216032981872559  val: loss: 2.12607741355896 acc: 0.8536698818206787\n",
      "step: 1890 time:0.0\n",
      "train: loss: 1.6628968715667725 acc: 0.8510151505470276  val: loss: 2.4232583045959473 acc: 0.825698733329773\n",
      "step: 1895 time:0.0\n",
      "train: loss: 1.6021170616149902 acc: 0.8067299127578735  val: loss: 4.247905731201172 acc: 0.8300436735153198\n",
      "step: 1900 time:0.0030019283294677734\n",
      "train: loss: 7.117019176483154 acc: 0.581479012966156  val: loss: 5.721661567687988 acc: 0.7696057558059692\n",
      "step: 1905 time:0.0\n",
      "train: loss: 2.1120400428771973 acc: 0.8300535082817078  val: loss: 7.673861026763916 acc: 0.7116926312446594\n",
      "step: 1910 time:0.003001689910888672\n",
      "train: loss: 1.0950509309768677 acc: 0.7517151832580566  val: loss: 3.17661714553833 acc: 0.7540359497070312\n",
      "step: 1915 time:0.0020012855529785156\n",
      "train: loss: 2.801980972290039 acc: 0.7886205315589905  val: loss: 3.2114005088806152 acc: 0.8155442476272583\n",
      "step: 1920 time:0.0050029754638671875\n",
      "train: loss: 1.5790300369262695 acc: 0.6644883155822754  val: loss: 2.75627064704895 acc: 0.7510120868682861\n",
      "step: 1925 time:0.0\n",
      "train: loss: 0.5801749229431152 acc: 0.8231031894683838  val: loss: 1.2868645191192627 acc: 0.8357864618301392\n",
      "step: 1930 time:0.004002809524536133\n",
      "train: loss: 0.9261677861213684 acc: 0.6814377307891846  val: loss: 17.881498336791992 acc: 0.745713472366333\n",
      "step: 1935 time:0.0030012130737304688\n",
      "train: loss: 2.561072826385498 acc: 0.7102798223495483  val: loss: 11.029869079589844 acc: 0.878048837184906\n",
      "step: 1940 time:0.0030019283294677734\n",
      "train: loss: 0.7543085813522339 acc: 0.7506532073020935  val: loss: 10.215271949768066 acc: 0.8051140308380127\n",
      "step: 1945 time:0.0030014514923095703\n",
      "train: loss: 1.0096502304077148 acc: 0.6705339550971985  val: loss: 19.26181411743164 acc: 0.7097367644309998\n",
      "step: 1950 time:0.00400233268737793\n",
      "train: loss: 0.6526349782943726 acc: 0.6504923105239868  val: loss: 2.7081215381622314 acc: 0.7286900281906128\n",
      "step: 1955 time:0.0030014514923095703\n",
      "train: loss: 0.8814964294433594 acc: 0.7006486654281616  val: loss: 11.582014083862305 acc: 0.8159303069114685\n",
      "step: 1960 time:0.003001689910888672\n",
      "train: loss: 0.5851929187774658 acc: 0.6798506379127502  val: loss: 2.4178614616394043 acc: 0.8389817476272583\n",
      "step: 1965 time:0.0030014514923095703\n",
      "train: loss: 0.4003318250179291 acc: 0.7334818243980408  val: loss: 3.015944480895996 acc: 0.7319841384887695\n",
      "step: 1970 time:0.0030074119567871094\n",
      "train: loss: 1.1851551532745361 acc: 0.4014037847518921  val: loss: 1.6924116611480713 acc: 0.7595201730728149\n",
      "step: 1975 time:0.0030014514923095703\n",
      "train: loss: 1.5633140802383423 acc: 0.7594020962715149  val: loss: 9.589469909667969 acc: 0.7955852746963501\n",
      "step: 1980 time:0.0030014514923095703\n",
      "train: loss: 0.8466137647628784 acc: 0.5827287435531616  val: loss: 17.079383850097656 acc: 0.8014516830444336\n",
      "step: 1985 time:0.0030014514923095703\n",
      "train: loss: 1.6461197137832642 acc: 0.6909305453300476  val: loss: 13.802607536315918 acc: 0.8174585103988647\n",
      "step: 1990 time:0.003001689910888672\n",
      "train: loss: 3.030280113220215 acc: 0.7709890604019165  val: loss: 6.028883934020996 acc: 0.781075119972229\n",
      "step: 1995 time:0.004002809524536133\n",
      "train: loss: 2.2094547748565674 acc: 0.7823744416236877  val: loss: 34.261573791503906 acc: 0.8113577365875244\n",
      "step: 2000 time:0.004003047943115234\n",
      "train: loss: 0.8881354331970215 acc: 0.8087109923362732  val: loss: 6.241767883300781 acc: 0.8017657995223999\n",
      "step: 2005 time:0.0030019283294677734\n",
      "train: loss: 0.9179750680923462 acc: 0.7575947642326355  val: loss: 3.523724317550659 acc: 0.8338024020195007\n",
      "step: 2010 time:0.003001689910888672\n",
      "train: loss: 2.0106451511383057 acc: 0.709722638130188  val: loss: 39.578636169433594 acc: 0.640109121799469\n",
      "step: 2015 time:0.003001689910888672\n",
      "train: loss: 0.7855018377304077 acc: 0.7599447965621948  val: loss: 8.62126636505127 acc: 0.7568182945251465\n",
      "step: 2020 time:0.004002809524536133\n",
      "train: loss: 0.6076573729515076 acc: 0.7719532251358032  val: loss: 5.461182594299316 acc: 0.6956307888031006\n",
      "step: 2025 time:0.003002643585205078\n",
      "train: loss: 1.099823236465454 acc: 0.7762514352798462  val: loss: 3.718674421310425 acc: 0.5821508169174194\n",
      "step: 2030 time:0.0\n",
      "train: loss: 1.3227746486663818 acc: 0.8099923729896545  val: loss: 9.845704078674316 acc: 0.8094382286071777\n",
      "step: 2035 time:0.003002166748046875\n",
      "train: loss: 1.8752071857452393 acc: 0.7174299955368042  val: loss: 18.118404388427734 acc: 0.756534218788147\n",
      "step: 2040 time:0.0\n",
      "train: loss: 2.589517593383789 acc: 0.8457741737365723  val: loss: 1.991426944732666 acc: 0.7240256667137146\n",
      "step: 2045 time:0.004002809524536133\n",
      "train: loss: 1.320491075515747 acc: 0.833075225353241  val: loss: 29.941802978515625 acc: 0.6679596304893494\n",
      "step: 2050 time:0.01562643051147461\n",
      "train: loss: 6.097089767456055 acc: 0.7974050641059875  val: loss: 3.1867008209228516 acc: 0.7814896106719971\n",
      "step: 2055 time:0.004002809524536133\n",
      "train: loss: 1.9055280685424805 acc: 0.8276699781417847  val: loss: 17.890321731567383 acc: 0.7446793913841248\n",
      "step: 2060 time:0.003002166748046875\n",
      "train: loss: 2.412130355834961 acc: 0.8522309064865112  val: loss: 26.25105857849121 acc: 0.8630427122116089\n",
      "step: 2065 time:0.0030007362365722656\n",
      "train: loss: 2.102299213409424 acc: 0.8191706538200378  val: loss: 3.9874861240386963 acc: 0.801529049873352\n",
      "step: 2070 time:0.015625715255737305\n",
      "train: loss: 0.8359067440032959 acc: 0.7299731969833374  val: loss: 5.923559665679932 acc: 0.811522901058197\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 2075 time:0.003001689910888672\n",
      "train: loss: 2.533998966217041 acc: 0.6717215180397034  val: loss: 11.149256706237793 acc: 0.7864757776260376\n",
      "step: 2080 time:0.0\n",
      "train: loss: 1.9910025596618652 acc: 0.8346338272094727  val: loss: 18.163713455200195 acc: 0.7694991827011108\n",
      "step: 2085 time:0.0\n",
      "train: loss: 0.8488659262657166 acc: 0.7800957560539246  val: loss: 1.8544199466705322 acc: 0.8073796033859253\n",
      "step: 2090 time:0.015853166580200195\n",
      "train: loss: 5.857549667358398 acc: 0.8145636916160583  val: loss: 3.7310423851013184 acc: 0.8436352610588074\n",
      "step: 2095 time:0.00400233268737793\n",
      "train: loss: 2.599337339401245 acc: 0.8150447607040405  val: loss: 15.135167121887207 acc: 0.8246349692344666\n",
      "step: 2100 time:0.0\n",
      "train: loss: 2.6761364936828613 acc: 0.8923125267028809  val: loss: 10.476320266723633 acc: 0.73121577501297\n",
      "step: 2105 time:0.015626192092895508\n",
      "train: loss: 2.210529088973999 acc: 0.8290526270866394  val: loss: 18.531875610351562 acc: 0.8715943098068237\n",
      "step: 2110 time:0.0\n",
      "train: loss: 6.854616165161133 acc: 0.7365202903747559  val: loss: 19.88571548461914 acc: 0.7381279468536377\n",
      "step: 2115 time:0.003002166748046875\n",
      "train: loss: 10.591806411743164 acc: 0.668841540813446  val: loss: 6.65594482421875 acc: 0.8215833902359009\n",
      "step: 2120 time:0.015625953674316406\n",
      "train: loss: 18.011051177978516 acc: 0.5382076501846313  val: loss: 9.956140518188477 acc: 0.7161378860473633\n",
      "step: 2125 time:0.0\n",
      "train: loss: 21.755502700805664 acc: 0.4798469543457031  val: loss: 11.222362518310547 acc: 0.8019778728485107\n",
      "step: 2130 time:0.0\n",
      "train: loss: 15.367551803588867 acc: 0.4760390520095825  val: loss: 16.260581970214844 acc: 0.7303529977798462\n",
      "step: 2135 time:0.0030014514923095703\n",
      "train: loss: 14.639912605285645 acc: 0.26237767934799194  val: loss: 4.988587379455566 acc: 0.7228707075119019\n",
      "step: 2140 time:0.0\n",
      "train: loss: 10.172075271606445 acc: -0.05189013481140137  val: loss: 3.486133098602295 acc: 0.8131638169288635\n",
      "step: 2145 time:0.0\n",
      "train: loss: 13.337751388549805 acc: 0.3876161575317383  val: loss: 6.442840576171875 acc: 0.7957034707069397\n",
      "step: 2150 time:0.0\n",
      "train: loss: 28.42459487915039 acc: 0.41343486309051514  val: loss: 6.392132759094238 acc: 0.8076714873313904\n",
      "step: 2155 time:0.0030019283294677734\n",
      "train: loss: 21.200586318969727 acc: 0.4851258397102356  val: loss: 2.6773319244384766 acc: 0.8394148349761963\n",
      "step: 2160 time:0.0030019283294677734\n",
      "train: loss: 30.42801284790039 acc: 0.33320438861846924  val: loss: 6.2083258628845215 acc: 0.8015268445014954\n",
      "step: 2165 time:0.015626192092895508\n",
      "train: loss: 13.100568771362305 acc: 0.8169293403625488  val: loss: 4.543801307678223 acc: 0.8126335740089417\n",
      "step: 2170 time:0.0\n",
      "train: loss: 10.676376342773438 acc: 0.7856101989746094  val: loss: 7.2757062911987305 acc: 0.8449140787124634\n",
      "step: 2175 time:0.003002166748046875\n",
      "train: loss: 18.25446319580078 acc: 0.7730073928833008  val: loss: 7.416167259216309 acc: 0.6929751634597778\n",
      "step: 2180 time:0.015625476837158203\n",
      "train: loss: 6.708078861236572 acc: 0.829291582107544  val: loss: 4.791694641113281 acc: 0.8199020624160767\n",
      "step: 2185 time:0.0\n",
      "train: loss: 5.779272556304932 acc: 0.7944515943527222  val: loss: 3.968660831451416 acc: 0.8345050811767578\n",
      "step: 2190 time:0.0\n",
      "train: loss: 10.828662872314453 acc: 0.8637056350708008  val: loss: 1.5995337963104248 acc: 0.8524012565612793\n",
      "step: 2195 time:0.003002166748046875\n",
      "train: loss: 13.340557098388672 acc: 0.7513939142227173  val: loss: 6.899941921234131 acc: 0.7923909425735474\n",
      "step: 2200 time:0.003002166748046875\n",
      "train: loss: 12.030896186828613 acc: 0.6148162484169006  val: loss: 10.688328742980957 acc: 0.8376001119613647\n",
      "step: 2205 time:0.0\n",
      "train: loss: 4.49884033203125 acc: 0.7494052052497864  val: loss: 11.549070358276367 acc: 0.842448353767395\n",
      "step: 2210 time:0.003001689910888672\n",
      "train: loss: 2.615145683288574 acc: 0.7809423804283142  val: loss: 7.623693943023682 acc: 0.8009020686149597\n",
      "step: 2215 time:0.003002166748046875\n",
      "train: loss: 3.8049116134643555 acc: 0.8128551244735718  val: loss: 8.201894760131836 acc: 0.6997516751289368\n",
      "step: 2220 time:0.0030019283294677734\n",
      "train: loss: 9.962532043457031 acc: 0.6102159023284912  val: loss: 11.226171493530273 acc: 0.7836252450942993\n",
      "step: 2225 time:0.015625953674316406\n",
      "train: loss: 3.748828411102295 acc: 0.8267407417297363  val: loss: 5.802894115447998 acc: 0.8163133859634399\n",
      "step: 2230 time:0.0\n",
      "train: loss: 7.118537902832031 acc: 0.42902910709381104  val: loss: 1.850226879119873 acc: 0.7992021441459656\n",
      "step: 2235 time:0.003001689910888672\n",
      "train: loss: 1.6276403665542603 acc: 0.9029690027236938  val: loss: 0.6164587736129761 acc: 0.8358461856842041\n",
      "step: 2240 time:0.0\n",
      "train: loss: 5.330049514770508 acc: 0.872750461101532  val: loss: 3.460968017578125 acc: 0.8314537405967712\n",
      "step: 2245 time:0.015626192092895508\n",
      "train: loss: 0.6266430616378784 acc: 0.8567107319831848  val: loss: 10.316935539245605 acc: 0.7953851819038391\n",
      "step: 2250 time:0.0\n",
      "train: loss: 1.9381306171417236 acc: 0.8991296291351318  val: loss: 4.70781946182251 acc: 0.8431391716003418\n",
      "step: 2255 time:0.004002571105957031\n",
      "train: loss: 8.225887298583984 acc: 0.9110043048858643  val: loss: 7.1408843994140625 acc: 0.8854947090148926\n",
      "step: 2260 time:0.0156252384185791\n",
      "train: loss: 10.088258743286133 acc: 0.8209954500198364  val: loss: 4.4675421714782715 acc: 0.7936627268791199\n",
      "step: 2265 time:0.0\n",
      "train: loss: 3.56839656829834 acc: 0.876038134098053  val: loss: 4.511906623840332 acc: 0.6735378503799438\n",
      "step: 2270 time:0.0\n",
      "train: loss: 4.900248050689697 acc: 0.8010385036468506  val: loss: 16.490671157836914 acc: 0.7865764498710632\n",
      "step: 2275 time:0.004002094268798828\n",
      "train: loss: 4.9110307693481445 acc: 0.7934727668762207  val: loss: 5.136079788208008 acc: 0.8374224305152893\n",
      "step: 2280 time:0.0030019283294677734\n",
      "train: loss: 8.113231658935547 acc: 0.4135996699333191  val: loss: 5.807605743408203 acc: 0.7700440883636475\n",
      "step: 2285 time:0.0030014514923095703\n",
      "train: loss: 2.340782642364502 acc: 0.8905491232872009  val: loss: 21.48346710205078 acc: 0.643927812576294\n",
      "step: 2290 time:0.003002166748046875\n",
      "train: loss: 1.4012064933776855 acc: 0.823991060256958  val: loss: 8.742112159729004 acc: 0.814853847026825\n",
      "step: 2295 time:0.003002166748046875\n",
      "train: loss: 5.033831596374512 acc: 0.7725003957748413  val: loss: 12.667769432067871 acc: 0.7398387789726257\n",
      "step: 2300 time:0.00400233268737793\n",
      "train: loss: 3.6318864822387695 acc: 0.8102787137031555  val: loss: 8.83332633972168 acc: 0.8237062692642212\n",
      "step: 2305 time:0.0\n",
      "train: loss: 7.076782703399658 acc: 0.5482100248336792  val: loss: 8.449007034301758 acc: 0.7451893091201782\n",
      "step: 2310 time:0.003001689910888672\n",
      "train: loss: 7.483891487121582 acc: 0.7397186756134033  val: loss: 3.741729736328125 acc: 0.7653621435165405\n",
      "step: 2315 time:0.015626192092895508\n",
      "train: loss: 10.872993469238281 acc: 0.7084865570068359  val: loss: 3.8050177097320557 acc: 0.7502871751785278\n",
      "step: 2320 time:0.0\n",
      "train: loss: 4.506933212280273 acc: 0.6868915557861328  val: loss: 9.489129066467285 acc: 0.7690628170967102\n",
      "step: 2325 time:0.0\n",
      "train: loss: 6.55246639251709 acc: 0.790349006652832  val: loss: 7.858699798583984 acc: 0.6908762454986572\n",
      "step: 2330 time:0.003001689910888672\n",
      "train: loss: 6.757859230041504 acc: 0.6990234851837158  val: loss: 4.545299530029297 acc: 0.8217178583145142\n",
      "step: 2335 time:0.002110004425048828\n",
      "train: loss: 3.754714012145996 acc: 0.8465601801872253  val: loss: 2.669907569885254 acc: 0.6725481748580933\n",
      "step: 2340 time:0.0\n",
      "train: loss: 19.739572525024414 acc: 0.7362745404243469  val: loss: 8.417335510253906 acc: 0.5531011819839478\n",
      "step: 2345 time:0.015625953674316406\n",
      "train: loss: 8.123672485351562 acc: 0.53538978099823  val: loss: 4.243414878845215 acc: 0.8212220668792725\n",
      "step: 2350 time:0.003002166748046875\n",
      "train: loss: 4.739030361175537 acc: 0.7842802405357361  val: loss: 18.722034454345703 acc: 0.616123616695404\n",
      "step: 2355 time:0.0\n",
      "train: loss: 5.683660507202148 acc: 0.7434654235839844  val: loss: 2.9951043128967285 acc: 0.8966242671012878\n",
      "step: 2360 time:0.0\n",
      "train: loss: 4.755528450012207 acc: 0.7313594222068787  val: loss: 5.79685115814209 acc: 0.8568180203437805\n",
      "step: 2365 time:0.0\n",
      "train: loss: 2.2469115257263184 acc: 0.705826461315155  val: loss: 3.8823928833007812 acc: 0.749011218547821\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 2370 time:0.0030019283294677734\n",
      "train: loss: 1.5256630182266235 acc: 0.82525634765625  val: loss: 1.896145224571228 acc: 0.7637963891029358\n",
      "step: 2375 time:0.015956878662109375\n",
      "train: loss: 5.534660339355469 acc: 0.7140494585037231  val: loss: 4.670705795288086 acc: 0.7312277555465698\n",
      "step: 2380 time:0.0\n",
      "train: loss: 3.7085514068603516 acc: 0.7696665525436401  val: loss: 19.1229248046875 acc: 0.8046857118606567\n",
      "step: 2385 time:0.01610708236694336\n",
      "train: loss: 2.786276340484619 acc: 0.8163400292396545  val: loss: 2.1388983726501465 acc: 0.896515965461731\n",
      "step: 2390 time:0.0\n",
      "train: loss: 2.268901824951172 acc: 0.8278855085372925  val: loss: 12.681147575378418 acc: 0.7361072301864624\n",
      "step: 2395 time:0.0\n",
      "train: loss: 2.5048670768737793 acc: 0.8819989562034607  val: loss: 6.013716220855713 acc: 0.8257608413696289\n",
      "step: 2400 time:0.0\n",
      "train: loss: 1.457382321357727 acc: 0.8780940175056458  val: loss: 30.34014892578125 acc: 0.6687015295028687\n",
      "step: 2405 time:0.0\n",
      "train: loss: 1.8275951147079468 acc: 0.8085232973098755  val: loss: 12.563041687011719 acc: 0.8307093381881714\n",
      "step: 2410 time:0.0\n",
      "train: loss: 1.8027420043945312 acc: 0.874895453453064  val: loss: 2.3652536869049072 acc: 0.7991560697555542\n",
      "step: 2415 time:0.0\n",
      "train: loss: 1.1404955387115479 acc: 0.7803111672401428  val: loss: 5.328280448913574 acc: 0.8745635747909546\n",
      "step: 2420 time:0.0\n",
      "train: loss: 3.525071144104004 acc: 0.4896141290664673  val: loss: 9.568122863769531 acc: 0.7880793809890747\n",
      "step: 2425 time:0.003002643585205078\n",
      "train: loss: 4.140865325927734 acc: 0.7614302039146423  val: loss: 5.961590766906738 acc: 0.7602701187133789\n",
      "step: 2430 time:0.0\n",
      "train: loss: 0.5215209722518921 acc: 0.7643011212348938  val: loss: 9.354372024536133 acc: 0.7177349925041199\n",
      "step: 2435 time:0.0\n",
      "train: loss: 1.2103720903396606 acc: 0.7807213068008423  val: loss: 13.797747611999512 acc: 0.8248496055603027\n",
      "step: 2440 time:0.0030019283294677734\n",
      "train: loss: 1.0251604318618774 acc: 0.7515934705734253  val: loss: 2.472987413406372 acc: 0.8700931072235107\n",
      "step: 2445 time:0.0\n",
      "train: loss: 2.1054623126983643 acc: 0.5941057205200195  val: loss: 12.669556617736816 acc: 0.7773797512054443\n",
      "step: 2450 time:0.0\n",
      "train: loss: 0.784746527671814 acc: 0.8595992922782898  val: loss: 7.37303352355957 acc: 0.8872680068016052\n",
      "step: 2455 time:0.003001689910888672\n",
      "train: loss: 1.327336072921753 acc: 0.8084560632705688  val: loss: 4.098883628845215 acc: 0.7687380909919739\n",
      "step: 2460 time:0.017137765884399414\n",
      "train: loss: 3.0721230506896973 acc: 0.7351170778274536  val: loss: 11.371190071105957 acc: 0.8130766749382019\n",
      "step: 2465 time:0.0\n",
      "train: loss: 0.8412086963653564 acc: 0.7431688904762268  val: loss: 3.948532819747925 acc: 0.7992726564407349\n",
      "step: 2470 time:0.0\n",
      "train: loss: 0.8752585649490356 acc: 0.7325387001037598  val: loss: 2.3686447143554688 acc: 0.8051480054855347\n",
      "step: 2475 time:0.0\n",
      "train: loss: 0.8670103549957275 acc: 0.5034032464027405  val: loss: 9.513070106506348 acc: 0.7288010716438293\n",
      "step: 2480 time:0.0030019283294677734\n",
      "train: loss: 0.26827704906463623 acc: 0.8409568071365356  val: loss: 15.276983261108398 acc: 0.7182455062866211\n",
      "step: 2485 time:0.003000497817993164\n",
      "train: loss: 1.103470802307129 acc: 0.4339274764060974  val: loss: 9.946455955505371 acc: 0.8860369920730591\n",
      "step: 2490 time:0.0\n",
      "train: loss: 0.5573388934135437 acc: 0.785422682762146  val: loss: 7.627224922180176 acc: 0.7645888924598694\n",
      "step: 2495 time:0.0\n",
      "train: loss: 1.3674895763397217 acc: 0.6722478270530701  val: loss: 18.103981018066406 acc: 0.5918914675712585\n",
      "step: 2500 time:0.0040018558502197266\n",
      "train: loss: 0.8961448669433594 acc: 0.6262931823730469  val: loss: 1.3663947582244873 acc: 0.8292258381843567\n",
      "step: 2505 time:0.004002571105957031\n",
      "train: loss: 1.1811548471450806 acc: 0.831049919128418  val: loss: 8.850534439086914 acc: 0.7679436802864075\n",
      "step: 2510 time:0.00400233268737793\n",
      "train: loss: 1.4772025346755981 acc: 0.8088344931602478  val: loss: 2.59625244140625 acc: 0.8632001280784607\n",
      "step: 2515 time:0.0030014514923095703\n",
      "train: loss: 1.1068665981292725 acc: 0.7512149810791016  val: loss: 8.131587982177734 acc: 0.7729576826095581\n",
      "step: 2520 time:0.004002571105957031\n",
      "train: loss: 1.6379483938217163 acc: 0.7934252023696899  val: loss: 8.478813171386719 acc: 0.7797375917434692\n",
      "step: 2525 time:0.0030019283294677734\n",
      "train: loss: 0.5748928785324097 acc: 0.8266410827636719  val: loss: 2.5888495445251465 acc: 0.8519891500473022\n",
      "step: 2530 time:0.00400233268737793\n",
      "train: loss: 1.0140299797058105 acc: 0.8423255085945129  val: loss: 6.669618606567383 acc: 0.7941124439239502\n",
      "step: 2535 time:0.015626192092895508\n",
      "train: loss: 0.4461054801940918 acc: 0.8326758742332458  val: loss: 2.0703728199005127 acc: 0.8015725612640381\n",
      "step: 2540 time:0.0\n",
      "train: loss: 1.3187689781188965 acc: 0.7660790681838989  val: loss: 1.4252924919128418 acc: 0.8613842129707336\n",
      "step: 2545 time:0.0\n",
      "train: loss: 0.5176914930343628 acc: 0.8316037654876709  val: loss: 2.9924798011779785 acc: 0.8403663635253906\n",
      "step: 2550 time:0.0\n",
      "train: loss: 1.9714279174804688 acc: 0.7189047336578369  val: loss: 21.467845916748047 acc: 0.6914324164390564\n",
      "step: 2555 time:0.004002571105957031\n",
      "train: loss: 1.4469630718231201 acc: 0.8751258254051208  val: loss: 15.328278541564941 acc: 0.8808069825172424\n",
      "step: 2560 time:0.0\n",
      "train: loss: 2.3787808418273926 acc: 0.8810754418373108  val: loss: 15.422662734985352 acc: 0.8565540313720703\n",
      "step: 2565 time:0.0\n",
      "train: loss: 3.479790449142456 acc: 0.6845086216926575  val: loss: 4.43651819229126 acc: 0.7661203145980835\n",
      "step: 2570 time:0.0\n",
      "train: loss: 2.5583372116088867 acc: 0.7732225656509399  val: loss: 19.440433502197266 acc: 0.7452077865600586\n",
      "step: 2575 time:0.003001689910888672\n",
      "train: loss: 2.8472542762756348 acc: 0.7769808173179626  val: loss: 7.204250335693359 acc: 0.7787480354309082\n",
      "step: 2580 time:0.004002094268798828\n",
      "train: loss: 1.4556020498275757 acc: 0.7801225185394287  val: loss: 3.1168508529663086 acc: 0.8885350227355957\n",
      "step: 2585 time:0.003001689910888672\n",
      "train: loss: 1.8267903327941895 acc: 0.7881321310997009  val: loss: 1.8504855632781982 acc: 0.8613072633743286\n",
      "step: 2590 time:0.004002094268798828\n",
      "train: loss: 2.838548183441162 acc: 0.8071705102920532  val: loss: 34.63814163208008 acc: 0.8169854879379272\n",
      "step: 2595 time:0.0030612945556640625\n",
      "train: loss: 3.4808835983276367 acc: 0.842320442199707  val: loss: 13.148982048034668 acc: 0.7968815565109253\n",
      "step: 2600 time:0.0029997825622558594\n",
      "train: loss: 1.3103437423706055 acc: 0.7913860082626343  val: loss: 24.48655891418457 acc: 0.8024088740348816\n",
      "step: 2605 time:0.0050029754638671875\n",
      "train: loss: 1.9463378190994263 acc: 0.8891778588294983  val: loss: 5.166473865509033 acc: 0.7358342409133911\n",
      "step: 2610 time:0.00400233268737793\n",
      "train: loss: 5.501375198364258 acc: 0.7607631683349609  val: loss: 5.804006576538086 acc: 0.7945514917373657\n",
      "step: 2615 time:0.0030014514923095703\n",
      "train: loss: 1.5958343744277954 acc: 0.840409517288208  val: loss: 1.2947834730148315 acc: 0.790189266204834\n",
      "step: 2620 time:0.003001689910888672\n",
      "train: loss: 5.622744560241699 acc: 0.8898800015449524  val: loss: 15.362272262573242 acc: 0.8405128717422485\n",
      "step: 2625 time:0.00400233268737793\n",
      "train: loss: 2.0769238471984863 acc: 0.806668758392334  val: loss: 2.3028812408447266 acc: 0.8317521810531616\n",
      "step: 2630 time:0.0030019283294677734\n",
      "train: loss: 15.138372421264648 acc: 0.7573888897895813  val: loss: 19.054880142211914 acc: 0.7692084312438965\n",
      "step: 2635 time:0.003002166748046875\n",
      "train: loss: 4.703625679016113 acc: 0.6275208592414856  val: loss: 2.7747294902801514 acc: 0.7651779055595398\n",
      "step: 2640 time:0.003001689910888672\n",
      "train: loss: 9.941877365112305 acc: 0.7065154910087585  val: loss: 9.626700401306152 acc: 0.7720712423324585\n",
      "step: 2645 time:0.0030019283294677734\n",
      "train: loss: 6.073350429534912 acc: 0.7514136433601379  val: loss: 7.001832008361816 acc: 0.6954329013824463\n",
      "step: 2650 time:0.0030014514923095703\n",
      "train: loss: 9.120401382446289 acc: 0.8142979145050049  val: loss: 10.172703742980957 acc: 0.8102862238883972\n",
      "step: 2655 time:0.003001689910888672\n",
      "train: loss: 9.05849838256836 acc: 0.6859415173530579  val: loss: 3.8916454315185547 acc: 0.8075779676437378\n",
      "step: 2660 time:0.003002166748046875\n",
      "train: loss: 7.7968430519104 acc: 0.8211497664451599  val: loss: 2.157499074935913 acc: 0.8257097601890564\n",
      "step: 2665 time:0.0\n",
      "train: loss: 13.53551197052002 acc: 0.8054620027542114  val: loss: 8.091341018676758 acc: 0.8692382574081421\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 2670 time:0.0030012130737304688\n",
      "train: loss: 19.055105209350586 acc: 0.5992276072502136  val: loss: 2.0736303329467773 acc: 0.7895888090133667\n",
      "step: 2675 time:0.0\n",
      "train: loss: 16.590951919555664 acc: 0.8198346495628357  val: loss: 4.130611419677734 acc: 0.8483582735061646\n",
      "step: 2680 time:0.0\n",
      "train: loss: 6.799002170562744 acc: 0.7971789240837097  val: loss: 18.20037841796875 acc: 0.7968840599060059\n",
      "step: 2685 time:0.0\n",
      "train: loss: 15.942484855651855 acc: 0.4935815930366516  val: loss: 6.070835113525391 acc: 0.8321210145950317\n",
      "step: 2690 time:0.0030024051666259766\n",
      "train: loss: 10.640875816345215 acc: 0.34599387645721436  val: loss: 10.430582046508789 acc: 0.8836637735366821\n",
      "step: 2695 time:0.0\n",
      "train: loss: 7.738192558288574 acc: 0.8236321210861206  val: loss: 14.132062911987305 acc: 0.7582093477249146\n",
      "step: 2700 time:0.0\n",
      "train: loss: 7.056191921234131 acc: 0.8824349045753479  val: loss: 2.2028491497039795 acc: 0.7748867273330688\n",
      "step: 2705 time:0.0\n",
      "train: loss: 5.9686102867126465 acc: 0.808679461479187  val: loss: 2.8077926635742188 acc: 0.7771902680397034\n",
      "step: 2710 time:0.003002166748046875\n",
      "train: loss: 5.082887649536133 acc: 0.8473272323608398  val: loss: 4.104581832885742 acc: 0.7711846232414246\n",
      "step: 2715 time:0.015626192092895508\n",
      "train: loss: 11.164349555969238 acc: 0.6750643253326416  val: loss: 6.598752498626709 acc: 0.7883691787719727\n",
      "step: 2720 time:0.0\n",
      "train: loss: 13.26557445526123 acc: 0.6242998242378235  val: loss: 4.2763671875 acc: 0.8280079364776611\n",
      "step: 2725 time:0.0\n",
      "train: loss: 6.590192794799805 acc: 0.5531318187713623  val: loss: 3.1205601692199707 acc: 0.6567234992980957\n",
      "step: 2730 time:0.0030019283294677734\n",
      "train: loss: 13.478681564331055 acc: 0.4793779253959656  val: loss: 2.4058327674865723 acc: 0.7845174074172974\n",
      "step: 2735 time:0.0030019283294677734\n",
      "train: loss: 5.172733783721924 acc: 0.7487481236457825  val: loss: 6.829529762268066 acc: 0.7483039498329163\n",
      "step: 2740 time:0.004003286361694336\n",
      "train: loss: 4.560175895690918 acc: 0.833558201789856  val: loss: 2.876680850982666 acc: 0.8003990054130554\n",
      "step: 2745 time:0.003001689910888672\n",
      "train: loss: 2.6699652671813965 acc: 0.8374290466308594  val: loss: 18.365890502929688 acc: 0.7584657669067383\n",
      "step: 2750 time:0.0030019283294677734\n",
      "train: loss: 3.75333309173584 acc: 0.692081868648529  val: loss: 7.665635585784912 acc: 0.7835748195648193\n",
      "step: 2755 time:0.003002166748046875\n",
      "train: loss: 2.664371967315674 acc: 0.8408952951431274  val: loss: 10.042165756225586 acc: 0.7687733173370361\n",
      "step: 2760 time:0.0\n",
      "train: loss: 4.639465808868408 acc: 0.8731938600540161  val: loss: 7.348208427429199 acc: 0.8521870970726013\n",
      "step: 2765 time:0.0\n",
      "train: loss: 3.299098491668701 acc: 0.8934890031814575  val: loss: 17.00107192993164 acc: 0.7367244958877563\n",
      "step: 2770 time:0.0\n",
      "train: loss: 4.787837028503418 acc: 0.858122706413269  val: loss: 1.940158724784851 acc: 0.8132134079933167\n",
      "step: 2775 time:0.0\n",
      "train: loss: 7.903752326965332 acc: 0.825767993927002  val: loss: 6.336368560791016 acc: 0.6719585061073303\n",
      "step: 2780 time:0.003001689910888672\n",
      "train: loss: 5.774358749389648 acc: 0.7884827852249146  val: loss: 5.064038276672363 acc: 0.5420775413513184\n",
      "step: 2785 time:0.0\n",
      "train: loss: 3.2751970291137695 acc: 0.8303595185279846  val: loss: 4.178531169891357 acc: 0.4819507598876953\n",
      "step: 2790 time:0.0\n",
      "train: loss: 4.766114711761475 acc: 0.857538104057312  val: loss: 6.699901103973389 acc: 0.6481400728225708\n",
      "step: 2795 time:0.0030019283294677734\n",
      "train: loss: 4.045659065246582 acc: 0.8441644906997681  val: loss: 7.458439826965332 acc: 0.8825234770774841\n",
      "step: 2800 time:0.003002166748046875\n",
      "train: loss: 3.2233142852783203 acc: 0.8716000914573669  val: loss: 6.953907012939453 acc: 0.8503319025039673\n",
      "step: 2805 time:0.0\n",
      "train: loss: 4.4676594734191895 acc: 0.7731617093086243  val: loss: 5.247426986694336 acc: 0.8838416934013367\n",
      "step: 2810 time:0.0\n",
      "train: loss: 4.643925666809082 acc: 0.5325049161911011  val: loss: 3.9044904708862305 acc: 0.7729876041412354\n",
      "step: 2815 time:0.0\n",
      "train: loss: 7.488297462463379 acc: 0.7121915817260742  val: loss: 3.1449456214904785 acc: 0.8474392294883728\n",
      "step: 2820 time:0.0030019283294677734\n",
      "train: loss: 4.848647117614746 acc: 0.8058736324310303  val: loss: 2.7867889404296875 acc: 0.6772478222846985\n",
      "step: 2825 time:0.01779961585998535\n",
      "train: loss: 5.278669357299805 acc: 0.7583107948303223  val: loss: 4.983950138092041 acc: 0.820070207118988\n",
      "step: 2830 time:0.0\n",
      "train: loss: 6.713536739349365 acc: 0.7015790343284607  val: loss: 5.464166641235352 acc: 0.7715646028518677\n",
      "step: 2835 time:0.0\n",
      "train: loss: 4.718957424163818 acc: 0.7559463977813721  val: loss: 7.651424407958984 acc: 0.8436617851257324\n",
      "step: 2840 time:0.003001689910888672\n",
      "train: loss: 3.1506567001342773 acc: 0.6575940847396851  val: loss: 3.7581405639648438 acc: 0.844481348991394\n",
      "step: 2845 time:0.015625953674316406\n",
      "train: loss: 11.830970764160156 acc: 0.7516381144523621  val: loss: 2.793729305267334 acc: 0.7353205680847168\n",
      "step: 2850 time:0.0\n",
      "train: loss: 7.811202049255371 acc: 0.7764506936073303  val: loss: 12.069018363952637 acc: 0.8288062214851379\n",
      "step: 2855 time:0.0\n",
      "train: loss: 5.908359527587891 acc: 0.7992542386054993  val: loss: 2.021639108657837 acc: 0.8405482769012451\n",
      "step: 2860 time:0.0030019283294677734\n",
      "train: loss: 14.491551399230957 acc: 0.739560604095459  val: loss: 5.3050537109375 acc: 0.7993859052658081\n",
      "step: 2865 time:0.0\n",
      "train: loss: 4.651246070861816 acc: 0.7322037220001221  val: loss: 17.803203582763672 acc: 0.7836965322494507\n",
      "step: 2870 time:0.003002166748046875\n",
      "train: loss: 5.114973068237305 acc: 0.8243297934532166  val: loss: 6.998422145843506 acc: 0.8621255159378052\n",
      "step: 2875 time:0.015625715255737305\n",
      "train: loss: 6.644274711608887 acc: 0.6842283010482788  val: loss: 7.358734607696533 acc: 0.7940084934234619\n",
      "step: 2880 time:0.003001689910888672\n",
      "train: loss: 3.6049742698669434 acc: 0.7873900532722473  val: loss: 4.319953918457031 acc: 0.747078001499176\n",
      "step: 2885 time:0.0\n",
      "train: loss: 2.7317469120025635 acc: 0.8208472728729248  val: loss: 3.7965219020843506 acc: 0.8144603371620178\n",
      "step: 2890 time:0.0\n",
      "train: loss: 6.344664096832275 acc: 0.8328658938407898  val: loss: 20.536758422851562 acc: 0.7260228395462036\n",
      "step: 2895 time:0.015625715255737305\n",
      "train: loss: 3.905391216278076 acc: 0.8063562512397766  val: loss: 4.9770708084106445 acc: 0.7702142596244812\n",
      "step: 2900 time:0.0030019283294677734\n",
      "train: loss: 3.785287857055664 acc: 0.7996748685836792  val: loss: 8.267505645751953 acc: 0.8109458684921265\n",
      "step: 2905 time:0.0\n",
      "train: loss: 4.321066856384277 acc: 0.8875575065612793  val: loss: 9.360573768615723 acc: 0.8498439788818359\n",
      "step: 2910 time:0.016951799392700195\n",
      "train: loss: 1.1078646183013916 acc: 0.9091490507125854  val: loss: 2.3584465980529785 acc: 0.777879536151886\n",
      "step: 2915 time:0.015743255615234375\n",
      "train: loss: 1.2358009815216064 acc: 0.881784975528717  val: loss: 19.24123191833496 acc: 0.777346134185791\n",
      "step: 2920 time:0.004002571105957031\n",
      "train: loss: 2.1524698734283447 acc: 0.8252646923065186  val: loss: 10.122082710266113 acc: 0.9087343811988831\n",
      "step: 2925 time:0.015625715255737305\n",
      "train: loss: 1.7880644798278809 acc: 0.8162721395492554  val: loss: 1.9152486324310303 acc: 0.763444721698761\n",
      "step: 2930 time:0.0\n",
      "train: loss: 3.46157169342041 acc: 0.7643207311630249  val: loss: 17.338417053222656 acc: 0.8096511960029602\n",
      "step: 2935 time:0.0\n",
      "train: loss: 2.465017795562744 acc: 0.8411006927490234  val: loss: 19.638307571411133 acc: 0.846613883972168\n",
      "step: 2940 time:0.0030012130737304688\n",
      "train: loss: 0.9526995420455933 acc: 0.8658195734024048  val: loss: 14.679052352905273 acc: 0.7931349873542786\n",
      "step: 2945 time:0.015625476837158203\n",
      "train: loss: 0.5709332823753357 acc: 0.8403705358505249  val: loss: 21.38167381286621 acc: 0.7656016945838928\n",
      "step: 2950 time:0.0\n",
      "train: loss: 0.6313483715057373 acc: 0.6854026913642883  val: loss: 8.036491394042969 acc: 0.7854337096214294\n",
      "step: 2955 time:0.0\n",
      "train: loss: 1.4771854877471924 acc: 0.6949777603149414  val: loss: 6.550290584564209 acc: 0.7262119650840759\n",
      "step: 2960 time:0.003002166748046875\n",
      "train: loss: 1.222627878189087 acc: 0.657114565372467  val: loss: 2.392296314239502 acc: 0.8325037956237793\n",
      "step: 2965 time:0.0\n",
      "train: loss: 0.8924018144607544 acc: 0.8236821889877319  val: loss: 2.0296943187713623 acc: 0.8279024362564087\n",
      "step: 2970 time:0.003002166748046875\n",
      "train: loss: 0.9964383244514465 acc: 0.7606767416000366  val: loss: 13.860491752624512 acc: 0.6943191289901733\n",
      "step: 2975 time:0.0\n",
      "train: loss: 0.9979011416435242 acc: 0.7908757925033569  val: loss: 11.1106595993042 acc: 0.7068792581558228\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 2980 time:0.004002571105957031\n",
      "train: loss: 1.4673817157745361 acc: 0.6793831586837769  val: loss: 9.0184907913208 acc: 0.7470436692237854\n",
      "step: 2985 time:0.0\n",
      "train: loss: 1.343119740486145 acc: 0.43763113021850586  val: loss: 13.012529373168945 acc: 0.6857703924179077\n",
      "step: 2990 time:0.0\n",
      "train: loss: 0.9329715967178345 acc: 0.5111206769943237  val: loss: 13.222599983215332 acc: 0.8002001047134399\n",
      "step: 2995 time:0.0\n",
      "train: loss: 0.38873040676116943 acc: 0.7693008780479431  val: loss: 2.4139153957366943 acc: 0.844526469707489\n",
      "step: 3000 time:0.00400233268737793\n",
      "train: loss: 0.5551910400390625 acc: 0.7009769082069397  val: loss: 2.2977452278137207 acc: 0.7999346256256104\n",
      "step: 3005 time:0.0\n",
      "train: loss: 0.6298048496246338 acc: 0.7310754060745239  val: loss: 11.666220664978027 acc: 0.798356294631958\n",
      "step: 3010 time:0.0\n",
      "train: loss: 0.6443647146224976 acc: 0.6995476484298706  val: loss: 23.46636199951172 acc: 0.742072582244873\n",
      "step: 3015 time:0.0\n",
      "train: loss: 1.1669598817825317 acc: 0.7135171890258789  val: loss: 21.1446533203125 acc: 0.6872600317001343\n",
      "step: 3020 time:0.0\n",
      "train: loss: 1.1902549266815186 acc: 0.7878464460372925  val: loss: 28.45819854736328 acc: 0.8098503351211548\n",
      "step: 3025 time:0.0\n",
      "train: loss: 2.2669296264648438 acc: 0.8247339725494385  val: loss: 1.9248745441436768 acc: 0.860670804977417\n",
      "step: 3030 time:0.0156252384185791\n",
      "train: loss: 1.6104333400726318 acc: 0.7340338826179504  val: loss: 1.8788856267929077 acc: 0.7906762361526489\n",
      "step: 3035 time:0.0\n",
      "train: loss: 1.4752604961395264 acc: 0.7582188248634338  val: loss: 0.7468790411949158 acc: 0.832972526550293\n",
      "step: 3040 time:0.003001689910888672\n",
      "train: loss: 1.749135136604309 acc: 0.6671956181526184  val: loss: 6.972416400909424 acc: 0.7813119292259216\n",
      "step: 3045 time:0.0\n",
      "train: loss: 1.2453398704528809 acc: 0.7485097646713257  val: loss: 4.780037879943848 acc: 0.8884504437446594\n",
      "step: 3050 time:0.0\n",
      "train: loss: 0.7104706168174744 acc: 0.8669862747192383  val: loss: 1.8336198329925537 acc: 0.8728277683258057\n",
      "step: 3055 time:0.0\n",
      "train: loss: 0.6837595701217651 acc: 0.7614419460296631  val: loss: 3.0907042026519775 acc: 0.7981212139129639\n",
      "step: 3060 time:0.0030024051666259766\n",
      "train: loss: 0.4150112271308899 acc: 0.7648254632949829  val: loss: 30.487714767456055 acc: 0.7844175696372986\n",
      "step: 3065 time:0.0\n",
      "train: loss: 0.5452415943145752 acc: 0.8400778770446777  val: loss: 14.239175796508789 acc: 0.870806872844696\n",
      "step: 3070 time:0.016597270965576172\n",
      "train: loss: 0.7522083520889282 acc: 0.7958036065101624  val: loss: 1.0282025337219238 acc: 0.8385016918182373\n",
      "step: 3075 time:0.001779317855834961\n",
      "train: loss: 2.1975293159484863 acc: 0.8428057432174683  val: loss: 5.976190567016602 acc: 0.7824568152427673\n",
      "step: 3080 time:0.003002166748046875\n",
      "train: loss: 5.498983383178711 acc: 0.8733717799186707  val: loss: 3.2034828662872314 acc: 0.6879761219024658\n",
      "step: 3085 time:0.0\n",
      "train: loss: 3.2452750205993652 acc: 0.8508878946304321  val: loss: 4.321650981903076 acc: 0.7661246061325073\n",
      "step: 3090 time:0.0030019283294677734\n",
      "train: loss: 1.2540242671966553 acc: 0.8582206964492798  val: loss: 10.097297668457031 acc: 0.8352106809616089\n",
      "step: 3095 time:0.0\n",
      "train: loss: 2.9952774047851562 acc: 0.8293853998184204  val: loss: 1.5972836017608643 acc: 0.8073642253875732\n",
      "step: 3100 time:0.0030031204223632812\n",
      "train: loss: 1.4104758501052856 acc: 0.7339134812355042  val: loss: 9.038047790527344 acc: 0.875927746295929\n",
      "step: 3105 time:0.0\n",
      "train: loss: 2.148625373840332 acc: 0.8663115501403809  val: loss: 3.615377187728882 acc: 0.821094274520874\n",
      "step: 3110 time:0.0\n",
      "train: loss: 1.3741565942764282 acc: 0.7297912836074829  val: loss: 2.7857649326324463 acc: 0.87843918800354\n",
      "step: 3115 time:0.015625\n",
      "train: loss: 1.0832804441452026 acc: 0.786595344543457  val: loss: 2.082581043243408 acc: 0.9024252891540527\n",
      "step: 3120 time:0.003001689910888672\n",
      "train: loss: 1.7623546123504639 acc: 0.7858616709709167  val: loss: 7.233780384063721 acc: 0.7668061852455139\n",
      "step: 3125 time:0.003002166748046875\n",
      "train: loss: 1.9987807273864746 acc: 0.824644923210144  val: loss: 2.930182933807373 acc: 0.7505837678909302\n",
      "step: 3130 time:0.0\n",
      "train: loss: 6.9521074295043945 acc: 0.8537457585334778  val: loss: 5.817019462585449 acc: 0.8709805011749268\n",
      "step: 3135 time:0.0030014514923095703\n",
      "train: loss: 1.3418726921081543 acc: 0.8594380021095276  val: loss: 17.078935623168945 acc: 0.8661243915557861\n",
      "step: 3140 time:0.003002166748046875\n",
      "train: loss: 1.1761482954025269 acc: 0.8634897470474243  val: loss: 13.184942245483398 acc: 0.7250136137008667\n",
      "step: 3145 time:0.003002166748046875\n",
      "train: loss: 2.0429458618164062 acc: 0.868212878704071  val: loss: 7.523158073425293 acc: 0.7625813484191895\n",
      "step: 3150 time:0.0\n",
      "train: loss: 3.735520601272583 acc: 0.7277995347976685  val: loss: 1.7852394580841064 acc: 0.8491714596748352\n",
      "step: 3155 time:0.017575740814208984\n",
      "train: loss: 14.621183395385742 acc: 0.6357066631317139  val: loss: 5.49391508102417 acc: 0.7364342212677002\n",
      "step: 3160 time:0.0030019283294677734\n",
      "train: loss: 22.481996536254883 acc: 0.25068169832229614  val: loss: 22.036113739013672 acc: 0.7522101402282715\n",
      "step: 3165 time:0.015626192092895508\n",
      "train: loss: 24.099594116210938 acc: 0.23116028308868408  val: loss: 8.632610321044922 acc: 0.7597126960754395\n",
      "step: 3170 time:0.0\n",
      "train: loss: 9.293903350830078 acc: 0.8482838869094849  val: loss: 9.65697193145752 acc: 0.8652325868606567\n",
      "step: 3175 time:0.0\n",
      "train: loss: 14.126598358154297 acc: 0.14491862058639526  val: loss: 5.572454452514648 acc: 0.8311449289321899\n",
      "step: 3180 time:0.003001689910888672\n",
      "train: loss: 18.353519439697266 acc: 0.5092671513557434  val: loss: 18.208139419555664 acc: 0.6283052563667297\n",
      "step: 3185 time:0.0\n",
      "train: loss: 18.539493560791016 acc: 0.47391390800476074  val: loss: 1.717705249786377 acc: 0.7954115867614746\n",
      "step: 3190 time:0.0\n",
      "train: loss: 15.966893196105957 acc: 0.8320884704589844  val: loss: 5.744043350219727 acc: 0.8282244801521301\n",
      "step: 3195 time:0.0\n",
      "train: loss: 15.372536659240723 acc: 0.7929354906082153  val: loss: 7.282449245452881 acc: 0.8511900305747986\n",
      "step: 3200 time:0.0030024051666259766\n",
      "train: loss: 6.839693069458008 acc: 0.8304154276847839  val: loss: 3.6464457511901855 acc: 0.8101053237915039\n",
      "step: 3205 time:0.0\n",
      "train: loss: 8.609901428222656 acc: 0.8948932886123657  val: loss: 1.9439499378204346 acc: 0.8422375321388245\n",
      "step: 3210 time:0.0\n",
      "train: loss: 5.993780612945557 acc: 0.8839651346206665  val: loss: 1.2578407526016235 acc: 0.8573160767555237\n",
      "step: 3215 time:0.015625953674316406\n",
      "train: loss: 24.196266174316406 acc: 0.46055763959884644  val: loss: 1.2273495197296143 acc: 0.7888296246528625\n",
      "step: 3220 time:0.0030024051666259766\n",
      "train: loss: 11.296449661254883 acc: 0.804198145866394  val: loss: 9.09274673461914 acc: 0.8633472919464111\n",
      "step: 3225 time:0.0\n",
      "train: loss: 7.550209999084473 acc: 0.8961622714996338  val: loss: 3.913180351257324 acc: 0.8543591499328613\n",
      "step: 3230 time:0.0031659603118896484\n",
      "train: loss: 6.149148941040039 acc: 0.8109411001205444  val: loss: 6.949901580810547 acc: 0.8613606691360474\n",
      "step: 3235 time:0.0\n",
      "train: loss: 8.718945503234863 acc: 0.8153097629547119  val: loss: 3.5060834884643555 acc: 0.8197873830795288\n",
      "step: 3240 time:0.003002166748046875\n",
      "train: loss: 3.6020264625549316 acc: 0.8119974136352539  val: loss: 10.683238983154297 acc: 0.7988348603248596\n",
      "step: 3245 time:0.0030956268310546875\n",
      "train: loss: 8.302884101867676 acc: 0.6198182106018066  val: loss: 4.479944229125977 acc: 0.8268924951553345\n",
      "step: 3250 time:0.015626192092895508\n",
      "train: loss: 5.546181678771973 acc: 0.7720351219177246  val: loss: 1.125842571258545 acc: 0.8785359263420105\n",
      "step: 3255 time:0.003001689910888672\n",
      "train: loss: 10.563692092895508 acc: 0.8590412139892578  val: loss: 4.11102294921875 acc: 0.84063720703125\n",
      "step: 3260 time:0.008005380630493164\n",
      "train: loss: 2.713470935821533 acc: 0.7816433906555176  val: loss: 1.1866977214813232 acc: 0.7485846281051636\n",
      "step: 3265 time:0.0030014514923095703\n",
      "train: loss: 8.747323989868164 acc: 0.4617100954055786  val: loss: 4.369172096252441 acc: 0.8386721014976501\n",
      "step: 3270 time:0.003001689910888672\n",
      "train: loss: 2.9311325550079346 acc: 0.8484229445457458  val: loss: 2.8486757278442383 acc: 0.7696722149848938\n",
      "step: 3275 time:0.0040018558502197266\n",
      "train: loss: 10.373738288879395 acc: 0.5103864669799805  val: loss: 3.0691561698913574 acc: 0.7486644983291626\n",
      "step: 3280 time:0.0030019283294677734\n",
      "train: loss: 2.2288272380828857 acc: 0.8626774549484253  val: loss: 6.269380569458008 acc: 0.7759947776794434\n",
      "step: 3285 time:0.00400233268737793\n",
      "train: loss: 1.5405652523040771 acc: 0.9083262085914612  val: loss: 5.340893745422363 acc: 0.85284823179245\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 3290 time:0.0030019283294677734\n",
      "train: loss: 3.7436978816986084 acc: 0.791752815246582  val: loss: 2.0417284965515137 acc: 0.8050150275230408\n",
      "step: 3295 time:0.0030014514923095703\n",
      "train: loss: 2.277735710144043 acc: 0.7838001251220703  val: loss: 9.729643821716309 acc: 0.8733716011047363\n",
      "step: 3300 time:0.003002166748046875\n",
      "train: loss: 9.015629768371582 acc: 0.8754110932350159  val: loss: 2.3932690620422363 acc: 0.7733826637268066\n",
      "step: 3305 time:0.0\n",
      "train: loss: 2.5413105487823486 acc: 0.9372755289077759  val: loss: 5.387879371643066 acc: 0.831327497959137\n",
      "step: 3310 time:0.0030019283294677734\n",
      "train: loss: 5.695169448852539 acc: 0.8031131625175476  val: loss: 9.939464569091797 acc: 0.5204722285270691\n",
      "step: 3315 time:0.004002571105957031\n",
      "train: loss: 5.450580596923828 acc: 0.5595742464065552  val: loss: 4.9298505783081055 acc: 0.8921037912368774\n",
      "step: 3320 time:0.004002809524536133\n",
      "train: loss: 0.979802668094635 acc: 0.8652145266532898  val: loss: 6.301164627075195 acc: 0.7702335119247437\n",
      "step: 3325 time:0.00400233268737793\n",
      "train: loss: 2.8445162773132324 acc: 0.6455033421516418  val: loss: 2.3304057121276855 acc: 0.8233790397644043\n",
      "step: 3330 time:0.003000974655151367\n",
      "train: loss: 2.307159185409546 acc: 0.7900899648666382  val: loss: 5.021062850952148 acc: 0.8292452096939087\n",
      "step: 3335 time:0.0030019283294677734\n",
      "train: loss: 3.400885581970215 acc: 0.7822238206863403  val: loss: 7.319954872131348 acc: 0.8248519897460938\n",
      "step: 3340 time:0.0013704299926757812\n",
      "train: loss: 8.082965850830078 acc: 0.6802780032157898  val: loss: 11.291691780090332 acc: 0.7970670461654663\n",
      "step: 3345 time:0.015625953674316406\n",
      "train: loss: 3.9341540336608887 acc: 0.7816950678825378  val: loss: 3.234210968017578 acc: 0.7778354287147522\n",
      "step: 3350 time:0.0\n",
      "train: loss: 11.255393981933594 acc: 0.6943159699440002  val: loss: 2.7909467220306396 acc: 0.6421449184417725\n",
      "step: 3355 time:0.0\n",
      "train: loss: 5.885105609893799 acc: 0.7895054221153259  val: loss: 6.319453239440918 acc: 0.8346232175827026\n",
      "step: 3360 time:0.0030014514923095703\n",
      "train: loss: 5.532410621643066 acc: 0.7421410083770752  val: loss: 1.7798209190368652 acc: 0.836536169052124\n",
      "step: 3365 time:0.003002166748046875\n",
      "train: loss: 4.807966232299805 acc: 0.8160735368728638  val: loss: 18.91707420349121 acc: 0.7473936080932617\n",
      "step: 3370 time:0.003002166748046875\n",
      "train: loss: 9.986478805541992 acc: 0.6363322734832764  val: loss: 9.870126724243164 acc: 0.6880096197128296\n",
      "step: 3375 time:0.0012981891632080078\n",
      "train: loss: 4.858259201049805 acc: 0.7832947373390198  val: loss: 4.800725936889648 acc: 0.8601088523864746\n",
      "step: 3380 time:0.003002166748046875\n",
      "train: loss: 6.661513328552246 acc: 0.8187596797943115  val: loss: 4.013192176818848 acc: 0.8050424456596375\n",
      "step: 3385 time:0.0\n",
      "train: loss: 7.496456146240234 acc: 0.8085344433784485  val: loss: 14.724508285522461 acc: 0.7875315546989441\n",
      "step: 3390 time:0.0\n",
      "train: loss: 3.6837000846862793 acc: 0.886138916015625  val: loss: 8.237959861755371 acc: 0.838063657283783\n",
      "step: 3395 time:0.003002643585205078\n",
      "train: loss: 3.3519723415374756 acc: 0.8778277635574341  val: loss: 4.789035320281982 acc: 0.8055933117866516\n",
      "step: 3400 time:0.015625476837158203\n",
      "train: loss: 2.6825599670410156 acc: 0.8694667816162109  val: loss: 3.0144474506378174 acc: 0.7760448455810547\n",
      "step: 3405 time:0.0\n",
      "train: loss: 2.1831727027893066 acc: 0.837348222732544  val: loss: 21.24557113647461 acc: 0.7437513470649719\n",
      "step: 3410 time:0.015625953674316406\n",
      "train: loss: 3.165980815887451 acc: 0.7512956857681274  val: loss: 1.355517029762268 acc: 0.8156015872955322\n",
      "step: 3415 time:0.0030019283294677734\n",
      "train: loss: 2.688901424407959 acc: 0.7890269756317139  val: loss: 1.7166248559951782 acc: 0.8401650190353394\n",
      "step: 3420 time:0.0\n",
      "train: loss: 1.2700366973876953 acc: 0.8287527561187744  val: loss: 17.92633819580078 acc: 0.646538257598877\n",
      "step: 3425 time:0.0\n",
      "train: loss: 5.249222755432129 acc: 0.8948144912719727  val: loss: 13.01219367980957 acc: 0.7583338022232056\n",
      "step: 3430 time:0.0\n",
      "train: loss: 2.6800527572631836 acc: 0.8700058460235596  val: loss: 6.302158355712891 acc: 0.7867442965507507\n",
      "step: 3435 time:0.003001689910888672\n",
      "train: loss: 3.7082407474517822 acc: 0.8824647665023804  val: loss: 6.144582748413086 acc: 0.8542320728302002\n",
      "step: 3440 time:0.015625476837158203\n",
      "train: loss: 5.500123500823975 acc: 0.8898518085479736  val: loss: 5.59067440032959 acc: 0.8050823211669922\n",
      "step: 3445 time:0.0030019283294677734\n",
      "train: loss: 0.5353348255157471 acc: 0.865835428237915  val: loss: 1.8925673961639404 acc: 0.8808150291442871\n",
      "step: 3450 time:0.00400233268737793\n",
      "train: loss: 4.365994930267334 acc: 0.7448477149009705  val: loss: 18.22835922241211 acc: 0.7569636702537537\n",
      "step: 3455 time:0.004001617431640625\n",
      "train: loss: 0.5693246126174927 acc: 0.8463107943534851  val: loss: 3.250718116760254 acc: 0.8435380458831787\n",
      "step: 3460 time:0.006003141403198242\n",
      "train: loss: 1.1482974290847778 acc: 0.8325173854827881  val: loss: 6.670405387878418 acc: 0.7665084600448608\n",
      "step: 3465 time:0.004002094268798828\n",
      "train: loss: 2.403895378112793 acc: 0.6240498423576355  val: loss: 3.163386106491089 acc: 0.5755513310432434\n",
      "step: 3470 time:0.003002166748046875\n",
      "train: loss: 0.7643425464630127 acc: 0.7889679074287415  val: loss: 2.7158050537109375 acc: 0.8663595914840698\n",
      "step: 3475 time:0.0030012130737304688\n",
      "train: loss: 1.3581913709640503 acc: 0.7381680011749268  val: loss: 15.411730766296387 acc: 0.7296314239501953\n",
      "step: 3480 time:0.003001689910888672\n",
      "train: loss: 0.6866644024848938 acc: 0.8203145861625671  val: loss: 2.646331548690796 acc: 0.8239943385124207\n",
      "step: 3485 time:0.003001689910888672\n",
      "train: loss: 3.7542643547058105 acc: 0.7798708081245422  val: loss: 4.490540504455566 acc: 0.7705130577087402\n",
      "step: 3490 time:0.0001544952392578125\n",
      "train: loss: 0.8083292245864868 acc: 0.8467735648155212  val: loss: 2.2191085815429688 acc: 0.8754496574401855\n",
      "step: 3495 time:0.003001689910888672\n",
      "train: loss: 1.318437099456787 acc: 0.7583509087562561  val: loss: 9.150819778442383 acc: 0.792457640171051\n",
      "step: 3500 time:0.003001689910888672\n",
      "train: loss: 0.6936748623847961 acc: 0.7867794632911682  val: loss: 7.658777713775635 acc: 0.823127269744873\n",
      "step: 3505 time:0.0030024051666259766\n",
      "train: loss: 0.5132426023483276 acc: 0.7911397218704224  val: loss: 10.752847671508789 acc: 0.7986146807670593\n",
      "step: 3510 time:0.015625953674316406\n",
      "train: loss: 0.4555402398109436 acc: 0.6333150863647461  val: loss: 6.026250839233398 acc: 0.8962864875793457\n",
      "step: 3515 time:0.0\n",
      "train: loss: 0.54188472032547 acc: 0.6802709102630615  val: loss: 1.7825682163238525 acc: 0.7903159260749817\n",
      "step: 3520 time:0.003002166748046875\n",
      "train: loss: 0.7722010612487793 acc: 0.7864081859588623  val: loss: 1.085231065750122 acc: 0.8788259029388428\n",
      "step: 3525 time:0.0\n",
      "train: loss: 0.6689329147338867 acc: 0.8164905309677124  val: loss: 8.71943473815918 acc: 0.8556104898452759\n",
      "step: 3530 time:0.0\n",
      "train: loss: 1.5082908868789673 acc: 0.6891646385192871  val: loss: 4.550246238708496 acc: 0.8587174415588379\n",
      "step: 3535 time:0.015625715255737305\n",
      "train: loss: 1.4076043367385864 acc: 0.7805359959602356  val: loss: 2.981273651123047 acc: 0.9020724296569824\n",
      "step: 3540 time:0.003001689910888672\n",
      "train: loss: 2.284684181213379 acc: 0.685645341873169  val: loss: 19.195335388183594 acc: 0.769639790058136\n",
      "step: 3545 time:0.0\n",
      "train: loss: 1.3043043613433838 acc: 0.7855452299118042  val: loss: 20.87515640258789 acc: 0.7261650562286377\n",
      "step: 3550 time:0.0030019283294677734\n",
      "train: loss: 1.964737057685852 acc: 0.8117614984512329  val: loss: 3.8797647953033447 acc: 0.7525292634963989\n",
      "step: 3555 time:0.003002166748046875\n",
      "train: loss: 0.5088593363761902 acc: 0.6999261379241943  val: loss: 9.89626693725586 acc: 0.8413379192352295\n",
      "step: 3560 time:0.003002166748046875\n",
      "train: loss: 0.9469431638717651 acc: 0.8779078722000122  val: loss: 2.8254668712615967 acc: 0.8430576324462891\n",
      "step: 3565 time:0.004002094268798828\n",
      "train: loss: 2.005037546157837 acc: 0.7534698247909546  val: loss: 4.951503753662109 acc: 0.8095789551734924\n",
      "step: 3570 time:0.0030024051666259766\n",
      "train: loss: 0.9170053601264954 acc: 0.8121668696403503  val: loss: 3.793247699737549 acc: 0.8243815302848816\n",
      "step: 3575 time:0.002251148223876953\n",
      "train: loss: 1.4684500694274902 acc: 0.7130190134048462  val: loss: 4.949827194213867 acc: 0.8206189274787903\n",
      "step: 3580 time:0.003002166748046875\n",
      "train: loss: 0.6906710267066956 acc: 0.850388765335083  val: loss: 5.375878810882568 acc: 0.8555898070335388\n",
      "step: 3585 time:0.0030019283294677734\n",
      "train: loss: 0.8002830743789673 acc: 0.8759374618530273  val: loss: 5.648759841918945 acc: 0.8192027807235718\n",
      "step: 3590 time:0.0030019283294677734\n",
      "train: loss: 2.4604415893554688 acc: 0.7987099289894104  val: loss: 1.974548101425171 acc: 0.8389819860458374\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 3595 time:0.0030014514923095703\n",
      "train: loss: 2.0987868309020996 acc: 0.573471188545227  val: loss: 4.767495155334473 acc: 0.8368099927902222\n",
      "step: 3600 time:0.0\n",
      "train: loss: 1.4867422580718994 acc: 0.8563647270202637  val: loss: 2.2954115867614746 acc: 0.6994141936302185\n",
      "step: 3605 time:0.00400233268737793\n",
      "train: loss: 1.4004799127578735 acc: 0.7953484654426575  val: loss: 6.873678207397461 acc: 0.7541749477386475\n",
      "step: 3610 time:0.004002809524536133\n",
      "train: loss: 2.138930320739746 acc: 0.8212067484855652  val: loss: 1.5608782768249512 acc: 0.8198432922363281\n",
      "step: 3615 time:0.00400233268737793\n",
      "train: loss: 1.109386682510376 acc: 0.9197546243667603  val: loss: 12.2396240234375 acc: 0.8138723373413086\n",
      "step: 3620 time:0.003001689910888672\n",
      "train: loss: 2.6413938999176025 acc: 0.8658052682876587  val: loss: 1.7020397186279297 acc: 0.7454850077629089\n",
      "step: 3625 time:0.0030019283294677734\n",
      "train: loss: 1.1588748693466187 acc: 0.7943387627601624  val: loss: 14.150588989257812 acc: 0.8211865425109863\n",
      "step: 3630 time:0.0\n",
      "train: loss: 1.7192412614822388 acc: 0.8787660598754883  val: loss: 17.086753845214844 acc: 0.8220341205596924\n",
      "step: 3635 time:0.015835285186767578\n",
      "train: loss: 0.8448265194892883 acc: 0.8372597694396973  val: loss: 1.2852799892425537 acc: 0.8692938089370728\n",
      "step: 3640 time:0.005002737045288086\n",
      "train: loss: 1.299858570098877 acc: 0.8142388463020325  val: loss: 6.345458030700684 acc: 0.8676779270172119\n",
      "step: 3645 time:0.003002643585205078\n",
      "train: loss: 7.9292144775390625 acc: 0.8382737636566162  val: loss: 4.088891983032227 acc: 0.7652677297592163\n",
      "step: 3650 time:0.004002571105957031\n",
      "train: loss: 2.2254395484924316 acc: 0.8648517727851868  val: loss: 10.673062324523926 acc: 0.7821725606918335\n",
      "step: 3655 time:0.00400233268737793\n",
      "train: loss: 1.319164752960205 acc: 0.9033490419387817  val: loss: 10.920843124389648 acc: 0.895653486251831\n",
      "step: 3660 time:0.004002094268798828\n",
      "train: loss: 3.9431540966033936 acc: 0.8776038885116577  val: loss: 6.4613447189331055 acc: 0.7773640155792236\n",
      "step: 3665 time:0.003001689910888672\n",
      "train: loss: 0.8463355302810669 acc: 0.8236743211746216  val: loss: 0.7785661220550537 acc: 0.8392897248268127\n",
      "step: 3670 time:0.0030024051666259766\n",
      "train: loss: 4.367770195007324 acc: 0.7494667768478394  val: loss: 1.2370814085006714 acc: 0.8004207611083984\n",
      "step: 3675 time:0.0\n",
      "train: loss: 7.207441329956055 acc: 0.6622647047042847  val: loss: 4.848576068878174 acc: 0.7639541029930115\n",
      "step: 3680 time:0.015625476837158203\n",
      "train: loss: 19.949539184570312 acc: 0.6184456944465637  val: loss: 6.707858085632324 acc: 0.7603477239608765\n",
      "step: 3685 time:0.0\n",
      "train: loss: 10.788716316223145 acc: 0.16414201259613037  val: loss: 7.485686302185059 acc: 0.853662371635437\n",
      "step: 3690 time:0.0030019283294677734\n",
      "train: loss: 6.283869743347168 acc: 0.7734204530715942  val: loss: 2.2603673934936523 acc: 0.8526420593261719\n",
      "step: 3695 time:0.0040018558502197266\n",
      "train: loss: 8.767305374145508 acc: 0.31334078311920166  val: loss: 2.8791863918304443 acc: 0.7709134221076965\n",
      "step: 3700 time:0.0\n",
      "train: loss: 18.223602294921875 acc: 0.8355002999305725  val: loss: 3.845330238342285 acc: 0.8765404224395752\n",
      "step: 3705 time:0.003002166748046875\n",
      "train: loss: 12.361732482910156 acc: 0.7933171987533569  val: loss: 4.489812850952148 acc: 0.6578497886657715\n",
      "step: 3710 time:0.004003286361694336\n",
      "train: loss: 15.230871200561523 acc: 0.7865341305732727  val: loss: 4.595233917236328 acc: 0.7886632084846497\n",
      "step: 3715 time:0.004003286361694336\n",
      "train: loss: 18.20419692993164 acc: 0.582361102104187  val: loss: 3.423982620239258 acc: 0.8294748663902283\n",
      "step: 3720 time:0.0040013790130615234\n",
      "train: loss: 14.168720245361328 acc: 0.7731442451477051  val: loss: 2.282813549041748 acc: 0.868862509727478\n",
      "step: 3725 time:0.004002094268798828\n",
      "train: loss: 10.749284744262695 acc: 0.8566280603408813  val: loss: 1.7662744522094727 acc: 0.8426003456115723\n",
      "step: 3730 time:0.0030012130737304688\n",
      "train: loss: 14.392606735229492 acc: 0.8570780158042908  val: loss: 3.3220102787017822 acc: 0.8356485366821289\n",
      "step: 3735 time:0.004002094268798828\n",
      "train: loss: 7.543763160705566 acc: 0.8504812717437744  val: loss: 2.7499520778656006 acc: 0.8230929970741272\n",
      "step: 3740 time:0.00400090217590332\n",
      "train: loss: 6.213491916656494 acc: 0.836638331413269  val: loss: 3.406707525253296 acc: 0.8322758078575134\n",
      "step: 3745 time:0.0030024051666259766\n",
      "train: loss: 5.844725131988525 acc: 0.7577755451202393  val: loss: 2.67838716506958 acc: 0.8725329637527466\n",
      "step: 3750 time:0.0\n",
      "train: loss: 6.5736403465271 acc: 0.830918550491333  val: loss: 3.9047203063964844 acc: 0.8380500078201294\n",
      "step: 3755 time:0.0030014514923095703\n",
      "train: loss: 10.941526412963867 acc: 0.7970888614654541  val: loss: 13.309527397155762 acc: 0.8174274563789368\n",
      "step: 3760 time:0.0\n",
      "train: loss: 5.6201701164245605 acc: 0.8642456531524658  val: loss: 2.165168285369873 acc: 0.8630266189575195\n",
      "step: 3765 time:0.0\n",
      "train: loss: 2.3523733615875244 acc: 0.7720959186553955  val: loss: 4.3069047927856445 acc: 0.8853625059127808\n",
      "step: 3770 time:0.0\n",
      "train: loss: 5.449807167053223 acc: 0.7988611459732056  val: loss: 2.325392723083496 acc: 0.8696366548538208\n",
      "step: 3775 time:0.0030024051666259766\n",
      "train: loss: 1.340785026550293 acc: 0.8706710338592529  val: loss: 2.5010101795196533 acc: 0.8085931539535522\n",
      "step: 3780 time:0.00400233268737793\n",
      "train: loss: 2.080108642578125 acc: 0.904572606086731  val: loss: 2.211717128753662 acc: 0.8433084487915039\n",
      "step: 3785 time:0.004003047943115234\n",
      "train: loss: 2.0246026515960693 acc: 0.8083484172821045  val: loss: 4.725978374481201 acc: 0.8817906379699707\n",
      "step: 3790 time:0.0030019283294677734\n",
      "train: loss: 5.592078685760498 acc: 0.7793799638748169  val: loss: 5.5079874992370605 acc: 0.8325856924057007\n",
      "step: 3795 time:0.003001689910888672\n",
      "train: loss: 2.7455763816833496 acc: 0.8315225839614868  val: loss: 4.155257701873779 acc: 0.8120843172073364\n",
      "step: 3800 time:0.00400233268737793\n",
      "train: loss: 3.2177677154541016 acc: 0.8129724264144897  val: loss: 3.0275449752807617 acc: 0.772128701210022\n",
      "step: 3805 time:0.0030028820037841797\n",
      "train: loss: 2.6334521770477295 acc: 0.9323753118515015  val: loss: 4.963521957397461 acc: 0.7837327122688293\n",
      "step: 3810 time:0.003001689910888672\n",
      "train: loss: 3.098074436187744 acc: 0.88852858543396  val: loss: 5.697168827056885 acc: 0.7274206280708313\n",
      "step: 3815 time:0.00400233268737793\n",
      "train: loss: 2.262845754623413 acc: 0.8890933394432068  val: loss: 6.279531955718994 acc: 0.7880322933197021\n",
      "step: 3820 time:0.0030019283294677734\n",
      "train: loss: 2.8779611587524414 acc: 0.8921582698822021  val: loss: 5.690132141113281 acc: 0.8260949850082397\n",
      "step: 3825 time:0.0030012130737304688\n",
      "train: loss: 7.609277725219727 acc: 0.8830469250679016  val: loss: 15.83831787109375 acc: 0.658157229423523\n",
      "step: 3830 time:0.0030019283294677734\n",
      "train: loss: 2.7895474433898926 acc: 0.8851633071899414  val: loss: 1.8556067943572998 acc: 0.749324381351471\n",
      "step: 3835 time:0.0\n",
      "train: loss: 11.079065322875977 acc: 0.8170218467712402  val: loss: 7.523196697235107 acc: 0.8298622965812683\n",
      "step: 3840 time:0.003002166748046875\n",
      "train: loss: 2.5332553386688232 acc: 0.822879433631897  val: loss: 14.323078155517578 acc: 0.8567496538162231\n",
      "step: 3845 time:0.0\n",
      "train: loss: 4.991486549377441 acc: 0.6394069790840149  val: loss: 3.608779191970825 acc: 0.8197062015533447\n",
      "step: 3850 time:0.0030014514923095703\n",
      "train: loss: 6.779047966003418 acc: 0.7026516199111938  val: loss: 5.118304252624512 acc: 0.7965987920761108\n",
      "step: 3855 time:0.0\n",
      "train: loss: 5.199939250946045 acc: 0.8701182007789612  val: loss: 8.061582565307617 acc: 0.7291971445083618\n",
      "step: 3860 time:0.003001689910888672\n",
      "train: loss: 3.202031373977661 acc: 0.838363528251648  val: loss: 8.295452117919922 acc: 0.7761809825897217\n",
      "step: 3865 time:0.003002166748046875\n",
      "train: loss: 8.119832992553711 acc: 0.7974011301994324  val: loss: 6.429821968078613 acc: 0.5781272649765015\n",
      "step: 3870 time:0.001378774642944336\n",
      "train: loss: 2.6935038566589355 acc: 0.7261723279953003  val: loss: 12.003891944885254 acc: 0.7995586395263672\n",
      "step: 3875 time:0.0030019283294677734\n",
      "train: loss: 7.321213722229004 acc: 0.7324094176292419  val: loss: 18.100831985473633 acc: 0.7031713724136353\n",
      "step: 3880 time:0.0030014514923095703\n",
      "train: loss: 5.664761543273926 acc: 0.6021426916122437  val: loss: 18.211740493774414 acc: 0.6840362548828125\n",
      "step: 3885 time:0.0\n",
      "train: loss: 4.866481781005859 acc: 0.7319531440734863  val: loss: 4.50523042678833 acc: 0.8286295533180237\n",
      "step: 3890 time:0.0\n",
      "train: loss: 1.9489102363586426 acc: 0.8480047583580017  val: loss: 3.4076809883117676 acc: 0.8315310478210449\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 3895 time:0.003001689910888672\n",
      "train: loss: 4.524112224578857 acc: 0.7987179756164551  val: loss: 6.0525898933410645 acc: 0.7780788540840149\n",
      "step: 3900 time:0.004002094268798828\n",
      "train: loss: 3.9982118606567383 acc: 0.7242239713668823  val: loss: 2.491158962249756 acc: 0.7834170460700989\n",
      "step: 3905 time:0.003999233245849609\n",
      "train: loss: 8.419271469116211 acc: 0.8334016799926758  val: loss: 8.66229248046875 acc: 0.8406227827072144\n",
      "step: 3910 time:0.004003047943115234\n",
      "train: loss: 4.271079063415527 acc: 0.8672423362731934  val: loss: 4.932062149047852 acc: 0.7183065414428711\n",
      "step: 3915 time:0.0030019283294677734\n",
      "train: loss: 5.599017143249512 acc: 0.5938000082969666  val: loss: 4.591418266296387 acc: 0.8081234693527222\n",
      "step: 3920 time:0.0\n",
      "train: loss: 3.5201163291931152 acc: 0.7860273122787476  val: loss: 5.614156723022461 acc: 0.847793698310852\n",
      "step: 3925 time:0.0\n",
      "train: loss: 3.0965421199798584 acc: 0.8411827087402344  val: loss: 5.235904693603516 acc: 0.8135848045349121\n",
      "step: 3930 time:0.00148773193359375\n",
      "train: loss: 5.47524881362915 acc: 0.7099583148956299  val: loss: 2.634823799133301 acc: 0.7945687770843506\n",
      "step: 3935 time:0.0\n",
      "train: loss: 3.497323751449585 acc: 0.8484642505645752  val: loss: 3.2290124893188477 acc: 0.8467996120452881\n",
      "step: 3940 time:0.003002166748046875\n",
      "train: loss: 4.119100093841553 acc: 0.8384873867034912  val: loss: 5.243264198303223 acc: 0.9024324417114258\n",
      "step: 3945 time:0.0030024051666259766\n",
      "train: loss: 6.958420276641846 acc: 0.8104944825172424  val: loss: 10.280426025390625 acc: 0.8172910213470459\n",
      "step: 3950 time:0.005003213882446289\n",
      "train: loss: 0.8651110529899597 acc: 0.9340119957923889  val: loss: 6.378012657165527 acc: 0.8394724130630493\n",
      "step: 3955 time:0.004002094268798828\n",
      "train: loss: 0.8736438155174255 acc: 0.8798456192016602  val: loss: 2.5523152351379395 acc: 0.8690381050109863\n",
      "step: 3960 time:0.005003452301025391\n",
      "train: loss: 2.3173131942749023 acc: 0.8389917612075806  val: loss: 6.772453308105469 acc: 0.8313673138618469\n",
      "step: 3965 time:0.003001689910888672\n",
      "train: loss: 2.0929694175720215 acc: 0.7908698320388794  val: loss: 10.758401870727539 acc: 0.7988365888595581\n",
      "step: 3970 time:0.003001689910888672\n",
      "train: loss: 3.185133457183838 acc: 0.8342939615249634  val: loss: 1.1244220733642578 acc: 0.8351552486419678\n",
      "step: 3975 time:0.0030014514923095703\n",
      "train: loss: 1.2860333919525146 acc: 0.896719753742218  val: loss: 21.36863136291504 acc: 0.7843573093414307\n",
      "step: 3980 time:0.0\n",
      "train: loss: 0.7829432487487793 acc: 0.7858155965805054  val: loss: 1.2375719547271729 acc: 0.7823287844657898\n",
      "step: 3985 time:0.0030012130737304688\n",
      "train: loss: 1.1451081037521362 acc: 0.8337000012397766  val: loss: 4.291369915008545 acc: 0.8372933268547058\n",
      "step: 3990 time:0.0030019283294677734\n",
      "train: loss: 0.7265458106994629 acc: 0.7496609091758728  val: loss: 10.672745704650879 acc: 0.8380157947540283\n",
      "step: 3995 time:0.0\n",
      "train: loss: 2.09818959236145 acc: 0.7924962639808655  val: loss: 4.024632453918457 acc: 0.8315991163253784\n",
      "step: 4000 time:0.0\n",
      "train: loss: 0.7932246327400208 acc: 0.6611130237579346  val: loss: 4.644213676452637 acc: 0.8444743156433105\n",
      "step: 4005 time:0.0\n",
      "train: loss: 2.9927401542663574 acc: 0.7898553609848022  val: loss: 9.05008602142334 acc: 0.8216623067855835\n",
      "step: 4010 time:0.003001689910888672\n",
      "train: loss: 1.038305401802063 acc: 0.6856653094291687  val: loss: 1.879410982131958 acc: 0.891075074672699\n",
      "step: 4015 time:0.015626907348632812\n",
      "train: loss: 1.453662633895874 acc: 0.8381922245025635  val: loss: 3.851147174835205 acc: 0.8194379806518555\n",
      "step: 4020 time:0.003001689910888672\n",
      "train: loss: 0.8534125089645386 acc: 0.6477848291397095  val: loss: 5.773728847503662 acc: 0.8756520748138428\n",
      "step: 4025 time:0.0\n",
      "train: loss: 0.6120584607124329 acc: 0.7058948874473572  val: loss: 3.818416118621826 acc: 0.8108691573143005\n",
      "step: 4030 time:0.00400233268737793\n",
      "train: loss: 2.231924057006836 acc: 0.5550267100334167  val: loss: 3.4264113903045654 acc: 0.7855962514877319\n",
      "step: 4035 time:0.0\n",
      "train: loss: 1.903280258178711 acc: 0.5360088348388672  val: loss: 3.751638412475586 acc: 0.8524559736251831\n",
      "step: 4040 time:0.003001689910888672\n",
      "train: loss: 0.4883255958557129 acc: 0.8262237310409546  val: loss: 1.1944074630737305 acc: 0.8708193302154541\n",
      "step: 4045 time:0.0\n",
      "train: loss: 0.4095040559768677 acc: 0.6563695669174194  val: loss: 2.899397850036621 acc: 0.8670447468757629\n",
      "step: 4050 time:0.00400233268737793\n",
      "train: loss: 0.6884921789169312 acc: 0.8091610670089722  val: loss: 5.694245338439941 acc: 0.8826388716697693\n",
      "step: 4055 time:0.003001689910888672\n",
      "train: loss: 0.9211084246635437 acc: 0.8690475821495056  val: loss: 3.2505602836608887 acc: 0.8355615139007568\n",
      "step: 4060 time:0.0060040950775146484\n",
      "train: loss: 1.5539665222167969 acc: 0.6231087446212769  val: loss: 2.5847067832946777 acc: 0.8731309771537781\n",
      "step: 4065 time:0.0030024051666259766\n",
      "train: loss: 1.220250129699707 acc: 0.7912862300872803  val: loss: 11.830810546875 acc: 0.8459764122962952\n",
      "step: 4070 time:0.0030019283294677734\n",
      "train: loss: 1.9833436012268066 acc: 0.8464906811714172  val: loss: 4.067023277282715 acc: 0.8583258390426636\n",
      "step: 4075 time:0.015625953674316406\n",
      "train: loss: 1.1449940204620361 acc: 0.8525498509407043  val: loss: 3.7775771617889404 acc: 0.7751417756080627\n",
      "step: 4080 time:0.0\n",
      "train: loss: 0.9198288917541504 acc: 0.7566310167312622  val: loss: 2.9099817276000977 acc: 0.9177860617637634\n",
      "step: 4085 time:0.00400233268737793\n",
      "train: loss: 1.0627802610397339 acc: 0.5301015973091125  val: loss: 8.099242210388184 acc: 0.715848982334137\n",
      "step: 4090 time:0.0\n",
      "train: loss: 0.7382266521453857 acc: 0.8106027245521545  val: loss: 8.303359985351562 acc: 0.8429069519042969\n",
      "step: 4095 time:0.0\n",
      "train: loss: 0.23772218823432922 acc: 0.7773522138595581  val: loss: 7.845470428466797 acc: 0.8995349407196045\n",
      "step: 4100 time:0.015625953674316406\n",
      "train: loss: 0.6706730723381042 acc: 0.6256293058395386  val: loss: 1.8306622505187988 acc: 0.7516690492630005\n",
      "step: 4105 time:0.0\n",
      "train: loss: 3.8690598011016846 acc: 0.8171911835670471  val: loss: 6.594386100769043 acc: 0.8107882738113403\n",
      "step: 4110 time:0.0\n",
      "train: loss: 1.607337474822998 acc: 0.8700582385063171  val: loss: 21.572973251342773 acc: 0.8562143445014954\n",
      "step: 4115 time:0.0\n",
      "train: loss: 2.451063394546509 acc: 0.7623226642608643  val: loss: 5.271773338317871 acc: 0.8453103303909302\n",
      "step: 4120 time:0.0030019283294677734\n",
      "train: loss: 1.936360478401184 acc: 0.7365239262580872  val: loss: 18.330829620361328 acc: 0.7266567945480347\n",
      "step: 4125 time:0.005003690719604492\n",
      "train: loss: 1.6626700162887573 acc: 0.8840849995613098  val: loss: 2.170140027999878 acc: 0.8498885035514832\n",
      "step: 4130 time:0.0\n",
      "train: loss: 1.086279034614563 acc: 0.7935957908630371  val: loss: 6.33128023147583 acc: 0.7994459867477417\n",
      "step: 4135 time:0.015625715255737305\n",
      "train: loss: 3.431783676147461 acc: 0.6931798458099365  val: loss: 4.779827117919922 acc: 0.7784950733184814\n",
      "step: 4140 time:0.0030019283294677734\n",
      "train: loss: 1.6422736644744873 acc: 0.909386396408081  val: loss: 11.599348068237305 acc: 0.8311122059822083\n",
      "step: 4145 time:0.0\n",
      "train: loss: 1.404880166053772 acc: 0.8054013252258301  val: loss: 6.892542839050293 acc: 0.8472162485122681\n",
      "step: 4150 time:0.0\n",
      "train: loss: 0.9481040835380554 acc: 0.8774834275245667  val: loss: 2.71158504486084 acc: 0.8031522035598755\n",
      "step: 4155 time:0.0\n",
      "train: loss: 1.0313230752944946 acc: 0.8429934978485107  val: loss: 1.0251878499984741 acc: 0.8640540838241577\n",
      "step: 4160 time:0.004002571105957031\n",
      "train: loss: 4.897705554962158 acc: 0.4643591046333313  val: loss: 3.023937702178955 acc: 0.7710657119750977\n",
      "step: 4165 time:0.015625715255737305\n",
      "train: loss: 3.2099905014038086 acc: 0.6999640464782715  val: loss: 6.478421688079834 acc: 0.8197848796844482\n",
      "step: 4170 time:0.0\n",
      "train: loss: 4.93750524520874 acc: 0.8242114186286926  val: loss: 2.0397891998291016 acc: 0.805436909198761\n",
      "step: 4175 time:0.0\n",
      "train: loss: 1.3510336875915527 acc: 0.7729846239089966  val: loss: 4.604848861694336 acc: 0.8398308753967285\n",
      "step: 4180 time:0.0030019283294677734\n",
      "train: loss: 3.047464370727539 acc: 0.7745938301086426  val: loss: 11.114965438842773 acc: 0.8377321362495422\n",
      "step: 4185 time:0.0\n",
      "train: loss: 2.5950253009796143 acc: 0.7266000509262085  val: loss: 5.2160444259643555 acc: 0.7766462564468384\n",
      "step: 4190 time:0.0\n",
      "train: loss: 9.50098991394043 acc: 0.6608423590660095  val: loss: 25.023378372192383 acc: 0.7062797546386719\n",
      "step: 4195 time:0.015625953674316406\n",
      "train: loss: 4.198361873626709 acc: 0.7496071457862854  val: loss: 4.564662456512451 acc: 0.8036328554153442\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 4200 time:0.0030019283294677734\n",
      "train: loss: 13.380413055419922 acc: 0.44305455684661865  val: loss: 10.960064888000488 acc: 0.798905611038208\n",
      "step: 4205 time:0.0\n",
      "train: loss: 13.232933044433594 acc: 0.4587000608444214  val: loss: 2.4471163749694824 acc: 0.8091524839401245\n",
      "step: 4210 time:0.0020012855529785156\n",
      "train: loss: 9.162510871887207 acc: 0.8147518634796143  val: loss: 2.1005640029907227 acc: 0.7973276972770691\n",
      "step: 4215 time:0.0\n",
      "train: loss: 13.606793403625488 acc: 0.7886098027229309  val: loss: 3.30739164352417 acc: 0.8309041261672974\n",
      "step: 4220 time:0.001270294189453125\n",
      "train: loss: 4.202683925628662 acc: 0.8008759021759033  val: loss: 1.8328537940979004 acc: 0.7911254167556763\n",
      "step: 4225 time:0.0030019283294677734\n",
      "train: loss: 9.931809425354004 acc: 0.7683138251304626  val: loss: 3.8103771209716797 acc: 0.8445671200752258\n",
      "step: 4230 time:0.0\n",
      "train: loss: 18.495342254638672 acc: 0.8524782061576843  val: loss: 4.94260311126709 acc: 0.8176203370094299\n",
      "step: 4235 time:0.0\n",
      "train: loss: 16.854000091552734 acc: 0.835709273815155  val: loss: 3.052624225616455 acc: 0.828824520111084\n",
      "step: 4240 time:0.003002166748046875\n",
      "train: loss: 8.10321044921875 acc: 0.5862157940864563  val: loss: 1.293634295463562 acc: 0.853847861289978\n",
      "step: 4245 time:0.015625476837158203\n",
      "train: loss: 15.22541618347168 acc: 0.5665286183357239  val: loss: 4.420694351196289 acc: 0.7738099694252014\n",
      "step: 4250 time:0.0\n",
      "train: loss: 2.676318645477295 acc: 0.8016578555107117  val: loss: 2.240661859512329 acc: 0.7380794286727905\n",
      "step: 4255 time:0.0\n",
      "train: loss: 10.236581802368164 acc: 0.8443120121955872  val: loss: 3.3821752071380615 acc: 0.8661952018737793\n",
      "step: 4260 time:0.002001523971557617\n",
      "train: loss: 6.908139705657959 acc: 0.8011152744293213  val: loss: 3.1232974529266357 acc: 0.7904151678085327\n",
      "step: 4265 time:0.01562643051147461\n",
      "train: loss: 3.0863308906555176 acc: 0.9219703078269958  val: loss: 16.869550704956055 acc: 0.7897363305091858\n",
      "step: 4270 time:0.0\n",
      "train: loss: 3.5461766719818115 acc: 0.7182420492172241  val: loss: 2.88063907623291 acc: 0.7700828313827515\n",
      "step: 4275 time:0.0\n",
      "train: loss: 6.525745391845703 acc: 0.6383349299430847  val: loss: 5.242833137512207 acc: 0.8551181554794312\n",
      "step: 4280 time:0.0030014514923095703\n",
      "train: loss: 6.65590763092041 acc: 0.8276971578598022  val: loss: 2.9040145874023438 acc: 0.6848894953727722\n",
      "step: 4285 time:0.0\n",
      "train: loss: 4.972270965576172 acc: 0.7313820719718933  val: loss: 1.7562251091003418 acc: 0.8454329967498779\n",
      "step: 4290 time:0.0\n",
      "train: loss: 2.7926015853881836 acc: 0.8573496341705322  val: loss: 17.06498146057129 acc: 0.7616205215454102\n",
      "step: 4295 time:0.0\n",
      "train: loss: 4.591765880584717 acc: 0.8920793533325195  val: loss: 3.342487335205078 acc: 0.8106799125671387\n",
      "step: 4300 time:0.0030019283294677734\n",
      "train: loss: 5.344435214996338 acc: 0.6859139204025269  val: loss: 5.347153186798096 acc: 0.8442825675010681\n",
      "step: 4305 time:0.0016474723815917969\n",
      "train: loss: 2.564525842666626 acc: 0.9040480256080627  val: loss: 9.348073959350586 acc: 0.7826188802719116\n",
      "step: 4310 time:0.01562666893005371\n",
      "train: loss: 9.775869369506836 acc: 0.7354132533073425  val: loss: 6.25391960144043 acc: 0.8015092611312866\n",
      "step: 4315 time:0.0030019283294677734\n",
      "train: loss: 2.040217399597168 acc: 0.8542282581329346  val: loss: 2.239377975463867 acc: 0.7254409790039062\n",
      "step: 4320 time:0.004002571105957031\n",
      "train: loss: 0.6329163908958435 acc: 0.9378308653831482  val: loss: 7.131964206695557 acc: 0.8595377802848816\n",
      "step: 4325 time:0.003000497817993164\n",
      "train: loss: 1.2572368383407593 acc: 0.9151356220245361  val: loss: 4.050892353057861 acc: 0.8323818445205688\n",
      "step: 4330 time:0.0\n",
      "train: loss: 3.3152830600738525 acc: 0.8782304525375366  val: loss: 8.16319751739502 acc: 0.8590425848960876\n",
      "step: 4335 time:0.0\n",
      "train: loss: 3.394040107727051 acc: 0.8657564520835876  val: loss: 11.337133407592773 acc: 0.79698646068573\n",
      "step: 4340 time:0.003001689910888672\n",
      "train: loss: 3.04934024810791 acc: 0.8563305139541626  val: loss: 2.2706027030944824 acc: 0.8240088820457458\n",
      "step: 4345 time:0.0\n",
      "train: loss: 10.740883827209473 acc: 0.9052242636680603  val: loss: 26.071672439575195 acc: 0.6322063207626343\n",
      "step: 4350 time:0.0\n",
      "train: loss: 2.4001359939575195 acc: 0.8840674161911011  val: loss: 2.446946859359741 acc: 0.7253636121749878\n",
      "step: 4355 time:0.015627145767211914\n",
      "train: loss: 2.4958572387695312 acc: 0.830212414264679  val: loss: 20.512876510620117 acc: 0.7981893420219421\n",
      "step: 4360 time:0.0030024051666259766\n",
      "train: loss: 1.7945470809936523 acc: 0.8469085693359375  val: loss: 2.83792781829834 acc: 0.9139463305473328\n",
      "step: 4365 time:0.015625476837158203\n",
      "train: loss: 1.4519636631011963 acc: 0.777281641960144  val: loss: 1.4111558198928833 acc: 0.8823326230049133\n",
      "step: 4370 time:0.0\n",
      "train: loss: 3.424776077270508 acc: 0.6515796184539795  val: loss: 2.5694632530212402 acc: 0.7813957333564758\n",
      "step: 4375 time:0.0\n",
      "train: loss: 5.284512519836426 acc: 0.7005897760391235  val: loss: 17.58138084411621 acc: 0.6904367208480835\n",
      "step: 4380 time:0.0030019283294677734\n",
      "train: loss: 2.2064828872680664 acc: 0.6757493019104004  val: loss: 21.447378158569336 acc: 0.7575894594192505\n",
      "step: 4385 time:0.0\n",
      "train: loss: 4.61229133605957 acc: 0.8071484565734863  val: loss: 3.5643343925476074 acc: 0.7858499884605408\n",
      "step: 4390 time:0.003002166748046875\n",
      "train: loss: 7.212443828582764 acc: 0.6558792591094971  val: loss: 6.874663829803467 acc: 0.8329095840454102\n",
      "step: 4395 time:0.0\n",
      "train: loss: 3.789869785308838 acc: 0.8196728825569153  val: loss: 4.308305740356445 acc: 0.8629946112632751\n",
      "step: 4400 time:0.0030019283294677734\n",
      "train: loss: 5.995530128479004 acc: 0.6280093193054199  val: loss: 2.2318973541259766 acc: 0.7240355610847473\n",
      "step: 4405 time:0.0\n",
      "train: loss: 2.9486846923828125 acc: 0.6923438310623169  val: loss: 4.2706122398376465 acc: 0.7360972166061401\n",
      "step: 4410 time:0.0\n",
      "train: loss: 11.662059783935547 acc: 0.8615177869796753  val: loss: 2.9928154945373535 acc: 0.8098580241203308\n",
      "step: 4415 time:0.0\n",
      "train: loss: 9.171241760253906 acc: 0.5516500473022461  val: loss: 3.0696046352386475 acc: 0.7029544711112976\n",
      "step: 4420 time:0.004002809524536133\n",
      "train: loss: 3.5140676498413086 acc: 0.8307961225509644  val: loss: 4.707469940185547 acc: 0.8081432580947876\n",
      "step: 4425 time:0.0\n",
      "train: loss: 9.326253890991211 acc: 0.7549237608909607  val: loss: 4.417062759399414 acc: 0.8021539449691772\n",
      "step: 4430 time:0.015625476837158203\n",
      "train: loss: 3.434896945953369 acc: 0.7550050020217896  val: loss: 1.887015461921692 acc: 0.8198405504226685\n",
      "step: 4435 time:0.0\n",
      "train: loss: 3.1342031955718994 acc: 0.8434715270996094  val: loss: 17.287071228027344 acc: 0.7429516911506653\n",
      "step: 4440 time:0.004003047943115234\n",
      "train: loss: 3.0149824619293213 acc: 0.8252004384994507  val: loss: 1.6514331102371216 acc: 0.9044453501701355\n",
      "step: 4445 time:0.0\n",
      "train: loss: 3.8254051208496094 acc: 0.8594125509262085  val: loss: 1.5716557502746582 acc: 0.8516809940338135\n",
      "step: 4450 time:0.015626192092895508\n",
      "train: loss: 1.8366880416870117 acc: 0.8059463500976562  val: loss: 3.461196184158325 acc: 0.7788441777229309\n",
      "step: 4455 time:0.0\n",
      "train: loss: 4.318921089172363 acc: 0.7900458574295044  val: loss: 4.6662702560424805 acc: 0.824688196182251\n",
      "step: 4460 time:0.004002571105957031\n",
      "train: loss: 3.9474120140075684 acc: 0.8659519553184509  val: loss: 6.303438663482666 acc: 0.8273319005966187\n",
      "step: 4465 time:0.003002166748046875\n",
      "train: loss: 1.231187343597412 acc: 0.87578284740448  val: loss: 1.6609669923782349 acc: 0.8103456497192383\n",
      "step: 4470 time:0.002138376235961914\n",
      "train: loss: 1.8211424350738525 acc: 0.8663625121116638  val: loss: 2.849945068359375 acc: 0.880254328250885\n",
      "step: 4475 time:0.0\n",
      "train: loss: 5.32444953918457 acc: 0.8781428933143616  val: loss: 6.442869186401367 acc: 0.8226412534713745\n",
      "step: 4480 time:0.0030019283294677734\n",
      "train: loss: 1.5199832916259766 acc: 0.8785848617553711  val: loss: 1.357100248336792 acc: 0.8123987913131714\n",
      "step: 4485 time:0.015625953674316406\n",
      "train: loss: 2.550290584564209 acc: 0.8471192717552185  val: loss: 3.4459168910980225 acc: 0.8026680946350098\n",
      "step: 4490 time:0.0\n",
      "train: loss: 0.4319708049297333 acc: 0.8587818741798401  val: loss: 3.1901683807373047 acc: 0.8439397811889648\n",
      "step: 4495 time:0.0\n",
      "train: loss: 1.8694318532943726 acc: 0.8521649837493896  val: loss: 15.044429779052734 acc: 0.8186861276626587\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 4500 time:0.003002166748046875\n",
      "train: loss: 1.9497629404067993 acc: 0.7882956862449646  val: loss: 4.162370681762695 acc: 0.8443853259086609\n",
      "step: 4505 time:0.0\n",
      "train: loss: 0.23399409651756287 acc: 0.8400793671607971  val: loss: 17.13747787475586 acc: 0.7687209844589233\n",
      "step: 4510 time:0.015628814697265625\n",
      "train: loss: 0.6500071883201599 acc: 0.8357175588607788  val: loss: 3.04801869392395 acc: 0.8720809817314148\n",
      "step: 4515 time:0.0\n",
      "train: loss: 2.8048970699310303 acc: 0.8133196830749512  val: loss: 3.8518168926239014 acc: 0.8490266799926758\n",
      "step: 4520 time:0.0\n",
      "train: loss: 3.6167962551116943 acc: 0.717923104763031  val: loss: 1.6109915971755981 acc: 0.7608692646026611\n",
      "step: 4525 time:0.002001047134399414\n",
      "train: loss: 1.643576979637146 acc: 0.7992025017738342  val: loss: 1.1095120906829834 acc: 0.7929384112358093\n",
      "step: 4530 time:0.0011911392211914062\n",
      "train: loss: 0.9325494766235352 acc: 0.7943972945213318  val: loss: 6.989137649536133 acc: 0.8417639136314392\n",
      "step: 4535 time:0.0\n",
      "train: loss: 0.49947771430015564 acc: 0.73163902759552  val: loss: 3.692793607711792 acc: 0.8702635765075684\n",
      "step: 4540 time:0.0\n",
      "train: loss: 0.980606198310852 acc: 0.7705961465835571  val: loss: 2.272641181945801 acc: 0.8501479625701904\n",
      "step: 4545 time:0.0\n",
      "train: loss: 0.49990999698638916 acc: 0.7346209287643433  val: loss: 7.464078903198242 acc: 0.8626776933670044\n",
      "step: 4550 time:0.0030019283294677734\n",
      "train: loss: 0.4121788740158081 acc: 0.7773605585098267  val: loss: 7.082749366760254 acc: 0.8747467994689941\n",
      "step: 4555 time:0.0\n",
      "train: loss: 0.6388791799545288 acc: 0.8211655616760254  val: loss: 1.2552614212036133 acc: 0.8330878019332886\n",
      "step: 4560 time:0.015625476837158203\n",
      "train: loss: 0.4019154906272888 acc: 0.8520808219909668  val: loss: 2.336899757385254 acc: 0.8454000949859619\n",
      "step: 4565 time:0.0\n",
      "train: loss: 0.897650957107544 acc: 0.5842259526252747  val: loss: 6.251675605773926 acc: 0.7925776839256287\n",
      "step: 4570 time:0.003001689910888672\n",
      "train: loss: 0.5395378470420837 acc: 0.8547120690345764  val: loss: 12.923707962036133 acc: 0.7525866627693176\n",
      "step: 4575 time:0.015625715255737305\n",
      "train: loss: 0.9160967469215393 acc: 0.8641011714935303  val: loss: 7.426607131958008 acc: 0.8144079446792603\n",
      "step: 4580 time:0.0\n",
      "train: loss: 0.9556315541267395 acc: 0.8324683904647827  val: loss: 2.623283624649048 acc: 0.8741053342819214\n",
      "step: 4585 time:0.0\n",
      "train: loss: 1.4456760883331299 acc: 0.8650592565536499  val: loss: 3.3567872047424316 acc: 0.8192015886306763\n",
      "step: 4590 time:0.003001689910888672\n",
      "train: loss: 1.9123001098632812 acc: 0.84136962890625  val: loss: 1.742786169052124 acc: 0.7957886457443237\n",
      "step: 4595 time:0.0\n",
      "train: loss: 2.0516357421875 acc: 0.7241067886352539  val: loss: 1.8861726522445679 acc: 0.8766905069351196\n",
      "step: 4600 time:0.0\n",
      "train: loss: 0.6860209703445435 acc: 0.8542283773422241  val: loss: 2.80898118019104 acc: 0.8615896105766296\n",
      "step: 4605 time:0.0030019283294677734\n",
      "train: loss: 0.9057509899139404 acc: 0.8413706421852112  val: loss: 2.019806385040283 acc: 0.8196451663970947\n",
      "step: 4610 time:0.0030019283294677734\n",
      "train: loss: 0.49222898483276367 acc: 0.8551392555236816  val: loss: 9.503938674926758 acc: 0.869994044303894\n",
      "step: 4615 time:0.015625953674316406\n",
      "train: loss: 0.2810339331626892 acc: 0.7915621995925903  val: loss: 13.373236656188965 acc: 0.8054682016372681\n",
      "step: 4620 time:0.0\n",
      "train: loss: 0.2835865914821625 acc: 0.8650831580162048  val: loss: 1.2937637567520142 acc: 0.8392847776412964\n",
      "step: 4625 time:0.0\n",
      "train: loss: 1.2280241250991821 acc: 0.8043503761291504  val: loss: 5.816007614135742 acc: 0.8518297076225281\n",
      "step: 4630 time:0.0030019283294677734\n",
      "train: loss: 2.031689167022705 acc: 0.7260708808898926  val: loss: 10.914289474487305 acc: 0.8430443406105042\n",
      "step: 4635 time:0.0\n",
      "train: loss: 0.8683620095252991 acc: 0.8035327196121216  val: loss: 4.235658645629883 acc: 0.8736897706985474\n",
      "step: 4640 time:0.0\n",
      "train: loss: 2.1716325283050537 acc: 0.81281578540802  val: loss: 1.747650146484375 acc: 0.8402032256126404\n",
      "step: 4645 time:0.0\n",
      "train: loss: 1.8851369619369507 acc: 0.8548696637153625  val: loss: 2.5951318740844727 acc: 0.8246762752532959\n",
      "step: 4650 time:0.0030019283294677734\n",
      "train: loss: 1.7951065301895142 acc: 0.8194403052330017  val: loss: 1.7523332834243774 acc: 0.842000424861908\n",
      "step: 4655 time:0.0\n",
      "train: loss: 1.1336538791656494 acc: 0.7760091423988342  val: loss: 8.59865951538086 acc: 0.9073746800422668\n",
      "step: 4660 time:0.015625715255737305\n",
      "train: loss: 1.6299006938934326 acc: 0.8259888291358948  val: loss: 17.50635528564453 acc: 0.696623682975769\n",
      "step: 4665 time:0.0\n",
      "train: loss: 1.4998788833618164 acc: 0.8167974352836609  val: loss: 1.9911184310913086 acc: 0.8496690988540649\n",
      "step: 4670 time:0.004002809524536133\n",
      "train: loss: 1.6432970762252808 acc: 0.783867359161377  val: loss: 5.3659491539001465 acc: 0.9032036662101746\n",
      "step: 4675 time:0.003002166748046875\n",
      "train: loss: 1.0694544315338135 acc: 0.7618758678436279  val: loss: 4.412402153015137 acc: 0.8575019836425781\n",
      "step: 4680 time:0.0\n",
      "train: loss: 3.178133964538574 acc: 0.8349906802177429  val: loss: 9.295902252197266 acc: 0.7650122046470642\n",
      "step: 4685 time:0.01562666893005371\n",
      "train: loss: 1.8901760578155518 acc: 0.7904391288757324  val: loss: 6.9756669998168945 acc: 0.8321220278739929\n",
      "step: 4690 time:0.0030019283294677734\n",
      "train: loss: 1.5847468376159668 acc: 0.7479938268661499  val: loss: 3.7607908248901367 acc: 0.7722813487052917\n",
      "step: 4695 time:0.0014719963073730469\n",
      "train: loss: 4.4611992835998535 acc: 0.8747919797897339  val: loss: 3.178288459777832 acc: 0.8644639849662781\n",
      "step: 4700 time:0.0\n",
      "train: loss: 0.8930051326751709 acc: 0.8478326797485352  val: loss: 1.4151358604431152 acc: 0.8617868423461914\n",
      "step: 4705 time:0.0\n",
      "train: loss: 2.070441246032715 acc: 0.7294836044311523  val: loss: 2.941941738128662 acc: 0.8760098218917847\n",
      "step: 4710 time:0.0030019283294677734\n",
      "train: loss: 3.291517972946167 acc: 0.7506051063537598  val: loss: 5.546072959899902 acc: 0.805628776550293\n",
      "step: 4715 time:0.015626192092895508\n",
      "train: loss: 15.887511253356934 acc: 0.1735820770263672  val: loss: 5.974143981933594 acc: 0.7588818669319153\n",
      "step: 4720 time:0.0\n",
      "train: loss: 15.612123489379883 acc: 0.2734779715538025  val: loss: 4.002777576446533 acc: 0.8313804268836975\n",
      "step: 4725 time:0.0\n",
      "train: loss: 7.488253593444824 acc: 0.7728356719017029  val: loss: 3.389530658721924 acc: 0.8472357988357544\n",
      "step: 4730 time:0.0030019283294677734\n",
      "train: loss: 5.176074504852295 acc: 0.7787891626358032  val: loss: 1.869957447052002 acc: 0.7969481348991394\n",
      "step: 4735 time:0.0\n",
      "train: loss: 5.735116004943848 acc: 0.7660910487174988  val: loss: 4.958133697509766 acc: 0.7734909653663635\n",
      "step: 4740 time:0.0030019283294677734\n",
      "train: loss: 6.164004325866699 acc: 0.8158998489379883  val: loss: 5.339934349060059 acc: 0.8235902190208435\n",
      "step: 4745 time:0.0\n",
      "train: loss: 23.293956756591797 acc: 0.6417604088783264  val: loss: 6.3830647468566895 acc: 0.7843894362449646\n",
      "step: 4750 time:0.0030019283294677734\n",
      "train: loss: 17.53850555419922 acc: 0.7842839360237122  val: loss: 3.0616188049316406 acc: 0.7753583192825317\n",
      "step: 4755 time:0.0\n",
      "train: loss: 9.050485610961914 acc: 0.7918665409088135  val: loss: 3.039499282836914 acc: 0.8762217164039612\n",
      "step: 4760 time:0.0020012855529785156\n",
      "train: loss: 7.620490550994873 acc: 0.8108075857162476  val: loss: 7.234121322631836 acc: 0.8725247383117676\n",
      "step: 4765 time:0.015625715255737305\n",
      "train: loss: 30.82241439819336 acc: 0.23545539379119873  val: loss: 27.370880126953125 acc: 0.6603488326072693\n",
      "step: 4770 time:0.0030024051666259766\n",
      "train: loss: 12.048810958862305 acc: 0.45621252059936523  val: loss: 4.434120178222656 acc: 0.8204173445701599\n",
      "step: 4775 time:0.003002166748046875\n",
      "train: loss: 7.758810997009277 acc: 0.623607873916626  val: loss: 3.0600292682647705 acc: 0.8636149764060974\n",
      "step: 4780 time:0.0030014514923095703\n",
      "train: loss: 8.061640739440918 acc: 0.8807898163795471  val: loss: 3.7115700244903564 acc: 0.8963736295700073\n",
      "step: 4785 time:0.003002166748046875\n",
      "train: loss: 4.617810249328613 acc: 0.8955854177474976  val: loss: 2.927424430847168 acc: 0.8329771757125854\n",
      "step: 4790 time:0.0030019283294677734\n",
      "train: loss: 6.227716445922852 acc: 0.6063988208770752  val: loss: 4.69591760635376 acc: 0.8232095241546631\n",
      "step: 4795 time:0.003002166748046875\n",
      "train: loss: 6.324201583862305 acc: 0.8754318356513977  val: loss: 27.27623748779297 acc: 0.6868515014648438\n",
      "step: 4800 time:0.0030019283294677734\n",
      "train: loss: 5.722877025604248 acc: 0.6427408456802368  val: loss: 8.381830215454102 acc: 0.7937161326408386\n",
      "step: 4805 time:0.0030019283294677734\n",
      "train: loss: 2.413973331451416 acc: 0.7425674200057983  val: loss: 2.2692129611968994 acc: 0.8352446556091309\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 4810 time:0.003001689910888672\n",
      "train: loss: 7.902046203613281 acc: 0.795872151851654  val: loss: 5.1980366706848145 acc: 0.6664074063301086\n",
      "step: 4815 time:0.0020012855529785156\n",
      "train: loss: 1.9654303789138794 acc: 0.8653257489204407  val: loss: 2.741225242614746 acc: 0.8570089936256409\n",
      "step: 4820 time:0.0030014514923095703\n",
      "train: loss: 0.8934230804443359 acc: 0.8863154053688049  val: loss: 3.044536590576172 acc: 0.8986026048660278\n",
      "step: 4825 time:0.003002166748046875\n",
      "train: loss: 1.5696260929107666 acc: 0.8847144842147827  val: loss: 1.974501609802246 acc: 0.8639883995056152\n",
      "step: 4830 time:0.0030019283294677734\n",
      "train: loss: 3.625842571258545 acc: 0.8744585514068604  val: loss: 3.8065242767333984 acc: 0.8318011164665222\n",
      "step: 4835 time:0.0030019283294677734\n",
      "train: loss: 4.784687042236328 acc: 0.8215159177780151  val: loss: 0.852836549282074 acc: 0.8654628992080688\n",
      "step: 4840 time:0.003002166748046875\n",
      "train: loss: 6.9138360023498535 acc: 0.85973060131073  val: loss: 4.827672958374023 acc: 0.855290412902832\n",
      "step: 4845 time:0.0030019283294677734\n",
      "train: loss: 1.6407572031021118 acc: 0.909605085849762  val: loss: 6.157878398895264 acc: 0.8307156562805176\n",
      "step: 4850 time:0.0\n",
      "train: loss: 2.082350730895996 acc: 0.898768424987793  val: loss: 2.138338327407837 acc: 0.8428351283073425\n",
      "step: 4855 time:0.004002094268798828\n",
      "train: loss: 1.6680142879486084 acc: 0.8951703310012817  val: loss: 1.6411958932876587 acc: 0.8034749627113342\n",
      "step: 4860 time:0.003002166748046875\n",
      "train: loss: 3.3894238471984863 acc: 0.9101206064224243  val: loss: 5.434776306152344 acc: 0.8810988068580627\n",
      "step: 4865 time:0.00400233268737793\n",
      "train: loss: 4.8191375732421875 acc: 0.8393036127090454  val: loss: 6.772345542907715 acc: 0.8768645524978638\n",
      "step: 4870 time:0.004002809524536133\n",
      "train: loss: 3.0419673919677734 acc: 0.8237267732620239  val: loss: 4.642916679382324 acc: 0.8232000470161438\n",
      "step: 4875 time:0.003000974655151367\n",
      "train: loss: 2.710777521133423 acc: 0.870328426361084  val: loss: 3.828640937805176 acc: 0.8244216442108154\n",
      "step: 4880 time:0.0030019283294677734\n",
      "train: loss: 7.886103630065918 acc: 0.4090946316719055  val: loss: 3.8477683067321777 acc: 0.8078625202178955\n",
      "step: 4885 time:0.004002571105957031\n",
      "train: loss: 2.939202308654785 acc: 0.42237913608551025  val: loss: 3.439121723175049 acc: 0.798731803894043\n",
      "step: 4890 time:0.0040018558502197266\n",
      "train: loss: 3.0459933280944824 acc: 0.822129487991333  val: loss: 1.710768461227417 acc: 0.8147741556167603\n",
      "step: 4895 time:0.0030019283294677734\n",
      "train: loss: 2.3792240619659424 acc: 0.8841171860694885  val: loss: 8.726755142211914 acc: 0.5727207064628601\n",
      "step: 4900 time:0.003002166748046875\n",
      "train: loss: 10.093341827392578 acc: 0.818558394908905  val: loss: 4.134620666503906 acc: 0.7721495628356934\n",
      "step: 4905 time:0.015625715255737305\n",
      "train: loss: 6.1916680335998535 acc: 0.7396926283836365  val: loss: 3.504687786102295 acc: 0.8211908340454102\n",
      "step: 4910 time:0.003001689910888672\n",
      "train: loss: 2.914137601852417 acc: 0.7661356329917908  val: loss: 10.731964111328125 acc: 0.7238211035728455\n",
      "step: 4915 time:0.003001689910888672\n",
      "train: loss: 2.6317901611328125 acc: 0.7233730554580688  val: loss: 0.8047650456428528 acc: 0.8591547608375549\n",
      "step: 4920 time:0.0030024051666259766\n",
      "train: loss: 2.0797009468078613 acc: 0.7486339807510376  val: loss: 6.175139904022217 acc: 0.8680379390716553\n",
      "step: 4925 time:0.0\n",
      "train: loss: 3.495965003967285 acc: 0.665817141532898  val: loss: 2.2349812984466553 acc: 0.8444597125053406\n",
      "step: 4930 time:0.003001689910888672\n",
      "train: loss: 2.969233989715576 acc: 0.7639936208724976  val: loss: 3.53004789352417 acc: 0.8715620636940002\n",
      "step: 4935 time:0.002184629440307617\n",
      "train: loss: 2.3131556510925293 acc: 0.8114502429962158  val: loss: 4.044675827026367 acc: 0.8260470628738403\n",
      "step: 4940 time:0.0030019283294677734\n",
      "train: loss: 3.4055254459381104 acc: 0.6367658972740173  val: loss: 3.1541483402252197 acc: 0.7538025379180908\n",
      "step: 4945 time:0.0\n",
      "train: loss: 4.973836898803711 acc: 0.8694437742233276  val: loss: 3.8473148345947266 acc: 0.8195382952690125\n",
      "step: 4950 time:0.003002643585205078\n",
      "train: loss: 7.361563205718994 acc: 0.8643993139266968  val: loss: 4.037689208984375 acc: 0.8891420364379883\n",
      "step: 4955 time:0.0\n",
      "train: loss: 3.3217263221740723 acc: 0.8808314204216003  val: loss: 4.7238593101501465 acc: 0.8494210839271545\n",
      "step: 4960 time:0.0\n",
      "train: loss: 3.218968391418457 acc: 0.8496586084365845  val: loss: 8.462924003601074 acc: 0.7662940621376038\n",
      "step: 4965 time:0.0\n",
      "train: loss: 2.9699301719665527 acc: 0.8631443381309509  val: loss: 1.8154263496398926 acc: 0.8866047263145447\n",
      "step: 4970 time:0.004002809524536133\n",
      "train: loss: 2.8149757385253906 acc: 0.858931839466095  val: loss: 1.1917521953582764 acc: 0.8637065887451172\n",
      "step: 4975 time:0.0\n",
      "train: loss: 6.666682243347168 acc: 0.7814053893089294  val: loss: 3.9151620864868164 acc: 0.8038988709449768\n",
      "step: 4980 time:0.0\n",
      "train: loss: 0.8902658224105835 acc: 0.8966607451438904  val: loss: 4.618190288543701 acc: 0.8922038674354553\n",
      "step: 4985 time:0.0\n",
      "train: loss: 2.4247124195098877 acc: 0.8381737470626831  val: loss: 4.622675895690918 acc: 0.8526941537857056\n",
      "step: 4990 time:0.00400233268737793\n",
      "train: loss: 1.6735732555389404 acc: 0.9074376821517944  val: loss: 1.0530714988708496 acc: 0.8496628403663635\n",
      "step: 4995 time:0.0\n",
      "train: loss: 3.2020676136016846 acc: 0.8932870626449585  val: loss: 3.0894529819488525 acc: 0.8495913147926331\n",
      "step: 5000 time:0.0\n",
      "train: loss: 1.3088946342468262 acc: 0.7935235500335693  val: loss: 1.900992512702942 acc: 0.852733850479126\n",
      "step: 5005 time:0.015626192092895508\n",
      "train: loss: 1.3225970268249512 acc: 0.8292911648750305  val: loss: 7.283570289611816 acc: 0.8780941963195801\n",
      "step: 5010 time:0.003002166748046875\n",
      "train: loss: 1.1224606037139893 acc: 0.8268788456916809  val: loss: 14.007431983947754 acc: 0.7790918350219727\n",
      "step: 5015 time:0.0030019283294677734\n",
      "train: loss: 0.948091983795166 acc: 0.7127784490585327  val: loss: 9.80360221862793 acc: 0.8894397616386414\n",
      "step: 5020 time:0.0015153884887695312\n",
      "train: loss: 0.6190240383148193 acc: 0.7442482709884644  val: loss: 2.186272144317627 acc: 0.8658594489097595\n",
      "step: 5025 time:0.0030019283294677734\n",
      "train: loss: 5.213951110839844 acc: 0.8299981951713562  val: loss: 2.814321994781494 acc: 0.8279125690460205\n",
      "step: 5030 time:0.003002643585205078\n",
      "train: loss: 0.39133042097091675 acc: 0.8404611945152283  val: loss: 2.9503090381622314 acc: 0.8662880659103394\n",
      "step: 5035 time:0.0030019283294677734\n",
      "train: loss: 2.3504860401153564 acc: 0.6082113981246948  val: loss: 5.376665115356445 acc: 0.7957617044448853\n",
      "step: 5040 time:0.0\n",
      "train: loss: 0.6856950521469116 acc: 0.8419311046600342  val: loss: 13.29372501373291 acc: 0.7270897030830383\n",
      "step: 5045 time:0.015626192092895508\n",
      "train: loss: 0.562336266040802 acc: 0.798313558101654  val: loss: 4.37454080581665 acc: 0.873334527015686\n",
      "step: 5050 time:0.00400233268737793\n",
      "train: loss: 0.24745164811611176 acc: 0.7565267086029053  val: loss: 2.025850534439087 acc: 0.8393855690956116\n",
      "step: 5055 time:0.0\n",
      "train: loss: 0.417905330657959 acc: 0.8029950857162476  val: loss: 5.003540515899658 acc: 0.8667140007019043\n",
      "step: 5060 time:0.0\n",
      "train: loss: 0.9003949165344238 acc: 0.7532757520675659  val: loss: 6.629369735717773 acc: 0.7953426837921143\n",
      "step: 5065 time:0.0\n",
      "train: loss: 0.6601251363754272 acc: 0.8304476141929626  val: loss: 2.3708531856536865 acc: 0.8614314794540405\n",
      "step: 5070 time:0.004002571105957031\n",
      "train: loss: 0.6385263800621033 acc: 0.6345183253288269  val: loss: 0.9049124717712402 acc: 0.8209857940673828\n",
      "step: 5075 time:0.003001689910888672\n",
      "train: loss: 0.5105372667312622 acc: 0.8630222678184509  val: loss: 5.147104263305664 acc: 0.875160276889801\n",
      "step: 5080 time:0.0030019283294677734\n",
      "train: loss: 0.6294108033180237 acc: 0.7363824248313904  val: loss: 2.5477347373962402 acc: 0.7532333135604858\n",
      "step: 5085 time:0.0\n",
      "train: loss: 0.5333655476570129 acc: 0.8119328022003174  val: loss: 7.809685230255127 acc: 0.8723382949829102\n",
      "step: 5090 time:0.003002166748046875\n",
      "train: loss: 1.4417072534561157 acc: 0.8806039094924927  val: loss: 5.236217498779297 acc: 0.883639395236969\n",
      "step: 5095 time:0.0030019283294677734\n",
      "train: loss: 1.0140036344528198 acc: 0.8640068769454956  val: loss: 4.820446968078613 acc: 0.8796964883804321\n",
      "step: 5100 time:0.003000974655151367\n",
      "train: loss: 1.714357614517212 acc: 0.6036590933799744  val: loss: 2.314466953277588 acc: 0.8668032288551331\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 5105 time:0.0030019283294677734\n",
      "train: loss: 1.9827649593353271 acc: 0.7146696448326111  val: loss: 5.901110649108887 acc: 0.7178232669830322\n",
      "step: 5110 time:0.003001689910888672\n",
      "train: loss: 1.6795002222061157 acc: 0.8734003305435181  val: loss: 3.209519386291504 acc: 0.7603835463523865\n",
      "step: 5115 time:0.003001689910888672\n",
      "train: loss: 0.8885923624038696 acc: 0.8176224231719971  val: loss: 17.14181137084961 acc: 0.8768038153648376\n",
      "step: 5120 time:0.004002809524536133\n",
      "train: loss: 0.700704038143158 acc: 0.879266619682312  val: loss: 5.057854652404785 acc: 0.8930972218513489\n",
      "step: 5125 time:0.0030019283294677734\n",
      "train: loss: 0.4341796934604645 acc: 0.8667711019515991  val: loss: 2.6925864219665527 acc: 0.8114432692527771\n",
      "step: 5130 time:0.004002094268798828\n",
      "train: loss: 0.6942383050918579 acc: 0.6802796125411987  val: loss: 0.8336515426635742 acc: 0.8807955980300903\n",
      "step: 5135 time:0.0030014514923095703\n",
      "train: loss: 0.22469604015350342 acc: 0.8473772406578064  val: loss: 2.818037986755371 acc: 0.8884453773498535\n",
      "step: 5140 time:0.015627145767211914\n",
      "train: loss: 0.7091524004936218 acc: 0.8122923970222473  val: loss: 9.772002220153809 acc: 0.8660200238227844\n",
      "step: 5145 time:0.0\n",
      "train: loss: 0.713565468788147 acc: 0.7817701697349548  val: loss: 11.38499641418457 acc: 0.8069792985916138\n",
      "step: 5150 time:0.0\n",
      "train: loss: 1.5440926551818848 acc: 0.8869986534118652  val: loss: 2.804744243621826 acc: 0.7846236228942871\n",
      "step: 5155 time:0.01603841781616211\n",
      "train: loss: 1.0212469100952148 acc: 0.8660642504692078  val: loss: 5.803796768188477 acc: 0.8464282155036926\n",
      "step: 5160 time:0.003002166748046875\n",
      "train: loss: 0.7515213489532471 acc: 0.8284591436386108  val: loss: 4.849323749542236 acc: 0.8063479065895081\n",
      "step: 5165 time:0.003002166748046875\n",
      "train: loss: 1.649793267250061 acc: 0.8627213835716248  val: loss: 3.51558518409729 acc: 0.8685989379882812\n",
      "step: 5170 time:0.016313552856445312\n",
      "train: loss: 2.13063383102417 acc: 0.8735440969467163  val: loss: 4.34573221206665 acc: 0.8445272445678711\n",
      "step: 5175 time:0.0030393600463867188\n",
      "train: loss: 2.2732198238372803 acc: 0.7279183864593506  val: loss: 5.500753402709961 acc: 0.8722371459007263\n",
      "step: 5180 time:0.0030019283294677734\n",
      "train: loss: 3.273744583129883 acc: 0.8663483262062073  val: loss: 14.887334823608398 acc: 0.8197422027587891\n",
      "step: 5185 time:0.0\n",
      "train: loss: 1.8316333293914795 acc: 0.8473707437515259  val: loss: 2.9801855087280273 acc: 0.8723260164260864\n",
      "step: 5190 time:0.0\n",
      "train: loss: 1.420247197151184 acc: 0.7981944680213928  val: loss: 1.993861436843872 acc: 0.8403836488723755\n",
      "step: 5195 time:0.0\n",
      "train: loss: 2.9055724143981934 acc: 0.8664320111274719  val: loss: 3.6219489574432373 acc: 0.8414843082427979\n",
      "step: 5200 time:0.0020008087158203125\n",
      "train: loss: 4.696283340454102 acc: 0.8029798865318298  val: loss: 1.633833646774292 acc: 0.8301125764846802\n",
      "step: 5205 time:0.0\n",
      "train: loss: 1.4351305961608887 acc: 0.8587372303009033  val: loss: 2.6348938941955566 acc: 0.8762946724891663\n",
      "step: 5210 time:0.0\n",
      "train: loss: 2.579103469848633 acc: 0.8802846670150757  val: loss: 0.677107036113739 acc: 0.9045241475105286\n",
      "step: 5215 time:0.017261505126953125\n",
      "train: loss: 1.8356075286865234 acc: 0.9168009757995605  val: loss: 2.9869565963745117 acc: 0.8773024678230286\n",
      "step: 5220 time:0.003002166748046875\n",
      "train: loss: 0.9290714263916016 acc: 0.7732123136520386  val: loss: 15.70091724395752 acc: 0.7339392900466919\n",
      "step: 5225 time:0.0\n",
      "train: loss: 7.653742790222168 acc: 0.6028550267219543  val: loss: 1.5073856115341187 acc: 0.8852830529212952\n",
      "step: 5230 time:0.0\n",
      "train: loss: 15.923274993896484 acc: 0.7587963342666626  val: loss: 3.608337879180908 acc: 0.7026118636131287\n",
      "step: 5235 time:0.0\n",
      "train: loss: 13.372303009033203 acc: 0.7388787269592285  val: loss: 4.100854396820068 acc: 0.7372326850891113\n",
      "step: 5240 time:0.0030019283294677734\n",
      "train: loss: 22.43920135498047 acc: 0.13265228271484375  val: loss: 4.724723815917969 acc: 0.7865556478500366\n",
      "step: 5245 time:0.0\n",
      "train: loss: 2.8005542755126953 acc: 0.8660739064216614  val: loss: 2.972254753112793 acc: 0.8583343625068665\n",
      "step: 5250 time:0.015625715255737305\n",
      "train: loss: 2.0048747062683105 acc: 0.8685663938522339  val: loss: 0.8149442672729492 acc: 0.8415877819061279\n",
      "step: 5255 time:0.003002166748046875\n",
      "train: loss: 2.6918296813964844 acc: 0.7244399785995483  val: loss: 3.8950281143188477 acc: 0.8141483068466187\n",
      "step: 5260 time:0.0030019283294677734\n",
      "train: loss: 8.635675430297852 acc: 0.8709832429885864  val: loss: 6.317708492279053 acc: 0.8632389903068542\n",
      "step: 5265 time:0.0\n",
      "train: loss: 13.030471801757812 acc: 0.5354822874069214  val: loss: 9.170005798339844 acc: 0.8194350004196167\n",
      "step: 5270 time:0.0\n",
      "train: loss: 17.059473037719727 acc: 0.6310303211212158  val: loss: 3.0239009857177734 acc: 0.820049524307251\n",
      "step: 5275 time:0.0\n",
      "train: loss: 6.704444408416748 acc: 0.869572103023529  val: loss: 3.8222718238830566 acc: 0.8639836311340332\n",
      "step: 5280 time:0.0030019283294677734\n",
      "train: loss: 14.142241477966309 acc: 0.7467551231384277  val: loss: 15.901325225830078 acc: 0.7670413255691528\n",
      "step: 5285 time:0.0\n",
      "train: loss: 9.143299102783203 acc: 0.8762333989143372  val: loss: 7.4587812423706055 acc: 0.8560129404067993\n",
      "step: 5290 time:0.0\n",
      "train: loss: 5.758373260498047 acc: 0.8286095261573792  val: loss: 4.495387077331543 acc: 0.8831818699836731\n",
      "step: 5295 time:0.0\n",
      "train: loss: 7.930850982666016 acc: 0.8929102420806885  val: loss: 1.619126796722412 acc: 0.88808274269104\n",
      "step: 5300 time:0.003002166748046875\n",
      "train: loss: 10.829139709472656 acc: 0.9014504551887512  val: loss: 16.826566696166992 acc: 0.6888895630836487\n",
      "step: 5305 time:0.0\n",
      "train: loss: 7.58715295791626 acc: 0.4002074599266052  val: loss: 5.667738914489746 acc: 0.8210605382919312\n",
      "step: 5310 time:0.0\n",
      "train: loss: 3.4666976928710938 acc: 0.7798137664794922  val: loss: 1.8479379415512085 acc: 0.8268883228302002\n",
      "step: 5315 time:0.0\n",
      "train: loss: 2.4975955486297607 acc: 0.8036144971847534  val: loss: 10.1443452835083 acc: 0.8757210969924927\n",
      "step: 5320 time:0.003002166748046875\n",
      "train: loss: 9.90298080444336 acc: 0.6609707474708557  val: loss: 1.6761155128479004 acc: 0.8641682863235474\n",
      "step: 5325 time:0.0\n",
      "train: loss: 4.140976428985596 acc: 0.779559314250946  val: loss: 2.1731998920440674 acc: 0.9042803049087524\n",
      "step: 5330 time:0.0\n",
      "train: loss: 5.342267990112305 acc: 0.649465799331665  val: loss: 6.502462863922119 acc: 0.7895098924636841\n",
      "step: 5335 time:0.0\n",
      "train: loss: 3.313270092010498 acc: 0.8629544377326965  val: loss: 3.090043544769287 acc: 0.8341776132583618\n",
      "step: 5340 time:0.003002166748046875\n",
      "train: loss: 1.5057551860809326 acc: 0.8370088934898376  val: loss: 2.0436882972717285 acc: 0.8564389944076538\n",
      "step: 5345 time:0.0\n",
      "train: loss: 2.621793270111084 acc: 0.8991028070449829  val: loss: 4.28140926361084 acc: 0.7909502983093262\n",
      "step: 5350 time:0.002001523971557617\n",
      "train: loss: 2.2004952430725098 acc: 0.8894428610801697  val: loss: 18.131826400756836 acc: 0.7566214203834534\n",
      "step: 5355 time:0.0\n",
      "train: loss: 0.599348247051239 acc: 0.9205079078674316  val: loss: 4.668811798095703 acc: 0.8792704343795776\n",
      "step: 5360 time:0.003002166748046875\n",
      "train: loss: 3.4310455322265625 acc: 0.7355161905288696  val: loss: 2.7542028427124023 acc: 0.8315832018852234\n",
      "step: 5365 time:0.0030012130737304688\n",
      "train: loss: 2.2144880294799805 acc: 0.8821934461593628  val: loss: 10.356744766235352 acc: 0.8174905776977539\n",
      "step: 5370 time:0.0030019283294677734\n",
      "train: loss: 2.3558597564697266 acc: 0.8175549507141113  val: loss: 3.741081953048706 acc: 0.8492567539215088\n",
      "step: 5375 time:0.0\n",
      "train: loss: 0.9208561778068542 acc: 0.8976920247077942  val: loss: 3.132645606994629 acc: 0.886445164680481\n",
      "step: 5380 time:0.0030019283294677734\n",
      "train: loss: 5.04001522064209 acc: 0.8446097373962402  val: loss: 3.7465641498565674 acc: 0.7819665670394897\n",
      "step: 5385 time:0.0\n",
      "train: loss: 2.7065629959106445 acc: 0.8035445213317871  val: loss: 5.831287384033203 acc: 0.8291770219802856\n",
      "step: 5390 time:0.0\n",
      "train: loss: 2.459156036376953 acc: 0.9171745181083679  val: loss: 2.7835679054260254 acc: 0.8023724555969238\n",
      "step: 5395 time:0.0\n",
      "train: loss: 1.249781847000122 acc: 0.8779957294464111  val: loss: 4.684185981750488 acc: 0.8349331617355347\n",
      "step: 5400 time:0.003001689910888672\n",
      "train: loss: 1.4007484912872314 acc: 0.8391121625900269  val: loss: 1.7042909860610962 acc: 0.844220757484436\n",
      "step: 5405 time:0.0\n",
      "train: loss: 3.8972575664520264 acc: 0.8084344863891602  val: loss: 2.4208974838256836 acc: 0.8470256328582764\n",
      "step: 5410 time:0.0\n",
      "train: loss: 1.6331665515899658 acc: 0.8656330108642578  val: loss: 6.577219009399414 acc: 0.744907021522522\n",
      "step: 5415 time:0.015625715255737305\n",
      "train: loss: 2.791943073272705 acc: 0.9169004559516907  val: loss: 3.194478988647461 acc: 0.846912145614624\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 5420 time:0.004003047943115234\n",
      "train: loss: 3.2202930450439453 acc: 0.7939950823783875  val: loss: 7.5091471672058105 acc: 0.7839232683181763\n",
      "step: 5425 time:0.0\n",
      "train: loss: 5.310196876525879 acc: 0.6794353723526001  val: loss: 3.843484878540039 acc: 0.802028238773346\n",
      "step: 5430 time:0.0\n",
      "train: loss: 8.287402153015137 acc: 0.7635355591773987  val: loss: 2.7438387870788574 acc: 0.7847279906272888\n",
      "step: 5435 time:0.0030019283294677734\n",
      "train: loss: 2.148449420928955 acc: 0.8515259027481079  val: loss: 3.3399319648742676 acc: 0.8838862180709839\n",
      "step: 5440 time:0.003376483917236328\n",
      "train: loss: 5.526897430419922 acc: 0.7947108149528503  val: loss: 1.439339518547058 acc: 0.7844004034996033\n",
      "step: 5445 time:0.0\n",
      "train: loss: 9.66659164428711 acc: 0.8230553865432739  val: loss: 2.4480040073394775 acc: 0.8956360220909119\n",
      "step: 5450 time:0.0030410289764404297\n",
      "train: loss: 6.207032203674316 acc: 0.594551682472229  val: loss: 9.588056564331055 acc: 0.8554953336715698\n",
      "step: 5455 time:0.0\n",
      "train: loss: 3.041992664337158 acc: 0.8271775245666504  val: loss: 2.3404994010925293 acc: 0.8896123170852661\n",
      "step: 5460 time:0.003001689910888672\n",
      "train: loss: 2.7896311283111572 acc: 0.7448990941047668  val: loss: 6.829848289489746 acc: 0.8574866056442261\n",
      "step: 5465 time:0.0\n",
      "train: loss: 7.492141246795654 acc: 0.8460801243782043  val: loss: 5.699522018432617 acc: 0.8856386542320251\n",
      "step: 5470 time:0.0030012130737304688\n",
      "train: loss: 4.1173624992370605 acc: 0.7940791845321655  val: loss: 15.76024055480957 acc: 0.7527422904968262\n",
      "step: 5475 time:0.0\n",
      "train: loss: 5.688023090362549 acc: 0.7789716720581055  val: loss: 2.5892510414123535 acc: 0.8739842176437378\n",
      "step: 5480 time:0.0030019283294677734\n",
      "train: loss: 2.122122049331665 acc: 0.7846004962921143  val: loss: 4.7269744873046875 acc: 0.8424094319343567\n",
      "step: 5485 time:0.017081499099731445\n",
      "train: loss: 2.60872483253479 acc: 0.8966108560562134  val: loss: 3.752173662185669 acc: 0.7607675790786743\n",
      "step: 5490 time:0.0\n",
      "train: loss: 2.815760612487793 acc: 0.8887258768081665  val: loss: 8.207734107971191 acc: 0.8198807239532471\n",
      "step: 5495 time:0.0\n",
      "train: loss: 1.3813271522521973 acc: 0.8961235284805298  val: loss: 2.927281141281128 acc: 0.8473660945892334\n",
      "step: 5500 time:0.003001689910888672\n",
      "train: loss: 0.8981022834777832 acc: 0.8746347427368164  val: loss: 1.6546670198440552 acc: 0.8296744227409363\n",
      "step: 5505 time:0.0\n",
      "train: loss: 5.165802001953125 acc: 0.8746305704116821  val: loss: 6.412813186645508 acc: 0.782189667224884\n",
      "step: 5510 time:0.0\n",
      "train: loss: 2.8452181816101074 acc: 0.8542830348014832  val: loss: 3.1636815071105957 acc: 0.8619939088821411\n",
      "step: 5515 time:0.003001689910888672\n",
      "train: loss: 0.9979610443115234 acc: 0.8357150554656982  val: loss: 4.232110977172852 acc: 0.8735166788101196\n",
      "step: 5520 time:0.0030019283294677734\n",
      "train: loss: 0.9698671102523804 acc: 0.7401190400123596  val: loss: 5.162806034088135 acc: 0.8672776222229004\n",
      "step: 5525 time:0.0030019283294677734\n",
      "train: loss: 1.1910572052001953 acc: 0.8517764806747437  val: loss: 3.1356983184814453 acc: 0.8486216068267822\n",
      "step: 5530 time:0.0\n",
      "train: loss: 3.6931118965148926 acc: 0.807340681552887  val: loss: 3.3123345375061035 acc: 0.8135949373245239\n",
      "step: 5535 time:0.0\n",
      "train: loss: 0.7457256317138672 acc: 0.9029396772384644  val: loss: 5.35667610168457 acc: 0.8498780131340027\n",
      "step: 5540 time:0.0030014514923095703\n",
      "train: loss: 1.048231601715088 acc: 0.807660698890686  val: loss: 13.565073013305664 acc: 0.7769891023635864\n",
      "step: 5545 time:0.0\n",
      "train: loss: 1.0451602935791016 acc: 0.7749102711677551  val: loss: 1.7978392839431763 acc: 0.8954198360443115\n",
      "step: 5550 time:0.0030024051666259766\n",
      "train: loss: 0.21179413795471191 acc: 0.8828744888305664  val: loss: 1.2934190034866333 acc: 0.9086013436317444\n",
      "step: 5555 time:0.0\n",
      "train: loss: 1.155221939086914 acc: 0.8172717094421387  val: loss: 12.017641067504883 acc: 0.8316038846969604\n",
      "step: 5560 time:0.0\n",
      "train: loss: 0.5187864303588867 acc: 0.8260826468467712  val: loss: 6.694632530212402 acc: 0.8757379055023193\n",
      "step: 5565 time:0.0\n",
      "train: loss: 0.4007618725299835 acc: 0.8311927318572998  val: loss: 3.7818453311920166 acc: 0.8666573166847229\n",
      "step: 5570 time:0.003001689910888672\n",
      "train: loss: 1.3795275688171387 acc: 0.8108971118927002  val: loss: 13.659181594848633 acc: 0.7472842931747437\n",
      "step: 5575 time:0.0\n",
      "train: loss: 1.0280191898345947 acc: 0.7833640575408936  val: loss: 6.651727676391602 acc: 0.8635656833648682\n",
      "step: 5580 time:0.0\n",
      "train: loss: 0.384358674287796 acc: 0.7939706444740295  val: loss: 3.0836148262023926 acc: 0.7368130087852478\n",
      "step: 5585 time:0.015625715255737305\n",
      "train: loss: 1.0674192905426025 acc: 0.8883886337280273  val: loss: 9.068002700805664 acc: 0.8945466876029968\n",
      "step: 5590 time:0.003000497817993164\n",
      "train: loss: 0.34282368421554565 acc: 0.7261877059936523  val: loss: 7.674427032470703 acc: 0.8816320300102234\n",
      "step: 5595 time:0.015626192092895508\n",
      "train: loss: 0.6896487474441528 acc: 0.7982214689254761  val: loss: 2.446118116378784 acc: 0.8808151483535767\n",
      "step: 5600 time:0.0\n",
      "train: loss: 0.8109354972839355 acc: 0.7822164297103882  val: loss: 1.565190076828003 acc: 0.8001831769943237\n",
      "step: 5605 time:0.0\n",
      "train: loss: 1.5251009464263916 acc: 0.8074679374694824  val: loss: 1.883249282836914 acc: 0.822091817855835\n",
      "step: 5610 time:0.0030012130737304688\n",
      "train: loss: 0.9103087186813354 acc: 0.8242993354797363  val: loss: 4.315338611602783 acc: 0.831305980682373\n",
      "step: 5615 time:0.003002166748046875\n",
      "train: loss: 1.1242491006851196 acc: 0.7515975832939148  val: loss: 7.751091003417969 acc: 0.7646426558494568\n",
      "step: 5620 time:0.003001689910888672\n",
      "train: loss: 1.2988005876541138 acc: 0.834021270275116  val: loss: 6.699794292449951 acc: 0.8701295852661133\n",
      "step: 5625 time:0.003002166748046875\n",
      "train: loss: 0.9479102492332458 acc: 0.881317675113678  val: loss: 10.157306671142578 acc: 0.8560280799865723\n",
      "step: 5630 time:0.004003286361694336\n",
      "train: loss: 0.6663917303085327 acc: 0.8321788311004639  val: loss: 1.8678898811340332 acc: 0.8344070911407471\n",
      "step: 5635 time:0.0\n",
      "train: loss: 0.9837688207626343 acc: 0.8760762214660645  val: loss: 2.476848602294922 acc: 0.8758862018585205\n",
      "step: 5640 time:0.0030014514923095703\n",
      "train: loss: 0.45023971796035767 acc: 0.8226524591445923  val: loss: 9.76217269897461 acc: 0.8484104871749878\n",
      "step: 5645 time:0.015625715255737305\n",
      "train: loss: 0.8187686204910278 acc: 0.8423342108726501  val: loss: 2.1647050380706787 acc: 0.899770975112915\n",
      "step: 5650 time:0.002001523971557617\n",
      "train: loss: 0.45904165506362915 acc: 0.7920251488685608  val: loss: 2.064085006713867 acc: 0.8421415686607361\n",
      "step: 5655 time:0.015625715255737305\n",
      "train: loss: 0.30658745765686035 acc: 0.8669321537017822  val: loss: 1.7018541097640991 acc: 0.8552777767181396\n",
      "step: 5660 time:0.0030019283294677734\n",
      "train: loss: 0.46749553084373474 acc: 0.695890486240387  val: loss: 6.012892723083496 acc: 0.8938765525817871\n",
      "step: 5665 time:0.0\n",
      "train: loss: 1.1622583866119385 acc: 0.8734661340713501  val: loss: 3.234227180480957 acc: 0.8475416898727417\n",
      "step: 5670 time:0.0030019283294677734\n",
      "train: loss: 2.1728620529174805 acc: 0.8904462456703186  val: loss: 1.7143141031265259 acc: 0.8128111958503723\n",
      "step: 5675 time:0.003002166748046875\n",
      "train: loss: 0.9198139905929565 acc: 0.8978258967399597  val: loss: 5.414388656616211 acc: 0.746071457862854\n",
      "step: 5680 time:0.0\n",
      "train: loss: 1.5709854364395142 acc: 0.867470383644104  val: loss: 1.9583916664123535 acc: 0.8947093486785889\n",
      "step: 5685 time:0.015625476837158203\n",
      "train: loss: 1.4587383270263672 acc: 0.8495786190032959  val: loss: 7.389250755310059 acc: 0.8530577421188354\n",
      "step: 5690 time:0.003001689910888672\n",
      "train: loss: 1.0526453256607056 acc: 0.8727365136146545  val: loss: 3.0689339637756348 acc: 0.7519205808639526\n",
      "step: 5695 time:0.003001689910888672\n",
      "train: loss: 2.3977413177490234 acc: 0.862872838973999  val: loss: 1.033063530921936 acc: 0.7784218788146973\n",
      "step: 5700 time:0.0171658992767334\n",
      "train: loss: 1.7115427255630493 acc: 0.9024045467376709  val: loss: 5.71061897277832 acc: 0.8364636301994324\n",
      "step: 5705 time:0.0\n",
      "train: loss: 1.3316566944122314 acc: 0.7850883603096008  val: loss: 1.184226155281067 acc: 0.8777207732200623\n",
      "step: 5710 time:0.003002166748046875\n",
      "train: loss: 1.4238972663879395 acc: 0.8203978538513184  val: loss: 2.8990705013275146 acc: 0.8751420378684998\n",
      "step: 5715 time:0.0030019283294677734\n",
      "train: loss: 4.711474418640137 acc: 0.8629857301712036  val: loss: 10.084283828735352 acc: 0.8427641987800598\n",
      "step: 5720 time:0.0030014514923095703\n",
      "train: loss: 2.2728424072265625 acc: 0.8370011448860168  val: loss: 4.036333084106445 acc: 0.8651217818260193\n",
      "step: 5725 time:0.0\n",
      "train: loss: 1.7807625532150269 acc: 0.9050050377845764  val: loss: 1.0636814832687378 acc: 0.910538375377655\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 5730 time:0.0030019283294677734\n",
      "train: loss: 1.9540565013885498 acc: 0.8033816814422607  val: loss: 7.702849864959717 acc: 0.8739728927612305\n",
      "step: 5735 time:0.0\n",
      "train: loss: 0.9279539585113525 acc: 0.8573660850524902  val: loss: 9.144964218139648 acc: 0.8910244703292847\n",
      "step: 5740 time:0.0030019283294677734\n",
      "train: loss: 1.6061995029449463 acc: 0.8337517380714417  val: loss: 4.834936618804932 acc: 0.8615332245826721\n",
      "step: 5745 time:0.0\n",
      "train: loss: 20.66670799255371 acc: 0.7749956846237183  val: loss: 5.211536884307861 acc: 0.8275482654571533\n",
      "step: 5750 time:0.003002166748046875\n",
      "train: loss: 8.340009689331055 acc: 0.8253318071365356  val: loss: 9.018659591674805 acc: 0.8300656080245972\n",
      "step: 5755 time:0.0\n",
      "train: loss: 21.42342185974121 acc: 0.6144905686378479  val: loss: 2.9796576499938965 acc: 0.7289837002754211\n",
      "step: 5760 time:0.0\n",
      "train: loss: 2.5569405555725098 acc: 0.7943424582481384  val: loss: 4.387338638305664 acc: 0.8162240982055664\n",
      "step: 5765 time:0.017839431762695312\n",
      "train: loss: 4.6370344161987305 acc: 0.7796752452850342  val: loss: 7.312445640563965 acc: 0.8667324185371399\n",
      "step: 5770 time:0.003001689910888672\n",
      "train: loss: 4.017875671386719 acc: 0.7570717930793762  val: loss: 3.2922303676605225 acc: 0.8750733733177185\n",
      "step: 5775 time:0.015625953674316406\n",
      "train: loss: 5.704198360443115 acc: 0.8072492480278015  val: loss: 4.521775245666504 acc: 0.8234457969665527\n",
      "step: 5780 time:0.0156252384185791\n",
      "train: loss: 21.684606552124023 acc: 0.3979092836380005  val: loss: 2.7443323135375977 acc: 0.8830382823944092\n",
      "step: 5785 time:0.0\n",
      "train: loss: 13.82463550567627 acc: 0.8254801034927368  val: loss: 18.708080291748047 acc: 0.7820209264755249\n",
      "step: 5790 time:0.003001689910888672\n",
      "train: loss: 6.59959077835083 acc: 0.8630582690238953  val: loss: 3.417264461517334 acc: 0.8451920747756958\n",
      "step: 5795 time:0.0\n",
      "train: loss: 4.913867950439453 acc: 0.8361403346061707  val: loss: 4.942109107971191 acc: 0.9070586562156677\n",
      "step: 5800 time:0.0030019283294677734\n",
      "train: loss: 5.220938205718994 acc: 0.8783959746360779  val: loss: 16.531869888305664 acc: 0.8087217211723328\n",
      "step: 5805 time:0.0021283626556396484\n",
      "train: loss: 11.11671257019043 acc: 0.8778614401817322  val: loss: 4.9173150062561035 acc: 0.8850108981132507\n",
      "step: 5810 time:0.003001689910888672\n",
      "train: loss: 4.612587928771973 acc: 0.9145076274871826  val: loss: 3.735675573348999 acc: 0.8725664019584656\n",
      "step: 5815 time:0.0030007362365722656\n",
      "train: loss: 4.255702018737793 acc: 0.8955173492431641  val: loss: 5.744687557220459 acc: 0.8553277254104614\n",
      "step: 5820 time:0.0\n",
      "train: loss: 1.9685921669006348 acc: 0.838746190071106  val: loss: 2.074753761291504 acc: 0.9061641693115234\n",
      "step: 5825 time:0.0\n",
      "train: loss: 3.9917798042297363 acc: 0.8657739162445068  val: loss: 2.3509321212768555 acc: 0.9164708256721497\n",
      "step: 5830 time:0.0030019283294677734\n",
      "train: loss: 1.7364758253097534 acc: 0.8510109782218933  val: loss: 1.646283745765686 acc: 0.8493499159812927\n",
      "step: 5835 time:0.0030019283294677734\n",
      "train: loss: 7.150002479553223 acc: 0.6458543539047241  val: loss: 4.2852091789245605 acc: 0.8415505886077881\n",
      "step: 5840 time:0.0\n",
      "train: loss: 3.9560294151306152 acc: 0.7825151681900024  val: loss: 4.462527751922607 acc: 0.8387084007263184\n",
      "step: 5845 time:0.0\n",
      "train: loss: 4.16818380355835 acc: 0.5822758674621582  val: loss: 1.4472535848617554 acc: 0.7493703365325928\n",
      "step: 5850 time:0.0030014514923095703\n",
      "train: loss: 1.394362449645996 acc: 0.8018514513969421  val: loss: 1.1354124546051025 acc: 0.8671188950538635\n",
      "step: 5855 time:0.0\n",
      "train: loss: 1.4412074089050293 acc: 0.8562933802604675  val: loss: 0.8463424444198608 acc: 0.8268535733222961\n",
      "step: 5860 time:0.0\n",
      "train: loss: 1.427621603012085 acc: 0.8787977695465088  val: loss: 3.0421509742736816 acc: 0.8130438327789307\n",
      "step: 5865 time:0.015625715255737305\n",
      "train: loss: 1.5705307722091675 acc: 0.8706848621368408  val: loss: 3.4118881225585938 acc: 0.8231852054595947\n",
      "step: 5870 time:0.0030012130737304688\n",
      "train: loss: 2.847360849380493 acc: 0.8371644616127014  val: loss: 18.270376205444336 acc: 0.7494578957557678\n",
      "step: 5875 time:0.0\n",
      "train: loss: 1.308060884475708 acc: 0.9339123964309692  val: loss: 20.86342430114746 acc: 0.6329131126403809\n",
      "step: 5880 time:0.015625953674316406\n",
      "train: loss: 1.1513944864273071 acc: 0.8651177883148193  val: loss: 3.8895955085754395 acc: 0.8751136064529419\n",
      "step: 5885 time:0.000274658203125\n",
      "train: loss: 2.4897584915161133 acc: 0.9060803055763245  val: loss: 1.9513028860092163 acc: 0.912329375743866\n",
      "step: 5890 time:0.0030019283294677734\n",
      "train: loss: 4.670074462890625 acc: 0.9043915271759033  val: loss: 3.052427053451538 acc: 0.6896275877952576\n",
      "step: 5895 time:0.004002571105957031\n",
      "train: loss: 9.605342864990234 acc: 0.8012282848358154  val: loss: 3.9812726974487305 acc: 0.752480685710907\n",
      "step: 5900 time:0.0\n",
      "train: loss: 3.323733329772949 acc: 0.8694711327552795  val: loss: 9.270391464233398 acc: 0.6272131204605103\n",
      "step: 5905 time:0.0030019283294677734\n",
      "train: loss: 1.3385623693466187 acc: 0.8735443353652954  val: loss: 3.6277637481689453 acc: 0.8228119611740112\n",
      "step: 5910 time:0.0030024051666259766\n",
      "train: loss: 2.285696029663086 acc: 0.7830158472061157  val: loss: 1.1966361999511719 acc: 0.8498044013977051\n",
      "step: 5915 time:0.0\n",
      "train: loss: 1.0369340181350708 acc: 0.9037593603134155  val: loss: 1.4485760927200317 acc: 0.760442852973938\n",
      "step: 5920 time:0.0030019283294677734\n",
      "train: loss: 1.2193865776062012 acc: 0.7350784540176392  val: loss: 1.7804207801818848 acc: 0.8661567568778992\n",
      "step: 5925 time:0.0030019283294677734\n",
      "train: loss: 0.7002182006835938 acc: 0.8535029888153076  val: loss: 3.9748401641845703 acc: 0.8207757472991943\n",
      "step: 5930 time:0.003002166748046875\n",
      "train: loss: 2.277434825897217 acc: 0.6219586133956909  val: loss: 1.8193907737731934 acc: 0.802044689655304\n",
      "step: 5935 time:0.0\n",
      "train: loss: 2.3960819244384766 acc: 0.7781630158424377  val: loss: 7.922898292541504 acc: 0.8751373887062073\n",
      "step: 5940 time:0.0\n",
      "train: loss: 1.403800129890442 acc: 0.8370311260223389  val: loss: 0.9046793580055237 acc: 0.8767676949501038\n",
      "step: 5945 time:0.015625476837158203\n",
      "train: loss: 10.865730285644531 acc: 0.566319465637207  val: loss: 2.4028210639953613 acc: 0.8521716594696045\n",
      "step: 5950 time:0.0030014514923095703\n",
      "train: loss: 8.102560043334961 acc: 0.7988930940628052  val: loss: 3.195061206817627 acc: 0.8652023673057556\n",
      "step: 5955 time:0.004002809524536133\n",
      "train: loss: 3.5174617767333984 acc: 0.7606175541877747  val: loss: 3.098254680633545 acc: 0.8070041537284851\n",
      "step: 5960 time:0.0\n",
      "train: loss: 4.934551239013672 acc: 0.8105838298797607  val: loss: 1.032518744468689 acc: 0.8122128248214722\n",
      "step: 5965 time:0.015625715255737305\n",
      "train: loss: 3.0590131282806396 acc: 0.8160860538482666  val: loss: 1.1062313318252563 acc: 0.8475909233093262\n",
      "step: 5970 time:0.003002166748046875\n",
      "train: loss: 5.731653213500977 acc: 0.7040098905563354  val: loss: 4.560144901275635 acc: 0.7809650897979736\n",
      "step: 5975 time:0.0030024051666259766\n",
      "train: loss: 3.993903875350952 acc: 0.8208681344985962  val: loss: 3.2722370624542236 acc: 0.8091739416122437\n",
      "step: 5980 time:0.003001689910888672\n",
      "train: loss: 5.45840311050415 acc: 0.8195966482162476  val: loss: 14.656244277954102 acc: 0.7659623026847839\n",
      "step: 5985 time:0.0\n",
      "train: loss: 2.8451638221740723 acc: 0.888738751411438  val: loss: 2.4200401306152344 acc: 0.7854886651039124\n",
      "step: 5990 time:0.003001689910888672\n",
      "train: loss: 0.8615195751190186 acc: 0.9001655578613281  val: loss: 4.75416898727417 acc: 0.924182653427124\n",
      "step: 5995 time:0.0\n",
      "train: loss: 9.149356842041016 acc: 0.783894419670105  val: loss: 3.4206666946411133 acc: 0.8744673132896423\n",
      "step: 6000 time:0.015625953674316406\n",
      "train: loss: 2.636695623397827 acc: 0.9037825465202332  val: loss: 3.1436946392059326 acc: 0.8684378862380981\n",
      "step: 6005 time:0.003001689910888672\n",
      "train: loss: 5.590346336364746 acc: 0.8333656191825867  val: loss: 5.336975574493408 acc: 0.8452643156051636\n",
      "step: 6010 time:0.0030014514923095703\n",
      "train: loss: 1.3881254196166992 acc: 0.8224062323570251  val: loss: 4.4902544021606445 acc: 0.8652012944221497\n",
      "step: 6015 time:0.0\n",
      "train: loss: 2.1206235885620117 acc: 0.8833903670310974  val: loss: 2.6740915775299072 acc: 0.8768630623817444\n",
      "step: 6020 time:0.0030007362365722656\n",
      "train: loss: 3.638176679611206 acc: 0.7392711043357849  val: loss: 2.0601401329040527 acc: 0.8656736016273499\n",
      "step: 6025 time:0.0\n",
      "train: loss: 2.0413591861724854 acc: 0.9050899744033813  val: loss: 2.444027900695801 acc: 0.8690434694290161\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 6030 time:0.003001689910888672\n",
      "train: loss: 2.74855637550354 acc: 0.9309409856796265  val: loss: 0.9675654172897339 acc: 0.8657562136650085\n",
      "step: 6035 time:0.0\n",
      "train: loss: 3.12113094329834 acc: 0.8591587543487549  val: loss: 1.319014072418213 acc: 0.7975948452949524\n",
      "step: 6040 time:0.0\n",
      "train: loss: 1.063279151916504 acc: 0.8413925170898438  val: loss: 13.962606430053711 acc: 0.8312127590179443\n",
      "step: 6045 time:0.0\n",
      "train: loss: 1.5218660831451416 acc: 0.908972978591919  val: loss: 1.3134180307388306 acc: 0.8984273076057434\n",
      "step: 6050 time:0.0030019283294677734\n",
      "train: loss: 0.21887750923633575 acc: 0.8881410360336304  val: loss: 10.388360023498535 acc: 0.8713456988334656\n",
      "step: 6055 time:0.0\n",
      "train: loss: 0.5978325009346008 acc: 0.8111124038696289  val: loss: 3.9164042472839355 acc: 0.8575104475021362\n",
      "step: 6060 time:0.0030591487884521484\n",
      "train: loss: 0.6037406921386719 acc: 0.8248255252838135  val: loss: 1.262702226638794 acc: 0.8843075633049011\n",
      "step: 6065 time:0.0\n",
      "train: loss: 0.9418140053749084 acc: 0.727520763874054  val: loss: 4.685590744018555 acc: 0.8243300914764404\n",
      "step: 6070 time:0.003001689910888672\n",
      "train: loss: 0.7220715880393982 acc: 0.8739074468612671  val: loss: 2.4168808460235596 acc: 0.8820721507072449\n",
      "step: 6075 time:0.015625715255737305\n",
      "train: loss: 0.8199482560157776 acc: 0.7655186057090759  val: loss: 5.8960466384887695 acc: 0.9130246639251709\n",
      "step: 6080 time:0.0\n",
      "train: loss: 0.6957712173461914 acc: 0.8303838968276978  val: loss: 3.0625224113464355 acc: 0.8850302696228027\n",
      "step: 6085 time:0.0\n",
      "train: loss: 0.47040876746177673 acc: 0.7695318460464478  val: loss: 5.826164245605469 acc: 0.8391645550727844\n",
      "step: 6090 time:0.0030019283294677734\n",
      "train: loss: 0.4864974021911621 acc: 0.8369335532188416  val: loss: 3.979144811630249 acc: 0.8484246730804443\n",
      "step: 6095 time:0.0\n",
      "train: loss: 2.234917402267456 acc: 0.8602027893066406  val: loss: 7.401164531707764 acc: 0.842595636844635\n",
      "step: 6100 time:0.0\n",
      "train: loss: 0.5447386503219604 acc: 0.6019547581672668  val: loss: 4.688835620880127 acc: 0.8461732268333435\n",
      "step: 6105 time:0.0030019283294677734\n",
      "train: loss: 0.884790301322937 acc: 0.8501560091972351  val: loss: 3.983750581741333 acc: 0.8317205905914307\n",
      "step: 6110 time:0.0030014514923095703\n",
      "train: loss: 0.34130266308784485 acc: 0.7875484228134155  val: loss: 5.775041103363037 acc: 0.8535427451133728\n",
      "step: 6115 time:0.0\n",
      "train: loss: 0.3638972043991089 acc: 0.7808645367622375  val: loss: 5.046856880187988 acc: 0.8362690210342407\n",
      "step: 6120 time:0.015626192092895508\n",
      "train: loss: 0.6258774995803833 acc: 0.7702627182006836  val: loss: 1.2367525100708008 acc: 0.8731468915939331\n",
      "step: 6125 time:0.0\n",
      "train: loss: 1.0997908115386963 acc: 0.8197965621948242  val: loss: 2.2366678714752197 acc: 0.8868936896324158\n",
      "step: 6130 time:0.0030024051666259766\n",
      "train: loss: 0.6328415870666504 acc: 0.6069948077201843  val: loss: 7.248026371002197 acc: 0.8703206777572632\n",
      "step: 6135 time:0.0\n",
      "train: loss: 1.3191659450531006 acc: 0.8751658797264099  val: loss: 8.417016983032227 acc: 0.8893781900405884\n",
      "step: 6140 time:0.0\n",
      "train: loss: 1.3431992530822754 acc: 0.7990090250968933  val: loss: 4.786992073059082 acc: 0.8442714214324951\n",
      "step: 6145 time:0.015625953674316406\n",
      "train: loss: 1.0757523775100708 acc: 0.7044463753700256  val: loss: 3.579881191253662 acc: 0.8801448345184326\n",
      "step: 6150 time:0.0030014514923095703\n",
      "train: loss: 1.3468865156173706 acc: 0.7875051498413086  val: loss: 3.4451823234558105 acc: 0.8549685478210449\n",
      "step: 6155 time:0.0\n",
      "train: loss: 0.9214312434196472 acc: 0.8506319522857666  val: loss: 13.522390365600586 acc: 0.6923874616622925\n",
      "step: 6160 time:0.0\n",
      "train: loss: 0.431050181388855 acc: 0.838425874710083  val: loss: 21.04238510131836 acc: 0.8237846493721008\n",
      "step: 6165 time:0.0156252384185791\n",
      "train: loss: 0.21584481000900269 acc: 0.8144588470458984  val: loss: 15.647215843200684 acc: 0.8403338193893433\n",
      "step: 6170 time:0.003002643585205078\n",
      "train: loss: 1.699924111366272 acc: 0.7592813372612  val: loss: 3.493549108505249 acc: 0.6889726519584656\n",
      "step: 6175 time:0.0\n",
      "train: loss: 0.7782590389251709 acc: 0.8652864098548889  val: loss: 1.187117576599121 acc: 0.8302611112594604\n",
      "step: 6180 time:0.0030014514923095703\n",
      "train: loss: 0.6085781455039978 acc: 0.8023802638053894  val: loss: 5.673117637634277 acc: 0.8788734674453735\n",
      "step: 6185 time:0.0030014514923095703\n",
      "train: loss: 1.2814228534698486 acc: 0.8571967482566833  val: loss: 22.374385833740234 acc: 0.7991616725921631\n",
      "step: 6190 time:0.004002571105957031\n",
      "train: loss: 0.9980659484863281 acc: 0.872117817401886  val: loss: 2.5839102268218994 acc: 0.8629683256149292\n",
      "step: 6195 time:0.0030019283294677734\n",
      "train: loss: 2.3951122760772705 acc: 0.7127503156661987  val: loss: 1.2341148853302002 acc: 0.833164632320404\n",
      "step: 6200 time:0.0\n",
      "train: loss: 0.6643712520599365 acc: 0.7826175093650818  val: loss: 2.213073253631592 acc: 0.7936059236526489\n",
      "step: 6205 time:0.0\n",
      "train: loss: 1.113279938697815 acc: 0.8865946531295776  val: loss: 4.408045768737793 acc: 0.8747332692146301\n",
      "step: 6210 time:0.0030019283294677734\n",
      "train: loss: 0.9215401411056519 acc: 0.8643245697021484  val: loss: 4.413510322570801 acc: 0.8437604904174805\n",
      "step: 6215 time:0.0\n",
      "train: loss: 1.318385362625122 acc: 0.8906651139259338  val: loss: 9.151628494262695 acc: 0.9074515700340271\n",
      "step: 6220 time:0.0\n",
      "train: loss: 2.6669983863830566 acc: 0.7675157785415649  val: loss: 7.926252365112305 acc: 0.8619251847267151\n",
      "step: 6225 time:0.015625715255737305\n",
      "train: loss: 0.7900059819221497 acc: 0.730420708656311  val: loss: 2.641700267791748 acc: 0.7782164812088013\n",
      "step: 6230 time:0.0030024051666259766\n",
      "train: loss: 2.541489601135254 acc: 0.8000873923301697  val: loss: 1.628735065460205 acc: 0.8697313070297241\n",
      "step: 6235 time:0.015625476837158203\n",
      "train: loss: 1.95262610912323 acc: 0.8819366693496704  val: loss: 5.6201629638671875 acc: 0.8980119824409485\n",
      "step: 6240 time:0.0\n",
      "train: loss: 2.485219955444336 acc: 0.7946391105651855  val: loss: 10.883970260620117 acc: 0.8082393407821655\n",
      "step: 6245 time:0.0\n",
      "train: loss: 2.5735764503479004 acc: 0.7758904695510864  val: loss: 2.2878923416137695 acc: 0.8886407613754272\n",
      "step: 6250 time:0.003002166748046875\n",
      "train: loss: 3.084425687789917 acc: 0.943168580532074  val: loss: 16.75211524963379 acc: 0.7784000635147095\n",
      "step: 6255 time:0.0\n",
      "train: loss: 2.5750231742858887 acc: 0.8931766152381897  val: loss: 8.02219009399414 acc: 0.8750108480453491\n",
      "step: 6260 time:0.003002166748046875\n",
      "train: loss: 4.443009853363037 acc: 0.8591766953468323  val: loss: 3.7575793266296387 acc: 0.8732486367225647\n",
      "step: 6265 time:0.0\n",
      "train: loss: 8.052848815917969 acc: 0.7204904556274414  val: loss: 1.5193939208984375 acc: 0.7091538906097412\n",
      "step: 6270 time:0.0030019283294677734\n",
      "train: loss: 2.2907495498657227 acc: 0.8377671837806702  val: loss: 2.5863351821899414 acc: 0.8548916578292847\n",
      "step: 6275 time:0.0\n",
      "train: loss: 23.561830520629883 acc: 0.10572987794876099  val: loss: 2.332913398742676 acc: 0.7170535326004028\n",
      "step: 6280 time:0.0\n",
      "train: loss: 13.719867706298828 acc: 0.5664063692092896  val: loss: 7.6276702880859375 acc: 0.7630681991577148\n",
      "step: 6285 time:0.0\n",
      "train: loss: 8.719890594482422 acc: 0.41865479946136475  val: loss: 1.3895103931427002 acc: 0.8814993500709534\n",
      "step: 6290 time:0.0030019283294677734\n",
      "train: loss: 12.346148490905762 acc: 0.7937694787979126  val: loss: 2.546203136444092 acc: 0.9109160304069519\n",
      "step: 6295 time:0.0030019283294677734\n",
      "train: loss: 5.874850273132324 acc: 0.6149392127990723  val: loss: 3.774348735809326 acc: 0.8168375492095947\n",
      "step: 6300 time:0.0\n",
      "train: loss: 17.47024154663086 acc: 0.6942190527915955  val: loss: 2.689941167831421 acc: 0.8605968952178955\n",
      "step: 6305 time:0.0156252384185791\n",
      "train: loss: 6.655883312225342 acc: 0.8547216653823853  val: loss: 1.9761534929275513 acc: 0.8488914370536804\n",
      "step: 6310 time:0.005003213882446289\n",
      "train: loss: 17.330970764160156 acc: 0.8900561332702637  val: loss: 2.249680519104004 acc: 0.8258533477783203\n",
      "step: 6315 time:0.0\n",
      "train: loss: 10.199901580810547 acc: 0.6505173444747925  val: loss: 20.35940170288086 acc: 0.7708602547645569\n",
      "step: 6320 time:0.0\n",
      "train: loss: 5.618069648742676 acc: 0.8194143772125244  val: loss: 2.7877087593078613 acc: 0.8927330374717712\n",
      "step: 6325 time:0.0024645328521728516\n",
      "train: loss: 6.492565155029297 acc: 0.8545786738395691  val: loss: 3.2017197608947754 acc: 0.838936448097229\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 6330 time:0.003002166748046875\n",
      "train: loss: 10.488849639892578 acc: 0.7416178584098816  val: loss: 15.314615249633789 acc: 0.8386123776435852\n",
      "step: 6335 time:0.0\n",
      "train: loss: 3.183790445327759 acc: 0.904193639755249  val: loss: 2.4897921085357666 acc: 0.8235719203948975\n",
      "step: 6340 time:0.0030019283294677734\n",
      "train: loss: 3.4508895874023438 acc: 0.6448825597763062  val: loss: 1.8419445753097534 acc: 0.8472236394882202\n",
      "step: 6345 time:0.0030019283294677734\n",
      "train: loss: 4.170978546142578 acc: 0.9168014526367188  val: loss: 1.0668593645095825 acc: 0.8839762806892395\n",
      "step: 6350 time:0.0030019283294677734\n",
      "train: loss: 6.964713096618652 acc: 0.8132915496826172  val: loss: 3.0591936111450195 acc: 0.9198273420333862\n",
      "step: 6355 time:0.0030019283294677734\n",
      "train: loss: 2.6954407691955566 acc: 0.9051278233528137  val: loss: 1.677733063697815 acc: 0.7965297102928162\n",
      "step: 6360 time:0.0030019283294677734\n",
      "train: loss: 3.8978171348571777 acc: 0.7191277742385864  val: loss: 2.2271578311920166 acc: 0.8376350402832031\n",
      "step: 6365 time:0.0\n",
      "train: loss: 1.5844916105270386 acc: 0.8984046578407288  val: loss: 1.232907772064209 acc: 0.8971505165100098\n",
      "step: 6370 time:0.0030012130737304688\n",
      "train: loss: 3.2950820922851562 acc: 0.9094693660736084  val: loss: 6.0608391761779785 acc: 0.8563019037246704\n",
      "step: 6375 time:0.0\n",
      "train: loss: 7.565816402435303 acc: 0.7410286068916321  val: loss: 3.461165428161621 acc: 0.763569712638855\n",
      "step: 6380 time:0.0\n",
      "train: loss: 1.2159905433654785 acc: 0.8793525695800781  val: loss: 4.240718364715576 acc: 0.7150521874427795\n",
      "step: 6385 time:0.015626192092895508\n",
      "train: loss: 0.7234213352203369 acc: 0.8959906101226807  val: loss: 1.8158752918243408 acc: 0.8323562741279602\n",
      "step: 6390 time:0.003002166748046875\n",
      "train: loss: 2.0146260261535645 acc: 0.8556386232376099  val: loss: 5.605419635772705 acc: 0.8256763219833374\n",
      "step: 6395 time:0.0\n",
      "train: loss: 3.9853734970092773 acc: 0.8677588701248169  val: loss: 2.089606285095215 acc: 0.814673662185669\n",
      "step: 6400 time:0.0\n",
      "train: loss: 1.940707802772522 acc: 0.9210512638092041  val: loss: 6.92607307434082 acc: 0.8753167390823364\n",
      "step: 6405 time:0.003002166748046875\n",
      "train: loss: 0.5318763256072998 acc: 0.8877953290939331  val: loss: 21.60635757446289 acc: 0.8009002804756165\n",
      "step: 6410 time:0.0030019283294677734\n",
      "train: loss: 4.821364402770996 acc: 0.8765676617622375  val: loss: 1.6616768836975098 acc: 0.9202868938446045\n",
      "step: 6415 time:0.003002166748046875\n",
      "train: loss: 2.092188835144043 acc: 0.8970099091529846  val: loss: 10.117228507995605 acc: 0.7501323223114014\n",
      "step: 6420 time:0.0\n",
      "train: loss: 3.1157751083374023 acc: 0.8435138463973999  val: loss: 5.1826300621032715 acc: 0.8156300783157349\n",
      "step: 6425 time:0.0\n",
      "train: loss: 0.9837477803230286 acc: 0.9398900270462036  val: loss: 2.211696147918701 acc: 0.7487856149673462\n",
      "step: 6430 time:0.0030019283294677734\n",
      "train: loss: 3.071331262588501 acc: 0.8395565748214722  val: loss: 1.7096502780914307 acc: 0.8661937713623047\n",
      "step: 6435 time:0.015626192092895508\n",
      "train: loss: 1.0918545722961426 acc: 0.8269606232643127  val: loss: 4.373634338378906 acc: 0.8305892944335938\n",
      "step: 6440 time:0.015625953674316406\n",
      "train: loss: 0.9671293497085571 acc: 0.7733784914016724  val: loss: 7.557045936584473 acc: 0.8790941834449768\n",
      "step: 6445 time:0.003001689910888672\n",
      "train: loss: 4.034620761871338 acc: 0.6073449850082397  val: loss: 4.6065168380737305 acc: 0.846718430519104\n",
      "step: 6450 time:0.0024569034576416016\n",
      "train: loss: 1.5893324613571167 acc: 0.9023738503456116  val: loss: 4.768252849578857 acc: 0.9040626883506775\n",
      "step: 6455 time:0.0\n",
      "train: loss: 9.103694915771484 acc: 0.870319128036499  val: loss: 7.479646682739258 acc: 0.6873482465744019\n",
      "step: 6460 time:0.0\n",
      "train: loss: 5.534724235534668 acc: 0.8404332995414734  val: loss: 3.564108371734619 acc: 0.761948823928833\n",
      "step: 6465 time:0.0030012130737304688\n",
      "train: loss: 9.369192123413086 acc: 0.6318535804748535  val: loss: 3.432499408721924 acc: 0.7384811043739319\n",
      "step: 6470 time:0.0030007362365722656\n",
      "train: loss: 12.346159934997559 acc: 0.5988786816596985  val: loss: 1.9015109539031982 acc: 0.7771528959274292\n",
      "step: 6475 time:0.0\n",
      "train: loss: 1.76039719581604 acc: 0.8769972920417786  val: loss: 5.112331390380859 acc: 0.8401103019714355\n",
      "step: 6480 time:0.0\n",
      "train: loss: 2.9153237342834473 acc: 0.7263485193252563  val: loss: 19.14232635498047 acc: 0.7391029596328735\n",
      "step: 6485 time:0.003002166748046875\n",
      "train: loss: 2.3864004611968994 acc: 0.7652470469474792  val: loss: 3.098231792449951 acc: 0.8168348670005798\n",
      "step: 6490 time:0.0\n",
      "train: loss: 3.958620071411133 acc: 0.7936052083969116  val: loss: 4.643270969390869 acc: 0.8534766435623169\n",
      "step: 6495 time:0.0\n",
      "train: loss: 1.7132160663604736 acc: 0.8650554418563843  val: loss: 5.254773139953613 acc: 0.8719764947891235\n",
      "step: 6500 time:0.01701641082763672\n",
      "train: loss: 2.355496406555176 acc: 0.8615928888320923  val: loss: 17.070026397705078 acc: 0.777288019657135\n",
      "step: 6505 time:0.0\n",
      "train: loss: 1.400518774986267 acc: 0.9134782552719116  val: loss: 2.040875196456909 acc: 0.8397172689437866\n",
      "step: 6510 time:0.0030019283294677734\n",
      "train: loss: 1.8567456007003784 acc: 0.8868955373764038  val: loss: 2.584709644317627 acc: 0.868250846862793\n",
      "step: 6515 time:0.0\n",
      "train: loss: 3.03120756149292 acc: 0.711586594581604  val: loss: 2.615778923034668 acc: 0.9001840353012085\n",
      "step: 6520 time:0.0\n",
      "train: loss: 1.920581340789795 acc: 0.8895933628082275  val: loss: 0.9930780529975891 acc: 0.8449984788894653\n",
      "step: 6525 time:0.0030014514923095703\n",
      "train: loss: 0.8197760581970215 acc: 0.8970924615859985  val: loss: 3.05153226852417 acc: 0.8619078397750854\n",
      "step: 6530 time:0.0030019283294677734\n",
      "train: loss: 1.7137699127197266 acc: 0.871100902557373  val: loss: 0.7010936737060547 acc: 0.8797262907028198\n",
      "step: 6535 time:0.015625953674316406\n",
      "train: loss: 5.927910327911377 acc: 0.8902763724327087  val: loss: 6.11623477935791 acc: 0.8560305833816528\n",
      "step: 6540 time:0.0\n",
      "train: loss: 1.2168776988983154 acc: 0.9054386615753174  val: loss: 1.0976207256317139 acc: 0.8474181890487671\n",
      "step: 6545 time:0.003001689910888672\n",
      "train: loss: 3.7801170349121094 acc: 0.9007720947265625  val: loss: 14.956087112426758 acc: 0.8834749460220337\n",
      "step: 6550 time:0.0\n",
      "train: loss: 1.5305559635162354 acc: 0.9042719006538391  val: loss: 2.84033465385437 acc: 0.8347545862197876\n",
      "step: 6555 time:0.0\n",
      "train: loss: 1.1096044778823853 acc: 0.9215127229690552  val: loss: 1.033977746963501 acc: 0.9420665502548218\n",
      "step: 6560 time:0.0\n",
      "train: loss: 0.9568340182304382 acc: 0.9340039491653442  val: loss: 14.774222373962402 acc: 0.7964007258415222\n",
      "step: 6565 time:0.004002809524536133\n",
      "train: loss: 1.4453059434890747 acc: 0.7795881628990173  val: loss: 3.7874279022216797 acc: 0.8273818492889404\n",
      "step: 6570 time:0.0\n",
      "train: loss: 2.553211212158203 acc: 0.8565536737442017  val: loss: 2.2914867401123047 acc: 0.902044415473938\n",
      "step: 6575 time:0.0\n",
      "train: loss: 0.8833081722259521 acc: 0.8818737864494324  val: loss: 1.3497493267059326 acc: 0.8798959851264954\n",
      "step: 6580 time:0.004002809524536133\n",
      "train: loss: 2.754910945892334 acc: 0.8253275752067566  val: loss: 3.786346197128296 acc: 0.8578452467918396\n",
      "step: 6585 time:0.003002166748046875\n",
      "train: loss: 1.1606128215789795 acc: 0.8017681241035461  val: loss: 4.858654975891113 acc: 0.8397406935691833\n",
      "step: 6590 time:0.0\n",
      "train: loss: 0.379571795463562 acc: 0.8620694875717163  val: loss: 4.200155735015869 acc: 0.9019472599029541\n",
      "step: 6595 time:0.0\n",
      "train: loss: 1.9523764848709106 acc: 0.7636961340904236  val: loss: 4.8723602294921875 acc: 0.8520376682281494\n",
      "step: 6600 time:0.0\n",
      "train: loss: 0.5049320459365845 acc: 0.8432028293609619  val: loss: 5.72813606262207 acc: 0.8590130805969238\n",
      "step: 6605 time:0.004002571105957031\n",
      "train: loss: 1.0108540058135986 acc: 0.6615368127822876  val: loss: 3.3238916397094727 acc: 0.7504506707191467\n",
      "step: 6610 time:0.0\n",
      "train: loss: 0.8304399847984314 acc: 0.7376347780227661  val: loss: 10.99233627319336 acc: 0.8542793393135071\n",
      "step: 6615 time:0.01616048812866211\n",
      "train: loss: 0.6121076345443726 acc: 0.7403755187988281  val: loss: 14.176959991455078 acc: 0.7523984313011169\n",
      "step: 6620 time:0.0020012855529785156\n",
      "train: loss: 1.1958222389221191 acc: 0.6325618624687195  val: loss: 1.887834906578064 acc: 0.6917371153831482\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 6625 time:0.004002809524536133\n",
      "train: loss: 0.24840334057807922 acc: 0.7611981630325317  val: loss: 8.031745910644531 acc: 0.9161279201507568\n",
      "step: 6630 time:0.0\n",
      "train: loss: 1.1435811519622803 acc: 0.5388085246086121  val: loss: 9.521061897277832 acc: 0.8783338665962219\n",
      "step: 6635 time:0.003002166748046875\n",
      "train: loss: 0.7236668467521667 acc: 0.8699169754981995  val: loss: 3.9857773780822754 acc: 0.8735755681991577\n",
      "step: 6640 time:0.0030014514923095703\n",
      "train: loss: 0.4376766085624695 acc: 0.8901866674423218  val: loss: 9.39592170715332 acc: 0.8553184866905212\n",
      "step: 6645 time:0.0030014514923095703\n",
      "train: loss: 1.5192439556121826 acc: 0.8804072737693787  val: loss: 2.5547749996185303 acc: 0.8723747730255127\n",
      "step: 6650 time:0.0156252384185791\n",
      "train: loss: 0.47525301575660706 acc: 0.8312260508537292  val: loss: 4.193970203399658 acc: 0.7650420069694519\n",
      "step: 6655 time:0.0\n",
      "train: loss: 1.2904338836669922 acc: 0.8233662843704224  val: loss: 3.782672643661499 acc: 0.8821436762809753\n",
      "step: 6660 time:0.0\n",
      "train: loss: 1.0471563339233398 acc: 0.7023670077323914  val: loss: 7.61569356918335 acc: 0.7820349335670471\n",
      "step: 6665 time:0.0030019283294677734\n",
      "train: loss: 1.9916660785675049 acc: 0.8210833072662354  val: loss: 4.700366973876953 acc: 0.8669949769973755\n",
      "step: 6670 time:0.003001689910888672\n",
      "train: loss: 0.44683709740638733 acc: 0.8980745673179626  val: loss: 4.548422336578369 acc: 0.7977790832519531\n",
      "step: 6675 time:0.0\n",
      "train: loss: 0.3282315135002136 acc: 0.8389402031898499  val: loss: 14.448857307434082 acc: 0.8809954524040222\n",
      "step: 6680 time:0.0\n",
      "train: loss: 0.5575685501098633 acc: 0.6207777261734009  val: loss: 2.6743662357330322 acc: 0.8485653400421143\n",
      "step: 6685 time:0.0030019283294677734\n",
      "train: loss: 1.0851013660430908 acc: 0.9100462794303894  val: loss: 0.8643956780433655 acc: 0.8805731534957886\n",
      "step: 6690 time:0.0\n",
      "train: loss: 1.1133735179901123 acc: 0.7840576171875  val: loss: 12.959358215332031 acc: 0.7992764711380005\n",
      "step: 6695 time:0.015625476837158203\n",
      "train: loss: 0.7183688282966614 acc: 0.8650287389755249  val: loss: 7.317293167114258 acc: 0.7680621147155762\n",
      "step: 6700 time:0.0\n",
      "train: loss: 0.4109158515930176 acc: 0.8956830501556396  val: loss: 9.670175552368164 acc: 0.8794623017311096\n",
      "step: 6705 time:0.0030019283294677734\n",
      "train: loss: 1.158432126045227 acc: 0.7947613000869751  val: loss: 1.8178584575653076 acc: 0.8757891654968262\n",
      "step: 6710 time:0.0030024051666259766\n",
      "train: loss: 1.178882360458374 acc: 0.8373265266418457  val: loss: 2.364053726196289 acc: 0.8577148914337158\n",
      "step: 6715 time:0.0\n",
      "train: loss: 1.0148041248321533 acc: 0.8963983058929443  val: loss: 11.273324966430664 acc: 0.8110880255699158\n",
      "step: 6720 time:0.015626192092895508\n",
      "train: loss: 0.9143627882003784 acc: 0.7648941278457642  val: loss: 3.7956974506378174 acc: 0.8887408971786499\n",
      "step: 6725 time:0.004002571105957031\n",
      "train: loss: 0.836165189743042 acc: 0.6750739216804504  val: loss: 14.299018859863281 acc: 0.7930644154548645\n",
      "step: 6730 time:0.015625953674316406\n",
      "train: loss: 1.7465901374816895 acc: 0.8657410144805908  val: loss: 1.6020532846450806 acc: 0.8954327702522278\n",
      "step: 6735 time:0.0\n",
      "train: loss: 0.7601156234741211 acc: 0.879095733165741  val: loss: 4.5795488357543945 acc: 0.858649730682373\n",
      "step: 6740 time:0.0\n",
      "train: loss: 0.8565919399261475 acc: 0.8429242372512817  val: loss: 4.639617919921875 acc: 0.8352870345115662\n",
      "step: 6745 time:0.0030019283294677734\n",
      "train: loss: 1.053574562072754 acc: 0.9219061732292175  val: loss: 6.054365158081055 acc: 0.8789078593254089\n",
      "step: 6750 time:0.017853736877441406\n",
      "train: loss: 2.013850212097168 acc: 0.7801661491394043  val: loss: 4.066967964172363 acc: 0.8483507633209229\n",
      "step: 6755 time:0.0\n",
      "train: loss: 1.5289647579193115 acc: 0.891645073890686  val: loss: 1.184187889099121 acc: 0.7310582399368286\n",
      "step: 6760 time:0.0030024051666259766\n",
      "train: loss: 4.590999126434326 acc: 0.8704556226730347  val: loss: 3.3178153038024902 acc: 0.8647722005844116\n",
      "step: 6765 time:0.0030014514923095703\n",
      "train: loss: 3.474639415740967 acc: 0.7315053343772888  val: loss: 0.9578847885131836 acc: 0.8720422983169556\n",
      "step: 6770 time:0.0030019283294677734\n",
      "train: loss: 2.3644511699676514 acc: 0.885208249092102  val: loss: 1.7708431482315063 acc: 0.8833377361297607\n",
      "step: 6775 time:0.015625953674316406\n",
      "train: loss: 1.6158145666122437 acc: 0.8758398294448853  val: loss: 3.3333606719970703 acc: 0.918083131313324\n",
      "step: 6780 time:0.0\n",
      "train: loss: 1.242011308670044 acc: 0.8400640487670898  val: loss: 8.475950241088867 acc: 0.8154400587081909\n",
      "step: 6785 time:0.003002166748046875\n",
      "train: loss: 3.6158342361450195 acc: 0.6960310339927673  val: loss: 2.5581493377685547 acc: 0.8528147339820862\n",
      "step: 6790 time:0.0\n",
      "train: loss: 14.125330924987793 acc: 0.5837302207946777  val: loss: 7.843842506408691 acc: 0.8600396513938904\n",
      "step: 6795 time:0.0\n",
      "train: loss: 17.606918334960938 acc: 0.1443711519241333  val: loss: 7.214142322540283 acc: 0.7393050193786621\n",
      "step: 6800 time:0.0\n",
      "train: loss: 2.34717059135437 acc: 0.7351148724555969  val: loss: 7.833328723907471 acc: 0.8178894519805908\n",
      "step: 6805 time:0.0030019283294677734\n",
      "train: loss: 5.782593727111816 acc: 0.7663132548332214  val: loss: 2.466003894805908 acc: 0.8821887373924255\n",
      "step: 6810 time:0.0030019283294677734\n",
      "train: loss: 2.6527915000915527 acc: 0.891403079032898  val: loss: 3.0510663986206055 acc: 0.8372061252593994\n",
      "step: 6815 time:0.015626192092895508\n",
      "train: loss: 3.4033923149108887 acc: 0.8112912774085999  val: loss: 3.4750170707702637 acc: 0.851956844329834\n",
      "step: 6820 time:0.0\n",
      "train: loss: 9.072415351867676 acc: 0.8998677134513855  val: loss: 2.360197067260742 acc: 0.8633676171302795\n",
      "step: 6825 time:0.003002166748046875\n",
      "train: loss: 19.325950622558594 acc: 0.7677046060562134  val: loss: 3.742638111114502 acc: 0.8025379180908203\n",
      "step: 6830 time:0.0\n",
      "train: loss: 8.774881362915039 acc: 0.831078052520752  val: loss: 1.1302587985992432 acc: 0.8973736763000488\n",
      "step: 6835 time:0.0\n",
      "train: loss: 5.210037708282471 acc: 0.841469943523407  val: loss: 1.6145703792572021 acc: 0.8343720436096191\n",
      "step: 6840 time:0.0\n",
      "train: loss: 6.575757026672363 acc: 0.8356673717498779  val: loss: 1.2159175872802734 acc: 0.8797949552536011\n",
      "step: 6845 time:0.0030019283294677734\n",
      "train: loss: 3.886347770690918 acc: 0.8366115093231201  val: loss: 2.8172872066497803 acc: 0.8661551475524902\n",
      "step: 6850 time:0.015625953674316406\n",
      "train: loss: 2.580015182495117 acc: 0.8705751895904541  val: loss: 1.7067162990570068 acc: 0.9246997237205505\n",
      "step: 6855 time:0.0\n",
      "train: loss: 4.840245723724365 acc: 0.8635051250457764  val: loss: 2.5654289722442627 acc: 0.8450141549110413\n",
      "step: 6860 time:0.0\n",
      "train: loss: 2.454533576965332 acc: 0.8393546342849731  val: loss: 0.7197779417037964 acc: 0.9260953068733215\n",
      "step: 6865 time:0.0030019283294677734\n",
      "train: loss: 2.05076265335083 acc: 0.8524138927459717  val: loss: 5.588269233703613 acc: 0.8807100653648376\n",
      "step: 6870 time:0.015625715255737305\n",
      "train: loss: 5.788933277130127 acc: 0.5241600871086121  val: loss: 1.7203667163848877 acc: 0.8319070935249329\n",
      "step: 6875 time:0.0030019283294677734\n",
      "train: loss: 0.9879761934280396 acc: 0.8471858501434326  val: loss: 2.7897720336914062 acc: 0.8673347234725952\n",
      "step: 6880 time:0.0030019283294677734\n",
      "train: loss: 3.2566747665405273 acc: 0.7902122735977173  val: loss: 1.5261151790618896 acc: 0.9153850078582764\n",
      "step: 6885 time:0.003002643585205078\n",
      "train: loss: 4.90243673324585 acc: 0.5687447786331177  val: loss: 3.424865245819092 acc: 0.7975354790687561\n",
      "step: 6890 time:0.0\n",
      "train: loss: 6.333822250366211 acc: 0.7872076034545898  val: loss: 2.436094284057617 acc: 0.8669906854629517\n",
      "step: 6895 time:0.0\n",
      "train: loss: 1.4733988046646118 acc: 0.8998214602470398  val: loss: 12.965324401855469 acc: 0.8103965520858765\n",
      "step: 6900 time:0.0\n",
      "train: loss: 4.802169322967529 acc: 0.921035647392273  val: loss: 2.426208972930908 acc: 0.7279319763183594\n",
      "step: 6905 time:0.0030024051666259766\n",
      "train: loss: 1.3890151977539062 acc: 0.8834562301635742  val: loss: 13.146302223205566 acc: 0.7921379804611206\n",
      "step: 6910 time:0.0\n",
      "train: loss: 3.5286295413970947 acc: 0.8870855569839478  val: loss: 6.00869607925415 acc: 0.8442273736000061\n",
      "step: 6915 time:0.0\n",
      "train: loss: 0.6200790405273438 acc: 0.8583768606185913  val: loss: 1.0418790578842163 acc: 0.8686585426330566\n",
      "step: 6920 time:0.0\n",
      "train: loss: 0.8742948770523071 acc: 0.9469921588897705  val: loss: 0.9962226152420044 acc: 0.9175362586975098\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 6925 time:0.003002166748046875\n",
      "train: loss: 5.266983985900879 acc: 0.8803287744522095  val: loss: 1.9990167617797852 acc: 0.8523526191711426\n",
      "step: 6930 time:0.0\n",
      "train: loss: 1.3043317794799805 acc: 0.9416236877441406  val: loss: 3.8772692680358887 acc: 0.8066511154174805\n",
      "step: 6935 time:0.003001689910888672\n",
      "train: loss: 1.3905880451202393 acc: 0.9346733093261719  val: loss: 3.030383348464966 acc: 0.7769861817359924\n",
      "step: 6940 time:0.0\n",
      "train: loss: 3.428807020187378 acc: 0.909525990486145  val: loss: 3.2321364879608154 acc: 0.7925085425376892\n",
      "step: 6945 time:0.0030019283294677734\n",
      "train: loss: 1.262723684310913 acc: 0.8847658634185791  val: loss: 0.6965670585632324 acc: 0.8775880932807922\n",
      "step: 6950 time:0.0\n",
      "train: loss: 0.6440377831459045 acc: 0.9415988326072693  val: loss: 1.5647947788238525 acc: 0.8845630288124084\n",
      "step: 6955 time:0.0\n",
      "train: loss: 1.761495590209961 acc: 0.8888537287712097  val: loss: 2.762056827545166 acc: 0.8721673488616943\n",
      "step: 6960 time:0.0\n",
      "train: loss: 0.4926181137561798 acc: 0.9203197360038757  val: loss: 1.071484923362732 acc: 0.8210557103157043\n",
      "step: 6965 time:0.0030024051666259766\n",
      "train: loss: 4.34033727645874 acc: 0.813746452331543  val: loss: 5.445152282714844 acc: 0.8628928065299988\n",
      "step: 6970 time:0.0\n",
      "train: loss: 3.97939395904541 acc: 0.786801815032959  val: loss: 1.5774822235107422 acc: 0.9075011610984802\n",
      "step: 6975 time:0.0\n",
      "train: loss: 5.649012088775635 acc: 0.6561044454574585  val: loss: 1.7627558708190918 acc: 0.7914653420448303\n",
      "step: 6980 time:0.0\n",
      "train: loss: 2.713533401489258 acc: 0.9004214406013489  val: loss: 2.4986085891723633 acc: 0.8818249702453613\n",
      "step: 6985 time:0.003002166748046875\n",
      "train: loss: 3.3818023204803467 acc: 0.8358124494552612  val: loss: 4.96097469329834 acc: 0.8759595155715942\n",
      "step: 6990 time:0.0\n",
      "train: loss: 2.3196887969970703 acc: 0.8388455510139465  val: loss: 1.7658624649047852 acc: 0.8194745779037476\n",
      "step: 6995 time:0.0\n",
      "train: loss: 7.4428253173828125 acc: 0.6468520164489746  val: loss: 2.5935914516448975 acc: 0.6594206094741821\n",
      "step: 7000 time:0.0\n",
      "train: loss: 2.7332377433776855 acc: 0.7764210104942322  val: loss: 6.145071029663086 acc: 0.8061966896057129\n",
      "step: 7005 time:0.003002166748046875\n",
      "train: loss: 7.512438774108887 acc: 0.7874218225479126  val: loss: 4.946381568908691 acc: 0.8106749057769775\n",
      "step: 7010 time:0.015625953674316406\n",
      "train: loss: 6.733255863189697 acc: 0.8273415565490723  val: loss: 2.6185462474823 acc: 0.8622104525566101\n",
      "step: 7015 time:0.002001523971557617\n",
      "train: loss: 1.5373129844665527 acc: 0.7502938508987427  val: loss: 3.887023448944092 acc: 0.898815393447876\n",
      "step: 7020 time:0.0020012855529785156\n",
      "train: loss: 3.6927566528320312 acc: 0.9342136383056641  val: loss: 2.485170364379883 acc: 0.8565555810928345\n",
      "step: 7025 time:0.0030014514923095703\n",
      "train: loss: 2.519981622695923 acc: 0.8994755148887634  val: loss: 1.6337707042694092 acc: 0.855796217918396\n",
      "step: 7030 time:0.0\n",
      "train: loss: 3.8501269817352295 acc: 0.8672741651535034  val: loss: 1.6473830938339233 acc: 0.8387957811355591\n",
      "step: 7035 time:0.0014874935150146484\n",
      "train: loss: 2.4431657791137695 acc: 0.6719499230384827  val: loss: 3.326333999633789 acc: 0.8240097761154175\n",
      "step: 7040 time:0.0030014514923095703\n",
      "train: loss: 1.1210618019104004 acc: 0.8899072408676147  val: loss: 1.2175607681274414 acc: 0.8752633333206177\n",
      "step: 7045 time:0.0030012130737304688\n",
      "train: loss: 3.5848989486694336 acc: 0.8465062975883484  val: loss: 4.138736724853516 acc: 0.8556623458862305\n",
      "step: 7050 time:0.0\n",
      "train: loss: 2.3599438667297363 acc: 0.894123375415802  val: loss: 4.015813827514648 acc: 0.8973063826560974\n",
      "step: 7055 time:0.0030019283294677734\n",
      "train: loss: 2.500366449356079 acc: 0.8966587781906128  val: loss: 6.696646213531494 acc: 0.8906872272491455\n",
      "step: 7060 time:0.0\n",
      "train: loss: 2.3971595764160156 acc: 0.8825977444648743  val: loss: 5.625533103942871 acc: 0.8878470063209534\n",
      "step: 7065 time:0.0030024051666259766\n",
      "train: loss: 1.7270532846450806 acc: 0.8868959546089172  val: loss: 2.9742653369903564 acc: 0.8521395921707153\n",
      "step: 7070 time:0.0\n",
      "train: loss: 0.60962975025177 acc: 0.9072722792625427  val: loss: 2.9002294540405273 acc: 0.8866462707519531\n",
      "step: 7075 time:0.0\n",
      "train: loss: 1.0309609174728394 acc: 0.7687698602676392  val: loss: 14.23803424835205 acc: 0.8041129112243652\n",
      "step: 7080 time:0.0\n",
      "train: loss: 4.993741035461426 acc: 0.8391220569610596  val: loss: 3.0552005767822266 acc: 0.8878355026245117\n",
      "step: 7085 time:0.004002809524536133\n",
      "train: loss: 1.0892295837402344 acc: 0.884117603302002  val: loss: 6.334053993225098 acc: 0.8819606304168701\n",
      "step: 7090 time:0.0\n",
      "train: loss: 0.7844663858413696 acc: 0.8877909779548645  val: loss: 2.849153757095337 acc: 0.9202858805656433\n",
      "step: 7095 time:0.0\n",
      "train: loss: 0.5065234303474426 acc: 0.8985213041305542  val: loss: 16.25665283203125 acc: 0.8250104784965515\n",
      "step: 7100 time:0.003001689910888672\n",
      "train: loss: 0.5894938707351685 acc: 0.8747789859771729  val: loss: 3.6995227336883545 acc: 0.8185345530509949\n",
      "step: 7105 time:0.003001689910888672\n",
      "train: loss: 1.1689558029174805 acc: 0.7946714758872986  val: loss: 3.268659830093384 acc: 0.8975515961647034\n",
      "step: 7110 time:0.0021653175354003906\n",
      "train: loss: 0.2793058156967163 acc: 0.8238950371742249  val: loss: 2.8921310901641846 acc: 0.8421002626419067\n",
      "step: 7115 time:0.0\n",
      "train: loss: 1.1113754510879517 acc: 0.7168794870376587  val: loss: 2.2776260375976562 acc: 0.8154109716415405\n",
      "step: 7120 time:0.0030002593994140625\n",
      "train: loss: 1.1106866598129272 acc: 0.7844237089157104  val: loss: 1.8715983629226685 acc: 0.9112085103988647\n",
      "step: 7125 time:0.0030019283294677734\n",
      "train: loss: 0.6871879696846008 acc: 0.8519378900527954  val: loss: 3.5751540660858154 acc: 0.8709660768508911\n",
      "step: 7130 time:0.0\n",
      "train: loss: 0.6418471932411194 acc: 0.777519166469574  val: loss: 2.324735403060913 acc: 0.837080717086792\n",
      "step: 7135 time:0.0020012855529785156\n",
      "train: loss: 0.2186032235622406 acc: 0.8222994804382324  val: loss: 8.27174186706543 acc: 0.8633791208267212\n",
      "step: 7140 time:0.003000497817993164\n",
      "train: loss: 0.24599067866802216 acc: 0.8358194828033447  val: loss: 3.9137229919433594 acc: 0.88822340965271\n",
      "step: 7145 time:0.0030019283294677734\n",
      "train: loss: 1.191703200340271 acc: 0.4977629780769348  val: loss: 5.703787326812744 acc: 0.7527746558189392\n",
      "step: 7150 time:0.0\n",
      "train: loss: 1.594365119934082 acc: 0.8047948479652405  val: loss: 3.3711276054382324 acc: 0.8600303530693054\n",
      "step: 7155 time:0.0\n",
      "train: loss: 0.6939963102340698 acc: 0.8449435830116272  val: loss: 10.079753875732422 acc: 0.8168615102767944\n",
      "step: 7160 time:0.0\n",
      "train: loss: 0.958429217338562 acc: 0.7428111433982849  val: loss: 6.371013641357422 acc: 0.7766047120094299\n",
      "step: 7165 time:0.003001689910888672\n",
      "train: loss: 0.6241494417190552 acc: 0.8132529258728027  val: loss: 1.0772364139556885 acc: 0.827297568321228\n",
      "step: 7170 time:0.015625476837158203\n",
      "train: loss: 1.5650016069412231 acc: 0.830740213394165  val: loss: 14.705994606018066 acc: 0.8017357587814331\n",
      "step: 7175 time:0.0\n",
      "train: loss: 1.4079949855804443 acc: 0.8107024431228638  val: loss: 1.2166643142700195 acc: 0.8010374903678894\n",
      "step: 7180 time:0.015625953674316406\n",
      "train: loss: 0.5137754678726196 acc: 0.8778453469276428  val: loss: 3.6869733333587646 acc: 0.9030482172966003\n",
      "step: 7185 time:0.003002166748046875\n",
      "train: loss: 1.3121644258499146 acc: 0.8282490968704224  val: loss: 7.983226776123047 acc: 0.9048756957054138\n",
      "step: 7190 time:0.0\n",
      "train: loss: 0.5272411704063416 acc: 0.8667454719543457  val: loss: 2.349453926086426 acc: 0.8987444639205933\n",
      "step: 7195 time:0.003002166748046875\n",
      "train: loss: 1.3941876888275146 acc: 0.8890227675437927  val: loss: 1.305166244506836 acc: 0.8624138832092285\n",
      "step: 7200 time:0.0\n",
      "train: loss: 0.5604897737503052 acc: 0.910403311252594  val: loss: 2.0563783645629883 acc: 0.8655601143836975\n",
      "step: 7205 time:0.0030014514923095703\n",
      "train: loss: 0.4242577850818634 acc: 0.76447594165802  val: loss: 11.348998069763184 acc: 0.8139459490776062\n",
      "step: 7210 time:0.0030019283294677734\n",
      "train: loss: 0.4213540554046631 acc: 0.9002475738525391  val: loss: 1.1400521993637085 acc: 0.9343180656433105\n",
      "step: 7215 time:0.0\n",
      "train: loss: 0.6574954390525818 acc: 0.7657869458198547  val: loss: 2.4593417644500732 acc: 0.7779458165168762\n",
      "step: 7220 time:0.0\n",
      "train: loss: 0.7401554584503174 acc: 0.841666042804718  val: loss: 2.547942638397217 acc: 0.8342262506484985\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 7225 time:0.003002166748046875\n",
      "train: loss: 1.2768447399139404 acc: 0.8868607878684998  val: loss: 5.8831939697265625 acc: 0.8875712156295776\n",
      "step: 7230 time:0.0\n",
      "train: loss: 0.9767758846282959 acc: 0.8941565752029419  val: loss: 7.677516937255859 acc: 0.8358651399612427\n",
      "step: 7235 time:0.0156252384185791\n",
      "train: loss: 0.9987245798110962 acc: 0.7595022916793823  val: loss: 1.21340811252594 acc: 0.9092994332313538\n",
      "step: 7240 time:0.0156252384185791\n",
      "train: loss: 0.8584914207458496 acc: 0.8521960377693176  val: loss: 1.5655009746551514 acc: 0.860517144203186\n",
      "step: 7245 time:0.0030014514923095703\n",
      "train: loss: 1.1257814168930054 acc: 0.9087420701980591  val: loss: 5.43760871887207 acc: 0.9082893133163452\n",
      "step: 7250 time:0.0\n",
      "train: loss: 1.5121216773986816 acc: 0.9407460689544678  val: loss: 2.1016597747802734 acc: 0.8010488152503967\n",
      "step: 7255 time:0.0\n",
      "train: loss: 0.7639003992080688 acc: 0.8539733290672302  val: loss: 4.656700134277344 acc: 0.9011080861091614\n",
      "step: 7260 time:0.0\n",
      "train: loss: 0.906039834022522 acc: 0.9238918423652649  val: loss: 2.52006196975708 acc: 0.9300728440284729\n",
      "step: 7265 time:0.0030024051666259766\n",
      "train: loss: 0.7178100347518921 acc: 0.8740664124488831  val: loss: 6.176156997680664 acc: 0.8029190301895142\n",
      "step: 7270 time:0.0\n",
      "train: loss: 1.2012900114059448 acc: 0.8171529769897461  val: loss: 1.4559104442596436 acc: 0.8809844255447388\n",
      "step: 7275 time:0.003002166748046875\n",
      "train: loss: 1.2067569494247437 acc: 0.872036337852478  val: loss: 19.34931182861328 acc: 0.8242084383964539\n",
      "step: 7280 time:0.0\n",
      "train: loss: 2.6131460666656494 acc: 0.8845863342285156  val: loss: 5.304314613342285 acc: 0.838828980922699\n",
      "step: 7285 time:0.0030019283294677734\n",
      "train: loss: 1.5219885110855103 acc: 0.9319061040878296  val: loss: 1.6567468643188477 acc: 0.9007313847541809\n",
      "step: 7290 time:0.0\n",
      "train: loss: 1.8040359020233154 acc: 0.9347926378250122  val: loss: 1.9819670915603638 acc: 0.8335564136505127\n",
      "step: 7295 time:0.0\n",
      "train: loss: 0.7620398998260498 acc: 0.8663500547409058  val: loss: 3.714369535446167 acc: 0.9270358085632324\n",
      "step: 7300 time:0.0\n",
      "train: loss: 1.0623137950897217 acc: 0.860068678855896  val: loss: 3.7508206367492676 acc: 0.850473165512085\n",
      "step: 7305 time:0.0030019283294677734\n",
      "train: loss: 2.531337261199951 acc: 0.8429811000823975  val: loss: 1.6973094940185547 acc: 0.7885180711746216\n",
      "step: 7310 time:0.003002166748046875\n",
      "train: loss: 5.071629524230957 acc: 0.7078330516815186  val: loss: 5.500998497009277 acc: 0.8797553181648254\n",
      "step: 7315 time:0.0\n",
      "train: loss: 5.816843032836914 acc: 0.8251396417617798  val: loss: 1.6618303060531616 acc: 0.8765161633491516\n",
      "step: 7320 time:0.0\n",
      "train: loss: 3.645582914352417 acc: 0.8304375410079956  val: loss: 1.2060356140136719 acc: 0.8635921478271484\n",
      "step: 7325 time:0.0030019283294677734\n",
      "train: loss: 3.4367434978485107 acc: 0.7982186675071716  val: loss: 5.68673849105835 acc: 0.8140746355056763\n",
      "step: 7330 time:0.015625953674316406\n",
      "train: loss: 9.641029357910156 acc: 0.7360799908638  val: loss: 1.9507429599761963 acc: 0.8467224836349487\n",
      "step: 7335 time:0.0\n",
      "train: loss: 29.30021095275879 acc: 0.8606816530227661  val: loss: 15.845212936401367 acc: 0.7295283675193787\n",
      "step: 7340 time:0.0\n",
      "train: loss: 9.087450981140137 acc: 0.7580244541168213  val: loss: 1.077371597290039 acc: 0.8546490669250488\n",
      "step: 7345 time:0.0030019283294677734\n",
      "train: loss: 7.34063720703125 acc: 0.6288318037986755  val: loss: 2.347175121307373 acc: 0.9046083092689514\n",
      "step: 7350 time:0.0\n",
      "train: loss: 10.680082321166992 acc: 0.9409225583076477  val: loss: 1.7200899124145508 acc: 0.8475600481033325\n",
      "step: 7355 time:0.0\n",
      "train: loss: 7.079453945159912 acc: 0.9107800126075745  val: loss: 1.2988438606262207 acc: 0.8044874668121338\n",
      "step: 7360 time:0.0\n",
      "train: loss: 2.7722434997558594 acc: 0.8495140075683594  val: loss: 1.4620190858840942 acc: 0.88319993019104\n",
      "step: 7365 time:0.003001689910888672\n",
      "train: loss: 4.982392311096191 acc: 0.8293262720108032  val: loss: 16.036161422729492 acc: 0.7687953114509583\n",
      "step: 7370 time:0.003002166748046875\n",
      "train: loss: 2.284013032913208 acc: 0.9316466450691223  val: loss: 4.9139251708984375 acc: 0.8735554218292236\n",
      "step: 7375 time:0.0\n",
      "train: loss: 2.8453989028930664 acc: 0.8908143043518066  val: loss: 2.3547775745391846 acc: 0.8709341883659363\n",
      "step: 7380 time:0.0\n",
      "train: loss: 5.735846042633057 acc: 0.8149104714393616  val: loss: 1.3099982738494873 acc: 0.8717179298400879\n",
      "step: 7385 time:0.0030019283294677734\n",
      "train: loss: 1.829306721687317 acc: 0.876268744468689  val: loss: 2.638960361480713 acc: 0.9015290141105652\n",
      "step: 7390 time:0.0\n",
      "train: loss: 1.326357364654541 acc: 0.8770414590835571  val: loss: 1.6742098331451416 acc: 0.8776900768280029\n",
      "step: 7395 time:0.0\n",
      "train: loss: 1.9809249639511108 acc: 0.8936918377876282  val: loss: 17.295246124267578 acc: 0.7937886714935303\n",
      "step: 7400 time:0.0018088817596435547\n",
      "train: loss: 5.235867500305176 acc: 0.44598281383514404  val: loss: 13.714495658874512 acc: 0.7650034427642822\n",
      "step: 7405 time:0.0030024051666259766\n",
      "train: loss: 2.5025880336761475 acc: 0.883867084980011  val: loss: 5.566739559173584 acc: 0.8420330286026001\n",
      "step: 7410 time:0.015625476837158203\n",
      "train: loss: 7.719619274139404 acc: 0.6461349129676819  val: loss: 6.367676734924316 acc: 0.8159765005111694\n",
      "step: 7415 time:0.0\n",
      "train: loss: 2.881720542907715 acc: 0.8132585287094116  val: loss: 3.605095386505127 acc: 0.8731494545936584\n",
      "step: 7420 time:0.015625476837158203\n",
      "train: loss: 1.350746512413025 acc: 0.8732677102088928  val: loss: 2.0371692180633545 acc: 0.7782296538352966\n",
      "step: 7425 time:0.004002094268798828\n",
      "train: loss: 1.9392054080963135 acc: 0.9538924098014832  val: loss: 5.838931083679199 acc: 0.8696550130844116\n",
      "step: 7430 time:0.0\n",
      "train: loss: 6.215411186218262 acc: 0.8136256337165833  val: loss: 5.8428754806518555 acc: 0.8662306070327759\n",
      "step: 7435 time:0.0\n",
      "train: loss: 0.5543445348739624 acc: 0.9114965200424194  val: loss: 3.7054967880249023 acc: 0.875943660736084\n",
      "step: 7440 time:0.0\n",
      "train: loss: 0.5332364439964294 acc: 0.9528103470802307  val: loss: 0.49744799733161926 acc: 0.8766857981681824\n",
      "step: 7445 time:0.004003047943115234\n",
      "train: loss: 1.710600733757019 acc: 0.8433573842048645  val: loss: 4.531391143798828 acc: 0.8689658641815186\n",
      "step: 7450 time:0.003002166748046875\n",
      "train: loss: 1.5504487752914429 acc: 0.9197655320167542  val: loss: 2.7661895751953125 acc: 0.8725095987319946\n",
      "step: 7455 time:0.0\n",
      "train: loss: 4.371934413909912 acc: 0.7442566752433777  val: loss: 4.8271918296813965 acc: 0.7584573030471802\n",
      "step: 7460 time:0.0\n",
      "train: loss: 1.224314570426941 acc: 0.9580690264701843  val: loss: 3.8138656616210938 acc: 0.878821074962616\n",
      "step: 7465 time:0.004002809524536133\n",
      "train: loss: 3.0131216049194336 acc: 0.8515413999557495  val: loss: 0.9819369912147522 acc: 0.8433197736740112\n",
      "step: 7470 time:0.003002166748046875\n",
      "train: loss: 1.0035128593444824 acc: 0.9015724658966064  val: loss: 1.7281007766723633 acc: 0.9121883511543274\n",
      "step: 7475 time:0.0\n",
      "train: loss: 1.7354443073272705 acc: 0.8783438205718994  val: loss: 5.495079040527344 acc: 0.79116290807724\n",
      "step: 7480 time:0.0\n",
      "train: loss: 0.9998621344566345 acc: 0.8742568492889404  val: loss: 4.212265968322754 acc: 0.8329353928565979\n",
      "step: 7485 time:0.004002571105957031\n",
      "train: loss: 1.2284022569656372 acc: 0.8393633961677551  val: loss: 7.294547080993652 acc: 0.8107990026473999\n",
      "step: 7490 time:0.0\n",
      "train: loss: 1.7270238399505615 acc: 0.9031786918640137  val: loss: 1.1958975791931152 acc: 0.8384044170379639\n",
      "step: 7495 time:0.0\n",
      "train: loss: 3.9038519859313965 acc: 0.6820557117462158  val: loss: 1.2734723091125488 acc: 0.8540830016136169\n",
      "step: 7500 time:0.0\n",
      "train: loss: 6.6382880210876465 acc: 0.8699192404747009  val: loss: 1.0812252759933472 acc: 0.8641303181648254\n",
      "step: 7505 time:0.004002094268798828\n",
      "train: loss: 4.418139934539795 acc: 0.8249253630638123  val: loss: 1.355412244796753 acc: 0.8702476620674133\n",
      "step: 7510 time:0.0\n",
      "train: loss: 2.086042642593384 acc: 0.7718989849090576  val: loss: 4.2587995529174805 acc: 0.8602116107940674\n",
      "step: 7515 time:0.0\n",
      "train: loss: 4.353238582611084 acc: 0.9025688767433167  val: loss: 1.9075005054473877 acc: 0.893635630607605\n",
      "step: 7520 time:0.0\n",
      "train: loss: 6.404697418212891 acc: 0.8120795488357544  val: loss: 6.979918479919434 acc: 0.6219937205314636\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 7525 time:0.0030019283294677734\n",
      "train: loss: 3.9401023387908936 acc: 0.7499998211860657  val: loss: 4.12243127822876 acc: 0.8418278694152832\n",
      "step: 7530 time:0.0030019283294677734\n",
      "train: loss: 5.193994045257568 acc: 0.8425467014312744  val: loss: 0.6147526502609253 acc: 0.9091373682022095\n",
      "step: 7535 time:0.0\n",
      "train: loss: 1.5041766166687012 acc: 0.7480655908584595  val: loss: 1.100541591644287 acc: 0.8553592562675476\n",
      "step: 7540 time:0.0030019283294677734\n",
      "train: loss: 3.272238254547119 acc: 0.8572880625724792  val: loss: 1.9698376655578613 acc: 0.8913450837135315\n",
      "step: 7545 time:0.003001689910888672\n",
      "train: loss: 2.1393277645111084 acc: 0.8908966183662415  val: loss: 1.7600147724151611 acc: 0.9274291396141052\n",
      "step: 7550 time:0.01582026481628418\n",
      "train: loss: 4.725214004516602 acc: 0.7569705843925476  val: loss: 3.0634512901306152 acc: 0.9093409180641174\n",
      "step: 7555 time:0.0\n",
      "train: loss: 5.723800182342529 acc: 0.8853171467781067  val: loss: 1.5619759559631348 acc: 0.8811248540878296\n",
      "step: 7560 time:0.0\n",
      "train: loss: 2.5899181365966797 acc: 0.9024762511253357  val: loss: 1.954670786857605 acc: 0.8959989547729492\n",
      "step: 7565 time:0.00400233268737793\n",
      "train: loss: 4.030704021453857 acc: 0.7984086871147156  val: loss: 0.8212305903434753 acc: 0.8346782326698303\n",
      "step: 7570 time:0.0\n",
      "train: loss: 4.183475494384766 acc: 0.8438231945037842  val: loss: 1.553311824798584 acc: 0.8682981133460999\n",
      "step: 7575 time:0.0\n",
      "train: loss: 1.8021111488342285 acc: 0.8541784286499023  val: loss: 13.863234519958496 acc: 0.8064919710159302\n",
      "step: 7580 time:0.0\n",
      "train: loss: 0.5302121043205261 acc: 0.905468761920929  val: loss: 4.505035877227783 acc: 0.905022144317627\n",
      "step: 7585 time:0.0030019283294677734\n",
      "train: loss: 1.3813539743423462 acc: 0.9057250022888184  val: loss: 1.5210736989974976 acc: 0.8958337306976318\n",
      "step: 7590 time:0.0\n",
      "train: loss: 2.6480941772460938 acc: 0.9093928337097168  val: loss: 1.2162269353866577 acc: 0.9055137634277344\n",
      "step: 7595 time:0.015625953674316406\n",
      "train: loss: 1.3741689920425415 acc: 0.8332952260971069  val: loss: 7.448310852050781 acc: 0.8731064796447754\n",
      "step: 7600 time:0.0\n",
      "train: loss: 2.3125319480895996 acc: 0.9078307151794434  val: loss: 4.368802070617676 acc: 0.8633808493614197\n",
      "step: 7605 time:0.0030019283294677734\n",
      "train: loss: 3.4212470054626465 acc: 0.8074663281440735  val: loss: 1.5254325866699219 acc: 0.894524097442627\n",
      "step: 7610 time:0.015625476837158203\n",
      "train: loss: 1.0228447914123535 acc: 0.8745737075805664  val: loss: 12.515442848205566 acc: 0.8356993198394775\n",
      "step: 7615 time:0.0\n",
      "train: loss: 0.37041857838630676 acc: 0.8889284729957581  val: loss: 1.2865192890167236 acc: 0.8778465986251831\n",
      "step: 7620 time:0.0\n",
      "train: loss: 0.1945849359035492 acc: 0.8005762696266174  val: loss: 2.2662947177886963 acc: 0.8991307020187378\n",
      "step: 7625 time:0.0\n",
      "train: loss: 0.27936363220214844 acc: 0.8988879919052124  val: loss: 2.8530232906341553 acc: 0.8901895880699158\n",
      "step: 7630 time:0.0\n",
      "train: loss: 3.220552682876587 acc: 0.6656099557876587  val: loss: 1.5249801874160767 acc: 0.8742700815200806\n",
      "step: 7635 time:0.0\n",
      "train: loss: 0.492667555809021 acc: 0.8421021699905396  val: loss: 2.470745086669922 acc: 0.7993028163909912\n",
      "step: 7640 time:0.015626192092895508\n",
      "train: loss: 0.3100224733352661 acc: 0.8157347440719604  val: loss: 2.1182737350463867 acc: 0.8507364988327026\n",
      "step: 7645 time:0.0030014514923095703\n",
      "train: loss: 0.8673670887947083 acc: 0.6994709968566895  val: loss: 4.304941177368164 acc: 0.8310602903366089\n",
      "step: 7650 time:0.0\n",
      "train: loss: 0.2420620620250702 acc: 0.8385369777679443  val: loss: 1.9203652143478394 acc: 0.8801543116569519\n",
      "step: 7655 time:0.0\n",
      "train: loss: 0.5118932723999023 acc: 0.6906085014343262  val: loss: 15.090923309326172 acc: 0.7941892147064209\n",
      "step: 7660 time:0.0\n",
      "train: loss: 1.2020386457443237 acc: 0.7528083324432373  val: loss: 4.010319709777832 acc: 0.8697803616523743\n",
      "step: 7665 time:0.0030014514923095703\n",
      "train: loss: 0.6082703471183777 acc: 0.6735880374908447  val: loss: 5.980278968811035 acc: 0.8879235982894897\n",
      "step: 7670 time:0.0\n",
      "train: loss: 0.5216997861862183 acc: 0.8215900659561157  val: loss: 1.2052264213562012 acc: 0.9036418199539185\n",
      "step: 7675 time:0.003000974655151367\n",
      "train: loss: 0.8020241260528564 acc: 0.7683271169662476  val: loss: 4.541277885437012 acc: 0.9048505425453186\n",
      "step: 7680 time:0.0017082691192626953\n",
      "train: loss: 1.8120038509368896 acc: 0.6331158876419067  val: loss: 5.9967122077941895 acc: 0.8377045392990112\n",
      "step: 7685 time:0.003000497817993164\n",
      "train: loss: 1.1032007932662964 acc: 0.8850311636924744  val: loss: 1.9487518072128296 acc: 0.8990215063095093\n",
      "step: 7690 time:0.003002166748046875\n",
      "train: loss: 0.9385876059532166 acc: 0.8438153266906738  val: loss: 2.3143837451934814 acc: 0.8573880791664124\n",
      "step: 7695 time:0.0\n",
      "train: loss: 1.096684455871582 acc: 0.781380295753479  val: loss: 0.8489077091217041 acc: 0.9124724268913269\n",
      "step: 7700 time:0.015625953674316406\n",
      "train: loss: 1.1075538396835327 acc: 0.8073165416717529  val: loss: 1.1185730695724487 acc: 0.9158558249473572\n",
      "step: 7705 time:0.0030019283294677734\n",
      "train: loss: 0.7293025255203247 acc: 0.7709176540374756  val: loss: 3.435001850128174 acc: 0.8982602953910828\n",
      "step: 7710 time:0.0\n",
      "train: loss: 0.7944087982177734 acc: 0.8603823184967041  val: loss: 2.2924444675445557 acc: 0.8349336981773376\n",
      "step: 7715 time:0.0\n",
      "train: loss: 0.5095071196556091 acc: 0.8488202095031738  val: loss: 1.9326348304748535 acc: 0.8820379376411438\n",
      "step: 7720 time:0.0\n",
      "train: loss: 0.9535932540893555 acc: 0.8267033696174622  val: loss: 3.006620168685913 acc: 0.8745267391204834\n",
      "step: 7725 time:0.0030014514923095703\n",
      "train: loss: 0.4804949164390564 acc: 0.8307659029960632  val: loss: 0.8859620094299316 acc: 0.851937472820282\n",
      "step: 7730 time:0.003001689910888672\n",
      "train: loss: 0.48311755061149597 acc: 0.8830488920211792  val: loss: 2.3520028591156006 acc: 0.9032204151153564\n",
      "step: 7735 time:0.0\n",
      "train: loss: 0.15100492537021637 acc: 0.9092187285423279  val: loss: 0.9223727583885193 acc: 0.8992643356323242\n",
      "step: 7740 time:0.0\n",
      "train: loss: 0.35980039834976196 acc: 0.8506871461868286  val: loss: 6.878203392028809 acc: 0.8354741334915161\n",
      "step: 7745 time:0.0\n",
      "train: loss: 2.146226406097412 acc: 0.8681817650794983  val: loss: 11.646636962890625 acc: 0.8053057789802551\n",
      "step: 7750 time:0.015625953674316406\n",
      "train: loss: 0.3602735996246338 acc: 0.9011266827583313  val: loss: 4.3696088790893555 acc: 0.9059492945671082\n",
      "step: 7755 time:0.0030019283294677734\n",
      "train: loss: 0.9115840196609497 acc: 0.8951498866081238  val: loss: 2.187021255493164 acc: 0.863987922668457\n",
      "step: 7760 time:0.001049041748046875\n",
      "train: loss: 3.520252227783203 acc: 0.7908787131309509  val: loss: 12.680153846740723 acc: 0.8033140301704407\n",
      "step: 7765 time:0.003001689910888672\n",
      "train: loss: 1.0548293590545654 acc: 0.8000537157058716  val: loss: 2.910212516784668 acc: 0.8557466864585876\n",
      "step: 7770 time:0.0\n",
      "train: loss: 0.6496942639350891 acc: 0.8678456544876099  val: loss: 2.617483377456665 acc: 0.8568340539932251\n",
      "step: 7775 time:0.003002166748046875\n",
      "train: loss: 1.5011382102966309 acc: 0.885136604309082  val: loss: 10.924490928649902 acc: 0.8603422045707703\n",
      "step: 7780 time:0.0\n",
      "train: loss: 1.0058300495147705 acc: 0.8162916898727417  val: loss: 1.6584358215332031 acc: 0.849196195602417\n",
      "step: 7785 time:0.0\n",
      "train: loss: 1.5883758068084717 acc: 0.7376438975334167  val: loss: 3.0386314392089844 acc: 0.9048085808753967\n",
      "step: 7790 time:0.0030019283294677734\n",
      "train: loss: 1.1538360118865967 acc: 0.9070733785629272  val: loss: 5.195943832397461 acc: 0.828563928604126\n",
      "step: 7795 time:0.0030019283294677734\n",
      "train: loss: 3.1758787631988525 acc: 0.8257343173027039  val: loss: 4.305164337158203 acc: 0.8407293558120728\n",
      "step: 7800 time:0.0\n",
      "train: loss: 1.861804723739624 acc: 0.8900954723358154  val: loss: 1.6372326612472534 acc: 0.7674474120140076\n",
      "step: 7805 time:0.0\n",
      "train: loss: 1.1748075485229492 acc: 0.8835376501083374  val: loss: 15.509683609008789 acc: 0.8382753729820251\n",
      "step: 7810 time:0.0\n",
      "train: loss: 2.90240216255188 acc: 0.8366055488586426  val: loss: 2.28922176361084 acc: 0.8610384464263916\n",
      "step: 7815 time:0.00400233268737793\n",
      "train: loss: 1.9686232805252075 acc: 0.9003857374191284  val: loss: 3.7953217029571533 acc: 0.9096526503562927\n",
      "step: 7820 time:0.0\n",
      "train: loss: 1.697285532951355 acc: 0.8216216564178467  val: loss: 4.186273574829102 acc: 0.8361068367958069\n",
      "step: 7825 time:0.0\n",
      "train: loss: 10.033023834228516 acc: 0.6983151435852051  val: loss: 4.536680221557617 acc: 0.8131527900695801\n",
      "step: 7830 time:0.003001689910888672\n",
      "train: loss: 11.698383331298828 acc: 0.6188172101974487  val: loss: 3.820650100708008 acc: 0.8535778522491455\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 7835 time:0.003001689910888672\n",
      "train: loss: 16.059829711914062 acc: 0.6090792417526245  val: loss: 6.1380696296691895 acc: 0.6388508081436157\n",
      "step: 7840 time:0.0\n",
      "train: loss: 2.7719295024871826 acc: 0.908115804195404  val: loss: 1.5063700675964355 acc: 0.884468138217926\n",
      "step: 7845 time:0.0\n",
      "train: loss: 1.2270941734313965 acc: 0.8979140520095825  val: loss: 4.286270618438721 acc: 0.9206814765930176\n",
      "step: 7850 time:0.0030019283294677734\n",
      "train: loss: 5.609952926635742 acc: 0.8200073838233948  val: loss: 3.7405900955200195 acc: 0.8149330615997314\n",
      "step: 7855 time:0.0030024051666259766\n",
      "train: loss: 4.607295513153076 acc: 0.8996975421905518  val: loss: 0.6119807958602905 acc: 0.9045882225036621\n",
      "step: 7860 time:0.0\n",
      "train: loss: 13.197021484375 acc: 0.9062475562095642  val: loss: 1.3905444145202637 acc: 0.865070104598999\n",
      "step: 7865 time:0.003054380416870117\n",
      "train: loss: 13.366264343261719 acc: 0.2068486213684082  val: loss: 3.6963210105895996 acc: 0.7260264754295349\n",
      "step: 7870 time:0.0020172595977783203\n",
      "train: loss: 5.78073263168335 acc: 0.8951342105865479  val: loss: 4.271759986877441 acc: 0.8791937232017517\n",
      "step: 7875 time:0.00400233268737793\n",
      "train: loss: 2.087400197982788 acc: 0.8582900762557983  val: loss: 1.4450377225875854 acc: 0.881273090839386\n",
      "step: 7880 time:0.0\n",
      "train: loss: 2.7382967472076416 acc: 0.8307712078094482  val: loss: 0.7257505655288696 acc: 0.9309719800949097\n",
      "step: 7885 time:0.0\n",
      "train: loss: 2.6227550506591797 acc: 0.8618407845497131  val: loss: 1.0770301818847656 acc: 0.8685621023178101\n",
      "step: 7890 time:0.0\n",
      "train: loss: 3.6504580974578857 acc: 0.809971034526825  val: loss: 20.108654022216797 acc: 0.8058282136917114\n",
      "step: 7895 time:0.003001689910888672\n",
      "train: loss: 9.279348373413086 acc: 0.7772168517112732  val: loss: 3.2429516315460205 acc: 0.861645519733429\n",
      "step: 7900 time:0.0\n",
      "train: loss: 5.231255054473877 acc: 0.8760500550270081  val: loss: 3.1251463890075684 acc: 0.8154438138008118\n",
      "step: 7905 time:0.0\n",
      "train: loss: 3.688969135284424 acc: 0.9262155890464783  val: loss: 1.7462401390075684 acc: 0.9015977382659912\n",
      "step: 7910 time:0.0\n",
      "train: loss: 13.608859062194824 acc: 0.8451687097549438  val: loss: 4.974058628082275 acc: 0.8593698740005493\n",
      "step: 7915 time:0.003001689910888672\n",
      "train: loss: 2.6655125617980957 acc: 0.8927603960037231  val: loss: 1.2748171091079712 acc: 0.8735535144805908\n",
      "step: 7920 time:0.0\n",
      "train: loss: 1.3059202432632446 acc: 0.9201210737228394  val: loss: 4.474682807922363 acc: 0.7858591675758362\n",
      "step: 7925 time:0.0030019283294677734\n",
      "train: loss: 3.2191476821899414 acc: 0.8321141600608826  val: loss: 2.6369950771331787 acc: 0.8460314273834229\n",
      "step: 7930 time:0.015625715255737305\n",
      "train: loss: 3.9777328968048096 acc: 0.8836655020713806  val: loss: 5.48684024810791 acc: 0.8470537662506104\n",
      "step: 7935 time:0.0030019283294677734\n",
      "train: loss: 5.51244592666626 acc: 0.9408692121505737  val: loss: 1.5051815509796143 acc: 0.8608298301696777\n",
      "step: 7940 time:0.0011241436004638672\n",
      "train: loss: 1.488265037536621 acc: 0.922675609588623  val: loss: 14.90522575378418 acc: 0.7736772298812866\n",
      "step: 7945 time:0.0\n",
      "train: loss: 0.846208930015564 acc: 0.8529233932495117  val: loss: 1.7203962802886963 acc: 0.8784916400909424\n",
      "step: 7950 time:0.0\n",
      "train: loss: 1.54925537109375 acc: 0.912083089351654  val: loss: 5.359468460083008 acc: 0.8293002843856812\n",
      "step: 7955 time:0.004002571105957031\n",
      "train: loss: 1.1790574789047241 acc: 0.8641645908355713  val: loss: 3.002809524536133 acc: 0.8376209735870361\n",
      "step: 7960 time:0.015625476837158203\n",
      "train: loss: 2.2290711402893066 acc: 0.9436088800430298  val: loss: 3.117892265319824 acc: 0.7665050625801086\n",
      "step: 7965 time:0.0030014514923095703\n",
      "train: loss: 1.2628388404846191 acc: 0.9509838819503784  val: loss: 1.2615342140197754 acc: 0.9167277812957764\n",
      "step: 7970 time:0.0\n",
      "train: loss: 2.9695358276367188 acc: 0.9020786285400391  val: loss: 1.8623441457748413 acc: 0.8043625950813293\n",
      "step: 7975 time:0.0030012130737304688\n",
      "train: loss: 2.526094675064087 acc: 0.945906937122345  val: loss: 2.3000309467315674 acc: 0.9013580679893494\n",
      "step: 7980 time:0.015625953674316406\n",
      "train: loss: 2.623225450515747 acc: 0.8874766230583191  val: loss: 10.092019081115723 acc: 0.8757683634757996\n",
      "step: 7985 time:0.0\n",
      "train: loss: 2.1029279232025146 acc: 0.7001373767852783  val: loss: 2.434910774230957 acc: 0.8714988827705383\n",
      "step: 7990 time:0.015625476837158203\n",
      "train: loss: 3.574289321899414 acc: 0.8899304866790771  val: loss: 1.4448060989379883 acc: 0.8962243795394897\n",
      "step: 7995 time:0.0030019283294677734\n",
      "train: loss: 0.6256633996963501 acc: 0.9391528964042664  val: loss: 3.0273215770721436 acc: 0.8754541277885437\n",
      "step: 8000 time:0.0\n",
      "train: loss: 1.0295164585113525 acc: 0.7245957851409912  val: loss: 2.5199217796325684 acc: 0.8292457461357117\n",
      "step: 8005 time:0.0\n",
      "train: loss: 1.9290639162063599 acc: 0.7522768378257751  val: loss: 1.5993781089782715 acc: 0.8592255115509033\n",
      "step: 8010 time:0.015625953674316406\n",
      "train: loss: 6.425930500030518 acc: 0.6833682060241699  val: loss: 1.8614423274993896 acc: 0.8171558380126953\n",
      "step: 8015 time:0.0030019283294677734\n",
      "train: loss: 3.9071366786956787 acc: 0.8074972629547119  val: loss: 10.220364570617676 acc: 0.858669102191925\n",
      "step: 8020 time:0.0\n",
      "train: loss: 3.577023506164551 acc: 0.7839111089706421  val: loss: 2.5462098121643066 acc: 0.8629050850868225\n",
      "step: 8025 time:0.0\n",
      "train: loss: 3.6305556297302246 acc: 0.8206175565719604  val: loss: 4.509253978729248 acc: 0.8959149718284607\n",
      "step: 8030 time:0.0\n",
      "train: loss: 2.5156216621398926 acc: 0.8332257866859436  val: loss: 2.522001028060913 acc: 0.8413251638412476\n",
      "step: 8035 time:0.00400233268737793\n",
      "train: loss: 2.7941091060638428 acc: 0.9309052228927612  val: loss: 2.673734188079834 acc: 0.8161213397979736\n",
      "step: 8040 time:0.0\n",
      "train: loss: 5.344109058380127 acc: 0.8170657753944397  val: loss: 4.369483947753906 acc: 0.8823046684265137\n",
      "step: 8045 time:0.0\n",
      "train: loss: 4.911462783813477 acc: 0.812614917755127  val: loss: 1.895888090133667 acc: 0.8766348958015442\n",
      "step: 8050 time:0.0030019283294677734\n",
      "train: loss: 1.1306207180023193 acc: 0.9230640530586243  val: loss: 3.687772750854492 acc: 0.8515074253082275\n",
      "step: 8055 time:0.004002571105957031\n",
      "train: loss: 2.4144089221954346 acc: 0.7895525693893433  val: loss: 2.6550726890563965 acc: 0.8677075505256653\n",
      "step: 8060 time:0.0\n",
      "train: loss: 4.423261642456055 acc: 0.9077282547950745  val: loss: 2.1016769409179688 acc: 0.8954896926879883\n",
      "step: 8065 time:0.0\n",
      "train: loss: 4.4202728271484375 acc: 0.8414207696914673  val: loss: 11.942852020263672 acc: 0.8152929544448853\n",
      "step: 8070 time:0.015625715255737305\n",
      "train: loss: 4.859699249267578 acc: 0.7752907276153564  val: loss: 0.9583568572998047 acc: 0.9064202904701233\n",
      "step: 8075 time:0.0030024051666259766\n",
      "train: loss: 1.295342206954956 acc: 0.8733010292053223  val: loss: 1.8168017864227295 acc: 0.8530756235122681\n",
      "step: 8080 time:0.0\n",
      "train: loss: 1.9826921224594116 acc: 0.8927919268608093  val: loss: 5.936943531036377 acc: 0.8580260276794434\n",
      "step: 8085 time:0.0\n",
      "train: loss: 3.897446870803833 acc: 0.8509408235549927  val: loss: 8.453993797302246 acc: 0.867234468460083\n",
      "step: 8090 time:0.0\n",
      "train: loss: 1.6112475395202637 acc: 0.9321489930152893  val: loss: 12.451144218444824 acc: 0.8447840809822083\n",
      "step: 8095 time:0.003002166748046875\n",
      "train: loss: 3.57293963432312 acc: 0.824648380279541  val: loss: 5.104816436767578 acc: 0.8524356484413147\n",
      "step: 8100 time:0.003002166748046875\n",
      "train: loss: 1.5021770000457764 acc: 0.9029489755630493  val: loss: 19.9112548828125 acc: 0.8663082122802734\n",
      "step: 8105 time:0.0\n",
      "train: loss: 0.6657232046127319 acc: 0.916587233543396  val: loss: 13.53890323638916 acc: 0.8159937262535095\n",
      "step: 8110 time:0.0\n",
      "train: loss: 1.9064948558807373 acc: 0.9079617261886597  val: loss: 1.610071063041687 acc: 0.9013797044754028\n",
      "step: 8115 time:0.0030019283294677734\n",
      "train: loss: 1.4662683010101318 acc: 0.8886153697967529  val: loss: 7.768338203430176 acc: 0.8814905881881714\n",
      "step: 8120 time:0.0030024051666259766\n",
      "train: loss: 1.144481897354126 acc: 0.7965372204780579  val: loss: 11.076238632202148 acc: 0.8868645429611206\n",
      "step: 8125 time:0.0\n",
      "train: loss: 1.07039475440979 acc: 0.7038951516151428  val: loss: 2.6122841835021973 acc: 0.8818925619125366\n",
      "step: 8130 time:0.003002166748046875\n",
      "train: loss: 1.7888295650482178 acc: 0.7915646433830261  val: loss: 2.7142863273620605 acc: 0.9138106107711792\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 8135 time:0.004002809524536133\n",
      "train: loss: 0.9123648405075073 acc: 0.8584891557693481  val: loss: 2.155223846435547 acc: 0.8277665376663208\n",
      "step: 8140 time:0.0\n",
      "train: loss: 0.2702659070491791 acc: 0.8786829113960266  val: loss: 5.367551326751709 acc: 0.8655447363853455\n",
      "step: 8145 time:0.0\n",
      "train: loss: 0.8443935513496399 acc: 0.8139046430587769  val: loss: 2.9028820991516113 acc: 0.8982701301574707\n",
      "step: 8150 time:0.0\n",
      "train: loss: 0.8615857362747192 acc: 0.7098323702812195  val: loss: 1.592289686203003 acc: 0.8362241983413696\n",
      "step: 8155 time:0.0030019283294677734\n",
      "train: loss: 1.462172508239746 acc: 0.7387310266494751  val: loss: 29.48489761352539 acc: 0.6652974486351013\n",
      "step: 8160 time:0.015625953674316406\n",
      "train: loss: 0.31236425042152405 acc: 0.854259192943573  val: loss: 1.522598147392273 acc: 0.8895191550254822\n",
      "step: 8165 time:0.0\n",
      "train: loss: 0.49876639246940613 acc: 0.8234078884124756  val: loss: 7.326972007751465 acc: 0.880597710609436\n",
      "step: 8170 time:0.0\n",
      "train: loss: 0.4835922420024872 acc: 0.8476176857948303  val: loss: 3.8778674602508545 acc: 0.8839723467826843\n",
      "step: 8175 time:0.0030019283294677734\n",
      "train: loss: 0.5948127508163452 acc: 0.8170788884162903  val: loss: 2.358707904815674 acc: 0.84537273645401\n",
      "step: 8180 time:0.0\n",
      "train: loss: 0.7327383756637573 acc: 0.8391879200935364  val: loss: 9.422438621520996 acc: 0.8723311424255371\n",
      "step: 8185 time:0.0030012130737304688\n",
      "train: loss: 0.654380202293396 acc: 0.858838677406311  val: loss: 3.0264828205108643 acc: 0.8157753944396973\n",
      "step: 8190 time:0.0\n",
      "train: loss: 0.338520348072052 acc: 0.8363953828811646  val: loss: 0.9621568322181702 acc: 0.8842393755912781\n",
      "step: 8195 time:0.0022890567779541016\n",
      "train: loss: 0.24675112962722778 acc: 0.8959704041481018  val: loss: 4.268616199493408 acc: 0.9032482504844666\n",
      "step: 8200 time:0.003002166748046875\n",
      "train: loss: 1.019080638885498 acc: 0.8895162343978882  val: loss: 1.4210443496704102 acc: 0.8159291744232178\n",
      "step: 8205 time:0.0\n",
      "train: loss: 2.3109402656555176 acc: 0.8791285157203674  val: loss: 1.7826359272003174 acc: 0.8208891153335571\n",
      "step: 8210 time:0.0\n",
      "train: loss: 0.3682357370853424 acc: 0.765990138053894  val: loss: 2.2023749351501465 acc: 0.8797705173492432\n",
      "step: 8215 time:0.003002166748046875\n",
      "train: loss: 2.5303609371185303 acc: 0.8554853200912476  val: loss: 3.022141695022583 acc: 0.8182356357574463\n",
      "step: 8220 time:0.0\n",
      "train: loss: 0.8161909580230713 acc: 0.847927987575531  val: loss: 2.0513453483581543 acc: 0.8655019998550415\n",
      "step: 8225 time:0.002000093460083008\n",
      "train: loss: 0.6123162508010864 acc: 0.8305072784423828  val: loss: 3.3013224601745605 acc: 0.8458607196807861\n",
      "step: 8230 time:0.0\n",
      "train: loss: 0.2998630106449127 acc: 0.8871341347694397  val: loss: 0.5339287519454956 acc: 0.8596969842910767\n",
      "step: 8235 time:0.004003286361694336\n",
      "train: loss: 0.4640936255455017 acc: 0.8335446119308472  val: loss: 2.6133008003234863 acc: 0.8886225819587708\n",
      "step: 8240 time:0.015625953674316406\n",
      "train: loss: 0.16336919367313385 acc: 0.8541525602340698  val: loss: 3.7530035972595215 acc: 0.9145277738571167\n",
      "step: 8245 time:0.0\n",
      "train: loss: 0.28316062688827515 acc: 0.9108259081840515  val: loss: 5.708943843841553 acc: 0.8927639126777649\n",
      "step: 8250 time:0.015625715255737305\n",
      "train: loss: 1.0425772666931152 acc: 0.6043605804443359  val: loss: 2.9946448802948 acc: 0.8287745714187622\n",
      "step: 8255 time:0.003002166748046875\n",
      "train: loss: 0.5916565656661987 acc: 0.8344519138336182  val: loss: 3.9499030113220215 acc: 0.8296947479248047\n",
      "step: 8260 time:0.0\n",
      "train: loss: 0.4587291479110718 acc: 0.9201720952987671  val: loss: 3.2083873748779297 acc: 0.8747839331626892\n",
      "step: 8265 time:0.0031681060791015625\n",
      "train: loss: 0.353799045085907 acc: 0.9113445281982422  val: loss: 9.343133926391602 acc: 0.9192582368850708\n",
      "step: 8270 time:0.0\n",
      "train: loss: 1.8439686298370361 acc: 0.8682565093040466  val: loss: 5.850331783294678 acc: 0.8226079344749451\n",
      "step: 8275 time:0.0030019283294677734\n",
      "train: loss: 0.568259596824646 acc: 0.8593749403953552  val: loss: 2.31969952583313 acc: 0.8734186291694641\n",
      "step: 8280 time:0.0\n",
      "train: loss: 0.4121752083301544 acc: 0.9304370880126953  val: loss: 1.7126543521881104 acc: 0.9074691534042358\n",
      "step: 8285 time:0.0030014514923095703\n",
      "train: loss: 0.9440143704414368 acc: 0.9179943203926086  val: loss: 6.1647515296936035 acc: 0.8832343816757202\n",
      "step: 8290 time:0.0\n",
      "train: loss: 1.6059305667877197 acc: 0.8882954716682434  val: loss: 1.8506735563278198 acc: 0.8613998293876648\n",
      "step: 8295 time:0.003001689910888672\n",
      "train: loss: 3.0641796588897705 acc: 0.9062484502792358  val: loss: 11.448519706726074 acc: 0.7298694252967834\n",
      "step: 8300 time:0.003001689910888672\n",
      "train: loss: 1.1331021785736084 acc: 0.7570353746414185  val: loss: 3.607741117477417 acc: 0.877293586730957\n",
      "step: 8305 time:0.015865087509155273\n",
      "train: loss: 0.9399037957191467 acc: 0.9065377712249756  val: loss: 4.708510398864746 acc: 0.8765404224395752\n",
      "step: 8310 time:0.0\n",
      "train: loss: 2.127281665802002 acc: 0.8538027405738831  val: loss: 3.035442590713501 acc: 0.8762471079826355\n",
      "step: 8315 time:0.003001689910888672\n",
      "train: loss: 1.5152539014816284 acc: 0.8341178894042969  val: loss: 11.55859375 acc: 0.8578377962112427\n",
      "step: 8320 time:0.015625953674316406\n",
      "train: loss: 3.4423487186431885 acc: 0.9114943742752075  val: loss: 18.982805252075195 acc: 0.8152279257774353\n",
      "step: 8325 time:0.0\n",
      "train: loss: 1.1848284006118774 acc: 0.8891469240188599  val: loss: 1.6717782020568848 acc: 0.8991366624832153\n",
      "step: 8330 time:0.0\n",
      "train: loss: 1.2629024982452393 acc: 0.896628737449646  val: loss: 1.9278843402862549 acc: 0.8739327192306519\n",
      "step: 8335 time:0.0030019283294677734\n",
      "train: loss: 2.7456419467926025 acc: 0.8550758361816406  val: loss: 3.4914121627807617 acc: 0.839975118637085\n",
      "step: 8340 time:0.0\n",
      "train: loss: 9.138643264770508 acc: 0.8224437236785889  val: loss: 1.8096387386322021 acc: 0.8967403173446655\n",
      "step: 8345 time:0.0\n",
      "train: loss: 19.04722023010254 acc: 0.7440019845962524  val: loss: 9.50756549835205 acc: 0.8223247528076172\n",
      "step: 8350 time:0.0\n",
      "train: loss: 6.827014923095703 acc: 0.7521164417266846  val: loss: 2.784423351287842 acc: 0.8763179183006287\n",
      "step: 8355 time:0.0030019283294677734\n",
      "train: loss: 5.688181400299072 acc: 0.6852860450744629  val: loss: 3.6664600372314453 acc: 0.7315054535865784\n",
      "step: 8360 time:0.0020012855529785156\n",
      "train: loss: 4.646142482757568 acc: 0.7315449118614197  val: loss: 2.832034111022949 acc: 0.8627804517745972\n",
      "step: 8365 time:0.0\n",
      "train: loss: 5.719965934753418 acc: 0.8698294162750244  val: loss: 6.805694580078125 acc: 0.8619521260261536\n",
      "step: 8370 time:0.0030019283294677734\n",
      "train: loss: 5.011247634887695 acc: 0.5893225073814392  val: loss: 1.4218783378601074 acc: 0.8886958956718445\n",
      "step: 8375 time:0.003001689910888672\n",
      "train: loss: 9.454498291015625 acc: 0.8794995546340942  val: loss: 2.7394495010375977 acc: 0.7573428750038147\n",
      "step: 8380 time:0.004002571105957031\n",
      "train: loss: 11.939413070678711 acc: 0.9299031496047974  val: loss: 3.2832112312316895 acc: 0.8808966875076294\n",
      "step: 8385 time:0.0\n",
      "train: loss: 7.231630325317383 acc: 0.9207842946052551  val: loss: 2.1033987998962402 acc: 0.8625107407569885\n",
      "step: 8390 time:0.0\n",
      "train: loss: 2.0871031284332275 acc: 0.8625534772872925  val: loss: 2.2095513343811035 acc: 0.9019202589988708\n",
      "step: 8395 time:0.0030019283294677734\n",
      "train: loss: 5.62971830368042 acc: 0.8553289771080017  val: loss: 6.32267951965332 acc: 0.8902705311775208\n",
      "step: 8400 time:0.003001689910888672\n",
      "train: loss: 2.9301767349243164 acc: 0.8616224527359009  val: loss: 1.7231050729751587 acc: 0.899803876876831\n",
      "step: 8405 time:0.015626192092895508\n",
      "train: loss: 5.091019153594971 acc: 0.850284218788147  val: loss: 1.070276141166687 acc: 0.9116446375846863\n",
      "step: 8410 time:0.0\n",
      "train: loss: 10.286993026733398 acc: 0.781834065914154  val: loss: 5.824585914611816 acc: 0.9397983551025391\n",
      "step: 8415 time:0.003001689910888672\n",
      "train: loss: 6.175941467285156 acc: 0.8269336223602295  val: loss: 4.515313148498535 acc: 0.8285098075866699\n",
      "step: 8420 time:0.0\n",
      "train: loss: 3.513650894165039 acc: 0.8540583252906799  val: loss: 4.466365814208984 acc: 0.8996100425720215\n",
      "step: 8425 time:0.0\n",
      "train: loss: 2.1055169105529785 acc: 0.9098287224769592  val: loss: 0.6843530535697937 acc: 0.9160006046295166\n",
      "step: 8430 time:0.015625476837158203\n",
      "train: loss: 1.751910924911499 acc: 0.8926026821136475  val: loss: 0.8785993456840515 acc: 0.8550102710723877\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 8435 time:0.0030019283294677734\n",
      "train: loss: 1.4625686407089233 acc: 0.8378690481185913  val: loss: 2.9264097213745117 acc: 0.876741886138916\n",
      "step: 8440 time:0.0\n",
      "train: loss: 6.855474948883057 acc: 0.8750733733177185  val: loss: 1.5873812437057495 acc: 0.835769534111023\n",
      "step: 8445 time:0.0030019283294677734\n",
      "train: loss: 4.089538097381592 acc: 0.7969224452972412  val: loss: 18.65704917907715 acc: 0.7676512598991394\n",
      "step: 8450 time:0.0\n",
      "train: loss: 1.3804960250854492 acc: 0.8446297645568848  val: loss: 4.107906818389893 acc: 0.7445879578590393\n",
      "step: 8455 time:0.0030019283294677734\n",
      "train: loss: 1.8077313899993896 acc: 0.8606466054916382  val: loss: 18.797792434692383 acc: 0.7938591241836548\n",
      "step: 8460 time:0.0030019283294677734\n",
      "train: loss: 0.955436646938324 acc: 0.9423512816429138  val: loss: 4.118372917175293 acc: 0.9072204828262329\n",
      "step: 8465 time:0.0\n",
      "train: loss: 1.1456129550933838 acc: 0.9412720799446106  val: loss: 22.065715789794922 acc: 0.8278533816337585\n",
      "step: 8470 time:0.0156252384185791\n",
      "train: loss: 2.9516968727111816 acc: 0.9101917147636414  val: loss: 6.170826435089111 acc: 0.8256702423095703\n",
      "step: 8475 time:0.003002166748046875\n",
      "train: loss: 0.7330352663993835 acc: 0.9489771127700806  val: loss: 0.9048522114753723 acc: 0.9040987491607666\n",
      "step: 8480 time:0.0030019283294677734\n",
      "train: loss: 1.3268089294433594 acc: 0.9347296357154846  val: loss: 2.3524370193481445 acc: 0.8081109523773193\n",
      "step: 8485 time:0.0\n",
      "train: loss: 0.8433120250701904 acc: 0.9061574339866638  val: loss: 1.8438820838928223 acc: 0.8742759823799133\n",
      "step: 8490 time:0.0030019283294677734\n",
      "train: loss: 2.7245233058929443 acc: 0.8554619550704956  val: loss: 5.229165077209473 acc: 0.8188836574554443\n",
      "step: 8495 time:0.0030012130737304688\n",
      "train: loss: 1.8184024095535278 acc: 0.8527214527130127  val: loss: 11.521682739257812 acc: 0.627343475818634\n",
      "step: 8500 time:0.0\n",
      "train: loss: 0.6591585874557495 acc: 0.9587453007698059  val: loss: 5.094205379486084 acc: 0.8346734046936035\n",
      "step: 8505 time:0.0\n",
      "train: loss: 0.9881400465965271 acc: 0.9231159090995789  val: loss: 1.034285306930542 acc: 0.8753895163536072\n",
      "step: 8510 time:0.015625953674316406\n",
      "train: loss: 3.399878978729248 acc: 0.8096441626548767  val: loss: 1.4939519166946411 acc: 0.854336142539978\n",
      "step: 8515 time:0.003001689910888672\n",
      "train: loss: 1.4064106941223145 acc: 0.8601377606391907  val: loss: 3.8744537830352783 acc: 0.8579090237617493\n",
      "step: 8520 time:0.0\n",
      "train: loss: 0.8820290565490723 acc: 0.7651189565658569  val: loss: 2.3589577674865723 acc: 0.9057218432426453\n",
      "step: 8525 time:0.0030019283294677734\n",
      "train: loss: 3.839156150817871 acc: 0.8793460130691528  val: loss: 4.354450225830078 acc: 0.8771761059761047\n",
      "step: 8530 time:0.0\n",
      "train: loss: 2.9559645652770996 acc: 0.8507142066955566  val: loss: 3.4372124671936035 acc: 0.7884183526039124\n",
      "step: 8535 time:0.0030019283294677734\n",
      "train: loss: 1.982419490814209 acc: 0.8002408742904663  val: loss: 3.978464365005493 acc: 0.828904390335083\n",
      "step: 8540 time:0.015626192092895508\n",
      "train: loss: 2.4495608806610107 acc: 0.7989126443862915  val: loss: 15.72995662689209 acc: 0.7826375961303711\n",
      "step: 8545 time:0.0030014514923095703\n",
      "train: loss: 3.700705051422119 acc: 0.810165524482727  val: loss: 4.595373630523682 acc: 0.7084143161773682\n",
      "step: 8550 time:0.0\n",
      "train: loss: 8.313436508178711 acc: 0.5088102221488953  val: loss: 8.240747451782227 acc: 0.8326953649520874\n",
      "step: 8555 time:0.003002166748046875\n",
      "train: loss: 3.0159807205200195 acc: 0.7891243100166321  val: loss: 4.111753463745117 acc: 0.732300877571106\n",
      "step: 8560 time:0.0030019283294677734\n",
      "train: loss: 3.594451904296875 acc: 0.8364083766937256  val: loss: 4.153848171234131 acc: 0.8469280004501343\n",
      "step: 8565 time:0.015626907348632812\n",
      "train: loss: 3.0132269859313965 acc: 0.8488402366638184  val: loss: 2.3227920532226562 acc: 0.8969274163246155\n",
      "step: 8570 time:0.015625\n",
      "train: loss: 0.7930321097373962 acc: 0.9092008471488953  val: loss: 1.3101699352264404 acc: 0.8831650614738464\n",
      "step: 8575 time:0.004002571105957031\n",
      "train: loss: 5.2808074951171875 acc: 0.8536390662193298  val: loss: 2.286757469177246 acc: 0.881866991519928\n",
      "step: 8580 time:0.0\n",
      "train: loss: 1.178114891052246 acc: 0.8869469165802002  val: loss: 0.685284435749054 acc: 0.8874109983444214\n",
      "step: 8585 time:0.0\n",
      "train: loss: 3.133113384246826 acc: 0.9247133135795593  val: loss: 3.3248090744018555 acc: 0.7581359148025513\n",
      "step: 8590 time:0.0\n",
      "train: loss: 4.849021911621094 acc: 0.8593577742576599  val: loss: 0.9950106739997864 acc: 0.8765722513198853\n",
      "step: 8595 time:0.003001689910888672\n",
      "train: loss: 1.029006004333496 acc: 0.8850160837173462  val: loss: 0.8021076321601868 acc: 0.8971254825592041\n",
      "step: 8600 time:0.0\n",
      "train: loss: 2.83736252784729 acc: 0.8887962102890015  val: loss: 2.9049298763275146 acc: 0.8828383088111877\n",
      "step: 8605 time:0.0\n",
      "train: loss: 0.4448446035385132 acc: 0.8794040083885193  val: loss: 1.0606471300125122 acc: 0.917751669883728\n",
      "step: 8610 time:0.0030024051666259766\n",
      "train: loss: 6.0710296630859375 acc: 0.895027756690979  val: loss: 2.873101234436035 acc: 0.7904711365699768\n",
      "step: 8615 time:0.0\n",
      "train: loss: 1.5670161247253418 acc: 0.9498221278190613  val: loss: 0.577396810054779 acc: 0.9118973016738892\n",
      "step: 8620 time:0.017160654067993164\n",
      "train: loss: 2.4173388481140137 acc: 0.9116664528846741  val: loss: 1.7219667434692383 acc: 0.90229731798172\n",
      "step: 8625 time:0.0030019283294677734\n",
      "train: loss: 1.7327687740325928 acc: 0.9412438273429871  val: loss: 1.1688520908355713 acc: 0.8671817779541016\n",
      "step: 8630 time:0.0\n",
      "train: loss: 2.4766130447387695 acc: 0.9128355979919434  val: loss: 6.594847679138184 acc: 0.8703309297561646\n",
      "step: 8635 time:0.00400233268737793\n",
      "train: loss: 1.0602210760116577 acc: 0.9035413861274719  val: loss: 0.695462703704834 acc: 0.8589439392089844\n",
      "step: 8640 time:0.0\n",
      "train: loss: 1.066990852355957 acc: 0.7979068160057068  val: loss: 1.2881622314453125 acc: 0.8895540237426758\n",
      "step: 8645 time:0.0\n",
      "train: loss: 0.2654982805252075 acc: 0.890597939491272  val: loss: 4.100028038024902 acc: 0.9286835193634033\n",
      "step: 8650 time:0.0\n",
      "train: loss: 0.9836657047271729 acc: 0.7661274075508118  val: loss: 1.534801721572876 acc: 0.8917096257209778\n",
      "step: 8655 time:0.003002166748046875\n",
      "train: loss: 0.237701877951622 acc: 0.8850786089897156  val: loss: 1.4760823249816895 acc: 0.8629275560379028\n",
      "step: 8660 time:0.0\n",
      "train: loss: 6.637246608734131 acc: 0.8385170698165894  val: loss: 4.5218048095703125 acc: 0.8583348393440247\n",
      "step: 8665 time:0.0\n",
      "train: loss: 0.301530122756958 acc: 0.8948249220848083  val: loss: 3.4459447860717773 acc: 0.859255850315094\n",
      "step: 8670 time:0.0\n",
      "train: loss: 0.29887858033180237 acc: 0.8698955774307251  val: loss: 1.0292794704437256 acc: 0.9238776564598083\n",
      "step: 8675 time:0.003002166748046875\n",
      "train: loss: 0.5247056484222412 acc: 0.8873906135559082  val: loss: 4.879200458526611 acc: 0.9129554629325867\n",
      "step: 8680 time:0.0\n",
      "train: loss: 1.2581579685211182 acc: 0.780143678188324  val: loss: 3.752180576324463 acc: 0.8286944031715393\n",
      "step: 8685 time:0.015625953674316406\n",
      "train: loss: 0.26795080304145813 acc: 0.873768150806427  val: loss: 1.5420953035354614 acc: 0.9125488996505737\n",
      "step: 8690 time:0.0\n",
      "train: loss: 1.6541392803192139 acc: 0.7822247743606567  val: loss: 3.1345579624176025 acc: 0.7558406591415405\n",
      "step: 8695 time:0.0030024051666259766\n",
      "train: loss: 0.5465276837348938 acc: 0.6586465835571289  val: loss: 1.5755506753921509 acc: 0.848847508430481\n",
      "step: 8700 time:0.003002166748046875\n",
      "train: loss: 0.8627164363861084 acc: 0.9030956029891968  val: loss: 1.1127057075500488 acc: 0.8774650692939758\n",
      "step: 8705 time:0.015625953674316406\n",
      "train: loss: 0.6482009887695312 acc: 0.7036964297294617  val: loss: 1.277621865272522 acc: 0.8446376919746399\n",
      "step: 8710 time:0.0\n",
      "train: loss: 0.5811408758163452 acc: 0.8606115579605103  val: loss: 1.7495825290679932 acc: 0.875637948513031\n",
      "step: 8715 time:0.0030019283294677734\n",
      "train: loss: 0.355695515871048 acc: 0.8612778186798096  val: loss: 3.7351064682006836 acc: 0.8882703185081482\n",
      "step: 8720 time:0.015626192092895508\n",
      "train: loss: 0.4002387225627899 acc: 0.8667623400688171  val: loss: 1.6205503940582275 acc: 0.9221281409263611\n",
      "step: 8725 time:0.0\n",
      "train: loss: 0.5497759580612183 acc: 0.8711817264556885  val: loss: 4.154830455780029 acc: 0.8680015802383423\n",
      "step: 8730 time:0.0\n",
      "train: loss: 2.2309675216674805 acc: 0.8566548824310303  val: loss: 4.160688400268555 acc: 0.8412583470344543\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 8735 time:0.0030014514923095703\n",
      "train: loss: 0.5582685470581055 acc: 0.8761193752288818  val: loss: 1.3313552141189575 acc: 0.9063164591789246\n",
      "step: 8740 time:0.0\n",
      "train: loss: 1.6915208101272583 acc: 0.8811846971511841  val: loss: 1.3264200687408447 acc: 0.8953093886375427\n",
      "step: 8745 time:0.0\n",
      "train: loss: 0.8086194396018982 acc: 0.8833602070808411  val: loss: 2.1736621856689453 acc: 0.8978628516197205\n",
      "step: 8750 time:0.0\n",
      "train: loss: 0.7067067623138428 acc: 0.7947510480880737  val: loss: 6.346366882324219 acc: 0.8832969069480896\n",
      "step: 8755 time:0.003002166748046875\n",
      "train: loss: 0.6306198239326477 acc: 0.8858863115310669  val: loss: 0.7253110408782959 acc: 0.9267133474349976\n",
      "step: 8760 time:0.0\n",
      "train: loss: 0.6312079429626465 acc: 0.7409583330154419  val: loss: 1.181487798690796 acc: 0.8107346296310425\n",
      "step: 8765 time:0.015625715255737305\n",
      "train: loss: 0.25732743740081787 acc: 0.8542013764381409  val: loss: 7.7947845458984375 acc: 0.9089728593826294\n",
      "step: 8770 time:0.0\n",
      "train: loss: 0.3863353729248047 acc: 0.8374656438827515  val: loss: 4.761195659637451 acc: 0.8959317207336426\n",
      "step: 8775 time:0.0030012130737304688\n",
      "train: loss: 1.450344443321228 acc: 0.8563065528869629  val: loss: 6.335326194763184 acc: 0.8755791783332825\n",
      "step: 8780 time:0.015625476837158203\n",
      "train: loss: 0.5832598805427551 acc: 0.9004533290863037  val: loss: 6.580236911773682 acc: 0.8753165602684021\n",
      "step: 8785 time:0.003069162368774414\n",
      "train: loss: 1.885427713394165 acc: 0.9105217456817627  val: loss: 6.360024452209473 acc: 0.8635489344596863\n",
      "step: 8790 time:0.0030019283294677734\n",
      "train: loss: 0.919782817363739 acc: 0.9008777141571045  val: loss: 0.6997747421264648 acc: 0.9061642289161682\n",
      "step: 8795 time:0.003001689910888672\n",
      "train: loss: 0.7081143856048584 acc: 0.8796600699424744  val: loss: 4.332254409790039 acc: 0.8501260280609131\n",
      "step: 8800 time:0.004002571105957031\n",
      "train: loss: 1.2780646085739136 acc: 0.7838868498802185  val: loss: 2.903118133544922 acc: 0.891147792339325\n",
      "step: 8805 time:0.0\n",
      "train: loss: 1.3522460460662842 acc: 0.9217383861541748  val: loss: 1.9168678522109985 acc: 0.8866783380508423\n",
      "step: 8810 time:0.0\n",
      "train: loss: 2.689138889312744 acc: 0.91330486536026  val: loss: 5.420126914978027 acc: 0.899065375328064\n",
      "step: 8815 time:0.0030019283294677734\n",
      "train: loss: 2.2114996910095215 acc: 0.8123137950897217  val: loss: 7.617129325866699 acc: 0.8550074696540833\n",
      "step: 8820 time:0.004002571105957031\n",
      "train: loss: 1.2883012294769287 acc: 0.833595871925354  val: loss: 1.5769144296646118 acc: 0.8698408007621765\n",
      "step: 8825 time:0.0\n",
      "train: loss: 1.4514429569244385 acc: 0.8185425996780396  val: loss: 3.799327850341797 acc: 0.9214882850646973\n",
      "step: 8830 time:0.0\n",
      "train: loss: 1.7531440258026123 acc: 0.9123446941375732  val: loss: 6.6435065269470215 acc: 0.8285801410675049\n",
      "step: 8835 time:0.003002166748046875\n",
      "train: loss: 2.232409715652466 acc: 0.9029057025909424  val: loss: 2.671332836151123 acc: 0.8744078874588013\n",
      "step: 8840 time:0.0\n",
      "train: loss: 0.5771432518959045 acc: 0.8993003368377686  val: loss: 1.5887078046798706 acc: 0.9055868983268738\n",
      "step: 8845 time:0.0\n",
      "train: loss: 1.0049781799316406 acc: 0.9241292476654053  val: loss: 0.5098345875740051 acc: 0.9130313396453857\n",
      "step: 8850 time:0.0\n",
      "train: loss: 3.354137897491455 acc: 0.9039245843887329  val: loss: 0.901436448097229 acc: 0.909186601638794\n",
      "step: 8855 time:0.0030019283294677734\n",
      "train: loss: 1.966155767440796 acc: 0.8294656276702881  val: loss: 1.4395021200180054 acc: 0.8754594326019287\n",
      "step: 8860 time:0.003001689910888672\n",
      "train: loss: 5.095370292663574 acc: 0.7337547540664673  val: loss: 5.532382965087891 acc: 0.788634717464447\n",
      "step: 8865 time:0.0\n",
      "train: loss: 8.50102424621582 acc: 0.7040271759033203  val: loss: 10.576013565063477 acc: 0.7843765616416931\n",
      "step: 8870 time:0.0\n",
      "train: loss: 6.5938849449157715 acc: 0.7476899027824402  val: loss: 2.839386463165283 acc: 0.8603324890136719\n",
      "step: 8875 time:0.0030024051666259766\n",
      "train: loss: 4.19014835357666 acc: 0.8798434138298035  val: loss: 16.55533218383789 acc: 0.6840291023254395\n",
      "step: 8880 time:0.0030019283294677734\n",
      "train: loss: 2.128790855407715 acc: 0.8011290431022644  val: loss: 0.8833627700805664 acc: 0.8583623766899109\n",
      "step: 8885 time:0.015625953674316406\n",
      "train: loss: 4.288519859313965 acc: 0.8091636896133423  val: loss: 2.2323646545410156 acc: 0.8878510594367981\n",
      "step: 8890 time:0.0\n",
      "train: loss: 8.928888320922852 acc: 0.8609024286270142  val: loss: 15.943480491638184 acc: 0.7984651923179626\n",
      "step: 8895 time:0.004001617431640625\n",
      "train: loss: 5.072399139404297 acc: 0.8924787044525146  val: loss: 2.089426279067993 acc: 0.89488285779953\n",
      "step: 8900 time:0.0\n",
      "train: loss: 5.808313369750977 acc: 0.8200671672821045  val: loss: 1.0677950382232666 acc: 0.8969405293464661\n",
      "step: 8905 time:0.015626192092895508\n",
      "train: loss: 7.630917549133301 acc: 0.8887031674385071  val: loss: 3.291130781173706 acc: 0.8493696451187134\n",
      "step: 8910 time:0.0030007362365722656\n",
      "train: loss: 3.68033766746521 acc: 0.8936857581138611  val: loss: 2.0945146083831787 acc: 0.9119393229484558\n",
      "step: 8915 time:0.003002166748046875\n",
      "train: loss: 5.716969966888428 acc: 0.7418285012245178  val: loss: 1.1955899000167847 acc: 0.9215860962867737\n",
      "step: 8920 time:0.003002166748046875\n",
      "train: loss: 2.6388461589813232 acc: 0.8883896470069885  val: loss: 4.145508289337158 acc: 0.8888071179389954\n",
      "step: 8925 time:0.0\n",
      "train: loss: 3.024829626083374 acc: 0.7807287573814392  val: loss: 17.26586151123047 acc: 0.7785268425941467\n",
      "step: 8930 time:0.0\n",
      "train: loss: 2.1126136779785156 acc: 0.900713324546814  val: loss: 17.05144500732422 acc: 0.7872021198272705\n",
      "step: 8935 time:0.003002166748046875\n",
      "train: loss: 1.8776910305023193 acc: 0.8752193450927734  val: loss: 2.013195276260376 acc: 0.9225751161575317\n",
      "step: 8940 time:0.0\n",
      "train: loss: 2.945342779159546 acc: 0.8206337690353394  val: loss: 2.2927539348602295 acc: 0.8758165240287781\n",
      "step: 8945 time:0.0\n",
      "train: loss: 6.0714521408081055 acc: 0.568087637424469  val: loss: 2.603203535079956 acc: 0.8428535461425781\n",
      "step: 8950 time:0.0\n",
      "train: loss: 0.7079485654830933 acc: 0.8610243797302246  val: loss: 6.150111198425293 acc: 0.8714134097099304\n",
      "step: 8955 time:0.003002643585205078\n",
      "train: loss: 2.2930266857147217 acc: 0.8937748074531555  val: loss: 2.7265892028808594 acc: 0.8762428164482117\n",
      "step: 8960 time:0.0\n",
      "train: loss: 9.000382423400879 acc: 0.7113393545150757  val: loss: 7.474634647369385 acc: 0.7815001606941223\n",
      "step: 8965 time:0.015626192092895508\n",
      "train: loss: 3.8065524101257324 acc: 0.8788515329360962  val: loss: 3.701228618621826 acc: 0.7623634934425354\n",
      "step: 8970 time:0.001415252685546875\n",
      "train: loss: 0.764594554901123 acc: 0.8282209634780884  val: loss: 3.782700538635254 acc: 0.884211540222168\n",
      "step: 8975 time:0.002001523971557617\n",
      "train: loss: 0.9915971755981445 acc: 0.9410973787307739  val: loss: 1.5754687786102295 acc: 0.9102048277854919\n",
      "step: 8980 time:0.0\n",
      "train: loss: 1.1223294734954834 acc: 0.9239944219589233  val: loss: 14.810640335083008 acc: 0.8004385232925415\n",
      "step: 8985 time:0.0\n",
      "train: loss: 2.0058979988098145 acc: 0.87746661901474  val: loss: 6.08167839050293 acc: 0.8703445196151733\n",
      "step: 8990 time:0.0\n",
      "train: loss: 2.8449621200561523 acc: 0.9403654932975769  val: loss: 1.311386227607727 acc: 0.8328067064285278\n",
      "step: 8995 time:0.003001689910888672\n",
      "train: loss: 2.288892984390259 acc: 0.9390315413475037  val: loss: 3.976832389831543 acc: 0.9019505977630615\n",
      "step: 9000 time:0.0030019283294677734\n",
      "train: loss: 2.02620792388916 acc: 0.90193110704422  val: loss: 4.526123523712158 acc: 0.8121782541275024\n",
      "step: 9005 time:0.0\n",
      "train: loss: 1.0607678890228271 acc: 0.9089158773422241  val: loss: 3.4240834712982178 acc: 0.8706454038619995\n",
      "step: 9010 time:0.0020008087158203125\n",
      "train: loss: 0.6713823676109314 acc: 0.9323237538337708  val: loss: 7.018570899963379 acc: 0.9026975631713867\n",
      "step: 9015 time:0.0030014514923095703\n",
      "train: loss: 1.2032527923583984 acc: 0.9106570482254028  val: loss: 3.0078256130218506 acc: 0.8727530837059021\n",
      "step: 9020 time:0.0\n",
      "train: loss: 1.4593710899353027 acc: 0.8801868557929993  val: loss: 3.6231634616851807 acc: 0.8954960107803345\n",
      "step: 9025 time:0.0\n",
      "train: loss: 2.3751018047332764 acc: 0.9410858154296875  val: loss: 0.8184617757797241 acc: 0.8362009525299072\n",
      "step: 9030 time:0.0\n",
      "train: loss: 0.742185115814209 acc: 0.8572138547897339  val: loss: 3.874545097351074 acc: 0.8704807162284851\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 9035 time:0.00400233268737793\n",
      "train: loss: 0.5874280333518982 acc: 0.9259472489356995  val: loss: 1.1862894296646118 acc: 0.8329399824142456\n",
      "step: 9040 time:0.0\n",
      "train: loss: 1.073245644569397 acc: 0.9187642335891724  val: loss: 3.639493465423584 acc: 0.8700761198997498\n",
      "step: 9045 time:0.017531871795654297\n",
      "train: loss: 2.527336597442627 acc: 0.8804194927215576  val: loss: 2.1152429580688477 acc: 0.8310191035270691\n",
      "step: 9050 time:0.0018639564514160156\n",
      "train: loss: 4.859942436218262 acc: 0.6377526521682739  val: loss: 1.531465768814087 acc: 0.8688608407974243\n",
      "step: 9055 time:0.0030014514923095703\n",
      "train: loss: 2.937899589538574 acc: 0.601006269454956  val: loss: 1.5917749404907227 acc: 0.8425034880638123\n",
      "step: 9060 time:0.0\n",
      "train: loss: 3.497157573699951 acc: 0.6614717245101929  val: loss: 8.781161308288574 acc: 0.8182483911514282\n",
      "step: 9065 time:0.017042160034179688\n",
      "train: loss: 3.543534755706787 acc: 0.8144620656967163  val: loss: 2.2193477153778076 acc: 0.8914890885353088\n",
      "step: 9070 time:0.0\n",
      "train: loss: 3.6771481037139893 acc: 0.7841386198997498  val: loss: 1.9623721837997437 acc: 0.8615995049476624\n",
      "step: 9075 time:0.003001689910888672\n",
      "train: loss: 1.4599432945251465 acc: 0.8454324007034302  val: loss: 2.038722515106201 acc: 0.8822725415229797\n",
      "step: 9080 time:0.0\n",
      "train: loss: 5.797664165496826 acc: 0.8594321012496948  val: loss: 3.031769275665283 acc: 0.8487297892570496\n",
      "step: 9085 time:0.0\n",
      "train: loss: 2.2298853397369385 acc: 0.8806140422821045  val: loss: 2.557657241821289 acc: 0.8483108282089233\n",
      "step: 9090 time:0.0\n",
      "train: loss: 3.7335519790649414 acc: 0.9078984260559082  val: loss: 6.4531450271606445 acc: 0.8752044439315796\n",
      "step: 9095 time:0.0030019283294677734\n",
      "train: loss: 1.286717414855957 acc: 0.8255921006202698  val: loss: 3.51831316947937 acc: 0.7732019424438477\n",
      "step: 9100 time:0.0\n",
      "train: loss: 1.3752481937408447 acc: 0.828105628490448  val: loss: 0.3562113642692566 acc: 0.9055707454681396\n",
      "step: 9105 time:0.015625476837158203\n",
      "train: loss: 2.2979111671447754 acc: 0.8752222657203674  val: loss: 3.472062110900879 acc: 0.9104896783828735\n",
      "step: 9110 time:0.0\n",
      "train: loss: 2.9330334663391113 acc: 0.8645896911621094  val: loss: 2.3905792236328125 acc: 0.8952021598815918\n",
      "step: 9115 time:0.003001689910888672\n",
      "train: loss: 2.245544672012329 acc: 0.9068748950958252  val: loss: 1.638132095336914 acc: 0.8435864448547363\n",
      "step: 9120 time:0.015625476837158203\n",
      "train: loss: 2.5730583667755127 acc: 0.8674852252006531  val: loss: 15.611858367919922 acc: 0.8438351154327393\n",
      "step: 9125 time:0.0\n",
      "train: loss: 3.0688834190368652 acc: 0.8429349660873413  val: loss: 1.3302171230316162 acc: 0.9001700282096863\n",
      "step: 9130 time:0.0030007362365722656\n",
      "train: loss: 2.1909077167510986 acc: 0.8998221755027771  val: loss: 1.0827608108520508 acc: 0.8426313400268555\n",
      "step: 9135 time:0.003000974655151367\n",
      "train: loss: 3.2634429931640625 acc: 0.8729340434074402  val: loss: 1.383995771408081 acc: 0.8832167387008667\n",
      "step: 9140 time:0.0\n",
      "train: loss: 2.190948486328125 acc: 0.8887909650802612  val: loss: 1.7936129570007324 acc: 0.8778976202011108\n",
      "step: 9145 time:0.015626192092895508\n",
      "train: loss: 0.5538849830627441 acc: 0.9077642560005188  val: loss: 13.960365295410156 acc: 0.8197500109672546\n",
      "step: 9150 time:0.0\n",
      "train: loss: 3.020270347595215 acc: 0.9437274932861328  val: loss: 1.9808719158172607 acc: 0.8589379191398621\n",
      "step: 9155 time:0.0030019283294677734\n",
      "train: loss: 0.4435540437698364 acc: 0.9049152731895447  val: loss: 2.448944091796875 acc: 0.9242680668830872\n",
      "step: 9160 time:0.0\n",
      "train: loss: 0.7570579051971436 acc: 0.8427849411964417  val: loss: 13.983891487121582 acc: 0.83430415391922\n",
      "step: 9165 time:0.0\n",
      "train: loss: 1.524846076965332 acc: 0.7758996486663818  val: loss: 6.109675407409668 acc: 0.7763178944587708\n",
      "step: 9170 time:0.003001689910888672\n",
      "train: loss: 0.40115073323249817 acc: 0.8498870730400085  val: loss: 1.2612535953521729 acc: 0.912426233291626\n",
      "step: 9175 time:0.0030024051666259766\n",
      "train: loss: 0.44498372077941895 acc: 0.8816922307014465  val: loss: 0.9628217220306396 acc: 0.92256760597229\n",
      "step: 9180 time:0.0\n",
      "train: loss: 0.3264017105102539 acc: 0.780088484287262  val: loss: 2.52817702293396 acc: 0.8885257840156555\n",
      "step: 9185 time:0.0\n",
      "train: loss: 0.34447038173675537 acc: 0.9268525242805481  val: loss: 1.9363147020339966 acc: 0.8987967371940613\n",
      "step: 9190 time:0.015625953674316406\n",
      "train: loss: 0.8453076481819153 acc: 0.8775504231452942  val: loss: 0.6364842653274536 acc: 0.8852036595344543\n",
      "step: 9195 time:0.003001689910888672\n",
      "train: loss: 0.4952879548072815 acc: 0.7812207937240601  val: loss: 4.360238075256348 acc: 0.8667792081832886\n",
      "step: 9200 time:0.015625476837158203\n",
      "train: loss: 1.010004997253418 acc: 0.8372933268547058  val: loss: 1.5356128215789795 acc: 0.8985297679901123\n",
      "step: 9205 time:0.0\n",
      "train: loss: 0.658726692199707 acc: 0.8499519228935242  val: loss: 2.0152385234832764 acc: 0.8508474826812744\n",
      "step: 9210 time:0.0\n",
      "train: loss: 0.8205655813217163 acc: 0.7756775617599487  val: loss: 2.6662256717681885 acc: 0.9007152318954468\n",
      "step: 9215 time:0.018948793411254883\n",
      "train: loss: 0.314003586769104 acc: 0.9227260947227478  val: loss: 4.240485668182373 acc: 0.8660944700241089\n",
      "step: 9220 time:0.0\n",
      "train: loss: 0.4810248017311096 acc: 0.8267599940299988  val: loss: 1.607433795928955 acc: 0.8865951299667358\n",
      "step: 9225 time:0.0\n",
      "train: loss: 0.9298757314682007 acc: 0.46992117166519165  val: loss: 2.0096030235290527 acc: 0.8635874390602112\n",
      "step: 9230 time:0.003002166748046875\n",
      "train: loss: 0.24746760725975037 acc: 0.8460190892219543  val: loss: 1.4088633060455322 acc: 0.9326730966567993\n",
      "step: 9235 time:0.003001689910888672\n",
      "train: loss: 0.22718611359596252 acc: 0.8977542519569397  val: loss: 0.5040130615234375 acc: 0.8844568729400635\n",
      "step: 9240 time:0.0\n",
      "train: loss: 0.3963313102722168 acc: 0.8090478181838989  val: loss: 3.8640036582946777 acc: 0.8958839178085327\n",
      "step: 9245 time:0.0\n",
      "train: loss: 0.9754844903945923 acc: 0.8448963165283203  val: loss: 5.339897155761719 acc: 0.8653425574302673\n",
      "step: 9250 time:0.015626192092895508\n",
      "train: loss: 0.9444034099578857 acc: 0.7331691384315491  val: loss: 2.0462806224823 acc: 0.9055206179618835\n",
      "step: 9255 time:0.0030014514923095703\n",
      "train: loss: 1.6042413711547852 acc: 0.8540250658988953  val: loss: 1.981549859046936 acc: 0.8395909070968628\n",
      "step: 9260 time:0.0030019283294677734\n",
      "train: loss: 0.23921382427215576 acc: 0.913479208946228  val: loss: 3.860759973526001 acc: 0.9039836525917053\n",
      "step: 9265 time:0.0\n",
      "train: loss: 0.24633750319480896 acc: 0.9066853523254395  val: loss: 1.3486106395721436 acc: 0.9162390232086182\n",
      "step: 9270 time:0.0030012130737304688\n",
      "train: loss: 0.7004185914993286 acc: 0.7906352877616882  val: loss: 3.4190404415130615 acc: 0.8704210519790649\n",
      "step: 9275 time:0.0030019283294677734\n",
      "train: loss: 0.7220752835273743 acc: 0.8673496842384338  val: loss: 14.746231079101562 acc: 0.809718906879425\n",
      "step: 9280 time:0.0\n",
      "train: loss: 0.6836180090904236 acc: 0.8789001107215881  val: loss: 0.9201732873916626 acc: 0.9139965176582336\n",
      "step: 9285 time:0.0\n",
      "train: loss: 0.3578701615333557 acc: 0.885604739189148  val: loss: 1.0152803659439087 acc: 0.8854508996009827\n",
      "step: 9290 time:0.0156252384185791\n",
      "train: loss: 0.3748370409011841 acc: 0.9210273623466492  val: loss: 14.880085945129395 acc: 0.7880212068557739\n",
      "step: 9295 time:0.004003286361694336\n",
      "train: loss: 0.9053955674171448 acc: 0.8816356062889099  val: loss: 2.5658016204833984 acc: 0.8940539956092834\n",
      "step: 9300 time:0.0\n",
      "train: loss: 0.7254360318183899 acc: 0.8061609268188477  val: loss: 6.173230171203613 acc: 0.8904334306716919\n",
      "step: 9305 time:0.0\n",
      "train: loss: 1.728948712348938 acc: 0.8620151281356812  val: loss: 11.351900100708008 acc: 0.835052490234375\n",
      "step: 9310 time:0.0\n",
      "train: loss: 1.5207821130752563 acc: 0.8881340622901917  val: loss: 2.6346731185913086 acc: 0.8653481006622314\n",
      "step: 9315 time:0.004003286361694336\n",
      "train: loss: 0.8116577863693237 acc: 0.8722920417785645  val: loss: 5.84201192855835 acc: 0.8896587491035461\n",
      "step: 9320 time:0.0\n",
      "train: loss: 0.5581434965133667 acc: 0.8938258290290833  val: loss: 0.935257077217102 acc: 0.8973535299301147\n",
      "step: 9325 time:0.0\n",
      "train: loss: 1.4109634160995483 acc: 0.8526967167854309  val: loss: 3.1554620265960693 acc: 0.8757621049880981\n",
      "step: 9330 time:0.0\n",
      "train: loss: 2.675100088119507 acc: 0.8285479545593262  val: loss: 0.6292528510093689 acc: 0.8765038251876831\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 9335 time:0.0030019283294677734\n",
      "train: loss: 0.870387613773346 acc: 0.9128314852714539  val: loss: 1.667883038520813 acc: 0.8773421049118042\n",
      "step: 9340 time:0.015625715255737305\n",
      "train: loss: 1.6579760313034058 acc: 0.8547124266624451  val: loss: 3.394627571105957 acc: 0.8342943787574768\n",
      "step: 9345 time:0.0\n",
      "train: loss: 0.683772623538971 acc: 0.8499786853790283  val: loss: 3.1419217586517334 acc: 0.8501286506652832\n",
      "step: 9350 time:0.0030019283294677734\n",
      "train: loss: 1.4514665603637695 acc: 0.8706809878349304  val: loss: 1.5915366411209106 acc: 0.8464644551277161\n",
      "step: 9355 time:0.003002166748046875\n",
      "train: loss: 1.1007810831069946 acc: 0.9058165550231934  val: loss: 5.283885955810547 acc: 0.9050137400627136\n",
      "step: 9360 time:0.0\n",
      "train: loss: 0.756781816482544 acc: 0.915081262588501  val: loss: 4.3720831871032715 acc: 0.9398596882820129\n",
      "step: 9365 time:0.0\n",
      "train: loss: 4.5414323806762695 acc: 0.9038331508636475  val: loss: 5.493170738220215 acc: 0.8638928532600403\n",
      "step: 9370 time:0.0\n",
      "train: loss: 1.4452953338623047 acc: 0.9031657576560974  val: loss: 2.6828694343566895 acc: 0.8882976770401001\n",
      "step: 9375 time:0.003002166748046875\n",
      "train: loss: 1.9173474311828613 acc: 0.9033775925636292  val: loss: 3.965158700942993 acc: 0.8892366886138916\n",
      "step: 9380 time:0.0\n",
      "train: loss: 3.365931987762451 acc: 0.7996071577072144  val: loss: 15.593256950378418 acc: 0.7882856726646423\n",
      "step: 9385 time:0.015625953674316406\n",
      "train: loss: 5.43895149230957 acc: 0.6344393491744995  val: loss: 3.2702276706695557 acc: 0.7936850786209106\n",
      "step: 9390 time:0.0\n",
      "train: loss: 5.138505935668945 acc: 0.8123866319656372  val: loss: 3.5297255516052246 acc: 0.8926868438720703\n",
      "step: 9395 time:0.0030014514923095703\n",
      "train: loss: 1.384499430656433 acc: 0.9035894274711609  val: loss: 3.7351768016815186 acc: 0.9046064615249634\n",
      "step: 9400 time:0.0\n",
      "train: loss: 12.742231369018555 acc: 0.782866358757019  val: loss: 2.781973361968994 acc: 0.856136679649353\n",
      "step: 9405 time:0.0030019283294677734\n",
      "train: loss: 4.654417991638184 acc: 0.6989609003067017  val: loss: 0.6833997964859009 acc: 0.880855917930603\n",
      "step: 9410 time:0.0\n",
      "train: loss: 10.535873413085938 acc: 0.6881303191184998  val: loss: 2.032487392425537 acc: 0.79679274559021\n",
      "step: 9415 time:0.0030014514923095703\n",
      "train: loss: 6.841950416564941 acc: 0.7028328776359558  val: loss: 17.544404983520508 acc: 0.7894814014434814\n",
      "step: 9420 time:0.0020732879638671875\n",
      "train: loss: 5.351274490356445 acc: 0.8379281163215637  val: loss: 4.374082088470459 acc: 0.821365475654602\n",
      "step: 9425 time:0.0\n",
      "train: loss: 7.071007251739502 acc: 0.9320505261421204  val: loss: 16.376026153564453 acc: 0.8246058225631714\n",
      "step: 9430 time:0.01562643051147461\n",
      "train: loss: 2.3668887615203857 acc: 0.8511558771133423  val: loss: 1.4428629875183105 acc: 0.8922885060310364\n",
      "step: 9435 time:0.003000974655151367\n",
      "train: loss: 2.3144640922546387 acc: 0.8441659212112427  val: loss: 4.0012102127075195 acc: 0.9122530221939087\n",
      "step: 9440 time:0.015625953674316406\n",
      "train: loss: 4.852514743804932 acc: 0.9343236684799194  val: loss: 3.438864231109619 acc: 0.9258237481117249\n",
      "step: 9445 time:0.0\n",
      "train: loss: 3.2264902591705322 acc: 0.9119589328765869  val: loss: 3.800363063812256 acc: 0.8371996283531189\n",
      "step: 9450 time:0.0\n",
      "train: loss: 3.0883471965789795 acc: 0.8101295232772827  val: loss: 1.965653896331787 acc: 0.9180936813354492\n",
      "step: 9455 time:0.003001689910888672\n",
      "train: loss: 2.125251293182373 acc: 0.9420100450515747  val: loss: 1.2349560260772705 acc: 0.8590229749679565\n",
      "step: 9460 time:0.0\n",
      "train: loss: 3.861295223236084 acc: 0.8771504759788513  val: loss: 3.436826467514038 acc: 0.894971489906311\n",
      "step: 9465 time:0.015625476837158203\n",
      "train: loss: 3.91986346244812 acc: 0.9052785038948059  val: loss: 1.2490178346633911 acc: 0.8667182922363281\n",
      "step: 9470 time:0.0\n",
      "train: loss: 4.046665668487549 acc: 0.9205513000488281  val: loss: 4.135272026062012 acc: 0.9131763577461243\n",
      "step: 9475 time:0.0030019283294677734\n",
      "train: loss: 1.8327497243881226 acc: 0.8104073405265808  val: loss: 0.617356538772583 acc: 0.8362836837768555\n",
      "step: 9480 time:0.015626192092895508\n",
      "train: loss: 1.7622300386428833 acc: 0.6795860528945923  val: loss: 4.323726654052734 acc: 0.8391990661621094\n",
      "step: 9485 time:0.0\n",
      "train: loss: 2.0606279373168945 acc: 0.8765426874160767  val: loss: 15.1314697265625 acc: 0.7891038656234741\n",
      "step: 9490 time:0.003001689910888672\n",
      "train: loss: 1.04335355758667 acc: 0.7464659214019775  val: loss: 5.970487594604492 acc: 0.8433769345283508\n",
      "step: 9495 time:0.003002166748046875\n",
      "train: loss: 0.26760581135749817 acc: 0.9344213604927063  val: loss: 1.6188461780548096 acc: 0.8898969292640686\n",
      "step: 9500 time:0.0\n",
      "train: loss: 0.23903970420360565 acc: 0.9173210859298706  val: loss: 1.8575615882873535 acc: 0.9129589200019836\n",
      "step: 9505 time:0.0\n",
      "train: loss: 0.5680094957351685 acc: 0.9336295127868652  val: loss: 1.3552697896957397 acc: 0.8705528974533081\n",
      "step: 9510 time:0.0\n",
      "train: loss: 2.796788454055786 acc: 0.9116479754447937  val: loss: 4.828974723815918 acc: 0.81804358959198\n",
      "step: 9515 time:0.0022881031036376953\n",
      "train: loss: 2.7898550033569336 acc: 0.920291543006897  val: loss: 1.2979884147644043 acc: 0.8477005958557129\n",
      "step: 9520 time:0.0\n",
      "train: loss: 1.2832647562026978 acc: 0.910546064376831  val: loss: 1.7148464918136597 acc: 0.765189528465271\n",
      "step: 9525 time:0.0\n",
      "train: loss: 3.055469512939453 acc: 0.8665664196014404  val: loss: 0.7899772524833679 acc: 0.846110999584198\n",
      "step: 9530 time:0.0\n",
      "train: loss: 0.5574766397476196 acc: 0.9213668704032898  val: loss: 2.045288324356079 acc: 0.8077985644340515\n",
      "step: 9535 time:0.0030019283294677734\n",
      "train: loss: 3.2016677856445312 acc: 0.8783063292503357  val: loss: 1.8107339143753052 acc: 0.8240441083908081\n",
      "step: 9540 time:0.0\n",
      "train: loss: 1.9208539724349976 acc: 0.9515572190284729  val: loss: 3.7787482738494873 acc: 0.8644741177558899\n",
      "step: 9545 time:0.0\n",
      "train: loss: 1.727024793624878 acc: 0.8763970136642456  val: loss: 4.556842803955078 acc: 0.8118852376937866\n",
      "step: 9550 time:0.0\n",
      "train: loss: 1.0135736465454102 acc: 0.9243324995040894  val: loss: 4.235405921936035 acc: 0.9043598771095276\n",
      "step: 9555 time:0.004003286361694336\n",
      "train: loss: 1.6704621315002441 acc: 0.6303213238716125  val: loss: 1.1751397848129272 acc: 0.768696665763855\n",
      "step: 9560 time:0.0\n",
      "train: loss: 0.7126920819282532 acc: 0.859963059425354  val: loss: 6.760654449462891 acc: 0.8147849440574646\n",
      "step: 9565 time:0.0\n",
      "train: loss: 5.477382183074951 acc: 0.8542696833610535  val: loss: 4.882856369018555 acc: 0.8384677767753601\n",
      "step: 9570 time:0.0\n",
      "train: loss: 3.503605365753174 acc: 0.8705517053604126  val: loss: 19.31955337524414 acc: 0.7868188619613647\n",
      "step: 9575 time:0.0029997825622558594\n",
      "train: loss: 6.035343170166016 acc: 0.7916902899742126  val: loss: 3.6701929569244385 acc: 0.8583917617797852\n",
      "step: 9580 time:0.0\n",
      "train: loss: 1.7302592992782593 acc: 0.8752871155738831  val: loss: 3.4895448684692383 acc: 0.9117043614387512\n",
      "step: 9585 time:0.0\n",
      "train: loss: 2.776761054992676 acc: 0.887570321559906  val: loss: 1.5978809595108032 acc: 0.8570307493209839\n",
      "step: 9590 time:0.0\n",
      "train: loss: 2.087794303894043 acc: 0.7711278200149536  val: loss: 1.2923766374588013 acc: 0.8793115019798279\n",
      "step: 9595 time:0.003001689910888672\n",
      "train: loss: 4.403009414672852 acc: 0.911873459815979  val: loss: 5.437154769897461 acc: 0.8366281986236572\n",
      "step: 9600 time:0.0\n",
      "train: loss: 3.8123793601989746 acc: 0.8332680463790894  val: loss: 1.6045265197753906 acc: 0.8770976066589355\n",
      "step: 9605 time:0.015625953674316406\n",
      "train: loss: 1.4425773620605469 acc: 0.9050356149673462  val: loss: 2.6385316848754883 acc: 0.9021316766738892\n",
      "step: 9610 time:0.015625476837158203\n",
      "train: loss: 5.21469783782959 acc: 0.8377464413642883  val: loss: 1.2280710935592651 acc: 0.8621248006820679\n",
      "step: 9615 time:0.003002166748046875\n",
      "train: loss: 1.5755212306976318 acc: 0.8257775902748108  val: loss: 1.734951376914978 acc: 0.9018816947937012\n",
      "step: 9620 time:0.0030002593994140625\n",
      "train: loss: 3.7324182987213135 acc: 0.8136016130447388  val: loss: 1.7289707660675049 acc: 0.8604787588119507\n",
      "step: 9625 time:0.0\n",
      "train: loss: 1.6152105331420898 acc: 0.9329112768173218  val: loss: 5.203967094421387 acc: 0.9127157330513\n",
      "step: 9630 time:0.0\n",
      "train: loss: 2.480956554412842 acc: 0.8986794948577881  val: loss: 1.3690457344055176 acc: 0.8424845933914185\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 9635 time:0.0030019283294677734\n",
      "train: loss: 3.1701011657714844 acc: 0.9240928888320923  val: loss: 2.631981372833252 acc: 0.8757471442222595\n",
      "step: 9640 time:0.0030019283294677734\n",
      "train: loss: 3.571476936340332 acc: 0.7489173412322998  val: loss: 2.881920576095581 acc: 0.8140062689781189\n",
      "step: 9645 time:0.0\n",
      "train: loss: 2.5728631019592285 acc: 0.8887264132499695  val: loss: 1.614027976989746 acc: 0.898961067199707\n",
      "step: 9650 time:0.015625953674316406\n",
      "train: loss: 4.541191101074219 acc: 0.8506029844284058  val: loss: 2.5643510818481445 acc: 0.8660123348236084\n",
      "step: 9655 time:0.0030019283294677734\n",
      "train: loss: 1.9514293670654297 acc: 0.8583509922027588  val: loss: 1.1109095811843872 acc: 0.8933162689208984\n",
      "step: 9660 time:0.0030019283294677734\n",
      "train: loss: 0.8908993601799011 acc: 0.8698872327804565  val: loss: 2.9193501472473145 acc: 0.9306463599205017\n",
      "step: 9665 time:0.0\n",
      "train: loss: 2.6194019317626953 acc: 0.8906172513961792  val: loss: 3.7515578269958496 acc: 0.8003346920013428\n",
      "step: 9670 time:0.015625715255737305\n",
      "train: loss: 0.8856588006019592 acc: 0.9314769506454468  val: loss: 2.4619619846343994 acc: 0.889567494392395\n",
      "step: 9675 time:0.004002809524536133\n",
      "train: loss: 0.520920991897583 acc: 0.8778989315032959  val: loss: 10.252055168151855 acc: 0.8556217551231384\n",
      "step: 9680 time:0.0\n",
      "train: loss: 0.3413071036338806 acc: 0.8962917923927307  val: loss: 2.788093328475952 acc: 0.9129951596260071\n",
      "step: 9685 time:0.0156252384185791\n",
      "train: loss: 0.28613078594207764 acc: 0.8440610766410828  val: loss: 4.443824768066406 acc: 0.8018829226493835\n",
      "step: 9690 time:0.0\n",
      "train: loss: 0.6844849586486816 acc: 0.9069601893424988  val: loss: 2.316913604736328 acc: 0.8844588994979858\n",
      "step: 9695 time:0.003002166748046875\n",
      "train: loss: 0.8414872884750366 acc: 0.8619130849838257  val: loss: 1.7765803337097168 acc: 0.8783391118049622\n",
      "step: 9700 time:0.0\n",
      "train: loss: 0.4866001009941101 acc: 0.892013669013977  val: loss: 0.7325342297554016 acc: 0.8552076816558838\n",
      "step: 9705 time:0.0\n",
      "train: loss: 0.5842028856277466 acc: 0.8491495847702026  val: loss: 1.4058457612991333 acc: 0.8303303718566895\n",
      "step: 9710 time:0.0\n",
      "train: loss: 0.2749786674976349 acc: 0.8392231464385986  val: loss: 3.197272300720215 acc: 0.8762410879135132\n",
      "step: 9715 time:0.0030019283294677734\n",
      "train: loss: 0.8078291416168213 acc: 0.8995677828788757  val: loss: 1.3652927875518799 acc: 0.9165657758712769\n",
      "step: 9720 time:0.0\n",
      "train: loss: 0.18901871144771576 acc: 0.8863479495048523  val: loss: 3.722123622894287 acc: 0.887134850025177\n",
      "step: 9725 time:0.0030014514923095703\n",
      "train: loss: 0.744183361530304 acc: 0.6648176908493042  val: loss: 5.245085716247559 acc: 0.7887542247772217\n",
      "step: 9730 time:0.0\n",
      "train: loss: 1.2126166820526123 acc: 0.728895902633667  val: loss: 14.719236373901367 acc: 0.8210831880569458\n",
      "step: 9735 time:0.003002166748046875\n",
      "train: loss: 0.6573240756988525 acc: 0.78441321849823  val: loss: 4.141168594360352 acc: 0.8507664203643799\n",
      "step: 9740 time:0.0\n",
      "train: loss: 1.039003849029541 acc: 0.7205168604850769  val: loss: 1.640745997428894 acc: 0.8904993534088135\n",
      "step: 9745 time:0.0030024051666259766\n",
      "train: loss: 0.30853962898254395 acc: 0.7039778828620911  val: loss: 12.057791709899902 acc: 0.8118517994880676\n",
      "step: 9750 time:0.0\n",
      "train: loss: 0.33554646372795105 acc: 0.8876272439956665  val: loss: 1.7153778076171875 acc: 0.9168825149536133\n",
      "step: 9755 time:0.0030014514923095703\n",
      "train: loss: 0.7495986819267273 acc: 0.8698223233222961  val: loss: 2.0033140182495117 acc: 0.8688534498214722\n",
      "step: 9760 time:0.0\n",
      "train: loss: 0.49039024114608765 acc: 0.867983877658844  val: loss: 2.964165210723877 acc: 0.9322965741157532\n",
      "step: 9765 time:0.0\n",
      "train: loss: 0.5103665590286255 acc: 0.842851459980011  val: loss: 2.5248520374298096 acc: 0.8733428120613098\n",
      "step: 9770 time:0.0\n",
      "train: loss: 0.9841348528862 acc: 0.8828564286231995  val: loss: 4.994342803955078 acc: 0.8074406981468201\n",
      "step: 9775 time:0.0030019283294677734\n",
      "train: loss: 0.642370343208313 acc: 0.8776456713676453  val: loss: 4.052735328674316 acc: 0.8844714164733887\n",
      "step: 9780 time:0.01725602149963379\n",
      "train: loss: 0.47029030323028564 acc: 0.9106254577636719  val: loss: 2.0059239864349365 acc: 0.8551620244979858\n",
      "step: 9785 time:0.0\n",
      "train: loss: 0.4320012331008911 acc: 0.6867148280143738  val: loss: 1.0678772926330566 acc: 0.8669763207435608\n",
      "step: 9790 time:0.0\n",
      "train: loss: 0.68284010887146 acc: 0.9105759263038635  val: loss: 1.325589895248413 acc: 0.9067668914794922\n",
      "step: 9795 time:0.003002166748046875\n",
      "train: loss: 0.21082863211631775 acc: 0.8256666660308838  val: loss: 4.748720645904541 acc: 0.88828045129776\n",
      "step: 9800 time:0.0\n",
      "train: loss: 0.9849883913993835 acc: 0.5052568316459656  val: loss: 1.8066743612289429 acc: 0.7191542983055115\n",
      "step: 9805 time:0.0\n",
      "train: loss: 2.1775875091552734 acc: 0.8729544878005981  val: loss: 15.81203556060791 acc: 0.8287253975868225\n",
      "step: 9810 time:0.015625715255737305\n",
      "train: loss: 0.5018731355667114 acc: 0.8773782253265381  val: loss: 2.606743812561035 acc: 0.8985500931739807\n",
      "step: 9815 time:0.003002166748046875\n",
      "train: loss: 0.7007653117179871 acc: 0.8408499360084534  val: loss: 2.879085063934326 acc: 0.8161933422088623\n",
      "step: 9820 time:0.0\n",
      "train: loss: 0.9493850469589233 acc: 0.8924646377563477  val: loss: 6.146491050720215 acc: 0.8955312371253967\n",
      "step: 9825 time:0.0\n",
      "train: loss: 0.6663802862167358 acc: 0.9063571691513062  val: loss: 5.136498928070068 acc: 0.9033229351043701\n",
      "step: 9830 time:0.0\n",
      "train: loss: 0.9658932685852051 acc: 0.8349648714065552  val: loss: 1.0221439599990845 acc: 0.8947968482971191\n",
      "step: 9835 time:0.003002166748046875\n",
      "train: loss: 1.433272123336792 acc: 0.8837490677833557  val: loss: 4.693710803985596 acc: 0.7817580699920654\n",
      "step: 9840 time:0.0030019283294677734\n",
      "train: loss: 1.0494310855865479 acc: 0.9089275002479553  val: loss: 2.743767738342285 acc: 0.9086551070213318\n",
      "step: 9845 time:0.0\n",
      "train: loss: 0.306766152381897 acc: 0.933901846408844  val: loss: 2.2960658073425293 acc: 0.9262468814849854\n",
      "step: 9850 time:0.0\n",
      "train: loss: 1.489956021308899 acc: 0.8861851692199707  val: loss: 3.295833110809326 acc: 0.8993350863456726\n",
      "step: 9855 time:0.003002166748046875\n",
      "train: loss: 0.9091717004776001 acc: 0.5510517358779907  val: loss: 1.7330135107040405 acc: 0.8720148205757141\n",
      "step: 9860 time:0.0030019283294677734\n",
      "train: loss: 1.080167293548584 acc: 0.8331570625305176  val: loss: 1.3704919815063477 acc: 0.9001723527908325\n",
      "step: 9865 time:0.0\n",
      "train: loss: 4.074873924255371 acc: 0.8486771583557129  val: loss: 3.7103207111358643 acc: 0.8932087421417236\n",
      "step: 9870 time:0.0022132396697998047\n",
      "train: loss: 2.9645421504974365 acc: 0.8607317805290222  val: loss: 0.813015341758728 acc: 0.8753529787063599\n",
      "step: 9875 time:0.0030024051666259766\n",
      "train: loss: 2.855501890182495 acc: 0.8377111554145813  val: loss: 0.9315091371536255 acc: 0.9034703969955444\n",
      "step: 9880 time:0.0\n",
      "train: loss: 2.71155047416687 acc: 0.9369565844535828  val: loss: 1.1903390884399414 acc: 0.8782286047935486\n",
      "step: 9885 time:0.0016939640045166016\n",
      "train: loss: 0.8336700201034546 acc: 0.9012064337730408  val: loss: 5.45742130279541 acc: 0.8856462240219116\n",
      "step: 9890 time:0.0\n",
      "train: loss: 1.4527428150177002 acc: 0.8452034592628479  val: loss: 1.796492338180542 acc: 0.9021550416946411\n",
      "step: 9895 time:0.00400233268737793\n",
      "train: loss: 3.1322128772735596 acc: 0.9234684109687805  val: loss: 6.85520076751709 acc: 0.9245516657829285\n",
      "step: 9900 time:0.0\n",
      "train: loss: 11.415460586547852 acc: 0.5906528234481812  val: loss: 9.677361488342285 acc: 0.7571963667869568\n",
      "step: 9905 time:0.015625\n",
      "train: loss: 7.576326370239258 acc: 0.805944561958313  val: loss: 4.180041790008545 acc: 0.8238728046417236\n",
      "step: 9910 time:0.0\n",
      "train: loss: 15.486749649047852 acc: 0.2503589391708374  val: loss: 14.851774215698242 acc: 0.7301195859909058\n",
      "step: 9915 time:0.003001689910888672\n",
      "train: loss: 4.842374801635742 acc: 0.8427053093910217  val: loss: 3.2354531288146973 acc: 0.8814593553543091\n",
      "step: 9920 time:0.0\n",
      "train: loss: 2.1249334812164307 acc: 0.8087159395217896  val: loss: 4.262025833129883 acc: 0.9010529518127441\n",
      "step: 9925 time:0.0\n",
      "train: loss: 4.49929141998291 acc: 0.8622186779975891  val: loss: 6.125607490539551 acc: 0.8904163837432861\n",
      "step: 9930 time:0.015625715255737305\n",
      "train: loss: 6.862918853759766 acc: 0.8652214407920837  val: loss: 2.5489501953125 acc: 0.8243368864059448\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 9935 time:0.003002166748046875\n",
      "train: loss: 9.12830924987793 acc: 0.7489796876907349  val: loss: 4.10566520690918 acc: 0.8415284156799316\n",
      "step: 9940 time:0.0\n",
      "train: loss: 9.602048873901367 acc: 0.8600641489028931  val: loss: 4.173794269561768 acc: 0.8944326043128967\n",
      "step: 9945 time:0.015625953674316406\n",
      "train: loss: 7.332580089569092 acc: 0.4915887117385864  val: loss: 2.131753444671631 acc: 0.8539701700210571\n",
      "step: 9950 time:0.0\n",
      "train: loss: 4.490780830383301 acc: 0.8488063812255859  val: loss: 2.801038980484009 acc: 0.8602145910263062\n",
      "step: 9955 time:0.0030014514923095703\n",
      "train: loss: 2.1023356914520264 acc: 0.8435816764831543  val: loss: 1.7890594005584717 acc: 0.8786427974700928\n",
      "step: 9960 time:0.003001689910888672\n",
      "train: loss: 4.468084335327148 acc: 0.8664002418518066  val: loss: 1.4750568866729736 acc: 0.9068279266357422\n",
      "step: 9965 time:0.0\n",
      "train: loss: 1.8257173299789429 acc: 0.8851666450500488  val: loss: 15.185174942016602 acc: 0.8142108917236328\n",
      "step: 9970 time:0.0030019283294677734\n",
      "train: loss: 2.346665143966675 acc: 0.91600501537323  val: loss: 1.7012869119644165 acc: 0.8744696378707886\n",
      "step: 9975 time:0.0030019283294677734\n",
      "train: loss: 2.718637466430664 acc: 0.9294846057891846  val: loss: 0.5821916460990906 acc: 0.8865056037902832\n",
      "step: 9980 time:0.0\n",
      "train: loss: 8.472186088562012 acc: 0.7637805342674255  val: loss: 1.5052859783172607 acc: 0.9158292412757874\n",
      "step: 9985 time:0.015625953674316406\n",
      "train: loss: 5.057483673095703 acc: 0.5846619606018066  val: loss: 0.9590997695922852 acc: 0.815479576587677\n",
      "step: 9990 time:0.0\n",
      "train: loss: 1.2271414995193481 acc: 0.8836086988449097  val: loss: 0.8116284608840942 acc: 0.8396599292755127\n",
      "step: 9995 time:0.0030019283294677734\n",
      "train: loss: 3.3084888458251953 acc: 0.8459097146987915  val: loss: 2.241349220275879 acc: 0.9001807570457458\n",
      "final time: 139.15451431274414\n"
     ]
    }
   ],
   "source": [
    "min_loss=1e10\n",
    "sess=tf.Session()\n",
    "# train_writer=tf.summary.FileWriter('D:/graph/wind-4-1/train/',sess.graph)\n",
    "# test_writer = tf.summary.FileWriter('D:/graph/wind-4-1/test/', sess.graph)\n",
    "saver=tf.train.Saver(max_to_keep=3)\n",
    "\n",
    "init_op = tf.global_variables_initializer()\n",
    "sess.run(init_op)\n",
    "coord = tf.train.Coordinator()\n",
    "threads = tf.train.start_queue_runners(sess, coord)\n",
    "max_ac=0.5\n",
    "t1=time.time()\n",
    "for training_itr in range(nb_iters):\n",
    "    x1, y1 = sess.run([batch_xt,batch_yt])\n",
    "    feed_dict1 = {enc_inp: x1}\n",
    "    feed_dict1.update({expected_sparse_output: y1})\n",
    "    feed_dict1.update({pharse:True})\n",
    "    _, loss1,acc1,summaries1 = sess.run([train_op, loss,acc,merged_summary], feed_dict1)\n",
    "\n",
    "#     train_writer.add_summary(summaries1, training_itr)\n",
    "    if training_itr %5==0:\n",
    "#             saver.save(sess=sess, save_path='model/hand_landmark_v6.1_model/model.ckpt',global_step=(global_step + 1))\n",
    "        mean_val_loss = 0\n",
    "\n",
    "        x2,y2=sess.run([batch_xv,batch_yv])\n",
    "        feed_dict2 = {enc_inp: x2}\n",
    "        feed_dict2.update({expected_sparse_output: y2})\n",
    "        feed_dict2.update({pharse:False})\n",
    "        tt=time.time()\n",
    "        loss2,acc2,summaries2,oout = sess.run([loss,acc,merged_summary,reshaped_outputs], feed_dict2)\n",
    "\n",
    "        print('step: {} time:{}'.format(training_itr,time.time()-tt))\n",
    "        print('train: loss: {} acc: {}  val: loss: {} acc: {}'.format(loss1,acc1,loss2,acc2))\n",
    "        if acc2>max_ac:\n",
    "            yy=y2*y_std+y_mean\n",
    "            ppre=oout*y_std+y_mean\n",
    "            max_ac=acc2\n",
    "#         test_writer.add_summary(summaries2, training_itr)\n",
    "        if loss1 < min_loss:\n",
    "            min_loss=loss1\n",
    "            saver.save(sess=sess, save_path='D:/model/twomonthes/model.ckpt',global_step=(training_itr + 1))\n",
    "sess.close()\n",
    "coord.request_stop()\n",
    "print('final time: {}'.format(time.time()-t1))\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.94206655"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_ac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[400.66],\n",
       "       [390.42],\n",
       "       [400.41],\n",
       "       [400.59],\n",
       "       [400.59],\n",
       "       [400.6 ],\n",
       "       [400.83],\n",
       "       [400.03],\n",
       "       [400.1 ],\n",
       "       [400.64]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yy[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[388.3565 ],\n",
       "       [376.9084 ],\n",
       "       [372.91177],\n",
       "       [392.57382],\n",
       "       [397.96802],\n",
       "       [391.1403 ],\n",
       "       [383.88507],\n",
       "       [387.4459 ],\n",
       "       [389.29532],\n",
       "       [388.3732 ]], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppre[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ac=np.mean(1-np.sqrt(np.square((yy*y_std+y_mean)-(ppre*y_std+y_mean)))/np.sqrt(np.square(yy*y_std+y_mean)),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.96502656],\n",
       "       [0.9447634 ],\n",
       "       [0.9414542 ],\n",
       "       [0.97032356],\n",
       "       [0.9695753 ],\n",
       "       [0.9678531 ],\n",
       "       [0.9462909 ],\n",
       "       [0.9531091 ],\n",
       "       [0.94567585],\n",
       "       [0.81940895]], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=yy[:-1]\n",
    "p=ppre[:-1]\n",
    "y=np.reshape(y,(-1,1))\n",
    "p=np.reshape(p,(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import *  \n",
    "mpl.rcParams['font.sans-serif'] = ['SimHei']  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAETCAYAAAAs4pGmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAIABJREFUeJzsvXmcVOWV//8+ta+90DT71ggCoiyKCgQTVFRi4hJ/RmKiE78xGvONk2Uy30nGmBidaJaZSUziGOOomcSYOKJG4xa3yKKyC7gAIksjDQ00vXfXXvf5/XFvVTdNQ1dLrd3P+/XqF1XPfe6t09XU/dQ5z3nOEaUUGo1Go9H0xFZoAzQajUZTnGiB0Gg0Gk2vaIHQaDQaTa9ogdBoNBpNr2iB0Gg0Gk2vaIHQaDQaTa9ogdAMaETEJSL2XsZERM7J4PxbRMTR7fldPc8TkWtF5L7sWd2rHTYRMUTE2ce8mIh4cmmLZvDg6HuKRlMaiMgo4HmgDQgCHcBy4BMiMhaIAfsBN/A54HsiEgL+BnwZCAE+4A/AvYAC/hH4sXX9IHA90FMMokD8GDZtteyIAsOt1/7QOnyyUmpYt7kjrHmpzUkdSqkEgFLKEJEOpVS82/z/Bt5USv2u20vGjmWLRtNftEBoBgxKqf0i8glgNPAg8C9KqVUAIvJr4AWl1POp+SJyMXAuMA1TEJ4ErgBGAA8DvwGSqms36VeB3wGdIjILUyiSwBCgTERmAx7gT0qpn1vnxIHPKqVqReTLwBil1A+t19/X41d4hS6BOAlYDKzpdtzoMb83MUgopZLW9ScCZyillh73jdNojoEWCM1AYwywEvi5UmqVdSO/F/OGO19EbgHWAz8BZiml/iYi06xz3wP+w3ocBxKpi4rIZOAbwBzg28A4pdRc69iVwAKl1DePYdNSEUl7ECKyqLdJSqlTu73eOuACEbkXCFvDARFZAzyulPp3ujyNIxARP/B14GvAr49hk0bTJ1ogNAMGa63gfeByYKg17AJ2AYswxcMG/BTwAj8TkQndLhHBDE95e7n8+ZiCsQwQYLblgczB9EDGi8itmB7Hj7ud58QMU+0DPguMBH5lHXuzh/2PAPcA7wAopX4kIncppQzreItS6uwedv1KRH4KBICbrbG3gT8DM5RSTb38LhpNRmiB0AwkzgPuwAwxtVsx+jusY6cAd2J6AUoptVtEzrfmLrDmJDnGt3Kl1H3AfSLyJPATpVSniGwDGjHXPVLYe5z6C2Ce9XgOUA5caT2/s8fc24GngUeAlSIyDPhfzDDYsfi6UuqPqSdWKG2GUqrzOOdoNBmhBUIzYFBKvQS8JCI/AVYDtwBvAWcCfkxPovv8BhH5P5heRfcb6lGfCxGxAd+3rjNPRM7A/KZ/d4+pm4E1InIy8FfruinRGYrp0VR0XVa+DMxXSrUrpbaLyHXA68BFSqlDItIkIpcqpf7aj/dBi4MmK+g0V82ARSl1FuZCLsB0YKf12AEgIvOAfwBWYX4W9gEbMMNIdszwUIqTgCXAAevYW5ihqHeVUnOUUnMwM6NOsubvBM7A9BouwRSWP2EunhvA561j52BmOaWoxMy0+r/W819Zc46LiNhFpKqveRpNf9ACoRnoHASewVyD2I654PugdezbwL8ppZZjhqDeBX6A6X38gW4eh1LqA6XUKUqpL1rnhzhGOMqan7S+yV+IuWj+L9b1kpgLyC8CVwFtqSwpERkO/CewEPCKyLVKqeVKqf93vF9QRC7FXHe4pJdjV4mI73jnazTHQguEZiDiwbp5K6VqrefVmN/U/wJsFZELgJmYGUZTMG/WP8BcqL5DKfWyUmoflhchIp8RkXUish/zZnyj9VqfFpH1IrLeujbW/KEisgr4GbBEKfUM5ufNrpRaDXwKuAmoF5GRIlIDvAZ8Vym1G3PB+aRu1xOO/ryejOlh3Ax8WSn1P0DS2k+BiJwE/BFzb4dG03+UUvpH/wyYH8yb9DuYGUsVwFLMtYBK6/gMYBumOFxpjb0KXG09rsS86dow015fs8bHABcDQ7q91qeA/+n2fBLwRrfniwFbt+dfA37aw96p1r8zgcuP8TvNxQxZvdpj/EvA4h5jdwFbrN9xC/CdQv9N9E/p/ohSuqOcZuAiItOUUlt7jFUrpRq6PXeqbjuUu437lFKh41zbDXiUUq1ZNfro17EDo5RSe3P5OhpNT7RAaDQajaZXcrIGISLlIvKCiLwkIn+xiqM9KCKrrM1EqXkZjWk0Go0m/+RqkfoLmKUOLsRMC/wc5uLcPGCiiEwWkSsyGcuRfRqNRqPpg5xslFNK3dvtaTVwDV0bil7C3Lk6G3gsg7EPul9bRG7EyiDx+/1nTJ06NQe/gUaj0QxcNmzYcFgpVd3XvJzupLY2IlUCtZibkACagNMxNw5lMnYESqn7gfsB5syZo9avX58j6zUajWZgIiJ7MpmXs30QIjIEs5LklzDzz1MF0ALW62Y6ptFoNJoCkKtFahdm/vm/KqX2YJYvSBVEm4npUWQ6ptFoNJoCkKsQ0/WY4aHvicj3MJusXGt1/Pok5sYfhVmxsq8xjUaj0RSAvO2DEJFK4AJghVLqQH/GjoVeg9BoNMciHo9TV1dHJBIptCkFw+PxMGbMGJzOI1uZi8gGZRaYPC55K/etlGqmK0OpX2MajUbTX+rq6ggGg0yYMAGzlNXgQilFY2MjdXV11NTUfKRr6EVgjUYzIIlEIlRVVQ1KcQAQEaqqqk7Ig9ICodFoBiyDVRxSnOjvrwVCo9Fo8sT+/fuJxWJ9TzwO3T2CeDxOPH5UncmsoQVCo9Fo8sT3vvc9Hn/88SPG3n77bUaMGMHcuXOZP38+Dz/8MOeeey6LFi1i0aJFXHrppUfMv/zyy1m+fDm1tbX87ne/40tf+hK1tbXs3LmTRCKRVXt1T2qNRqPJIVOmTGHs2LEAhMNhtmzZwkMPPQRAbW0tzzzzDIsXL2bJkiWsWrWKq6++mi984QvYbOb39+4ex86dO3G73USjUZYuXcq6deuIRqM8/vjjJBIJvva1rxEMBrNmuxYIjUYz4Pnm377JpgObsnrNWSNmcffiu/uc53A4eOyxx9i+fTtz587lscceY9q0aZx22mmcfvrp6XWC1157jcWLFwOkxSF1fiKRwOFwcMsttzBt2jQWLVrEj3/8Y+rq6rDZbLS2tvL9738/q+IAWiA0Go0mp7zwwgvU19fzxBNP8Oyzz9LW1saIESMAeOutt9i2bRsAr7/+OjfeeCPnnnsudrudzZs3c+qpp2Kz2fjyl7+Mx+Nh8+bN1NTUYLPZ6Ozs5OGHHwbgueeeo7m5Oeu2a4HQaDQDnky+6eeKzs5OfvSjH3H11VdzySWXcNVVV3HyyScfNe+qq65ixYoVvPzyy7jdbi644AKeeuopbDYbPp+PLVu2cPfdd7Ns2TLADFft2LEDgEOHDuXEdi0QGo1GkyOeeeYZfvazn+H3+7nnnnu49957WbduHddddx1gisddd90FwGWXXcY3vvEN9u3bx8iRIwHTM3j55Zd54IEHOOWUUwiFujrg1tfX88ADDwBw4MABLrjggqzbrwVCo9FocsQll1zCJZdcApg7m2+55RYWLlzI8OHDueaaa3A6nWzbto22tjauvPJKbrjhBh544AFWrlzJo48+yuLFi7n99tvZsWMHkyZNOuLaQ4cO5fLLLwdg7dq1ObFfp7lqNBpNDtm3bx8PPfQQCxYsIBgMcvPNN7Nv3z7mz5/PE088QTKZpKysjOXLlwOmJ+H1ekkkEtjtdr797W+nU2OVUhiGQTKZpLy8nAULFrBgwYJ0yCqZTGbVdu1BaDQaTY6IRqP827/9G1OnTuWJJ55IL07feuut3HDDDfziF79IewaBQIDrrruOZDLJNddcg1IKn8/H9ddff8T1Dh8+zMUXX0x1dTU//OEP08fWrVtHIpHgc5/7XNbsz1s111ygq7lqNJpjsXXrVqZNm1ZoMwpOb+9DptVcdYhJo9FoNL2iBUKj0Wg0vaIFQqPRaDS9ogVCo9Fo8kQ8HscwjPTzRCKBYRi0t7cf85xdu3YdsUtaV3PVaDSaAcDKlSu54IILuOSSSxg9ejQPPvggl112GVVVVVx++eVcfvnlvPnmmyxatIhly5bx2c9+luuuu44lS5awceNGAB566KH0Y8hvNdecCYSIDBeRldbj20VkmfWzTUT+VURGi0hdt/Fqa+6DIrJKRG7NlW0ajUaTD8455xy+853vMGXKFO655x5uuukmnnnmGebMmcNTTz3Fs88+y4IFC3j++ecZM2YMdrudO++8kzPOOIPDhw9z4YUX4nA40sX7elZzfeWVV2hra+Pxxx9n6dKlhMPhrNqfE4EQkUrg94AfQCl1m1JqoVJqIfAu8AfgbODO1LhSqkFErgDsSql5wEQRmZwL+zQajSZf+Hw+1qxZw2c+8xlWr17NWWedxfjx47npppuYMWMGq1evZu3aten9EDfddBMtLS04nU5cLtcR1+pezfX5559n48aNbNu2jWeffZYzzzyzZKq5JoElwNPdB0XkTKBOKbVPROYC54vIDcDflFK3AAuBx6zpLwELgA96XONG4EaAcePG5ch8jUYzoPjmN2FTdst9M2sW3H38IoCPPPII999/P0opFi5cyOLFi7n44osJBALMmzePuro6pk+fzhVXXJEWCLvdTllZ2VHXWrp0ad6ruebEg1BKtSmlWns59A3g19bjFzAF4UxgnojMwPQ49lnHm4DhvVz7fqXUHKXUnOrq6qzbrtFoNNni6quvZtmyZVRUVHDWWWcxatQowCyuN3ToUACCwSB//etfmTBhAkop4vE4DoeDnpuYp0+fzt3dBClVzXXHjh2lX81VRCqAYUqpndbQm0qpqHVsIzAZ6AC81vEAehFdo9Fkgz6+6eeK7o1/fvzjH/PWW2+xa9cu9u7dy7hx49Ii8PTTT7N9+3YcDgft7e0Eg8G0WKQY6NVcLwOe7/b8RRG5GmgFLgR+i+lBLABWAzOB9/Non0aj0eQMu92Oz+dj/PjxrFixArfbzbx58zAMg1/96lc8/vjjbNq0iSeffJLdu3dzww03YBgGr7/+eq/XG2jVXC8CVnR7fjvwGqYY3KeUeh94CrhWRH4OXAU8l0f7NBqNJusopdKewvTp02lvb+f888/n2muv5fzzz+eBBx5g/vz5BINBfvCDH/DDH/4Qj8fDfffdx5QpU9i2bRt2uz19rQFTzdXKWko9/nyPY68BU3uMtYnIQuAC4GfHWMfQaDSakiAWizF//nyuvvpqkskkN998M4ZhcO+99xIKhViyZAk//OEPueqqqzh06BC33XYbY8aM4ZZbbqG+vp6DBw9y+umnc9ZZZwG6mmu/0NVcNRrNsSjGaq779u1j9OjR6eehUAi32532EHKBruaq0WjS1LfX8+k/fZqWSEuhTSk4xfYFuLs4gLlHIpficKK/vxYIjWaAsaF+A8998BxbGrYU2pSC4vF4aGxsLDqRyBdKKRobG/F4PB/5GrqjnEYzwEgYCWqaIJaMFdqUgjJmzBjq6upoaGgotCkFw+PxMGbMmI98vhYIjWaAUb5xK7t+BW98aidMWFhocwqG0+mkpqam0GaUNDrEpNEMMByNZskFOdxYYEs0pY4WCI1mgGHEzdBSMhbpY6ZGc3y0QGg0AwyVMMszGFEtEJoTQwuERjPQsJrGGLFogQ3RlDpaIDSaAYayCrxpgdCcKFogNJoBhpHUAqHJDlogNJqBRtwKMek1CM0JogVCoxloWGsQKj64N8ppThwtEBrNACOVxaRiWiA0J4YWCI1moGGFmLRAaE4ULRAazQBDJU2BQIeYNCeIFgiNZqCR1GsQmuygBUKjGWhYi9TE4sefp9H0gRYIjWagkRKIuBaIAcfatfDOO3l7uZwJhIgMF5GV1uPRIlInIsusn2pr/EERWSUit3Y776gxjUaTOZIwG9eLFogBx8FLzycydw689VZeXi8nAiEilcDvAb81dDZwp1JqofXTICJXAHal1DxgoohM7m0sF/ZpNAMZlTQFQnsQA4vm2m0MP9iBKxyDxYth+/acv2auPIgksARos57PBb4sIm+JyF3W2ELgMevxS8CCY4wdgYjcKCLrRWT9YO4UpdEcC7FCTKl/NQODd/76AADXXwpJZcAXvgA5bqeaE4FQSrUppVq7Db2AefM/E5gnIjMwvYt91vEmYPgxxnpe+36l1Byl1Jzq6upcmK/RlDbpEJMWiIFE8/IXiNvg0VPhtfu+A3/8I4jk9DXztUj9plKqXSmVBDYCk4EOwGsdD1i29Dam0Wj6gSS1QAw0kkaSys3b2TthCHg9PB+ohylTcv66+boBvygiI0XEB1wIvAtsoCuENBOoPcaYRqPpB6lFapsWiKJn72tPs/+J3xNf/hocZ+f76j1vMHtvAuPsMzlz1Jm8uffNvNjnyMurwO3Aa0AMuE8p9b6I1AMrRWQU8EnMdQrVy5hGo+kPKQ/CEgpNcbJjzQtMPP9ybNYywqZvXc2sn/+p17nrXvk9H4uB/YIr+NjY3fznqv8kHA/jdXp7nZ8tcupBKKUWWv++ppSaqpSaoZS6xxprw1yXWA2cq5Rq7W0sl/ZpNAORVIjJpgWiqNnyb18nKfDCr7/B3gob0TVvHHNu87IXAPCdcx7zx84nbsRZv399zm0saIxfKdWslHpMKXXgeGMajSZzbFogip71O1ay4NUdbP/4dD55893sO6maoTt7v+W9d+g9xm2tJ1zuh5NOYt7YeQB5CTPlK8SkKUHq2+t5+v2nOdBxgJqKGr4464uFNkmTAZIwALBrgShaXv33m5gTAc93fwpA9JQpTHhrBR2tDQTKj8zO/N/Nj/B/doPt7LkgwlDfUKZUTeHNOi0QmgJy27Lb+O+3/hsAn9OnBaJEEMMUBi0QxUltSy0ff34LjeOqqbrwYgD8Z8zF/vAKdrz+DLM+9aX0XKUUkd89QE0L8JX/mx7/9rxv43a4c26rTiPVHJMPmj7g7NFnc+s5txKKhzCUUWiTNBmQ8iBsCf33KkZ2b17OvDpovebK9D6GcQs+BcChNX8/Yu763W/wtecbaDhlAnzmM+nxG864gX+Y+Q85t1ULhOaY7G7ezaQhkyhzlwEQjocLbJEmE2xJK8SU1AJRjLRuMBejK8+9OD02bOZ8Ig5Ivr3xiLl7//37jG8F789+nvNNcb2hBUKTRimFsrbux5Nx9rbtpaaiBp/TB0AoHiqkeZoMSS1SO7QHUZQY75rVWCtmz+sadDioG1NG2fY96aFkIs7cP6/g3WlVBC6+PN9mAlogNBYJI8Fnl36WKx67AoC9bXsxlEFNZQ0j97Yw44AWiFIh5UE4tAdRlPg+qOVwuROpqjpivP3kCdTUdaY/Z5v/ch+jWg3ar/t8QbwH0IvUGkzP4esvfJ0ntj6B1+ElaSTZ3bybs/fCVVffSeD9XXzcC4fu6Cy0qZoMECMVYsptITfNR2PYh40cHFfF0B7jzpmnM+qlt1m3ZTlnzvwkjY/cT9QOM6+/pSB2gvYgNMC96+7lN+t/w+zKU1DhMDubd7K7ZTffeQO8Dc00nnUqQ8MQinYU2lRNBtgsYXBogSg62iNtTD4QJzx5wlHHhs89H4Dalc8QS0SZunwL780aha9qRJ6t7EJ7EBoefe9RZo+YzatPBtmyFd75/Dvsat7FeQdBFi2iadIQqta+S6StqdCmajIgFWJyJrRAFBu731nJjBjYp5921LGhZ58LwHvPPEinS7iuxaDxys/m28Qj0B6EhsOhw1zUVEnlSyuYWwfbatdzYN/7TGwG26xZ2INmFlO0pbHAlmoyIZW95EyaVUA1xUPjhpUAlJ8+76hjMmoU0bPncOsrMabdcS9xG5xy/XfzbeIRaIHQ0NDZwOf/shMAu4LI6tdxvrvVPDhrFs7yIQAkWpsLZaKmH6RCTE4D4obuKldMRK001lFnLzr6oAjul/9O58KPcfY+2DFrHK7qwoWXQAvEoCdpJKnZ0chp6/bAP/0TAMFNWxjywV5zwsyZOMoqAIi3aYEoBWxGlwcRSx67hLQm/zi376DRb8MzcmzvE4JByl9cRuddtzPpt0vza1wvaIEY5DSGG/nmaoiU+eC222gaPYSTdjRx0p4OQuU+GDUKl+VBJNt0cd1SwGaYHoQrae5n0RQPVbsPsn9s+fEnORz4//UHOOeclR+jjoMWiEHO4dBhTm6EptMmQ1kZnbNP5ew6mHkQ2qbWgAieCjMhL9muBaIUsHcLMWkPongwjCTj93fSNnFMoU3JGC0Qg5yGzgaGdYIMGwaA72MLGdMOs+vBmDEDIO1BqPb2gtmpyRy75UHoEFNxcXDHZiojYEzLfavQbKEFYpDT0HmIYZ1gHzkKgMqFiwFwKPCfOR8AZ3klAKpD74MoBVIehA2IxXT9rGIh1LAfAPuIUQW2JHO0QAxyWhvq8CbAPWocALZZs4nbzW39ZWedY04KBMx/tUCUBKk1CIBERJdHKRaS8SgANruzwJZkjhaIQU54n1kczD+mxhzweDh88hgSDhsybZo5ZgmErVOX2igFHN0EIh7VAlEsGDEz3CdOLRCIyHARWWk9Hiciy0Tk7yJyv5iMFpE6a3yZiFRbcx8UkVUicmuubNN0Ea+vA8DRze0d+a3v4/jKV8HlMgdcLuI2sHXqcEUpYO9Woy8R1X+zYsGImwJhc7oKbEnm5KTUhohUAr8H/NbQV4CvKqW2isgLwGnAJOBOpdRvup13BWBXSs0TkYdEZLJS6oNc2KgxMQ4dNB8MH941eMMNR04SIey24wjpm00pYE8qEjZwGJCI6L9ZsZAOMZWQQOTKg0gCS4A2AKXU95RS1tZcqoDDwFzgyyLylojcZR1bCDxmPX4JWNDzwiJyo4isF5H1DQ0NOTJ/8OBoOGw+sLKYjkXYY8cRiuTBIs2J4jAg6jQ/2tqDKB5SHsSgDzEppdqUUkclzYvIEuA9pdR+4AVMQTgTmCciMzA9jn3W9CZgeM9rKKXuV0rNUUrNqa6u7nlY00+ch63d0X28lxGPA2c4mgeLNCeKXUHMZQf0InUxYSTMTYul5EHkrZqriEwE/hlIFSF5UykVtY5tBCYDHYDXOh5AL6LnHF9TOx1BN4E+vtXEvE5cYb0rt9hRSuFIQshlB+IkY9rrKxZ0iOkYWGsSfwa+1M2zeFFERoqID7gQeBfYQFdYaSZQmw/7BitKKYKtYTorA33OjXnduCNaIIodQxk4DIi7TcHXAlE8pEJMdqe7wJZkTr48iO8C44Bfi9k67zbgduA1IAbcp5R6X0TqgZUiMgr4JOY6hSZHtMfaqW5XxKr6qA0DxL1uvC26WF+xk0jGcStIuM2PdjKqBaJYUHEdYjoCpdRC69/vAN/pZcrUHvPbRGQhcAHws97WMTTZI1VmIzG1Z/PDo0n6PASiurdAsZNIxHADCbd5E0rqReqioRTTXIsuxq+UalZKPaaUOlBoWwY6DaEGhnerw3Q8En4vvqiZYP/Szpc47/fn6WY0RUjCCiklLYEwYjqxoFhQ1iJ1KYWYik4gNPmjsaWeygg4Ro7uc64K+AlEzf4Ry2qX8Vrta3TG9c7qYiO1EGp4zJuQEdUCUSykQkx2lxYITQnQUbcLAM/IcX3OVX4/gRiE4yEaOs39J9GEvvkUG0nLY1CWQOhF6uLBSOgQk6aEiOz/EAD/2Il9zlWBADYg1NZIQ8gUCF1KuvhIhZiU12P+qz2IokF7EJqSIl5v7kn0jOrbg7AFgwCEmxvSAhFN6ptPsZG0vqUqr7mdSMW1iBcLKpEAwOHQAqEpBaw6TDKi78botkAZANGWRg6HzPIcOsRUfKQrhloCoRepiwjtQWhKCVuGdZgA7GXmXolYa1PXGoT2IIqO1CK18lghJu1BFA06i0lTUtgPHSbitnc1BDreXEsgwi0NNEfMDXPagyg+UovUqSwmYlogioV0iMntKbAlmaMFYpASioewHW4knEGZDejqS93YsCc9pj2I4iPtQaQWqbVAFA9WiMnhGkACISLuHs8dIvKl3JmkyQebD2xmWIdCDTuqYG6vOMvMvtSth/elx7QHUXykKoZ2LVLr+lnFgkqaG0sHjECIiB1YISK3W13grgO+DXwmH8Zpcsf6/euZchg8k6dlNN9dUQVAW9P+9Jj2IIqPVDkHrDUI9BpE8ZAYYB6EUioJhIGdwOXAbMyqrIncm6bJJVu3v8GEVvCeOS+j+Z4Ks15TZ9PB9JjeB1F89AwxEdcf1aLBWoOwOQZWwyCF2cTneaAS+A9rTFPCRNavBkBOPz2j+d4Ks6FQtPlwekyHmIqPdEjJCjGJ9iCKh0SCpAC20ln67SvEtARTDMYCjwK/BVzAaBG5SkQ+n3sTNdmmPdpO1VZrsXn27IzO8ZYNwQAS7S3pMR1iKj7SbS09Vt8t7UEUD4kkidLRBqDvct/DMfs4TMTs+PYVIAh4gJFA6ST0atJsPLCRWQcgPGIo3qF9l/oGcNidtLnBF+1yHrUHUXykdlLbXG4SNhAtEEWDJBIkbKV10+xrDeJXwF5gF9AJPAi0AjuVUr9USv0s9yZqss36/euZXQ9y+hn9Oq/TJQRiUOU1F6y1B1F8pD0Ip5O4w4YkdBZT0ZBMkiwxDyITc21AA/BF4CLgyzm1SJNz3tm1mqmN4Dmzfw37wm4bgRiMKRsDaA+iGEl3LXM4SdgFm/YgigZJJEjYpdBm9Iu+1iAcgBc4C9gNPAbcaY1pSpTYxvXYFBmvP6QIeRxHCoT2IIoOo1s5h6TDpkNMxUQiiSGlJRDHXYNQSiUwxSHFJhH5DvD/5dQqTc5QSlG9vc580k+BiHodBGJRRgZGIohOcy1CVLe2lkm7DVtCd/0rFiSZJGkvtBX9I6OImIhME5EaEakEOpRSv8vgnOEistJ67BSRZ0TkjdQu7EzHNNmlIdTAqXVxwmU+GDu2X+dGPU4CMaj2V+N2uHWIqQhJeRA2p4ukQwtEMSGJJElbaXkQmS6Z/B64Ffgd8JqIrBaRDSJyUW+TLSH5PeC3hv4R2KCU+hhwpYgE+zGmySK7m3dzRj10TJ8E/XR3Y143gRgM9Q3FZXcNzBCTUlBfX2grPjLL17zvAAAgAElEQVTKCimZAmHHFtcCUSyYHsTAFIgOpdT1SqnLlVKfUErNBa7BTHvtjSSwBGizni/EXL8AWAHM6cfYEYjIjSKyXkTWNzQ0ZGi+JsW+vVuYeQCM+fP7fW7CZwpEta8at32AehArV8KYMbB9e6Et+UgoK83V7nBhOGzYk1ogigVbIkmyhDbJwXHWIETECzwOtAPTROQ3wAFgK+au6lrglt7OVUq1WddIDfkxd2MDNGHur8h0rOe17wfuB5gzZ07x7eiOxcwt9T5foS3pleTK5diA4KJP9fvchM9zZIhpIHoQtbVgGPDuu3DyyYW2pt90b2uZdNix6TTX4sEwBpQHEQO+BNyMuQ/iLuAFYALwMrBEKbUtw9fpoCvzKWC9bqZjJcWmK+azY9b4QptxTMrXbCJmB9/Hz+v3uUmfl2AUqj1VpgcxAAUidPgAANHtmf7XLi5SPQdsThfKYceeMApskSaFLZnEGCgCoZRKKqUOAtXAi0qpvUqptcADwI+BOSJyZ4avswFYYD2eiel9ZDpWOijFqDfepmbHYVRnZ6Gt6ZVxm3azZYL/I3k47SOH4FAwojE6YBepd+7ZCMA7a54usCUfDZU0BcL0IBw49CJ10SAJY+CEmLpxp1Lqim7PW4F/UEpdKSJfy/B1fg88LyLnAKcAa7AKAGYwVjIkd+9iWIvp0jetX8mQTywusEU96OhgUm0bf71sKrM+wuktE0cDULX74ID1IIxWs1te05a3aAw1UuWrKrBF/aRbUxrltGOPFl8UdrAyoDyIbowUkTdTP8DrwFwRcSql/ut4JyqlFlr/7gEuAN4AFlneSUZjH/k3KwAHX/pL+nHzqtcKaEnvGG+8jsOAxjNP/Ujnz150DQCe93fgdrgH5j6INjOvYlxjgp+8/pMCG9N/0n2PHS4MhwNHsluIqa0NPv3pkl2AL3UkaWDYS8uDyMTaA0qp+d1+5gGPYGYcZYxSar9S6jGlVGt/x0qF8LKXaXNBixsSGzcU2pyj6HzleRICan5mPSB6cvapF8GoUfDee2aa6wAMMUlbBwAT22z815pfc6DjQIEt6h/d1yBwOrAnFEpZXsTy5fDcc/DqqwW0cPBiG6AC8dNexp4Gvikik7NsT0njX7eZNWNh0wjwvft+oc05CmP5ctaPgrGjMusi1yvTp8OWLQM2xGRvNwXCFTeobImypWFLgS3qJ5ZAOFweDKcTVxKSKUd8jRWx3bfvGCdrcsmAFAil1Oruz63Naz5gFHBjjuwqPVpbGbb7IFtPrmLnuADDdtZD9xz0p56Cq64qnH2AfV8926tgQsWEj36R6dNh61Y8toHpQTg7QmZTF6CmGdqibcc/odjoJhA4HDgNiCetVNeUQNTVFci4wY29BAXiuIvUItKAmVmUCmQK5r6IWuD7Sqlnc2pdKbF6NTYFzadPI3SgAffK92HHDpgyxTz+yiuwdKmZY1+oTIZImLAzCwIRCjGmKcFO+8ATCFdnhNphLk46GGNiCQuE3eVGOZ04k2ZrWK/dDWvXmnO0B1EQbEmFKjGB6MvabcCVwJPAZUqpTwJfw2wYVGK+d26Jr1hGUsA5fwHRU60QzqZNXRNarE5s4XD+jbOwRaKI14fXeQLFeE85BYCJ+8MD0oNwh2LUjg2gRKhpKV2BsDmc4HTiNKze4du3m4vUdrsWiAJhMwwMe2lV6+tLIJQ1xwU8JiK/Bv4E/EUptSvXxpUSneveYOtQmDJhDp4ZpxOzQXT92q4JKYEo4P4IZzSBJ1hxYhexBGLCvs4BuQbhDcfpLPfD6FGl6UFY+yCw28FlehBxI94VXjr3XB1iKhD2AehBiFU2449AFAhj1lnamWvDSo1wcwNNXjht+GlMHD6V94ZBdEO3bRyFFohkEmdS4Q5Wnth1Kipg9GjG7m0beGmuSuEPJUgGfUjNRCa2SOkJRKrvsQg4XbisEBNr1kAwCOefD+3t6XReTf6wJw3UAPQgwGw1epdS6l+Am4BHRWR0Ti0rMWLtLUTdNk6qPImThpzEO8PAsbVbJlNKIDo6CmOgFdoSbxZqRE2fzqi9LQMvxBSJ4DTACAahpqY0BSJp9j0GEFe3ENOaNSTOmM09B/5qHtRhprxjNxSGY2AJxHARuQb4KzBVRD6NmcF0I3A418aVEirUic0fxG6zc1LlSRwMgLO5aytHvMl6uwrlQaQEwuM58WtNn86Ivc3E4pETv1YxYX2rVmVlUFPDyDaDUEdzgY3qJykPAsDpMkNMHW3w9tu8PcHL0tZV5jEtEHnHXKQuLYHoq9TGDzBDSnbMInpDMXc6TwVCIvJjqz7ToMcZiZP0lgNQ7iknXObDGQ2ZN2avF6O5CQDV0UFBNttHzJu5ZKPK7NixOGMJ3J0JlFLdq/aWNMmWZuyArbwcJk7EpsCz72D/LxQOg8tlrgPkGUkkMaymNDanC6cBjve2QiLBn707qHNaE7VA5B27oVAl5kH01XL0f491TERm0Es57sGKK5rA8LrTz53Vw4BaaGyE4cNxR8xc9GhLI1n4Dt9vjFAnNsDm9fc5t08CAQD8MUgYCZx2Zx8nlAbhwwcIAPaKIelue56Djf27iFIwdSp885vwrW9l38g+kES3EJPbjTsJjg/MJcNnbDvZn2rBpReq847doCBfGk6Ej7ykrpR6Wyn1cjaNKWU8MQPD25U+6hk+xnzQ1AStXaGmcGthInPRdnMNxObPgkAEzbtMIMaAymTqbDQ7yTkqhkCluZjvaG3v30U6OuDDD2FngfI4DCPd1lKc5hcW2/vvY9iEuqFOJo46hdaAQ3sQBcD0IDKpj1o8lJa1RYwnZoCvSyB8lkCow4ePCOtE25rybhtApL0ZL2D3BU78YpYHEYxBNBEl4MrCNYuASNMhAFyVQ9MC4WztZ1KB1eVw//7tjMqqdZkhiUS6KU0wYP4O7y5biqqAT536GexiZ3/ZDsq1B5F3HEkFJRZiKq2k3GIlFsNpgPi6vp0HRowDoL2+tiuDCYi1FkYgoh2mDQ5/Ftp8WwIx0DyIWLPp3XmGDDPTeQFXe6hf1wjtqwWg4WBhtgnZkl0exLAKM9HwzBYfH1QJN595M2PLxlLrj6O0B5F37AYlt0itBSILJDvMMITN3/VNunz0RADa9u8m1ngoPR5va6EQxDrMMFdWBKJbiGkg7YVIZZp5h46AYBDDJng7+pep1Vi7FQBHZ/+EJVt0X6TG5QJgxIEOLvrkzZwz/hzGlo9lb1Bh1O0tiH2DGYcBlFiISQtEFuhsMcMKtkDXzXfoGLOfcehgHa0H96THE+2FqWIe7zBTOJ2BshO/WCrEFCXreyF2N+8mHC9MOZKE1SzIP2QE2GxEAh6CoWTfv2NdXbrEReveDwBwdhYmBViSya6+x04reUApmGwWXh5bNpZ9ZWA/1GD2T9fkBaWUFojBSmerKRCObjffEdU1hBwQPbifjoNd8V6jvTAbr+Kdppfj9GdPILIdYjKUwYqLpvD0ffnP/gFQrS1E7BAsGwpAPOijIgLtseMsVLe2wsknw0MPARDeb34ZcIUKc/OVbiGmtECAaSMwrnwcdan/AvX1+TVuEGMowxQIHWIafIRbzVTI7gIxMjCSRh8kDzcQajA/iC1uMDoLs5M6ETJvcu5A+YlfrHsWUxY9iHCojS+ui1O9Yn3WrtkfVFsbbW4od5vvUbwsSGUY2qPHFojEu+9AOEzdajOhL1G/HwBPOJ57g3tBDKOrrWV3gUh5EOVj2adTXfNOIhHDrtAexGAkYmUmuboVwnM73LT67UhzM7GmgyQFDgYo2E5qI2S+rjtwgsX6AKxU2WCWPYhOKwXYVqD4va2tjVYP+F3m72dUlFEZOX7BvoNvrQCgcbtVudfKYvJGErk19hjYEkmMVDl5aw0ClwvGmUkTVd4qDlda43qhOm8k4tbnxFFae4byJhAi8lURWWb9bBKRB0Xkw25jp1nzbheRdSJy3H7XxUS03Yxd9yyEFwp6cDa3kmxqpMUDnS6whQpz80tanovnRIv1AdjtJD3urHsQnW2mQNhDhVmDsHd00umxYRPzY6EqKqjoQyDa3zG9HU+9mYjgbDKTEPxRw4z95xmza1kPD2LSpHRoQ0TwDLfKqDWXWBmREiYRsyoZaA+id5RSv1FKLVRKLQRWAr8F/pwaU0q9IyJnAAuAs4BDIrIoX/adCKm9DZ7yIUeOVwTxtoVRzc20eW1EPE7snYW5+Rlh04Pwlg3pY2ZmJAO+rK9BhNvN99ERKswCr6MjTMjb9Q1PKodQGT6+QKj3zYKMlQ2mAPubzffZYZAub5JPxDBIpkpKpwRi8pGdgauqzF3ihexNMthICQROLRDHxaoCOxyYA3xaRNZa3oQD+ATwhDK7rL8InNPL+TeKyHoRWd9gufOFJm5lJnnLqo4YT1ZWEOiIYWtrJ+R3EvM6sYcLs29AhUJE7eBzZ2dTm/L7CUazm+YatoTWWaD3yNUZIeJzpZ/bq4aaIabIsTPPArvNMM3QdoO29sNUtMXS7RfjLfnf83JE3+OUQFgL1CmGVY83HxTImx2MJGPm/2nRIaY++RrwG2AdsEgpdRbgBC4G/EAqMNpEL7WelFL3K6XmKKXmVFdX58nk45NKIe0pELaqKipCBs62DiIBLwmPC2e4QKmF4TBhB/icWSjWByi/L+shptRmPleB3iNPZ5RooFs9rapq3EkItR2jHlMyybD6Ng77zA/S+rVPUd0JByvMb4mdTQfyYPWR9CoQPTyIkUPGY0hX2FGTe5IJ6/+0DjEdGxGxAecCy4C3lVKpPLv1wGSgA7NqLEAg3/Z9VAxro5yv8kjBclSPwGnAkIYOEmV+El53umhf3gmb/ajddnffczNABYNZDzGl6kW5C7TA6wvHiQe6BNRVZX4/iTb2XtHV2FOLO6HYctoIAHa+9iQOBY0jzUSAzsZCCIRCpRapx441b0hnn33EnLHl4wg5oKNNV+zPF+k1CKf2II7HOcAaK4T0sIjMFBE7cDmwGdiAuQYBMBOozbN9H4mktcfAX36kQHitekyjWhWqvJyk34s7Wpibn0QiRJyStdLcEghk3YNI7fb2FEIglMIXTpLsJhDuoaZAJBt7v5E2vPW6efz88wCIr1sNQGy8uQgcbjrU63m55AgPYsoUs3jgjBlHzBlbPpawE0ItxRGiHQzoEFNmXASssB7fATwMbAJWKaVeAV4HZovIL4HvAn/Os30fCWWlkPbstRAcaaYW2gCpqET5fHijRs/T84KEo8Sc2ftzSyCY9TTXVKjOV4j3qHs3OQsZYi7oG029h5iaNpnNdwKLLwVgwi4zK8g5eZp5yQLcgG1Gj77H7qM9xnHl4wg7upICNLknaaW5lpoHkdeAmFLqlm6P3wVm9DhuWJlLnwJ+qZTanU/7Pioq1EnYAV7bkTfgitEnpR87hgyF9hjuhDLLMuQ5FmmLxYi5sigQZeVZ9yASKU8sqvLfiChVkr2s205zq2CfaumWDtreDpdcAnfcQWzLOzR7oOb08+jwOzmj3gwf+qaeCpi9P/KNPWlg9LFbt8pbRbMT7KECdTcchBhxcw1CexAniFIqrJR6XClVmHKYHwHpDBPp5eZbOXpS+rFn6AjEKuanCtCX2h6JEXNlT5RsOViDSIfq4hCN5TfDJmmJgJR320holfy2tXRlMUU3rIHly4l9fgll733AzmoHQ/3VdAyvZLh1v3VMngJAvDX/+wyO8iB6we/yE3aC6CymvNG1BuHqY2ZxUXQCUYrYIhGi7qO/tdmHdq1JeKtHYQua304L0RPCEY0Td2dPIOzB8qwX6zO6faPtzHN4pvOwmS9h70Ug7K1d+yDqrZ3Trn0HqNneQMNYKww1enR6jmfSVACSBRAIe7JvgfA5fYQdIJGBU6q92EmFmGzagxh82MMRYr3dfId0bUoLDhuDI2jW+Olszv/ipSMaJ+HO3n9OKSvDZUAykr3NVqnNfAChPL9H4SYzU8lZObRr0AoxOdu67Eq8v5W4Df5znvk8MtFcZ3LXmOHE9qCboNULxGjLf+XeTPoe28RG1G3DVoCNfIOVVIhJC8QgxBGJEff08od3OOj0msJRPmI8zjIr/bG597TJXOKKJUhmUSBSFV3JZi59t5BHOM8eROSQWWTP1V0gHA7CHgfubk2DbDt2sbsCGm75Bv8xD1ovWwxA2SRz3SFWVYHXEyTsAFWAyr1miKnviqExlwN7ofbkDELSaxB6kXrw4YjESXh636EcKffjD7fiqhqGyypzEW7Jf/65M5Yk6c5i/NMSCGnPokB0K/0QacqjQDz6KKP++XZa3WCvmXjEoVDAjbe965u2b89+NlTB7Yt/youTz2PhhIUAOMfXAFA59mREhA63IIVYa0qqPhepARJuB442LRD5Iu1B6DWIwYcrGifp6X0DWjqTqaICt1WrKdKa/+wWd8zAOIaNHwkrHVSyWJ1Wwl034mie3qPYI3+Aq6+mccwQzrwBfMNGH3E8GvTi67RupEpRse8we4d5cDvcXDrlUsrcVtbTWLO+kW24uXci5LFj68j/IrDDyKzvccLjwlGgPTmDkWSJCoT2ILKAO5rE8PZ+87VXWSGLigq85ebjQvSldsezLBCWB5HN0ty2boum+coA+vAXP0QqYcoVe0jaYYj3yGKGsaCfYOthkkYS+4GDeCIJmkb3UuLFEgiGDQMg4nXgKEBhRnuGTWmSbheuqE5zzRdKC8TgJGkk8cYUEa+39wlVVWCzQSCAr8K8scTa89+X2hNX4PFk74KWQNiz+C3ZHukKeSTy0Lu7fe9Oat7azdJLT+JPS+4ilowxacikI+YkygNUHoCOWAflO3aY540bcfTFxowBrxfGm4XwIl4XzgJUpTWzmPoWCMPjxhVL5sEiDZRuiEkLxAnSEevAF4eI/xhF8CZNgpoaEME/xAw/5OPmdwSJhFl++lgi9lGwQkzZ7N1gj8aJOm244wbJPPTuXn33P3OBglO//iNOnX5Vr3OMivJ0T4jyD8x+0/GTJhw90eOBt95KN+ZJ+Nx4WvL/Dd2u6DOLCUB5PbjiVs+KfG5IHKSUqkDoNYgTpD3Wji8O4jtGGe3vfQ/WrgUgUGnV9slzdkuqaqd4s1PJFUh7EI5Q9nLpndE4HeWmiOW6d3ckESH4l+fYPdrPqed97tgTKyrSXeXU9u3E7OCqmdT73KlTwSq3Evd5C9J21GGQ0S59w+s158YLVDxykGEkzPfZ7sximDcPaIE4QdqjpkDY/McQCLc7vR/CY2UxqY5j9zjOBRGr413PWlEnhCUQriyFUZRSOGMJIkNMzyTX79Fzr97H3N1xjKs+e9x5UjmEYAzaO5qIbXuPXRUwvHz0cc8BMAI+fJH8h3AcSTISCEl5k7ppUF5QlhDbXNqDGFS0h5pxJ8EeCPY5VxwOQk5Qea7DnyrKZvP5s3dRK8SUreY+kUQEbxziwQBxG2YV0hySePovAEz8ynePO88x1Fx0bjlYi7HjA3YMgZHBkX1e3wj4zbajecRQhukVZLAGkfYmtUDkBe1BDFJSXdAcgbI+ZlrzXTYki5k/mZDqs5BVgfB6MQScoa6F5daXniFU4SdZv7/fl+uIdeCNm7HxTrfk/D1yv7+DNp8d6dFtrScjxphlM3bt2oBz9x4+qIJRwVF9v0AwSCAK0Xj+FqoTiZj5gc7Ag0h5vErXY8oLqSwmLRClzuuvQ1PmaahhK1/faZXR6Iuo244tzx/KVKc2h69vLydjbDaibgeebrtxt//iVnytIbavfKrfl+uIdeBNmKGPsNuOPccCUbm3gcOjh/S5QOutNr2Fk373NI5wlA+GwMhA3x6ErawcG9DWVN/n3GyRyrXPSCCsLwuFyKgbjKRCTHaXFoiSRXV0kFz4CQ7c/v8yPiditaN0BSv6mGkS9Tix5zn9MeVBOPxZFAgg7HWkO+TFYmFqVr4DQMvO9/p9rZQHIV4fYY8dRw7fo0Odh6g5GCdq1VE6LmPMpk8XvVpLy4hKXqvJLMRkt8qqdDTmTyBSFUMzEYjU/wXdEyI/KCvE5HBlMdU8D2iB6MbhzauwJw0Orf57xudE28wFYHfZkD5mmsS8Lhx5roET7zQzgpwZhsEyJeZxpru/vbH05wztVABE9+zs97VSHoTN5yfqcWZtbaM3tu7ZwLg2cE6b3vfkWbP4w+++yfB/hq/ecxH7x5Rl1NfbWWH+f8hn29F+CYT1fyHSlv+Ks4MRlTA/J3ad5lo6dMQ6uOnZm9jSsAWAuvWmMAyp7fpQh+NhEsaxSxKkvp17g5kJRMLrwhXJr0AkrIwglz/LAuF1pQXi0KMPErdBpxNUXV2/r5XaT2LzB4l5XbhyKKL7N5olu4fMmpfR/LGfuJRDAfjbjr9lFF4CcFplVcLN+asplW5rae9bIFJfFqLtWiDyQlwvUpcUxqGDXPuXa/ntht/yP5v+B4D2d9YDMLIhggqFUEox474Z/OC1H6TPW/L4Em577bb084TVR9ldVpnR6ya9Htx57rkcD1kCEchsnSRToj43vojB7ubdzFi9m72zaqiv9uI60P9S3akQk8MfIO51487hHoL2dzcAUDnj7Izmnz7ydABaIi0ZhZcA3FZV2Ggeq9ImE5aoZlAx1B0wQ2DRjvyXJB+M6BBTKbFvH2rUKL56y1Ncu83Nmx++YY5vN3fK2hU0bl5FbUstO5p28MquVwDTm3hy65M89X7XImzC2tAlgWPsg+iB4ffhieY3Pz61US51U8gWCZ8bX9Tgiad/zLTDULnkOjqqywg09H+TW2dnCw4FDn8ZSb8nt+/R+9sBkMmTM5pe7ilPl+DI1IPwWZsiY3lsO5rue5xBiCm1ZhbXApEfUiEmvUhd/Gxu3sYd5xic0e7nD49GOe2ZtUQTUSr2HOBg0MxqqV/7d1bXreayrRB5ZyORRISNBzYy6WCC1h3vpTuppdpkkukmNL8ff1QRSeRvoToZsgQiw4X0TIl7PfiiBrE//RGAyiVfJDZyGNXNUQxlQGMjXHMNNPcdxkht5nMFykj4fXhztMlMKUVwTz3NVf6unhYZkPIiMhaIVFmVPHaVS4WYyKApjSdoeryJjvz3rBiMpNYgtAfRCyLiEJEPRWSZ9XOaiNwuIutE5L+6zTtqLBfMPPV8Zt77BIG9hwiNHMr52xOsqF3OuINRds6bSlKgc/N6Nm9fwdKl8JMXEmw+sJm1e1bxyh/g3qeT6XULw7r5ZioQzmAF/riZSZMvUq08UzeFbJHwe6iIwDVrwjSfMwfGj8c2Ziwj2mF/0x548UV45BFYtarPa6VScV2BcpTfhz+mUEpl1V6AAx0HmHAoRseEDPYydOOMkWcAmWUwAfirzIJ+eRWItAfRt0Ck1swSnfnd1T9o0RvljssM4M9KqYVKqYWAC1gAnAUcEpFFInJGz7FcGnTFtCtwu33IwoV8fA88+vdfUhGF4JyPUVtlx/H+BzheegWnAYt2weaty2h99TlGt8PH98DmveZ6heron0C4y4fgj8GB9iPTHxNGgmW1y3JyUzSsfRfeDDOtMiXp9zKyA8a1QcU3zB3J3gmTsQH7tm8gsXkjAInDfYthzAp1OANlEAgQiEIsi/2uU7x78B2mHAaZMrVf56UEIqNNcnStQai2/H1DN2KptpZ9h5i85VVAl3epyTGWB1FqHeXyJRBzgU+LyFoReRA4H3hCmXfDF4FzgE/0MnYUInKjiKwXkfUNDSe+AOg97yKGhcDx7AsADD/9HA6MraRydz3TV+8i4bDhMsB45mlqXl4HQCAOzW++ilKK9harfWiGAuGpHolDQcOBXUeM//f6+7n8vnP51ovfyr5IhE2BcPuzu0htWL9zaGg5cumlAJRPPAWAxh1vc2DVywDs+GBtn9eKW9Vbbf4ABILYgI7W7C/w7vhgDUMiUH7anH6dt3DCQu6+6G4um3JZZidY4atYHptDpRapM7kJ+cpMgTDyXPYlkoiws6n/adAlT8IKmWYg3sVEvgRiHbBIKXUW4AS8wD7rWBMwHPD3MnYUSqn7lVJzlFJzqqt7adzSXz7xCQCu32DelKtnL6Bz0jjGHYyweLtB3ac+TkOVl4kvb+CiTR3snWW2pPS+uZb3Gt4jlLoBZFhK2zfeXOzsqN1+xHj4kf/h0H/Ayr/8ku++cvz6QP0mHCbsALFl9889dtQ0AJw33JTOnKmeMhuAzl3v49pmLvrHGvreC5Daq4HXi82q8xRqzn4YbtfavwEQPO2Mfp1nt9n5xtxv4HdlWK7EZiPsthNqyl8osT8hpoCnjKi9y7vMF79d/1tm3DeDWHKQtTtNJDAEszdMCZEva99WSqViKuuBDkyRAAhYdvQ2lnsmTSJUVcZZ+yHmsCHjx+OYPgOnAZURCF51DbXnzebCbTGGd0LHjddxYHQFNW/v5cmtT+KPg3I6M0otBCibYIY2Ih92eRCGMjj7mY24kvC/K4fzszd+xvr967P2K0okQsSZ/Zr/o06bB14vzq98NT3mHm8KqGzdxrAG8+ZjNPbtCSSsVFy8XhzWYno4ywIRTURps1KZ6aMGUzZoqw4SqD+ck7Bhb6RCTJl4EC67i5CTtHeZL+ra6gjFQ7RHB9naRzJBorS0AcifQDwsIjNFxA5cjuktLLCOzQRqgQ29jOUeEeIfMzdMNY6uBLudqjNMMyIOqLrsc9iuNJvJdDph/Oe/StNZpzG3NsF/r7uPie4RiD/zIniOsWZ5B2Nf12aybWue42O7EzRNGcekrQe56j3SqbXZQCJRYs4c/KmXLIF9+9Jd1AAYMoSo08aYVe92vX5j3+UcUqm4eL04rDIVkZbDWTV35Ycruei9GLEyv9nEKcd0njSeyQeT1LX1f+PgR8FIZN6URkSIOAXJczXXloiZjNAZH2TtThNJLRDH4Q7gYWATsAr4ETBbRH4JfBf4M/B6L2N5IXjhJQD4ps8EYPzciwB4b8YI8PuZfMkXqS2H5XTBd2QAACAASURBVLMr8ZUPxXnu+ZRHYdgH9Uzxjcs8xRVglLnIaTtwMD3Udv+vMYDo4/8LM2fyi787ef39l7Pzy2H2eo66cvCnttmgskdmlAgtVT7O3GuWuj4UEOwtfS/UpjKt8HpxlpvXjGZjD8H995thxPZ2Nr38MFdsA/7xH/MSC7ZNP5XJTfB+/bt9T84C6a5lGYSYAKIuGxLOb12wlqglELHBJRCSSJC0lV7nvrwIhFLqXaXUDKXUaUqp7ymlDGARsBL4pFJqd29j+bANwGatQ5TPOAuAiqFjePZLC0j+678CUOat4LZ//yTbfvQtAEZ/+vMAfKIWJrqH908ggkFCHjueQ9bNzzCY+MzrvDHVy8hT58JPfsKopjjBV19P77U44d8vGiPm6rtHQLYIDR+CXUHIY+eDcX7crX0vhHYXCLeVYRPPQp2gyD2/hBUrUNdfz6n3P0Wnx47rnzIvxngiVMyei9OA+k0r8/J6qZ4DmaxBAMRcdiSSu5pXvTFYPQhJJEmWoAdRsCV1pVQYeLyvsbxwyinw/e/D1Venhz794JEf6t/f8Hz6sa9mMnuqnXxzoxAcvbd/AgG0VPkJWruNk39/lWGHw/z1i+eZaVuLFhErD7B4Swdr963lnPG9JnP1C0c0RtyVvz+1MWokbP6Q1kljCQfD+Br6Liktqd7WPh/uipRAnGAp6ro6PO9s4d1qOHXpUhYDa649j7OHZDfd91iUz5oLQOjtDXCcrqbZor99j2MuB45CCcQg8yBIJkjYtQdRmthscMcdMG1axqd0/NfdjLSVwaZN/RaI0NByKpvDKKWof/EJACquutY86HCgPvUpPr0dXtuRnTCTIxon4c6fQFRNNkN1/397Zx5eVXU17nfdMcnNHEjABMIsCMooqMggglpxrjhVra1WRWu1tdaxVRw6WNvf59dK+znVOrS1WqFitaBYFASHQBEIAmGeMkDIPN5h/f44NyEhCQlwknsj+32e++Rkn332WecOZ5219tprJY2biD85kfjKDuRVqg0riNhYYpOt6LTgsSqIf/0LgOuudPOPYXAgBpIfePTYxjwCJPx9cm0IR6ytWAEbN3ba+Y5UQQS8Lpy1XVuTuqTGsgqPRwsiZFxMxw/DZ92GO3cD3HQTzDp8XeND8Wf0pFe5Ul5XTsWaL9ieBBOHndu433vZLNJqoPj9f9oiq7suQMDbdWmGkwdaabTjxkxAU1NIqgk1LhRqC0dDeu/YWHzhNBWhimNcZLZgAbt6eOhz+rnsf2kuj734bYaceMaxjXkk+Hzs7+kjeXuBlc1z5kxrYr+Topoa6x53WEG4cdV3rYI4Xi0ICQa75RxE91q1EW2kpcFzzx3xYXpCb06ogO0V+cRs3s62Xl7OaprC4dxz8budDPh4HVX1Vfg8PoKhIB/v+JiK+gpUlZCGULTFttPhxO1w43a6cTvcBEIB+tb5qe9CBdEY1TRyJLLZWoBYX1yEJ6PtVciO2oMKItZpWRAH9u1k8dbFjdfW8Hdi34nEe5rnUcqvyCcQCtAnqY/VUF0Nixfz7lghK6kPt4yfDeNn09WUD8ii/46NVPz7bRJKSqy8VB9/3Lj+BqCstozP93yOiCAIIoJDHI3bgnVjCdbXEQj68TtBUeLccaT70hnWYxgicsST1IEYL57irlsop6qdOgexrmgdO8t2NoYVK80VsQSCxO0pImHbHir7Z1LTzyoG1fQ9btgekjaE7ORs7EICIYLd0MVkFEQEcGVl4w1C8c6NjNpdwppph3wR4+MpP/NULvzvp4z+4yhuGXcrL335EuuK1uEIwewv4JINcOZO+CITXhoFb5wEFU3zgCkkhu+5OXVQfwShuMfMzJnw5pswaRLuj/4EQMmeLWS0oSBqA7V4/CFCTgcOtxuHy0VIIOWTVSy5aTq9KiG7FEpiYVVveGbKycy/bzUOcfDR9o94YukTjWHBM4fMZM7UOYzJ2QO1tbwxAKYlZnXZpbdg2FCGrtxIxV/+TFxcDHUuIe+BGyl8eS7nDDyHukAd82YOoMeuA9w/HdY1LA9V+OUHcPNKK9xaFHpWQ7kXbr4Q3hwOk7fDqXtg5fSTuHDSjWTs28gEOm5BhGK9eOq7LrNwlb+KoFrns9uCCAT9rJsxit2+IL+YBAfCXt+Uanj1LThjFyQ3mW6pd8DDZ8Fbw6xUOr0q4UAs7E2Ar3pA6MTBrLtrU+snOwok2D1dTEZBRIDYftZq6qrlS4irVxjaMi9Q2lXfIe0/n/LDf5fxw6If0zdjMK9e/DLn/vINery3gJohA6j81lhOXZHDpLe38dz7MVR8YxoaCBKTuwHP7gIcdQd/EaH+HSuOYwsuF3zzmwB40y2lUL53GxljWp9w/2rfV8T6Iej1WD5PEQKDBjAjbysztoI/KYG6zAzchaVct2Y/pUvWsjjlPvpfcTMXvjaTkVUJ/M19GTHl1SxYsITPXprOmDVeggnxfJxdyXUJmV104S2JH3kqcS/9E8db/+LNwSG2psBPPtnCrLkX8MoPl1Lw+19yw4cHCLpdzPy/EAVXzmTH7deSPn8RAz55gX3TTyeYZoX97k7vQeqyHN54Yz01/80mdvMOACqXbuDpU++mLDyN443pWJZajfHirQ91ynW3RoP1APZbEOUlBVy1xlI+d63zse+aSymdcSbZD/2amJ072X/VRezpkUrdCenU9etD+ktv8It3FvOLxdbxKoI0cf191mcL3GWffI5gkKCzG3r0VbXbvsaOHavdkbLF76qCvnPRMFXQj158pGWn6mrVK69UBa3tl6WBu+5Sve46VVB9+OGD/UIh1U8/Vb31VtW0NNX+/VUvuUT17rtVn3pK9Te/UX30UdXc3C67vqasmv9HVdBVzz/eZp/nVj6nfxiL+numHWysq1MtL1etr2/WN7hxg27KjFW/A1090KflXqz3pMkrIGjg/G/oqr//r/II+sGWDzrr8tol8NGSRrn+9+5JWrV5g4acTl02JEbvvjxJK93o2uHpqkVFqnfeqep2Wy9QvfZa1WCw+YD19ar33qs6ZIjq00+rrl5tfd7hc4RENLRzZ4dkW3rxaC2OxZbr3Fq8RVfvXnnYPmsL1yqPoDyC/uzDn9ly3gZ2rP9UFXTd1dNVL79c1eWy3hOfT/WDNj7/BQtU585Vzcuz3ucDB1RXr9YNZw7VEi9a66+1Tb6l49I1LzPWtvGOFSBHO3CPjfhN/lhe3VVBhLZsUQX98gSnKuj29Sva7rxokeqkSaoxMdbH9eMfW0qhm5D36XuqoMsfv7XNPrcuuFX/MtqtoezsDo25IneRvjQSXZ6Fbr76PNXnn1ddtUp1zx59Z/6vNeNudF3hOn3ly1eUR9AN+zbYdDVHwf79qqB+r1tD5eVW2xNPaCAhXhW0MF60MG/1wf47dqjOnq16ww2WkuwoJSXWsfn5HT5k2VUTtcaJBoKBdvue9+p5eu/797a6LxQK6fwJSbq8r0PXF1oPIh9u/VBfXPVis35LdyxtVBB3L7y7w3J2hPUfv6UK+tmTd1kNhYWqzzxjfS+OkM9unqkKml+yyzb5PhnVQzf28dk23rHSUQVhXEwRQMKrqUfsDVLmhT4nntp25xkzrJffD4WFkBVBf/pRkJJludP8+wvb7LMyfyVXupKRDiY8PO2kGbzz2wfZ5YrhockPNdvX69SzKFwNm4o3safcyv2YmRg5FxNpaZCZiWvCBAgnIeSBB3Decw+57/yJYM8enDJo5MH+ffvC3LlHfp7kZOt1BEhcHDFBqKirJCG27Uy/wVCQxVsXsyp/FT8/++c4pLmr5JM173BeThneIMx+/AKm3vwLrpt3HV6XlxtG3YCI5Xtv5mKyeQ6iNpwU0ZNqpVknPR1uu+2oxnKmWUESZQU76JVsz+9NQtYcW3fDKIhIEBNDmc9JUlWQPZkJnOTowCpnt7vbKQeAlF79CAoE97eesM8f9LOmcA3pjswOZ8QFeHza4622D06zyohuKt7E7vLdJHmTWkQ8dTmLF8Ohi/PcboZfenNk5AkjcVbgQlVF8WEVxI6yHfhDfoqqivh8z+eclnVas/25zzzCmUHw+2KZtWAbZ6ddRawrlsr6SvZV7yPdlw507hxEbbH1ABKTmn7MY7l7WpECFQU7YOjEYx4PwBEMEeqGUUzdT6V9TShJtcIsKvt3v5v+keBwuiiNFeRA6wn7cvflUhesI5W4I1IQbZHoTaRXfC9LQVTsJiuSEUwNnHgi2JGa3macPktxVpcdPiliXnEe9y6FWevg7Y1vN9u3u3w3IxauoiA7DfcjjzJtO9zjnMwLF70AwNaSg1mLGxREz7ietiuIhtrfseFKfsdCbLplcdYU7mmnZ8exFET3u912P4m/JlT2sJ7YnMOGR1iSzqci3t1mwr6Ve1cCkKQeWxQEwJC0IWw6YLmYIupeinJcPsvlVdtOzqtNxZu4Zzk8vdjFO181X7z597d/ycSd4Ln+O3DLLZCczJMv5zPzgZe4c0VzBdGwijozMdP+MNcSS0HE2aAg4jKstTS1RXuPeawGjIIwHBH1GVa+odTRXbiyN0JUx3vbTNi3Mn8lid5EYupDR5yypC2GpA5pdDFlJUSBBRGluHyJANRWHl5B7NiTS1oN9C4J0Puz9Y0V4VQVee01AFJvusOaY3nySYiPJ+Gz//LzxbBlf17jOKW1pcS540iOSbbdggiVWteQ0PPYHwgSe1vrkjpSKrejWAqi6xJm2oVREBEiffAoALLGnx1hSTqf2iQfceWt1x1Ymb+SMb3HWHUJbLQgiqqKyK/Mjw4XU5TijrcURF07FkTFpoPpym9cddDNtLZoLVNXl5I/erA1uQ7wve/BqlXInDnEBaBka27jsaW1paTEpOBz+2y3ILTcKllrxxxEfC/rWoLF9tUjcQZDqKv73W67n8RfE7Jm3QgzZ+I+seMJArsr/uRE4qta5vzxB/18WfAlY3uPBZsVRAPGxdQ2nnjLzVlfWXbYfoFt4RrS48dz6Ubh7eUvoqos+vQ1RhdA/IWXtzwoXLEvtHFDY1NpXSnJMcn4PD7bLQhHeQVVbmyp8yHJyYSENufNjgZHSNFuVm4UjIKIHJMmwTvvdLhUaXdGU5JJqQoRCDVP2Lds5zJG7Kxjco9xtiqIhkgmwFgQh8GTYK3QPpyCqA/WE7MnHKI8Zw7uoHLKB+tYuGUh+9/5OwAJMy9peeBg6zPwbtvV2FRaG1YQnWBBOCqqqIy1yYXjdFIe68BZcozZhJsOGVTjYjIYWkPSepJUB8VlBc3acxY8S85zcP4Df7KS69mkIAamDGxMvmYURNvEhhWEv7LtrLlbS7bSp1QJupxwzjmETj2V769ycc+/72bgqu3UxsfA2LEtD8zKwu9xkb6nlBq/5V5spiBstiBcldXUxNoXtV/pc+PpQKGrjuIMKuoyCsJgaIG7p+UXLt67pbFNVRnwytsEHYJr4SKoqrJNQXhdXvol9wMgM4J5mKKdxGQr3r+qvG1fe15xHtll4D+hFzgcOH7wAwYXBcj8dD0ztkBgyiRo7cnY4aCqby8GF8P20u1AEwXhsd+C8FbVUBtnX8biqoQYvOXVto3nDIVQY0EYDC1pTNiXf7CK7Lov3+fCVdVsvGIafOc7VqNNCgKseYgYVwypsV1TPa474kmw5iDKSvLb7LOpeBPZpeDs399quOIKQr0y+M1iJ/3KIP78VtxLYUKDBjL4AGwpsR4MmloQ/pAff9C+WhTeqjrLmrGJusQ4fBX2VdszFkQ7iEiSiLwnIotEZJ6IeERkp4gsCb9ODvebIyJfiMgzXSWboXNJyRwIwL5dB6upFf+/J3ApZNz/hJVa4vbb4YILbDvn5SddztUjrm5M82BohXBYcUVZ2+GceQfy6F8uuPtbKVPweHDMvo3hBeE04dOnt3ms96STGXgAtu3LQ1WbWRBg72rquGo/AZ99Dxj+pAQSWgmsOFqcIVq3tKKcrrQgvgX8VlXPAQqA+4C/qurU8GutiIwFzgTGA0Ui0va3z9BtyOg3AoDihpDHujpOeWs5K05JJe2UCRATA7//PYwaZds5bxpzEy9e/KJt430tSUkh5BBc+W3nydpauIGMCj1YBAqsBXEej5X6ZfDgNo+NGzYSTwhKN62hsr6SkIYaw1zB3nxMcbVBAgn2pVQJpiSRXN0ysOJocYaMBXFYVHWuqjYUWe4JBIALRORzEXlBRFzAFOAf4WyDC4EWBQRE5GYRyRGRnH37Ws/vY4gunEOtUF7HBivkseiTRaRWBiieNTOSYhliYtiXlUq/HWUEQy0LB6kq5ZtzcSjNFURGhrUg7uGH4TAWmpx4IgCBjespqbXWWnSGBRHSEAm1itqoIEhNJaUGSqvbDnWtC9Txn23/sdJit4MzpKiz+6W+6/I5CBE5HUgB3gemq+p4wA2cD/iAhgQoB4CMQ49X1WdVdZyqjusZhfltDK0QH09Ruo+ULdZHu2vpvwDIPuvSSEplAMqHDWDkXiW/suU8RO6+XOL2hiewmyoIgDvvtOqxH46wdeHZurMxD1PDHATYZ0FU1paTUAckJtoyHoAjrScOoLRwR5t9nvzkSaa9PI3L37icstrDryVxhgBjQRweEUkFfgd8F1ijqg3fyhxgMFAJNDgS47taPkPnUTooi/67q6mqr6J+1ReUe2HouPMiLdZxT3DkKfQthz3b1rTY917ee2Q33PcOVRAdISODulgPSTsLWb9vPdA5FkT5/r04sBa42YW7Rzija37rCkJVeW/Fy/z0v4ksWzmfCc9POKzCc4VAbVjE19V05SS1B3gDuF9VdwCviMhIEXEClwBfAiux5iAARgLbu0o+Q+cSGjGcE4th4961+DZsZVuWD6/HvklFw9ERN84qRVvx+bIW+97b/B7j/emWG6lPnyMfXASGDGbQfuXnS38OwIAFyzjzG7fgDNpnQVTtt5LquZLti1iLSe8NQHXh7lb3ry1ay1XzNvPoP8vZ/YyXS+dv5JOdLd9DsFxgriDGgmiHG4ExwIMisgTIBV4BVgMrVPUDYBkwWkSeJjyJ3YXyGTqRhDGn4w7B7pzFZO8spXTIUTyRGmynxxlWLjBdvbpZe0VdBct2LuPUQDr07m1NSh8F3qEjGF0ex9rCtaCQ+fuXicvbTu9K+yyIqmLLEeFK6WHLeABxGdYCy5qi1lN+/2PVa1y7FurPmgyTJ/OLxbDjw3mt9g2GgrhC2JIGpKvpMolV9Q/AHw5pnnNIn1A4cmkm8LSqbsPwtSB9wjQAyuf9jaRacI0aE2GJDABxmf3IT3SQsD4PAgGYNQuuv57FwwR/yM/ACvfRuZcamDKFjNdf57zNUOcEz2Yr/XdmuX0WREM1Oa+NCiK+l3XN/n0tI7xUlZLXXyK1Brj/IaveR3Y2/uVLoZUaUIFQAHc3VRBR5+NX1RpVfVNVt7bf29BdcJ80Ar8DRi62Ql3Tz5gRYYkMDWzJTiBjcwGBl/8M8+ez+/breD7nWRJd8STv3n9sCuLGG9FBg3jmwxju+JzGhHVZ5fZZEPUHrGjG2NQWMS1HTUPK71ArGV1X5q9k5rIiKnulwtlnQ58+lKbE0mPN5lYjmgJBPy4FMVFMBkMbeDwUnJDAyYXWD6j/pAsjLJChgcLBJ9BnbyV1jzxEhQey8qvQ997j0b0nIrt2wcxjCEf2eJAnn2RAfi2XbgC5/nogrCBssiD8DcWCeva2ZTwAZw8rQlJbyej657cfY8ZWcH73JnA4QISSUUMZvaOezQc2t+gf8IdXZBsLwmBom9JB1kTn7vQYXEkpEZbG0EDVSYNwhcC3q4Dvz/IRzDyBZ9f147Z5u2DcOLjmmmM7wSWXwOTJ1vYDD6Ber60WRLDMWmPhS7NPQeByUREjOEubZ3Rdvms5vf5i1cOI/d7sxvaYiVMYfABWr/ugxVD++prGMbsbRkEYugwdfhIAxYNMAr1owhGeD1rdC2KuuAbnnXeRuWY77vwi+J//sZ6SjwURePVVmD8fBg9GsrLoU+mwzYIIhW/icT1sVBBAhc+Nu7Ti4Hk0xE/n3cEdXwihSy+Bfv0a96VPsyzi/f/5V4txgvWWBSHdMLW/URCGLiN1nPUU6Rw1OsKSGJqSMnwsz46B28+Ha075llUVLikJrroKJk605yR9+sDFF1vbWVn0rXDYl4upvMwq8JOQYM94YQ7N6PrCqhc47Z+rSKxVXA/9rFlf5/gJBAVcOatajBOor7U2uqEF0f0kNnRbsmZcRtB3Hydd+f1Ii2JoQnZqfy64yEqNPil7EogDcnMhLa1zTpiVReZatU1BOMorqfQKiTZXbKtNjCWuwlopuGLXCn7yz9vZ9oUbPX86MvqQhxyfj8IBGfTbUECNv4ZY98E1PsHwHIS4jAVhMLRNZibOyiock6dEWhJDE/ol98PtcHPNydfgkPAtITPTSqLYGWRm0qs8RHWtPQV5XJVVVNtYLKiBYHIScRW1nPXns7j09Ut5OCee5Ao/8uBDrfYPjB/L+N3Korx/Nx/HuJgMBkN3Jd4Tz+ff+5w5U+e039kOsrLwBBTngRJbhnNX1VATa//N96ThZzGo3EXiqvWM21TJnQvLLLfbGWe02v+EGZeRVAcfvvnrZu3GgjAYDN2aUb1GNXOLdCpZ1iplX6E9CsJbWUutz2vLWE3x3HMv7j7ZzH+ugrfnxSBDhsCzz7bZ33XpN6mL9TD2Hysaq+gBBP31gLEgDAaDoX3CCiJhf9u1sI+EmGo/fl8nuMP694fly5GTT8ZRUwtvvgmHmwhPTqbu29dyVS78beFvG5srqi1F6HJ3ksuuEzEKwmAwdC2ZVphz0v6WcxD+oJ/Vu3M6VGOhAV+NH39CnG3iNSM9HZYtg23bYPjwdrsn3vMgzhDEPPsi9UHLcvho82IABmUM6xwZOxGjIAwGQ9eSkUHQIaQVHwwhVVUWbFzAQzf0YcDAU3n393d2aChVxVcbIhRvY7GgQ3G7oaO1ZwYMoOiciVy/vIr5Oa+iqny4eREA8XFJnSdjJ2EUhMFg6FqcTsrSfPQ4YE3e1gXquPHtG3nsqYt47G9FxPvhtHt/x8ache0O9W7euyTWgiRFz803Y85TJNdB4h0/Zm3hGoqKd1k7zDoIg8FgaJ/yHomklxZQUFnAZa9fxq7cFaybn4A7K40DL87Fdf5M9JKL+WzGZGIcHrZPGUnBsD7EeXzEe+JxiIPcfbn8cf5D7PTDsIGnRfqSGnFMOI3Pb7uE856Zz1vXX8DfVoI6nchh6ndHK3Ikvr5oY9y4cZqTkxNpMQwGwxGSO3U4znXrmfFgFiNy9zFvnpcYv8LSpTByJKuff5xBt/+UuHoICbgU1qZDjQvSaqDQB8VxcN4WwYUDWbQIpk2L9GU1UlNfzfvjUrhobT1FKR7SF3xo36p0GxCRlao6rr1+xsVkMBi6nJqMNPqWwdOvHuDdP/mJyciEL76AkSMBGHXTQ8RW1lFWVczeHes48JsnGDL0DIYPm0TK5HM5ue9YzvH3xXXLbCQvL6qUA0CsJ451v7qbH5wH8177aVQphyPBuJgMBkOXc8LIicT9fSmXbHUj378JHn+8RQip0+0hxZ1KSlwq/Gg4/OgBvIAvMiIfMbOn3sMjVDFr6m2RFuWoMS4mg8HQ9VRVwSefwKRJEGtqk3c1HXUxGQvCYDB0PT4fnHNOpKUwtENUzkGIyAsiskJEWs+KZTAYDIZOJ+oUhIhcBjhV9XRggIh0v9gwg8Fg+BoQjS6mqcDfw9uLgDOBvIadInIzcHP430oR2XgM5+oBtKxKHj1Eu3xgZLQLI6M9GBk7RnZHOkWjgvABe8LbB4AxTXeq6rNA2ykVjwARyenIRE2kiHb5wMhoF0ZGezAy2kvUuZiASqAhrCGe6JTRYDAYvvZE4813JZZbCWAksD1yohgMBsPxSzS6mOYDS0XkBOAbQGcmWbHFVdWJRLt8YGS0CyOjPRgZbSQqF8qJSAowA/hYVQsiLY/BYDAcj0SlgjAYDAZD5InGOQiDwWAwRAFGQRgMBoOhVY5LBRGtqTxEJElE3hORRSIyT0Q80SiriGSIyH/D21EnXwMiMldELgxvR42cIpIiIu+KSI6I/F8UypchIkvD224RWSAin4jId9tqi7CMfUVkiYh8KCLPikVUydikbYSIvB/ejriM7XHcKYgoT+XxLeC3qnoOUABcRXTK+hQQG83vpYhMAnqp6oIolPM64LXwYqkEEfkJUSJfOEDkzxzMqn0HsFJVJwKXi0hCG22RlPEWYLaqTgP6ACdHoYyIiAC/BdzhpojK2BGOOwVB66k8ogJVnauq74f/7QlcS5TJKiLTgCosBTaVKJMPrCcz4Dlgu4hcTPTJWQyMEJFkrBtaf6JHviBwJVAe/n8qB2X7GBjXRltX0kxGVX1QVb8K70vDSmMRVTKG+Q7wnyb/TyWyMrbL8aggDk3lkRFBWVpFRE4HUoBdRJGsIuIBfgrcF26K1vfyemA98CQwHrid6JJzGVYunB8AXwEeokQ+VS1X1bImTa19xhH93FuREQARuRLIVdW9RJmMIpKG9cD3VJNu0fr7aeR4VBBRncpDRFKB3wHfJfpkvQ+Yq6ql4f+jTb4GRgPPhtfQvIr1dBZNcj4M3KqqjwIbgGuILvma0tpnHHWfu4gMAH4M3BVuijYZfwncr6r+Jm3RJmMLok6gLiBqU3mEn9DfwPoi7SD6ZJ0O3C4iS4BRwIVEl3wNbAYGhLfHAf2ILjlTgJNFxAlMwLp5RJN8TWntOxhV38uwv/+vwHebPLVHlYzAFOBXDb8dEXmc6JOxJap6XL2AROBLrMmir4CkSMvURLbZQAmwJPz6dhTLuiRa30sgAUvRfgyswHLnRI2cWG6vXKwnyPej8X0EloT/ZodlfRr4AnC21hZhGX8F5Df53UyJNhk7+t5G+nM/9HVcrqTuTqk8ol3WaJevgWiXM5rlC+dFOxNYqOEnZisRCgAAAqVJREFU9Nbaog0j47FzXCoIg8FgMLTP8TgHYTAYDIYOYBSEwWAwGFrFKAiD4SgQkTQRuTq87Q6vkj3qfocc423aT0Sc4Ygng6FLMXMQBkMHEZE7gTpV/aOIeIFNWKG+92MtcgqFu44F+qlqWUf6ARdhLUAMYaVnuBIIND018IiqLui8qzMYWhKNFeUMhoghIlOAvwB5wFBV7dVkdwDwh5/mU4EfAQWqevUhYywB6jvaD6jDCnEdDLyFFf76zSZdFxvlYIgERkEYDM0JAPNU9fsi8kU4y+aIcPtIrKf8AHCzqk4UkYWHuH/OC/9VrNWxb3agX6NFoaobReQW4A9YMfIZWOthDIYuxygIg6E5QeBSERkBpKvqiyLSR1V3icitQC3wOlbiNQCXqp4NlkWgqoEm0wd1gL8D/cCaDwyE10PMB07BSuYG8FpnXKjB0B5GQRgMzQly0IL4VERigQVh11NrDBWRD8LbIw8zbnv9QsCvgYewrIt94XYB4kXEq6pvH9GVGAzHiFEQBkNzmkb2iarWiMgztJ2K+StVnQ6Ncwpt0V4/L5CMNQ+xFmsO5K/A4+G/niO4BoPBFoyCMBia4+KgiykTQFWfA2ijkM+oppaBiLT1m2qvXzzWxPSvsdKTnw9chjVHUQwsPMrrMRiOGqMgDIbmODnoYnr4kH0NkwaOhm1V7XHoAOGsvCGs31dH+xWr6hUi0jvcf6yIfISVn+mHWIrCYOhSzEI5g6E5OcBjAKo6p6FRRGZhFfjJw3ra97Z2sIi8hjUhXX8E/dxNd3GwCtljWOnAbwHWHP0lGQxHh1koZzB0gPBkdUhV69rpl6CqFR0Yr0P9DIZIYhSEwWAwGFrFuJgMBoPB0CpGQRgMBoOhVYyCMBgMBkOrGAVhMBgMhlb5/+2LKY/pm61WAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1f01a1a6cf8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y=np.squeeze(y)\n",
    "p=np.squeeze(p)\n",
    "x=range(150)\n",
    "plt.title('预测结果对比')\n",
    "plt.ylim(0,2000)\n",
    "plt.plot(x,y, color='green', label='实际值')\n",
    "plt.plot(x, p, color='red', label='预测值')\n",
    "plt.legend() # 显示图例\n",
    "plt.xlabel('时间间隔')\n",
    "plt.ylabel('风功率')\n",
    "# plt.savefig(\"filename.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "   \n",
    "\n",
    "#字典中的key值即为csv中列名\n",
    "dataframe = pd.DataFrame({'y_true':y,'y_predict':p})\n",
    "\n",
    "#将DataFrame存储为csv,index表示是否显示行名，default=True\n",
    "dataframe.to_csv(\"test.csv\",index=False,sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
