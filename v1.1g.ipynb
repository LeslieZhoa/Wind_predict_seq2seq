{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import h5py\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#读取数据\n",
    "f=h5py.File('D:/data/wind/data_4.h5','r')\n",
    "data_x=f['x']\n",
    "data_y=f['y']\n",
    "data_y=np.expand_dims(data_y,axis=2)\n",
    "\n",
    "#分配训练测试集\n",
    "num=list(range(data_x.shape[0]))\n",
    "num1=random.sample(num,20)\n",
    "num2=set(num)-set(num1)\n",
    "num2=list(num2)\n",
    "num1.sort()\n",
    "\n",
    "\n",
    "x_train=data_x[num2]\n",
    "y_train=data_y[num2]\n",
    "\n",
    "x_test=data_x[num1]\n",
    "y_test=data_y[num1]\n",
    "#生成批次\n",
    "train_queue = tf.train.slice_input_producer([x_train,y_train],shuffle=None)\n",
    "val_queue = tf.train.slice_input_producer([x_test,y_test],shuffle=None)\n",
    "batch_xt,batch_yt=tf.train.shuffle_batch(train_queue,batch_size=16,capacity=500,min_after_dequeue=150)\n",
    "batch_xv,batch_yv=tf.train.shuffle_batch(val_queue,batch_size=16,capacity=500,min_after_dequeue=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 搭建模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 超参数设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = batch_xt.shape[1]  # Time series will have the same past and future (to be predicted) lenght. \n",
    "batch_size = batch_xt.shape[0]  # Low value used for live demo purposes - 100 and 1000 would be possible too, crank that up!\n",
    "\n",
    "output_dim = 1\n",
    "input_dim = batch_xt.shape[-1]  # Output dimension (e.g.: multiple signals at once, tied in time)\n",
    "hidden_dim = 24  # Count of hidden neurons in the recurrent units. \n",
    "layers_stacked_count = 3  # Number of stacked recurrent cells, on the neural depth axis. \n",
    "\n",
    "# Optmizer: \n",
    "learning_rate = 0.007  # Small lr helps not to diverge during training. \n",
    "nb_iters = 10000  # How many times we perform a training step (therefore how many times we show a batch). \n",
    "lr_decay = 0.92  # default: 0.9 . Simulated annealing.\n",
    "momentum = 0.5  # default: 0.0 . Momentum technique in weights update\n",
    "lambda_l2_reg = 0.003  # L2 regularization of weights - avoids overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 搭建s2s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('encoder'):\n",
    "\n",
    "    # Encoder: inputs\n",
    "    enc_inp = tf.placeholder(tf.float32, shape=(None, 10,10), name=\"inp\")\n",
    "\n",
    "    # Decoder: expected outputs\n",
    "    expected_sparse_output = tf.placeholder(tf.float32, shape=(None, 10,1), name=\"expected_sparse_output\")\n",
    "    pharse=tf.placeholder(tf.bool,name='if_train')\n",
    "    \n",
    "   \n",
    "\n",
    "    # encoder cell \n",
    "    cells = []\n",
    "    for i in range(layers_stacked_count):\n",
    "        with tf.variable_scope('RNN_{}'.format(i)):\n",
    "            cells.append(tf.contrib.rnn.GRUCell(hidden_dim))\n",
    "            # cells.append(tf.nn.rnn_cell.BasicLSTMCell(...))\n",
    "    cell = tf.contrib.rnn.MultiRNNCell(cells)\n",
    "    \n",
    "    \n",
    "    enc_outputs, enc_memory = tf.nn.dynamic_rnn(cell,enc_inp,dtype=tf.float32)\n",
    "\n",
    "with tf.variable_scope('decoder'):  \n",
    "    decells=[]\n",
    "    for i in range(layers_stacked_count):\n",
    "        with tf.variable_scope('RNN_{}'.format(i)):\n",
    "            decells.append(tf.contrib.rnn.GRUCell(hidden_dim))\n",
    "            \n",
    "    decell = tf.contrib.rnn.MultiRNNCell(decells)\n",
    "    dec_state=enc_memory\n",
    "    dec_inp=enc_outputs[:,-1,:]\n",
    "    dec_outputs=[]\n",
    "    #reshape expected_sparse_output\n",
    "    w_h = tf.Variable(tf.random_normal([output_dim,hidden_dim]))\n",
    "    b_h = tf.Variable(tf.random_normal([hidden_dim]))\n",
    "    dec_inputs=[tf.matmul(expected_sparse_output[:,i,:],w_h)+b_h for i in range(seq_length)]\n",
    "    \n",
    "    for time_step in range(seq_length):\n",
    "        if time_step>0:tf.get_variable_scope().reuse_variables()\n",
    "        (dec_output,dec_state)=decell(dec_inp,dec_state)\n",
    "        if pharse==True:\n",
    "            dec_inp=dec_inputs[time_step]\n",
    "        else:\n",
    "            dec_inp=dec_output\n",
    "        dec_outputs.append(dec_output)\n",
    "    \n",
    "    # reshape seq2seq输出 \n",
    "    w_out = tf.Variable(tf.random_normal([hidden_dim, output_dim]))\n",
    "    b_out = tf.Variable(tf.random_normal([output_dim]))\n",
    "    \n",
    "    # 最终输出\n",
    "    output_scale_factor = tf.Variable(1.0, name=\"Output_ScaleFactor\")\n",
    "    \n",
    "    reshaped_outputs = [output_scale_factor*(tf.matmul(i, w_out) + b_out) for i in dec_outputs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 搭建损失函数 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loss and optimizer\n",
    "\n",
    "with tf.variable_scope('Loss'):\n",
    "    # L2 loss\n",
    "    output_loss = 0\n",
    "    for i in range(len(reshaped_outputs)):\n",
    "        output_loss += tf.reduce_mean(tf.nn.l2_loss(reshaped_outputs[i] - expected_sparse_output[:,i,:]))\n",
    "        \n",
    "    # L2 regularization (to avoid overfitting and to have a  better generalization capacity)\n",
    "    reg_loss = 0\n",
    "    for tf_var in tf.trainable_variables():\n",
    "        if not (\"Bias\" in tf_var.name or \"Output_\" in tf_var.name):\n",
    "            reg_loss += tf.reduce_mean(tf.nn.l2_loss(tf_var))\n",
    "            \n",
    "    loss = output_loss + lambda_l2_reg * reg_loss\n",
    "    tf.summary.scalar('loss',loss)\n",
    "\n",
    "with tf.variable_scope('acc'):\n",
    "    acc=0\n",
    "    num=0\n",
    "    for i in range(len(reshaped_outputs)):\n",
    "        acc += tf.reduce_mean(tf.nn.l2_loss(reshaped_outputs[i] - expected_sparse_output[:,i,:]))/tf.reduce_mean(tf.nn.l2_loss( expected_sparse_output[:,i,:]-0))\n",
    "        num=num+1\n",
    "    acc=acc/num\n",
    "    acc=1-acc\n",
    "    tf.summary.scalar('acc',acc)\n",
    "\n",
    "with tf.variable_scope('Optimizer'):\n",
    "    optimizer = tf.train.RMSPropOptimizer(learning_rate, decay=lr_decay, momentum=momentum)\n",
    "    train_op = optimizer.minimize(loss)\n",
    "    merged_summary = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0\n",
      "train: loss: 4574.5302734375 acc: -0.5366077423095703  val: loss: 3006.13623046875 acc: 0.6289818286895752\n",
      "step: 5\n",
      "train: loss: 1145.770263671875 acc: 0.8569021224975586  val: loss: 1012.7208862304688 acc: 0.8355456590652466\n",
      "step: 10\n",
      "train: loss: 1494.511474609375 acc: 0.8006479740142822  val: loss: 1523.33837890625 acc: 0.6977113485336304\n",
      "step: 15\n",
      "train: loss: 886.3568115234375 acc: 0.8470785617828369  val: loss: 1098.5958251953125 acc: 0.7923693060874939\n",
      "step: 20\n",
      "train: loss: 655.4415893554688 acc: 0.8601730465888977  val: loss: 725.5482788085938 acc: 0.8896011710166931\n",
      "step: 25\n",
      "train: loss: 953.4411010742188 acc: 0.8364667296409607  val: loss: 758.8779907226562 acc: 0.9028199315071106\n",
      "step: 30\n",
      "train: loss: 1226.9429931640625 acc: 0.7341808080673218  val: loss: 1428.004150390625 acc: 0.7949180603027344\n",
      "step: 35\n",
      "train: loss: 436.9747314453125 acc: 0.9013840556144714  val: loss: 928.2267456054688 acc: 0.873285710811615\n",
      "step: 40\n",
      "train: loss: 1213.9058837890625 acc: 0.79722660779953  val: loss: 955.0809326171875 acc: 0.7987954020500183\n",
      "step: 45\n",
      "train: loss: 436.54132080078125 acc: 0.8668654561042786  val: loss: 1552.5703125 acc: 0.810637354850769\n",
      "step: 50\n",
      "train: loss: 761.4331665039062 acc: 0.825107216835022  val: loss: 998.41845703125 acc: 0.7904841899871826\n",
      "step: 55\n",
      "train: loss: 374.2558898925781 acc: 0.9354748129844666  val: loss: 537.2269897460938 acc: 0.9056456089019775\n",
      "step: 60\n",
      "train: loss: 916.700439453125 acc: 0.8459073305130005  val: loss: 735.000244140625 acc: 0.877213180065155\n",
      "step: 65\n",
      "train: loss: 681.7813110351562 acc: 0.8850116729736328  val: loss: 812.521240234375 acc: 0.8427233099937439\n",
      "step: 70\n",
      "train: loss: 1114.2205810546875 acc: 0.8520620465278625  val: loss: 817.5706176757812 acc: 0.8736643195152283\n",
      "step: 75\n",
      "train: loss: 2079.1376953125 acc: 0.7397133708000183  val: loss: 821.6248168945312 acc: 0.8861790895462036\n",
      "step: 80\n",
      "train: loss: 1480.923095703125 acc: 0.732865035533905  val: loss: 1501.534423828125 acc: 0.7931532859802246\n",
      "step: 85\n",
      "train: loss: 707.3076782226562 acc: 0.8476144075393677  val: loss: 1127.3651123046875 acc: 0.8064634799957275\n",
      "step: 90\n",
      "train: loss: 580.9572143554688 acc: 0.8762471079826355  val: loss: 1548.8726806640625 acc: 0.7842676639556885\n",
      "step: 95\n",
      "train: loss: 716.3349609375 acc: 0.8726398944854736  val: loss: 862.8702392578125 acc: 0.86532062292099\n",
      "step: 100\n",
      "train: loss: 828.6243286132812 acc: 0.869581937789917  val: loss: 1080.2313232421875 acc: 0.8324897885322571\n",
      "step: 105\n",
      "train: loss: 565.8165283203125 acc: 0.8323156237602234  val: loss: 2718.16259765625 acc: 0.6929231286048889\n",
      "step: 110\n",
      "train: loss: 686.1344604492188 acc: 0.8764758110046387  val: loss: 1348.4888916015625 acc: 0.7697399258613586\n",
      "step: 115\n",
      "train: loss: 649.7164916992188 acc: 0.9355286359786987  val: loss: 993.2650146484375 acc: 0.7994266748428345\n",
      "step: 120\n",
      "train: loss: 1360.5081787109375 acc: 0.7401556968688965  val: loss: 990.5374755859375 acc: 0.8787111043930054\n",
      "step: 125\n",
      "train: loss: 645.5737915039062 acc: 0.8743910789489746  val: loss: 2582.916259765625 acc: 0.7185930013656616\n",
      "step: 130\n",
      "train: loss: 1146.4129638671875 acc: 0.8622753620147705  val: loss: 1081.70458984375 acc: 0.8105286955833435\n",
      "step: 135\n",
      "train: loss: 1241.1475830078125 acc: 0.7360750436782837  val: loss: 1227.95556640625 acc: 0.7761743068695068\n",
      "step: 140\n",
      "train: loss: 1395.0169677734375 acc: 0.6719321608543396  val: loss: 1400.6451416015625 acc: 0.8243123888969421\n",
      "step: 145\n",
      "train: loss: 518.0498046875 acc: 0.8880909085273743  val: loss: 424.22735595703125 acc: 0.9308028817176819\n",
      "step: 150\n",
      "train: loss: 843.9075927734375 acc: 0.8547406792640686  val: loss: 1019.1854248046875 acc: 0.8714216351509094\n",
      "step: 155\n",
      "train: loss: 652.3574829101562 acc: 0.8697876334190369  val: loss: 1356.0634765625 acc: 0.8080968260765076\n",
      "step: 160\n",
      "train: loss: 624.5369262695312 acc: 0.8876286149024963  val: loss: 749.5445556640625 acc: 0.8734091520309448\n",
      "step: 165\n",
      "train: loss: 534.46630859375 acc: 0.8945230841636658  val: loss: 700.8358764648438 acc: 0.8562464714050293\n",
      "step: 170\n",
      "train: loss: 463.38916015625 acc: 0.9228069186210632  val: loss: 826.1052856445312 acc: 0.8976360559463501\n",
      "step: 175\n",
      "train: loss: 1089.407958984375 acc: 0.8268560767173767  val: loss: 568.2022094726562 acc: 0.8987876176834106\n",
      "step: 180\n",
      "train: loss: 470.5501403808594 acc: 0.9338476061820984  val: loss: 1842.01171875 acc: 0.7268236875534058\n",
      "step: 185\n",
      "train: loss: 755.1830444335938 acc: 0.8315436840057373  val: loss: 640.11572265625 acc: 0.9040963053703308\n",
      "step: 190\n",
      "train: loss: 811.168212890625 acc: 0.8345636129379272  val: loss: 2089.767578125 acc: 0.7590087652206421\n",
      "step: 195\n",
      "train: loss: 711.09814453125 acc: 0.8615403175354004  val: loss: 980.5445556640625 acc: 0.8123906850814819\n",
      "step: 200\n",
      "train: loss: 767.3515014648438 acc: 0.8433175086975098  val: loss: 1312.158447265625 acc: 0.8087402582168579\n",
      "step: 205\n",
      "train: loss: 817.5769653320312 acc: 0.8383933901786804  val: loss: 2617.859375 acc: 0.7178314328193665\n",
      "step: 210\n",
      "train: loss: 737.5370483398438 acc: 0.875917375087738  val: loss: 1681.7203369140625 acc: 0.7701655626296997\n",
      "step: 215\n",
      "train: loss: 298.4959716796875 acc: 0.9377999901771545  val: loss: 1499.56201171875 acc: 0.8286831974983215\n",
      "step: 220\n",
      "train: loss: 644.0197143554688 acc: 0.8729247450828552  val: loss: 637.5540771484375 acc: 0.8444348573684692\n",
      "step: 225\n",
      "train: loss: 496.1646728515625 acc: 0.8759511113166809  val: loss: 710.7340087890625 acc: 0.8632482290267944\n",
      "step: 230\n",
      "train: loss: 346.77484130859375 acc: 0.9449328184127808  val: loss: 944.5284423828125 acc: 0.8372209072113037\n",
      "step: 235\n",
      "train: loss: 1348.033203125 acc: 0.7855068445205688  val: loss: 697.1469116210938 acc: 0.8900238871574402\n",
      "step: 240\n",
      "train: loss: 1014.3959350585938 acc: 0.8579209446907043  val: loss: 659.3181762695312 acc: 0.8875982761383057\n",
      "step: 245\n",
      "train: loss: 790.3872680664062 acc: 0.9203094840049744  val: loss: 715.3053588867188 acc: 0.9026162624359131\n",
      "step: 250\n",
      "train: loss: 1092.974853515625 acc: 0.6289891004562378  val: loss: 1309.80810546875 acc: 0.7948751449584961\n",
      "step: 255\n",
      "train: loss: 1245.158203125 acc: 0.7991026639938354  val: loss: 731.4553833007812 acc: 0.8707969784736633\n",
      "step: 260\n",
      "train: loss: 1146.923583984375 acc: 0.812131941318512  val: loss: 930.054931640625 acc: 0.8521653413772583\n",
      "step: 265\n",
      "train: loss: 734.1671142578125 acc: 0.8704155683517456  val: loss: 973.05517578125 acc: 0.8373681306838989\n",
      "step: 270\n",
      "train: loss: 916.9592895507812 acc: 0.7676226496696472  val: loss: 1512.9266357421875 acc: 0.7428935170173645\n",
      "step: 275\n",
      "train: loss: 533.7506103515625 acc: 0.9136999249458313  val: loss: 1084.158447265625 acc: 0.8456633687019348\n",
      "step: 280\n",
      "train: loss: 503.5694885253906 acc: 0.8780540823936462  val: loss: 1493.981201171875 acc: 0.7870453596115112\n",
      "step: 285\n",
      "train: loss: 669.8240356445312 acc: 0.8455654382705688  val: loss: 911.6903076171875 acc: 0.8627874255180359\n",
      "step: 290\n",
      "train: loss: 706.6145629882812 acc: 0.8332041501998901  val: loss: 622.768310546875 acc: 0.8999001383781433\n",
      "step: 295\n",
      "train: loss: 517.1799926757812 acc: 0.8967772126197815  val: loss: 487.43609619140625 acc: 0.8895506858825684\n",
      "step: 300\n",
      "train: loss: 412.9304504394531 acc: 0.945446789264679  val: loss: 1250.8197021484375 acc: 0.8474711775779724\n",
      "step: 305\n",
      "train: loss: 884.0997924804688 acc: 0.8879706263542175  val: loss: 1275.57470703125 acc: 0.8318489193916321\n",
      "step: 310\n",
      "train: loss: 554.9332885742188 acc: 0.9083114862442017  val: loss: 593.8814086914062 acc: 0.8942561745643616\n",
      "step: 315\n",
      "train: loss: 1161.6302490234375 acc: 0.7485783696174622  val: loss: 587.5574951171875 acc: 0.925200343132019\n",
      "step: 320\n",
      "train: loss: 835.8604125976562 acc: 0.8403716087341309  val: loss: 965.2803955078125 acc: 0.8208972811698914\n",
      "step: 325\n",
      "train: loss: 892.0440063476562 acc: 0.8614450693130493  val: loss: 1465.6319580078125 acc: 0.7812497615814209\n",
      "step: 330\n",
      "train: loss: 704.1260986328125 acc: 0.8715842366218567  val: loss: 825.0615844726562 acc: 0.8715326189994812\n",
      "step: 335\n",
      "train: loss: 663.3416137695312 acc: 0.8662260174751282  val: loss: 2007.67724609375 acc: 0.7485888004302979\n",
      "step: 340\n",
      "train: loss: 535.7787475585938 acc: 0.8490961194038391  val: loss: 1817.93505859375 acc: 0.7474074363708496\n",
      "step: 345\n",
      "train: loss: 700.4938354492188 acc: 0.8545234799385071  val: loss: 1347.9996337890625 acc: 0.8092791438102722\n",
      "step: 350\n",
      "train: loss: 834.0906982421875 acc: 0.847665548324585  val: loss: 860.8936767578125 acc: 0.8700302839279175\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 355\n",
      "train: loss: 399.9646911621094 acc: 0.9098789095878601  val: loss: 1086.09814453125 acc: 0.8705436587333679\n",
      "step: 360\n",
      "train: loss: 829.163330078125 acc: 0.8933086395263672  val: loss: 1347.9613037109375 acc: 0.8198961019515991\n",
      "step: 365\n",
      "train: loss: 1108.79150390625 acc: 0.7231707572937012  val: loss: 1211.6453857421875 acc: 0.8331088423728943\n",
      "step: 370\n",
      "train: loss: 942.7859497070312 acc: 0.8076850175857544  val: loss: 2445.3876953125 acc: 0.6823176145553589\n",
      "step: 375\n",
      "train: loss: 752.0897827148438 acc: 0.8933994770050049  val: loss: 424.9921569824219 acc: 0.9280794262886047\n",
      "step: 380\n",
      "train: loss: 520.6289672851562 acc: 0.9164575934410095  val: loss: 1407.4329833984375 acc: 0.8149899244308472\n",
      "step: 385\n",
      "train: loss: 826.4861450195312 acc: 0.8457160592079163  val: loss: 900.789794921875 acc: 0.845691442489624\n",
      "step: 390\n",
      "train: loss: 787.5335083007812 acc: 0.883771538734436  val: loss: 2362.63525390625 acc: 0.7575620412826538\n",
      "step: 395\n",
      "train: loss: 685.496826171875 acc: 0.8780579566955566  val: loss: 1159.2684326171875 acc: 0.837922215461731\n",
      "step: 400\n",
      "train: loss: 868.415283203125 acc: 0.7842715382575989  val: loss: 1092.6080322265625 acc: 0.833236813545227\n",
      "step: 405\n",
      "train: loss: 721.6766357421875 acc: 0.8636627793312073  val: loss: 1621.6409912109375 acc: 0.7985801696777344\n",
      "step: 410\n",
      "train: loss: 1040.647705078125 acc: 0.8763389587402344  val: loss: 972.2225952148438 acc: 0.8798813819885254\n",
      "step: 415\n",
      "train: loss: 505.9713439941406 acc: 0.933661937713623  val: loss: 1551.712646484375 acc: 0.8246526718139648\n",
      "step: 420\n",
      "train: loss: 732.7060546875 acc: 0.8751276731491089  val: loss: 1521.77490234375 acc: 0.8367033004760742\n",
      "step: 425\n",
      "train: loss: 941.6500854492188 acc: 0.8365008234977722  val: loss: 1131.6573486328125 acc: 0.8250083923339844\n",
      "step: 430\n",
      "train: loss: 730.6100463867188 acc: 0.8949177861213684  val: loss: 563.5394897460938 acc: 0.8965144753456116\n",
      "step: 435\n",
      "train: loss: 900.6331787109375 acc: 0.7069555521011353  val: loss: 1379.2489013671875 acc: 0.8034281730651855\n",
      "step: 440\n",
      "train: loss: 339.3167419433594 acc: 0.9402187466621399  val: loss: 886.2282104492188 acc: 0.8943113088607788\n",
      "step: 445\n",
      "train: loss: 274.8553771972656 acc: 0.950658917427063  val: loss: 1683.38037109375 acc: 0.8093419671058655\n",
      "step: 450\n",
      "train: loss: 323.68975830078125 acc: 0.9090064167976379  val: loss: 668.3724365234375 acc: 0.8613138794898987\n",
      "step: 455\n",
      "train: loss: 701.5552978515625 acc: 0.7731531858444214  val: loss: 1336.0537109375 acc: 0.8478212356567383\n",
      "step: 460\n",
      "train: loss: 646.1199951171875 acc: 0.889393150806427  val: loss: 2413.53515625 acc: 0.7355884909629822\n",
      "step: 465\n",
      "train: loss: 531.671630859375 acc: 0.9328258633613586  val: loss: 1498.2252197265625 acc: 0.8064603805541992\n",
      "step: 470\n",
      "train: loss: 1019.1614990234375 acc: 0.8779439926147461  val: loss: 425.59088134765625 acc: 0.8665918111801147\n",
      "step: 475\n",
      "train: loss: 896.4107666015625 acc: 0.8375568389892578  val: loss: 1661.5992431640625 acc: 0.7934041619300842\n",
      "step: 480\n",
      "train: loss: 773.3703002929688 acc: 0.8786591291427612  val: loss: 899.8564453125 acc: 0.8728005290031433\n",
      "step: 485\n",
      "train: loss: 717.5883178710938 acc: 0.8198641538619995  val: loss: 605.4691772460938 acc: 0.867637038230896\n",
      "step: 490\n",
      "train: loss: 1181.866455078125 acc: 0.8235824108123779  val: loss: 481.0356140136719 acc: 0.9208837747573853\n",
      "step: 495\n",
      "train: loss: 793.6494140625 acc: 0.865730881690979  val: loss: 790.6475219726562 acc: 0.8613181114196777\n",
      "step: 500\n",
      "train: loss: 786.8187866210938 acc: 0.9022150039672852  val: loss: 1433.279052734375 acc: 0.8404529690742493\n",
      "step: 505\n",
      "train: loss: 706.996826171875 acc: 0.8740341067314148  val: loss: 699.4161987304688 acc: 0.8900764584541321\n",
      "step: 510\n",
      "train: loss: 802.5535888671875 acc: 0.8598507642745972  val: loss: 1191.1031494140625 acc: 0.8388252258300781\n",
      "step: 515\n",
      "train: loss: 611.8568725585938 acc: 0.869530439376831  val: loss: 1939.523193359375 acc: 0.7329784631729126\n",
      "step: 520\n",
      "train: loss: 627.5429077148438 acc: 0.8872365355491638  val: loss: 642.8207397460938 acc: 0.9018890261650085\n",
      "step: 525\n",
      "train: loss: 787.7117309570312 acc: 0.8154200315475464  val: loss: 962.2639770507812 acc: 0.8348418474197388\n",
      "step: 530\n",
      "train: loss: 1058.6865234375 acc: 0.8250945806503296  val: loss: 1520.342041015625 acc: 0.8082510828971863\n",
      "step: 535\n",
      "train: loss: 317.0666198730469 acc: 0.9480777978897095  val: loss: 514.2650146484375 acc: 0.9108333587646484\n",
      "step: 540\n",
      "train: loss: 474.86907958984375 acc: 0.913987398147583  val: loss: 689.9006958007812 acc: 0.8888185620307922\n",
      "step: 545\n",
      "train: loss: 487.3890075683594 acc: 0.8842993974685669  val: loss: 992.5203247070312 acc: 0.8681023120880127\n",
      "step: 550\n",
      "train: loss: 818.316650390625 acc: 0.8736084699630737  val: loss: 1197.6573486328125 acc: 0.8111335039138794\n",
      "step: 555\n",
      "train: loss: 533.2848510742188 acc: 0.8992770910263062  val: loss: 1431.636474609375 acc: 0.8137292861938477\n",
      "step: 560\n",
      "train: loss: 585.4803466796875 acc: 0.912689208984375  val: loss: 1256.817138671875 acc: 0.8398342132568359\n",
      "step: 565\n",
      "train: loss: 415.5201110839844 acc: 0.919954776763916  val: loss: 1726.2442626953125 acc: 0.760965883731842\n",
      "step: 570\n",
      "train: loss: 619.9619750976562 acc: 0.8935922980308533  val: loss: 1165.910400390625 acc: 0.839890718460083\n",
      "step: 575\n",
      "train: loss: 964.8714599609375 acc: 0.6603225469589233  val: loss: 1691.0050048828125 acc: 0.7755818963050842\n",
      "step: 580\n",
      "train: loss: 672.9374389648438 acc: 0.8454363346099854  val: loss: 2068.0234375 acc: 0.7748071551322937\n",
      "step: 585\n",
      "train: loss: 397.8313293457031 acc: 0.9339274168014526  val: loss: 1259.466796875 acc: 0.8470296263694763\n",
      "step: 590\n",
      "train: loss: 812.7415771484375 acc: 0.8511235117912292  val: loss: 2022.3494873046875 acc: 0.7678913474082947\n",
      "step: 595\n",
      "train: loss: 438.64117431640625 acc: 0.9323571920394897  val: loss: 2161.37255859375 acc: 0.6802635788917542\n",
      "step: 600\n",
      "train: loss: 528.7398071289062 acc: 0.9235158562660217  val: loss: 1396.83642578125 acc: 0.8438988924026489\n",
      "step: 605\n",
      "train: loss: 700.3030395507812 acc: 0.8609384298324585  val: loss: 1446.4979248046875 acc: 0.80173259973526\n",
      "step: 610\n",
      "train: loss: 770.7456665039062 acc: 0.8690572381019592  val: loss: 3656.85791015625 acc: 0.565703809261322\n",
      "step: 615\n",
      "train: loss: 1053.8634033203125 acc: 0.769257664680481  val: loss: 952.6155395507812 acc: 0.8455506563186646\n",
      "step: 620\n",
      "train: loss: 423.29217529296875 acc: 0.9511482119560242  val: loss: 923.3837280273438 acc: 0.8306928873062134\n",
      "step: 625\n",
      "train: loss: 663.5672607421875 acc: 0.8970893621444702  val: loss: 452.49566650390625 acc: 0.9177652597427368\n",
      "step: 630\n",
      "train: loss: 652.1497802734375 acc: 0.7306073307991028  val: loss: 1024.181884765625 acc: 0.8163166046142578\n",
      "step: 635\n",
      "train: loss: 409.5416259765625 acc: 0.9044426679611206  val: loss: 488.4544982910156 acc: 0.9092739820480347\n",
      "step: 640\n",
      "train: loss: 267.0212707519531 acc: 0.9626219868659973  val: loss: 1408.7183837890625 acc: 0.7876648902893066\n",
      "step: 645\n",
      "train: loss: 686.1034545898438 acc: 0.9142047762870789  val: loss: 1093.7579345703125 acc: 0.8407622575759888\n",
      "step: 650\n",
      "train: loss: 415.4599914550781 acc: 0.9193449020385742  val: loss: 1103.1219482421875 acc: 0.8626928329467773\n",
      "step: 655\n",
      "train: loss: 671.8544921875 acc: 0.8960567712783813  val: loss: 954.5242919921875 acc: 0.8632202744483948\n",
      "step: 660\n",
      "train: loss: 700.718994140625 acc: 0.9051144123077393  val: loss: 268.2214660644531 acc: 0.9541857838630676\n",
      "step: 665\n",
      "train: loss: 977.3751220703125 acc: 0.8475258350372314  val: loss: 1383.737548828125 acc: 0.8239689469337463\n",
      "step: 670\n",
      "train: loss: 819.5526733398438 acc: 0.8342056274414062  val: loss: 1745.1109619140625 acc: 0.8043093681335449\n",
      "step: 675\n",
      "train: loss: 777.4031372070312 acc: 0.8805813789367676  val: loss: 1405.703857421875 acc: 0.8068264126777649\n",
      "step: 680\n",
      "train: loss: 849.4966430664062 acc: 0.8538877964019775  val: loss: 1228.14404296875 acc: 0.805999219417572\n",
      "step: 685\n",
      "train: loss: 771.3658447265625 acc: 0.8846932053565979  val: loss: 992.9303588867188 acc: 0.8630581498146057\n",
      "step: 690\n",
      "train: loss: 909.809814453125 acc: 0.8625079989433289  val: loss: 417.494140625 acc: 0.926656186580658\n",
      "step: 695\n",
      "train: loss: 474.9355773925781 acc: 0.8580924868583679  val: loss: 588.3609008789062 acc: 0.8909596800804138\n",
      "step: 700\n",
      "train: loss: 750.2655029296875 acc: 0.7569330334663391  val: loss: 996.2657470703125 acc: 0.8050064444541931\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 705\n",
      "train: loss: 963.4010009765625 acc: 0.8473922610282898  val: loss: 610.527099609375 acc: 0.9130348563194275\n",
      "step: 710\n",
      "train: loss: 471.2832946777344 acc: 0.9258319735527039  val: loss: 1353.1058349609375 acc: 0.8210476636886597\n",
      "step: 715\n",
      "train: loss: 921.9700317382812 acc: 0.8227491974830627  val: loss: 501.0044250488281 acc: 0.9166448712348938\n",
      "step: 720\n",
      "train: loss: 874.2536010742188 acc: 0.8066072463989258  val: loss: 726.2216796875 acc: 0.849925696849823\n",
      "step: 725\n",
      "train: loss: 798.969970703125 acc: 0.8434537649154663  val: loss: 2302.61669921875 acc: 0.7688347697257996\n",
      "step: 730\n",
      "train: loss: 712.1490478515625 acc: 0.8575664162635803  val: loss: 662.0975341796875 acc: 0.879362165927887\n",
      "step: 735\n",
      "train: loss: 766.4188842773438 acc: 0.8654465675354004  val: loss: 987.3771362304688 acc: 0.8126519322395325\n",
      "step: 740\n",
      "train: loss: 958.3605346679688 acc: 0.8164791464805603  val: loss: 1182.1121826171875 acc: 0.8681948184967041\n",
      "step: 745\n",
      "train: loss: 475.1675720214844 acc: 0.8659921884536743  val: loss: 1573.569580078125 acc: 0.774155855178833\n",
      "step: 750\n",
      "train: loss: 1177.2200927734375 acc: 0.8348498344421387  val: loss: 349.000244140625 acc: 0.937048614025116\n",
      "step: 755\n",
      "train: loss: 771.5531616210938 acc: 0.8609054088592529  val: loss: 938.7840576171875 acc: 0.8607619404792786\n",
      "step: 760\n",
      "train: loss: 376.80352783203125 acc: 0.9256616830825806  val: loss: 607.7081298828125 acc: 0.8744415044784546\n",
      "step: 765\n",
      "train: loss: 428.4817199707031 acc: 0.9327741861343384  val: loss: 2260.357421875 acc: 0.7524136304855347\n",
      "step: 770\n",
      "train: loss: 382.1708679199219 acc: 0.9318534731864929  val: loss: 1862.1669921875 acc: 0.7058756351470947\n",
      "step: 775\n",
      "train: loss: 662.4346923828125 acc: 0.9091408848762512  val: loss: 559.482177734375 acc: 0.9005732536315918\n",
      "step: 780\n",
      "train: loss: 1230.3197021484375 acc: 0.7413771748542786  val: loss: 1590.813720703125 acc: 0.8072798252105713\n",
      "step: 785\n",
      "train: loss: 834.8941650390625 acc: 0.8844051361083984  val: loss: 498.9896545410156 acc: 0.9167895913124084\n",
      "step: 790\n",
      "train: loss: 885.0389404296875 acc: 0.8360211849212646  val: loss: 1237.6885986328125 acc: 0.8574215173721313\n",
      "step: 795\n",
      "train: loss: 756.556884765625 acc: 0.8862461447715759  val: loss: 704.023193359375 acc: 0.8546231985092163\n",
      "step: 800\n",
      "train: loss: 494.42205810546875 acc: 0.9031736254692078  val: loss: 1715.4825439453125 acc: 0.7892603278160095\n",
      "step: 805\n",
      "train: loss: 442.90008544921875 acc: 0.8959106206893921  val: loss: 1459.1669921875 acc: 0.8132131695747375\n",
      "step: 810\n",
      "train: loss: 797.8365478515625 acc: 0.8436951637268066  val: loss: 788.9890747070312 acc: 0.8584555387496948\n",
      "step: 815\n",
      "train: loss: 770.7611694335938 acc: 0.8178207874298096  val: loss: 705.9739990234375 acc: 0.8865672945976257\n",
      "step: 820\n",
      "train: loss: 460.59930419921875 acc: 0.9203438758850098  val: loss: 1331.060302734375 acc: 0.8199889063835144\n",
      "step: 825\n",
      "train: loss: 983.8665771484375 acc: 0.8836224675178528  val: loss: 829.8331298828125 acc: 0.8616453409194946\n",
      "step: 830\n",
      "train: loss: 518.6718139648438 acc: 0.9286544919013977  val: loss: 969.9228515625 acc: 0.8265597224235535\n",
      "step: 835\n",
      "train: loss: 591.120361328125 acc: 0.9037332534790039  val: loss: 1552.4395751953125 acc: 0.8144851326942444\n",
      "step: 840\n",
      "train: loss: 736.3885498046875 acc: 0.9033821821212769  val: loss: 646.1446533203125 acc: 0.9144882559776306\n",
      "step: 845\n",
      "train: loss: 1644.211181640625 acc: 0.6913591623306274  val: loss: 1715.453125 acc: 0.7786291837692261\n",
      "step: 850\n",
      "train: loss: 856.6083374023438 acc: 0.781525194644928  val: loss: 1136.6953125 acc: 0.8809727430343628\n",
      "step: 855\n",
      "train: loss: 817.0833129882812 acc: 0.8602306842803955  val: loss: 1039.316162109375 acc: 0.8369203805923462\n",
      "step: 860\n",
      "train: loss: 743.8406982421875 acc: 0.8709218502044678  val: loss: 1852.697998046875 acc: 0.795608639717102\n",
      "step: 865\n",
      "train: loss: 858.6278076171875 acc: 0.8570030927658081  val: loss: 1728.8018798828125 acc: 0.7478699684143066\n",
      "step: 870\n",
      "train: loss: 939.2306518554688 acc: 0.8309131264686584  val: loss: 899.4783325195312 acc: 0.8773007392883301\n",
      "step: 875\n",
      "train: loss: 693.7922973632812 acc: 0.8735675811767578  val: loss: 1406.0260009765625 acc: 0.7898785471916199\n",
      "step: 880\n",
      "train: loss: 751.5307006835938 acc: 0.8297449350357056  val: loss: 1558.2615966796875 acc: 0.7812127470970154\n",
      "step: 885\n",
      "train: loss: 586.7872924804688 acc: 0.9082009792327881  val: loss: 454.73095703125 acc: 0.9342573881149292\n",
      "step: 890\n",
      "train: loss: 743.1809692382812 acc: 0.9046610593795776  val: loss: 1487.4073486328125 acc: 0.8111974000930786\n",
      "step: 895\n",
      "train: loss: 1247.7025146484375 acc: 0.7918179631233215  val: loss: 2052.61474609375 acc: 0.7405257225036621\n",
      "step: 900\n",
      "train: loss: 522.85546875 acc: 0.9210299253463745  val: loss: 918.5303955078125 acc: 0.870134711265564\n",
      "step: 905\n",
      "train: loss: 789.9798583984375 acc: 0.8302392959594727  val: loss: 2700.78662109375 acc: 0.7086788415908813\n",
      "step: 910\n",
      "train: loss: 563.502685546875 acc: 0.9125074148178101  val: loss: 950.8719482421875 acc: 0.8725409507751465\n",
      "step: 915\n",
      "train: loss: 600.1201171875 acc: 0.8776557445526123  val: loss: 1333.330078125 acc: 0.7818649411201477\n",
      "step: 920\n",
      "train: loss: 459.802490234375 acc: 0.9252068400382996  val: loss: 1285.1959228515625 acc: 0.7842270135879517\n",
      "step: 925\n",
      "train: loss: 319.4771728515625 acc: 0.9329513311386108  val: loss: 1105.44921875 acc: 0.8206688761711121\n",
      "step: 930\n",
      "train: loss: 362.3993835449219 acc: 0.9067081212997437  val: loss: 636.3283081054688 acc: 0.8809484243392944\n",
      "step: 935\n",
      "train: loss: 775.5824584960938 acc: 0.833219587802887  val: loss: 819.7080688476562 acc: 0.8297651410102844\n",
      "step: 940\n",
      "train: loss: 610.6370849609375 acc: 0.9166468381881714  val: loss: 1649.203857421875 acc: 0.7829058170318604\n",
      "step: 945\n",
      "train: loss: 359.923095703125 acc: 0.9525352716445923  val: loss: 815.8814086914062 acc: 0.8811066746711731\n",
      "step: 950\n",
      "train: loss: 1393.2274169921875 acc: 0.7898613214492798  val: loss: 1747.2781982421875 acc: 0.7872784733772278\n",
      "step: 955\n",
      "train: loss: 890.8318481445312 acc: 0.8341653347015381  val: loss: 891.961181640625 acc: 0.8103203177452087\n",
      "step: 960\n",
      "train: loss: 502.9045104980469 acc: 0.8958057165145874  val: loss: 1987.79736328125 acc: 0.7387062907218933\n",
      "step: 965\n",
      "train: loss: 1177.31689453125 acc: 0.7296707034111023  val: loss: 1487.6656494140625 acc: 0.7722755670547485\n",
      "step: 970\n",
      "train: loss: 481.4687805175781 acc: 0.9297139048576355  val: loss: 814.9014892578125 acc: 0.8444877862930298\n",
      "step: 975\n",
      "train: loss: 822.4408569335938 acc: 0.859820544719696  val: loss: 989.4031982421875 acc: 0.8821452856063843\n",
      "step: 980\n",
      "train: loss: 984.8143310546875 acc: 0.6370645761489868  val: loss: 1002.9445190429688 acc: 0.7691463232040405\n",
      "step: 985\n",
      "train: loss: 515.9692993164062 acc: 0.863242506980896  val: loss: 1637.48583984375 acc: 0.8158441185951233\n",
      "step: 990\n",
      "train: loss: 670.5908203125 acc: 0.8840295076370239  val: loss: 1359.2843017578125 acc: 0.8113001585006714\n",
      "step: 995\n",
      "train: loss: 265.8084716796875 acc: 0.9409627914428711  val: loss: 1061.2445068359375 acc: 0.8639780879020691\n",
      "step: 1000\n",
      "train: loss: 638.9638671875 acc: 0.8937292695045471  val: loss: 1658.122314453125 acc: 0.7663044929504395\n",
      "step: 1005\n",
      "train: loss: 543.0137939453125 acc: 0.9048702716827393  val: loss: 846.1866455078125 acc: 0.8681505918502808\n",
      "step: 1010\n",
      "train: loss: 694.9737548828125 acc: 0.8894667625427246  val: loss: 2295.73974609375 acc: 0.6940557956695557\n",
      "step: 1015\n",
      "train: loss: 474.57794189453125 acc: 0.9051597118377686  val: loss: 1497.528076171875 acc: 0.8161744475364685\n",
      "step: 1020\n",
      "train: loss: 590.2094116210938 acc: 0.9049140214920044  val: loss: 2708.413818359375 acc: 0.7034776210784912\n",
      "step: 1025\n",
      "train: loss: 571.1307373046875 acc: 0.8569706678390503  val: loss: 1706.5030517578125 acc: 0.7363970279693604\n",
      "step: 1030\n",
      "train: loss: 951.450927734375 acc: 0.8554983735084534  val: loss: 1944.9742431640625 acc: 0.7265110611915588\n",
      "step: 1035\n",
      "train: loss: 1177.7322998046875 acc: 0.8503650426864624  val: loss: 686.4879760742188 acc: 0.9076452255249023\n",
      "step: 1040\n",
      "train: loss: 218.2279052734375 acc: 0.9624138474464417  val: loss: 1153.814697265625 acc: 0.780945897102356\n",
      "step: 1045\n",
      "train: loss: 682.4251098632812 acc: 0.8764632344245911  val: loss: 747.5938110351562 acc: 0.8634083867073059\n",
      "step: 1050\n",
      "train: loss: 652.4087524414062 acc: 0.8758022785186768  val: loss: 2004.3861083984375 acc: 0.7554743885993958\n",
      "step: 1055\n",
      "train: loss: 432.4170837402344 acc: 0.9227983355522156  val: loss: 1216.4930419921875 acc: 0.8395336270332336\n",
      "step: 1060\n",
      "train: loss: 719.1676025390625 acc: 0.878947377204895  val: loss: 2404.3388671875 acc: 0.7333700060844421\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 1065\n",
      "train: loss: 461.7975769042969 acc: 0.907728374004364  val: loss: 1575.2274169921875 acc: 0.815043568611145\n",
      "step: 1070\n",
      "train: loss: 757.7072143554688 acc: 0.8366124033927917  val: loss: 795.0546264648438 acc: 0.8608638048171997\n",
      "step: 1075\n",
      "train: loss: 1185.7755126953125 acc: 0.8287404179573059  val: loss: 1000.2051391601562 acc: 0.8686047196388245\n",
      "step: 1080\n",
      "train: loss: 803.8359375 acc: 0.8704373836517334  val: loss: 807.6805419921875 acc: 0.8639222383499146\n",
      "step: 1085\n",
      "train: loss: 455.0972900390625 acc: 0.9348124265670776  val: loss: 1375.14208984375 acc: 0.8362721800804138\n",
      "step: 1090\n",
      "train: loss: 819.0202026367188 acc: 0.8673251867294312  val: loss: 1267.1165771484375 acc: 0.8439034223556519\n",
      "step: 1095\n",
      "train: loss: 598.73974609375 acc: 0.8904616832733154  val: loss: 1948.0523681640625 acc: 0.7477340698242188\n",
      "step: 1100\n",
      "train: loss: 575.5997314453125 acc: 0.9019302725791931  val: loss: 830.4699096679688 acc: 0.8903321027755737\n",
      "step: 1105\n",
      "train: loss: 728.716064453125 acc: 0.8560795783996582  val: loss: 1399.9759521484375 acc: 0.8180035352706909\n",
      "step: 1110\n",
      "train: loss: 447.29290771484375 acc: 0.879318118095398  val: loss: 397.8668518066406 acc: 0.9252821207046509\n",
      "step: 1115\n",
      "train: loss: 498.3542175292969 acc: 0.9137242436408997  val: loss: 427.449951171875 acc: 0.9280956983566284\n",
      "step: 1120\n",
      "train: loss: 488.270263671875 acc: 0.913837730884552  val: loss: 1293.2607421875 acc: 0.8079862594604492\n",
      "step: 1125\n",
      "train: loss: 661.83154296875 acc: 0.865393877029419  val: loss: 1542.1593017578125 acc: 0.7683136463165283\n",
      "step: 1130\n",
      "train: loss: 783.1255493164062 acc: 0.8586516976356506  val: loss: 532.5350952148438 acc: 0.9181156754493713\n",
      "step: 1135\n",
      "train: loss: 748.353271484375 acc: 0.7695100903511047  val: loss: 372.2655944824219 acc: 0.945979118347168\n",
      "step: 1140\n",
      "train: loss: 917.1100463867188 acc: 0.8860915899276733  val: loss: 1541.298583984375 acc: 0.7436844110488892\n",
      "step: 1145\n",
      "train: loss: 809.595947265625 acc: 0.8234798908233643  val: loss: 1520.3646240234375 acc: 0.8012053966522217\n",
      "step: 1150\n",
      "train: loss: 600.4559326171875 acc: 0.8893863558769226  val: loss: 515.3951416015625 acc: 0.9151636362075806\n",
      "step: 1155\n",
      "train: loss: 588.9693603515625 acc: 0.9010492563247681  val: loss: 960.861572265625 acc: 0.833228349685669\n",
      "step: 1160\n",
      "train: loss: 636.3146362304688 acc: 0.8703398108482361  val: loss: 498.4729919433594 acc: 0.9087008833885193\n",
      "step: 1165\n",
      "train: loss: 517.5671997070312 acc: 0.8233824968338013  val: loss: 947.9697265625 acc: 0.8375658392906189\n",
      "step: 1170\n",
      "train: loss: 249.16664123535156 acc: 0.9486292004585266  val: loss: 490.087646484375 acc: 0.9063559174537659\n",
      "step: 1175\n",
      "train: loss: 523.1617431640625 acc: 0.8782835006713867  val: loss: 498.29388427734375 acc: 0.8924641609191895\n",
      "step: 1180\n",
      "train: loss: 1036.11474609375 acc: 0.8567788600921631  val: loss: 465.2483215332031 acc: 0.9022666811943054\n",
      "step: 1185\n",
      "train: loss: 462.86395263671875 acc: 0.9326793551445007  val: loss: 1303.35693359375 acc: 0.8387140035629272\n",
      "step: 1190\n",
      "train: loss: 438.0882263183594 acc: 0.9404545426368713  val: loss: 1542.0513916015625 acc: 0.7004015445709229\n",
      "step: 1195\n",
      "train: loss: 903.4661254882812 acc: 0.8382716178894043  val: loss: 1705.3809814453125 acc: 0.7861146926879883\n",
      "step: 1200\n",
      "train: loss: 505.8304443359375 acc: 0.8204090595245361  val: loss: 1796.8919677734375 acc: 0.7729259729385376\n",
      "step: 1205\n",
      "train: loss: 858.625 acc: 0.8686694502830505  val: loss: 1246.8662109375 acc: 0.8163281083106995\n",
      "step: 1210\n",
      "train: loss: 588.0858764648438 acc: 0.8968188166618347  val: loss: 1104.675048828125 acc: 0.8361138105392456\n",
      "step: 1215\n",
      "train: loss: 515.3438110351562 acc: 0.9119219183921814  val: loss: 853.9439697265625 acc: 0.8455272912979126\n",
      "step: 1220\n",
      "train: loss: 438.76153564453125 acc: 0.9405432939529419  val: loss: 841.3909912109375 acc: 0.8525055646896362\n",
      "step: 1225\n",
      "train: loss: 352.8121032714844 acc: 0.92531418800354  val: loss: 839.6896362304688 acc: 0.8945614695549011\n",
      "step: 1230\n",
      "train: loss: 384.0577087402344 acc: 0.8859446048736572  val: loss: 1143.0003662109375 acc: 0.8538728952407837\n",
      "step: 1235\n",
      "train: loss: 484.5641174316406 acc: 0.9243673086166382  val: loss: 776.5040893554688 acc: 0.897347092628479\n",
      "step: 1240\n",
      "train: loss: 405.9083251953125 acc: 0.9062823057174683  val: loss: 1202.7550048828125 acc: 0.8430513739585876\n",
      "step: 1245\n",
      "train: loss: 885.6582641601562 acc: 0.8781422972679138  val: loss: 979.0950927734375 acc: 0.8560730814933777\n",
      "step: 1250\n",
      "train: loss: 925.2455444335938 acc: 0.8283321261405945  val: loss: 1036.7969970703125 acc: 0.8782346248626709\n",
      "step: 1255\n",
      "train: loss: 1132.5770263671875 acc: 0.8156696557998657  val: loss: 922.383544921875 acc: 0.8113253116607666\n",
      "step: 1260\n",
      "train: loss: 546.7918701171875 acc: 0.8569083213806152  val: loss: 684.7833862304688 acc: 0.8848588466644287\n",
      "step: 1265\n",
      "train: loss: 906.658447265625 acc: 0.8350155353546143  val: loss: 1024.9815673828125 acc: 0.7645341157913208\n",
      "step: 1270\n",
      "train: loss: 368.4309387207031 acc: 0.9358694553375244  val: loss: 1162.0977783203125 acc: 0.8315887451171875\n",
      "step: 1275\n",
      "train: loss: 655.3001098632812 acc: 0.8835492730140686  val: loss: 807.5375366210938 acc: 0.8812559843063354\n",
      "step: 1280\n",
      "train: loss: 531.859130859375 acc: 0.8837968707084656  val: loss: 1277.6973876953125 acc: 0.8139888048171997\n",
      "step: 1285\n",
      "train: loss: 696.6270751953125 acc: 0.8547304272651672  val: loss: 888.2584228515625 acc: 0.8396078944206238\n",
      "step: 1290\n",
      "train: loss: 580.2750854492188 acc: 0.8776402473449707  val: loss: 340.189453125 acc: 0.9364338517189026\n",
      "step: 1295\n",
      "train: loss: 409.0599670410156 acc: 0.9403375387191772  val: loss: 1439.7752685546875 acc: 0.8342938423156738\n",
      "step: 1300\n",
      "train: loss: 448.4804382324219 acc: 0.9462494850158691  val: loss: 1519.6009521484375 acc: 0.8005974888801575\n",
      "step: 1305\n",
      "train: loss: 1091.554931640625 acc: 0.8248056769371033  val: loss: 837.5499877929688 acc: 0.8734182715415955\n",
      "step: 1310\n",
      "train: loss: 759.4091186523438 acc: 0.8782932758331299  val: loss: 973.5201416015625 acc: 0.8734931945800781\n",
      "step: 1315\n",
      "train: loss: 942.0331420898438 acc: 0.8386765718460083  val: loss: 1051.817626953125 acc: 0.8251993656158447\n",
      "step: 1320\n",
      "train: loss: 761.1878051757812 acc: 0.8606849312782288  val: loss: 1051.771240234375 acc: 0.8307496309280396\n",
      "step: 1325\n",
      "train: loss: 947.32177734375 acc: 0.7843220829963684  val: loss: 837.7886962890625 acc: 0.8672020435333252\n",
      "step: 1330\n",
      "train: loss: 459.3345642089844 acc: 0.9312514066696167  val: loss: 513.1503295898438 acc: 0.9120086431503296\n",
      "step: 1335\n",
      "train: loss: 498.60406494140625 acc: 0.9192793369293213  val: loss: 1301.6021728515625 acc: 0.7868523597717285\n",
      "step: 1340\n",
      "train: loss: 781.9974365234375 acc: 0.8390160202980042  val: loss: 2065.966552734375 acc: 0.7563198804855347\n",
      "step: 1345\n",
      "train: loss: 633.71923828125 acc: 0.8042412996292114  val: loss: 2321.280029296875 acc: 0.6762770414352417\n",
      "step: 1350\n",
      "train: loss: 521.5857543945312 acc: 0.8966352939605713  val: loss: 1412.1693115234375 acc: 0.7825157046318054\n",
      "step: 1355\n",
      "train: loss: 599.8937377929688 acc: 0.8955782651901245  val: loss: 1442.567138671875 acc: 0.7662457823753357\n",
      "step: 1360\n",
      "train: loss: 644.6305541992188 acc: 0.9152703881263733  val: loss: 766.6875 acc: 0.8792406320571899\n",
      "step: 1365\n",
      "train: loss: 865.0010986328125 acc: 0.8681210279464722  val: loss: 597.218505859375 acc: 0.9139825105667114\n",
      "step: 1370\n",
      "train: loss: 871.50634765625 acc: 0.8724908828735352  val: loss: 1094.618408203125 acc: 0.8416452407836914\n",
      "step: 1375\n",
      "train: loss: 1264.1820068359375 acc: 0.714629054069519  val: loss: 819.8475341796875 acc: 0.8862239718437195\n",
      "step: 1380\n",
      "train: loss: 988.0220336914062 acc: 0.7856450080871582  val: loss: 834.115966796875 acc: 0.8638859987258911\n",
      "step: 1385\n",
      "train: loss: 771.2244873046875 acc: 0.852770209312439  val: loss: 1164.8018798828125 acc: 0.8269967436790466\n",
      "step: 1390\n",
      "train: loss: 515.358642578125 acc: 0.9005043506622314  val: loss: 1129.675048828125 acc: 0.8682624101638794\n",
      "step: 1395\n",
      "train: loss: 733.5818481445312 acc: 0.8613462448120117  val: loss: 1074.3795166015625 acc: 0.8467859029769897\n",
      "step: 1400\n",
      "train: loss: 530.0250244140625 acc: 0.8808764815330505  val: loss: 560.7421264648438 acc: 0.888224184513092\n",
      "step: 1405\n",
      "train: loss: 516.0594482421875 acc: 0.8954696655273438  val: loss: 1174.008544921875 acc: 0.8203693628311157\n",
      "step: 1410\n",
      "train: loss: 653.32177734375 acc: 0.900972306728363  val: loss: 596.232421875 acc: 0.8881385922431946\n",
      "step: 1415\n",
      "train: loss: 1024.4698486328125 acc: 0.8308073878288269  val: loss: 2543.27490234375 acc: 0.6251362562179565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 1420\n",
      "train: loss: 448.4581604003906 acc: 0.9294744729995728  val: loss: 1266.992431640625 acc: 0.7658417224884033\n",
      "step: 1425\n",
      "train: loss: 582.9100341796875 acc: 0.8962897062301636  val: loss: 894.8624877929688 acc: 0.8514227867126465\n",
      "step: 1430\n",
      "train: loss: 873.2326049804688 acc: 0.7945483326911926  val: loss: 459.2456359863281 acc: 0.914553701877594\n",
      "step: 1435\n",
      "train: loss: 665.5408325195312 acc: 0.8886141180992126  val: loss: 859.3973999023438 acc: 0.8380088806152344\n",
      "step: 1440\n",
      "train: loss: 1252.9765625 acc: 0.7883152961730957  val: loss: 834.0591430664062 acc: 0.8956588506698608\n",
      "step: 1445\n",
      "train: loss: 621.1747436523438 acc: 0.9138297438621521  val: loss: 1365.9991455078125 acc: 0.828635573387146\n",
      "step: 1450\n",
      "train: loss: 845.2828979492188 acc: 0.8828544616699219  val: loss: 1590.5091552734375 acc: 0.7664828896522522\n",
      "step: 1455\n",
      "train: loss: 498.974853515625 acc: 0.8968296647071838  val: loss: 1541.1724853515625 acc: 0.7289314866065979\n",
      "step: 1460\n",
      "train: loss: 474.3211669921875 acc: 0.892645001411438  val: loss: 1996.805908203125 acc: 0.7791445851325989\n",
      "step: 1465\n",
      "train: loss: 515.8438110351562 acc: 0.8694924712181091  val: loss: 454.4259033203125 acc: 0.9064443111419678\n",
      "step: 1470\n",
      "train: loss: 341.8303527832031 acc: 0.9420160055160522  val: loss: 1998.320556640625 acc: 0.7702476978302002\n",
      "step: 1475\n",
      "train: loss: 1014.2408447265625 acc: 0.8414212465286255  val: loss: 1180.155029296875 acc: 0.8384958505630493\n",
      "step: 1480\n",
      "train: loss: 648.1065673828125 acc: 0.9190812110900879  val: loss: 1057.744140625 acc: 0.8336045145988464\n",
      "step: 1485\n",
      "train: loss: 1240.3118896484375 acc: 0.8153352737426758  val: loss: 1328.85107421875 acc: 0.8332133889198303\n",
      "step: 1490\n",
      "train: loss: 486.015625 acc: 0.9025242328643799  val: loss: 931.3890991210938 acc: 0.8522108793258667\n",
      "step: 1495\n",
      "train: loss: 1045.9290771484375 acc: 0.7917450666427612  val: loss: 674.4459838867188 acc: 0.8728152513504028\n",
      "step: 1500\n",
      "train: loss: 661.8787841796875 acc: 0.8669248819351196  val: loss: 1143.1978759765625 acc: 0.8107097148895264\n",
      "step: 1505\n",
      "train: loss: 441.9424133300781 acc: 0.909710705280304  val: loss: 1273.8653564453125 acc: 0.837885856628418\n",
      "step: 1510\n",
      "train: loss: 333.22491455078125 acc: 0.9197962880134583  val: loss: 436.9576110839844 acc: 0.9095854163169861\n",
      "step: 1515\n",
      "train: loss: 811.4146728515625 acc: 0.8124352693557739  val: loss: 1380.4432373046875 acc: 0.7990099191665649\n",
      "step: 1520\n",
      "train: loss: 315.35821533203125 acc: 0.9278027415275574  val: loss: 1654.6092529296875 acc: 0.780320405960083\n",
      "step: 1525\n",
      "train: loss: 303.2447509765625 acc: 0.926310658454895  val: loss: 774.7744140625 acc: 0.8649711012840271\n",
      "step: 1530\n",
      "train: loss: 460.04412841796875 acc: 0.9226396083831787  val: loss: 923.9463500976562 acc: 0.8806120753288269\n",
      "step: 1535\n",
      "train: loss: 592.1476440429688 acc: 0.9061572551727295  val: loss: 887.9727783203125 acc: 0.8494426012039185\n",
      "step: 1540\n",
      "train: loss: 623.260986328125 acc: 0.9093174934387207  val: loss: 897.7162475585938 acc: 0.836449921131134\n",
      "step: 1545\n",
      "train: loss: 1084.7451171875 acc: 0.8342242240905762  val: loss: 745.5612182617188 acc: 0.8710914850234985\n",
      "step: 1550\n",
      "train: loss: 985.6294555664062 acc: 0.7183181047439575  val: loss: 726.0247802734375 acc: 0.8795441389083862\n",
      "step: 1555\n",
      "train: loss: 767.235595703125 acc: 0.8521372079849243  val: loss: 1507.5755615234375 acc: 0.8050192594528198\n",
      "step: 1560\n",
      "train: loss: 1019.2476196289062 acc: 0.8251444101333618  val: loss: 851.8451538085938 acc: 0.868503749370575\n",
      "step: 1565\n",
      "train: loss: 645.6905517578125 acc: 0.9089125394821167  val: loss: 1526.2579345703125 acc: 0.8238722681999207\n",
      "step: 1570\n",
      "train: loss: 930.6989135742188 acc: 0.7777025699615479  val: loss: 1348.5306396484375 acc: 0.8087335228919983\n",
      "step: 1575\n",
      "train: loss: 719.8154296875 acc: 0.8856555819511414  val: loss: 1296.6295166015625 acc: 0.8077631592750549\n",
      "step: 1580\n",
      "train: loss: 546.206298828125 acc: 0.8994249105453491  val: loss: 505.0885314941406 acc: 0.9198336005210876\n",
      "step: 1585\n",
      "train: loss: 359.6307373046875 acc: 0.9426891207695007  val: loss: 1075.606201171875 acc: 0.8213798999786377\n",
      "step: 1590\n",
      "train: loss: 456.8572692871094 acc: 0.9123191833496094  val: loss: 1458.3577880859375 acc: 0.7997337579727173\n",
      "step: 1595\n",
      "train: loss: 420.656005859375 acc: 0.9097468256950378  val: loss: 1217.669921875 acc: 0.8297199010848999\n",
      "step: 1600\n",
      "train: loss: 912.6093139648438 acc: 0.8179576396942139  val: loss: 491.8060302734375 acc: 0.917478621006012\n",
      "step: 1605\n",
      "train: loss: 526.9391479492188 acc: 0.9108356833457947  val: loss: 1758.0406494140625 acc: 0.8212219476699829\n",
      "step: 1610\n",
      "train: loss: 1069.8450927734375 acc: 0.7547426223754883  val: loss: 856.1846923828125 acc: 0.8855387568473816\n",
      "step: 1615\n",
      "train: loss: 1124.6802978515625 acc: 0.8322083950042725  val: loss: 963.833984375 acc: 0.8155118227005005\n",
      "step: 1620\n",
      "train: loss: 580.901123046875 acc: 0.88469398021698  val: loss: 747.31396484375 acc: 0.8903981447219849\n",
      "step: 1625\n",
      "train: loss: 950.203369140625 acc: 0.8537207245826721  val: loss: 905.2080078125 acc: 0.8364930152893066\n",
      "step: 1630\n",
      "train: loss: 383.6348571777344 acc: 0.929080069065094  val: loss: 550.9248046875 acc: 0.9071062207221985\n",
      "step: 1635\n",
      "train: loss: 1185.9942626953125 acc: 0.825721025466919  val: loss: 1487.887939453125 acc: 0.7997968792915344\n",
      "step: 1640\n",
      "train: loss: 800.4529418945312 acc: 0.8272098302841187  val: loss: 1586.128173828125 acc: 0.787283182144165\n",
      "step: 1645\n",
      "train: loss: 403.2953186035156 acc: 0.9300985336303711  val: loss: 1856.420654296875 acc: 0.7584236264228821\n",
      "step: 1650\n",
      "train: loss: 691.32373046875 acc: 0.8680469989776611  val: loss: 395.7530212402344 acc: 0.926144003868103\n",
      "step: 1655\n",
      "train: loss: 404.1427917480469 acc: 0.9089191555976868  val: loss: 1385.330078125 acc: 0.8148205876350403\n",
      "step: 1660\n",
      "train: loss: 509.6471862792969 acc: 0.9121126532554626  val: loss: 913.1704711914062 acc: 0.8692227602005005\n",
      "step: 1665\n",
      "train: loss: 676.6251831054688 acc: 0.8441365361213684  val: loss: 389.8828430175781 acc: 0.9493491649627686\n",
      "step: 1670\n",
      "train: loss: 1668.2642822265625 acc: 0.6553614139556885  val: loss: 1512.4810791015625 acc: 0.8042581081390381\n",
      "step: 1675\n",
      "train: loss: 539.624267578125 acc: 0.8799131512641907  val: loss: 1578.5269775390625 acc: 0.7871521711349487\n",
      "step: 1680\n",
      "train: loss: 391.2012939453125 acc: 0.9330466985702515  val: loss: 571.594482421875 acc: 0.8743898868560791\n",
      "step: 1685\n",
      "train: loss: 932.7881469726562 acc: 0.8924554586410522  val: loss: 1880.33251953125 acc: 0.7725285291671753\n",
      "step: 1690\n",
      "train: loss: 486.1011047363281 acc: 0.8427571058273315  val: loss: 1664.72314453125 acc: 0.7964521646499634\n",
      "step: 1695\n",
      "train: loss: 218.7788543701172 acc: 0.9387662410736084  val: loss: 1756.4869384765625 acc: 0.8426812291145325\n",
      "step: 1700\n",
      "train: loss: 569.8017578125 acc: 0.8711546659469604  val: loss: 1366.255859375 acc: 0.7956202030181885\n",
      "step: 1705\n",
      "train: loss: 648.2638549804688 acc: 0.8671800494194031  val: loss: 597.7568969726562 acc: 0.8901978135108948\n",
      "step: 1710\n",
      "train: loss: 377.8431701660156 acc: 0.9421077370643616  val: loss: 619.8504638671875 acc: 0.8548518419265747\n",
      "step: 1715\n",
      "train: loss: 705.0424194335938 acc: 0.850567102432251  val: loss: 1086.4149169921875 acc: 0.8429948091506958\n",
      "step: 1720\n",
      "train: loss: 297.02252197265625 acc: 0.9572049379348755  val: loss: 1452.87939453125 acc: 0.8290249109268188\n",
      "step: 1725\n",
      "train: loss: 492.220458984375 acc: 0.881027340888977  val: loss: 1053.5648193359375 acc: 0.8596003651618958\n",
      "step: 1730\n",
      "train: loss: 1008.5311889648438 acc: 0.8183528780937195  val: loss: 1913.90185546875 acc: 0.7396194934844971\n",
      "step: 1735\n",
      "train: loss: 534.6497192382812 acc: 0.8534433841705322  val: loss: 857.824951171875 acc: 0.8674762845039368\n",
      "step: 1740\n",
      "train: loss: 887.224853515625 acc: 0.855193018913269  val: loss: 526.2313842773438 acc: 0.9128724336624146\n",
      "step: 1745\n",
      "train: loss: 291.6714782714844 acc: 0.9417463541030884  val: loss: 1100.215087890625 acc: 0.8581581115722656\n",
      "step: 1750\n",
      "train: loss: 441.207763671875 acc: 0.9379936456680298  val: loss: 1115.2366943359375 acc: 0.8251254558563232\n",
      "step: 1755\n",
      "train: loss: 566.8099975585938 acc: 0.92431640625  val: loss: 528.9710083007812 acc: 0.9157357215881348\n",
      "step: 1760\n",
      "train: loss: 478.66363525390625 acc: 0.826640248298645  val: loss: 1683.1312255859375 acc: 0.7536405920982361\n",
      "step: 1765\n",
      "train: loss: 608.0852661132812 acc: 0.9033461213111877  val: loss: 1279.9578857421875 acc: 0.7435368299484253\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 1770\n",
      "train: loss: 439.4986267089844 acc: 0.9372483491897583  val: loss: 996.1610107421875 acc: 0.8157490491867065\n",
      "step: 1775\n",
      "train: loss: 1440.93359375 acc: 0.7206742763519287  val: loss: 1400.3604736328125 acc: 0.8153177499771118\n",
      "step: 1780\n",
      "train: loss: 448.4891662597656 acc: 0.9247130155563354  val: loss: 2401.729248046875 acc: 0.7624602317810059\n",
      "step: 1785\n",
      "train: loss: 684.3486938476562 acc: 0.8768077492713928  val: loss: 1046.45263671875 acc: 0.8504904508590698\n",
      "step: 1790\n",
      "train: loss: 1002.2855224609375 acc: 0.7679161429405212  val: loss: 1284.1226806640625 acc: 0.8359699249267578\n",
      "step: 1795\n",
      "train: loss: 708.1788330078125 acc: 0.8600412011146545  val: loss: 814.3406372070312 acc: 0.8847379684448242\n",
      "step: 1800\n",
      "train: loss: 595.9468994140625 acc: 0.8775945901870728  val: loss: 1670.0997314453125 acc: 0.8422402143478394\n",
      "step: 1805\n",
      "train: loss: 243.50686645507812 acc: 0.9614700675010681  val: loss: 1084.396240234375 acc: 0.8484945893287659\n",
      "step: 1810\n",
      "train: loss: 594.6873168945312 acc: 0.910908043384552  val: loss: 2473.7001953125 acc: 0.7368833422660828\n",
      "step: 1815\n",
      "train: loss: 1425.3829345703125 acc: 0.8028563261032104  val: loss: 749.9930419921875 acc: 0.8717784881591797\n",
      "step: 1820\n",
      "train: loss: 471.8883972167969 acc: 0.9064050912857056  val: loss: 656.3781127929688 acc: 0.9209949374198914\n",
      "step: 1825\n",
      "train: loss: 566.923583984375 acc: 0.8706920146942139  val: loss: 966.404541015625 acc: 0.8141494393348694\n",
      "step: 1830\n",
      "train: loss: 366.19537353515625 acc: 0.9315871596336365  val: loss: 918.3173217773438 acc: 0.8278667330741882\n",
      "step: 1835\n",
      "train: loss: 985.536376953125 acc: 0.8569458723068237  val: loss: 715.810546875 acc: 0.8976237177848816\n",
      "step: 1840\n",
      "train: loss: 874.7127075195312 acc: 0.8741325736045837  val: loss: 1524.7462158203125 acc: 0.8188496828079224\n",
      "step: 1845\n",
      "train: loss: 574.1165771484375 acc: 0.9116091728210449  val: loss: 1198.3338623046875 acc: 0.842517614364624\n",
      "step: 1850\n",
      "train: loss: 467.90118408203125 acc: 0.9044565558433533  val: loss: 774.7239379882812 acc: 0.8804956674575806\n",
      "step: 1855\n",
      "train: loss: 1450.289306640625 acc: 0.7828793525695801  val: loss: 1350.5323486328125 acc: 0.793073296546936\n",
      "step: 1860\n",
      "train: loss: 361.8257751464844 acc: 0.9495300054550171  val: loss: 1577.7266845703125 acc: 0.7626680135726929\n",
      "step: 1865\n",
      "train: loss: 354.6721496582031 acc: 0.9285553693771362  val: loss: 1119.4765625 acc: 0.850831151008606\n",
      "step: 1870\n",
      "train: loss: 551.0494995117188 acc: 0.8835477828979492  val: loss: 1199.0498046875 acc: 0.8088194727897644\n",
      "step: 1875\n",
      "train: loss: 762.5668334960938 acc: 0.8298349976539612  val: loss: 1534.8592529296875 acc: 0.7934076189994812\n",
      "step: 1880\n",
      "train: loss: 269.81561279296875 acc: 0.9356739521026611  val: loss: 1701.1068115234375 acc: 0.7761709094047546\n",
      "step: 1885\n",
      "train: loss: 500.19659423828125 acc: 0.9319348931312561  val: loss: 952.048583984375 acc: 0.819401204586029\n",
      "step: 1890\n",
      "train: loss: 537.4495239257812 acc: 0.9258623123168945  val: loss: 1533.1522216796875 acc: 0.772455096244812\n",
      "step: 1895\n",
      "train: loss: 966.3095092773438 acc: 0.8869467377662659  val: loss: 405.352294921875 acc: 0.918822705745697\n",
      "step: 1900\n",
      "train: loss: 1034.3544921875 acc: 0.859449028968811  val: loss: 1028.847900390625 acc: 0.840509831905365\n",
      "step: 1905\n",
      "train: loss: 433.1933288574219 acc: 0.9008870124816895  val: loss: 1188.9200439453125 acc: 0.8283078074455261\n",
      "step: 1910\n",
      "train: loss: 827.4733276367188 acc: 0.8565751314163208  val: loss: 2054.15625 acc: 0.7930355072021484\n",
      "step: 1915\n",
      "train: loss: 312.08984375 acc: 0.9322540163993835  val: loss: 871.2642822265625 acc: 0.85450279712677\n",
      "step: 1920\n",
      "train: loss: 724.2821044921875 acc: 0.814452588558197  val: loss: 2258.286376953125 acc: 0.7370918989181519\n",
      "step: 1925\n",
      "train: loss: 438.2805480957031 acc: 0.9109161496162415  val: loss: 1030.2374267578125 acc: 0.8155771493911743\n",
      "step: 1930\n",
      "train: loss: 747.04833984375 acc: 0.8301584124565125  val: loss: 1231.9459228515625 acc: 0.8119414448738098\n",
      "step: 1935\n",
      "train: loss: 476.6652526855469 acc: 0.8677270412445068  val: loss: 1467.7999267578125 acc: 0.8262594938278198\n",
      "step: 1940\n",
      "train: loss: 488.37042236328125 acc: 0.8655029535293579  val: loss: 1528.9522705078125 acc: 0.7725679874420166\n",
      "step: 1945\n",
      "train: loss: 468.5999450683594 acc: 0.9403162002563477  val: loss: 955.9707641601562 acc: 0.8340200185775757\n",
      "step: 1950\n",
      "train: loss: 301.0547790527344 acc: 0.9546167254447937  val: loss: 1669.1993408203125 acc: 0.8016084432601929\n",
      "step: 1955\n",
      "train: loss: 786.313720703125 acc: 0.876662015914917  val: loss: 1649.792236328125 acc: 0.7665870189666748\n",
      "step: 1960\n",
      "train: loss: 936.0048828125 acc: 0.7885634899139404  val: loss: 1748.701416015625 acc: 0.8049991130828857\n",
      "step: 1965\n",
      "train: loss: 524.7300415039062 acc: 0.8996928334236145  val: loss: 958.6006469726562 acc: 0.8950680494308472\n",
      "step: 1970\n",
      "train: loss: 537.4379272460938 acc: 0.916966438293457  val: loss: 325.4478454589844 acc: 0.9503369927406311\n",
      "step: 1975\n",
      "train: loss: 425.9610595703125 acc: 0.9149317145347595  val: loss: 891.513671875 acc: 0.8701127767562866\n",
      "step: 1980\n",
      "train: loss: 506.9249267578125 acc: 0.9106701016426086  val: loss: 938.4219360351562 acc: 0.8748534917831421\n",
      "step: 1985\n",
      "train: loss: 847.713623046875 acc: 0.8225122690200806  val: loss: 1161.5050048828125 acc: 0.8354109525680542\n",
      "step: 1990\n",
      "train: loss: 226.01519775390625 acc: 0.9527034163475037  val: loss: 1008.146484375 acc: 0.8737080097198486\n",
      "step: 1995\n",
      "train: loss: 505.8128662109375 acc: 0.8743475079536438  val: loss: 448.91705322265625 acc: 0.9134620428085327\n",
      "step: 2000\n",
      "train: loss: 530.0003662109375 acc: 0.920968770980835  val: loss: 1328.31591796875 acc: 0.8189658522605896\n",
      "step: 2005\n",
      "train: loss: 470.2337951660156 acc: 0.9173084497451782  val: loss: 2144.63232421875 acc: 0.7771623730659485\n",
      "step: 2010\n",
      "train: loss: 503.4848937988281 acc: 0.9149284958839417  val: loss: 1342.7047119140625 acc: 0.848051905632019\n",
      "step: 2015\n",
      "train: loss: 889.7926635742188 acc: 0.7995032668113708  val: loss: 641.22314453125 acc: 0.7946337461471558\n",
      "step: 2020\n",
      "train: loss: 721.2962646484375 acc: 0.8265605568885803  val: loss: 1516.1173095703125 acc: 0.7763329148292542\n",
      "step: 2025\n",
      "train: loss: 920.3529663085938 acc: 0.841907799243927  val: loss: 618.6446533203125 acc: 0.8946910500526428\n",
      "step: 2030\n",
      "train: loss: 830.1690673828125 acc: 0.8239474892616272  val: loss: 1055.654296875 acc: 0.8128618001937866\n",
      "step: 2035\n",
      "train: loss: 936.5637817382812 acc: 0.7986631989479065  val: loss: 821.371337890625 acc: 0.8908494710922241\n",
      "step: 2040\n",
      "train: loss: 444.91455078125 acc: 0.9135638475418091  val: loss: 1254.8509521484375 acc: 0.7872576117515564\n",
      "step: 2045\n",
      "train: loss: 471.6043701171875 acc: 0.9142637252807617  val: loss: 314.7087707519531 acc: 0.9325046539306641\n",
      "step: 2050\n",
      "train: loss: 709.9489135742188 acc: 0.8359225988388062  val: loss: 1688.3873291015625 acc: 0.7200710773468018\n",
      "step: 2055\n",
      "train: loss: 364.7735595703125 acc: 0.9223802089691162  val: loss: 1735.0318603515625 acc: 0.7937700748443604\n",
      "step: 2060\n",
      "train: loss: 538.923095703125 acc: 0.9051463603973389  val: loss: 453.0516357421875 acc: 0.9284894466400146\n",
      "step: 2065\n",
      "train: loss: 381.3298034667969 acc: 0.9429191946983337  val: loss: 1177.1170654296875 acc: 0.7657191753387451\n",
      "step: 2070\n",
      "train: loss: 665.934326171875 acc: 0.894821286201477  val: loss: 711.4179077148438 acc: 0.8964307308197021\n",
      "step: 2075\n",
      "train: loss: 1044.4693603515625 acc: 0.8294691443443298  val: loss: 1059.9769287109375 acc: 0.8526167869567871\n",
      "step: 2080\n",
      "train: loss: 871.4568481445312 acc: 0.8537040948867798  val: loss: 1073.538818359375 acc: 0.8782870769500732\n",
      "step: 2085\n",
      "train: loss: 584.17822265625 acc: 0.7753058075904846  val: loss: 566.8599853515625 acc: 0.8932892084121704\n",
      "step: 2090\n",
      "train: loss: 937.9356689453125 acc: 0.8555030226707458  val: loss: 543.6282348632812 acc: 0.902122437953949\n",
      "step: 2095\n",
      "train: loss: 550.2449951171875 acc: 0.9127232432365417  val: loss: 1736.21630859375 acc: 0.809916615486145\n",
      "step: 2100\n",
      "train: loss: 273.61041259765625 acc: 0.9424446821212769  val: loss: 864.3447875976562 acc: 0.8567940592765808\n",
      "step: 2105\n",
      "train: loss: 528.0150756835938 acc: 0.8701460361480713  val: loss: 1267.32763671875 acc: 0.8310794830322266\n",
      "step: 2110\n",
      "train: loss: 834.5756225585938 acc: 0.8071961402893066  val: loss: 1604.239990234375 acc: 0.7756636738777161\n",
      "step: 2115\n",
      "train: loss: 531.2605590820312 acc: 0.8843602538108826  val: loss: 483.75653076171875 acc: 0.8898483514785767\n",
      "step: 2120\n",
      "train: loss: 599.9419555664062 acc: 0.9302351474761963  val: loss: 1400.78955078125 acc: 0.817920446395874\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 2125\n",
      "train: loss: 722.6018676757812 acc: 0.8731470704078674  val: loss: 763.9263916015625 acc: 0.8929231762886047\n",
      "step: 2130\n",
      "train: loss: 530.1802978515625 acc: 0.9165366291999817  val: loss: 1425.0423583984375 acc: 0.8123098611831665\n",
      "step: 2135\n",
      "train: loss: 618.46533203125 acc: 0.9196399450302124  val: loss: 445.6300048828125 acc: 0.9046719670295715\n",
      "step: 2140\n",
      "train: loss: 800.1832885742188 acc: 0.8105037212371826  val: loss: 650.1934204101562 acc: 0.8865736126899719\n",
      "step: 2145\n",
      "train: loss: 884.6904296875 acc: 0.8606715202331543  val: loss: 563.2677612304688 acc: 0.9194833636283875\n",
      "step: 2150\n",
      "train: loss: 559.6463623046875 acc: 0.9079489707946777  val: loss: 1092.6812744140625 acc: 0.8018904328346252\n",
      "step: 2155\n",
      "train: loss: 772.6886596679688 acc: 0.8720323443412781  val: loss: 1174.3199462890625 acc: 0.8552508354187012\n",
      "step: 2160\n",
      "train: loss: 396.8241882324219 acc: 0.9327900409698486  val: loss: 1211.7030029296875 acc: 0.8492621779441833\n",
      "step: 2165\n",
      "train: loss: 1060.7933349609375 acc: 0.7000716328620911  val: loss: 639.974853515625 acc: 0.8828760385513306\n",
      "step: 2170\n",
      "train: loss: 1010.5997314453125 acc: 0.780403196811676  val: loss: 1515.7706298828125 acc: 0.7970755696296692\n",
      "step: 2175\n",
      "train: loss: 589.8450927734375 acc: 0.8871361613273621  val: loss: 817.983642578125 acc: 0.865014910697937\n",
      "step: 2180\n",
      "train: loss: 471.6243896484375 acc: 0.9156588315963745  val: loss: 1036.32421875 acc: 0.8584665060043335\n",
      "step: 2185\n",
      "train: loss: 288.3891296386719 acc: 0.9485786557197571  val: loss: 1094.4007568359375 acc: 0.8262249231338501\n",
      "step: 2190\n",
      "train: loss: 1312.22314453125 acc: 0.7827338576316833  val: loss: 685.6079711914062 acc: 0.9066981673240662\n",
      "step: 2195\n",
      "train: loss: 1419.11376953125 acc: 0.7904309630393982  val: loss: 1603.1949462890625 acc: 0.7801640629768372\n",
      "step: 2200\n",
      "train: loss: 1060.085693359375 acc: 0.8161466121673584  val: loss: 695.8699951171875 acc: 0.8892979025840759\n",
      "step: 2205\n",
      "train: loss: 562.5650024414062 acc: 0.8746376633644104  val: loss: 798.0738525390625 acc: 0.8349180221557617\n",
      "step: 2210\n",
      "train: loss: 634.7736206054688 acc: 0.8525742292404175  val: loss: 811.9139404296875 acc: 0.8511315584182739\n",
      "step: 2215\n",
      "train: loss: 710.8471069335938 acc: 0.8541749715805054  val: loss: 1207.731689453125 acc: 0.8340821266174316\n",
      "step: 2220\n",
      "train: loss: 566.3502807617188 acc: 0.8341927528381348  val: loss: 2680.04248046875 acc: 0.7099695801734924\n",
      "step: 2225\n",
      "train: loss: 349.1901550292969 acc: 0.9389594793319702  val: loss: 786.2523803710938 acc: 0.8969751596450806\n",
      "step: 2230\n",
      "train: loss: 362.6121826171875 acc: 0.9235778450965881  val: loss: 1295.0517578125 acc: 0.7987940311431885\n",
      "step: 2235\n",
      "train: loss: 450.902099609375 acc: 0.9229286909103394  val: loss: 879.3666381835938 acc: 0.8473151922225952\n",
      "step: 2240\n",
      "train: loss: 537.4950561523438 acc: 0.9089751243591309  val: loss: 515.8508911132812 acc: 0.9261826276779175\n",
      "step: 2245\n",
      "train: loss: 817.425537109375 acc: 0.8924588561058044  val: loss: 1934.03662109375 acc: 0.7171273231506348\n",
      "step: 2250\n",
      "train: loss: 721.6841430664062 acc: 0.9103849530220032  val: loss: 1589.166748046875 acc: 0.7994682192802429\n",
      "step: 2255\n",
      "train: loss: 503.2443542480469 acc: 0.8941097259521484  val: loss: 1433.526611328125 acc: 0.8008348941802979\n",
      "step: 2260\n",
      "train: loss: 848.0127563476562 acc: 0.8892053961753845  val: loss: 906.024169921875 acc: 0.8551225662231445\n",
      "step: 2265\n",
      "train: loss: 598.1273193359375 acc: 0.9044716358184814  val: loss: 1271.4796142578125 acc: 0.8491395115852356\n",
      "step: 2270\n",
      "train: loss: 573.78369140625 acc: 0.9068928360939026  val: loss: 1098.8192138671875 acc: 0.8085798025131226\n",
      "step: 2275\n",
      "train: loss: 737.0716552734375 acc: 0.8612975478172302  val: loss: 1747.432373046875 acc: 0.7832621335983276\n",
      "step: 2280\n",
      "train: loss: 681.2899780273438 acc: 0.8477425575256348  val: loss: 557.6224365234375 acc: 0.917747974395752\n",
      "step: 2285\n",
      "train: loss: 911.7947387695312 acc: 0.844916045665741  val: loss: 897.669921875 acc: 0.8700245022773743\n",
      "step: 2290\n",
      "train: loss: 476.72772216796875 acc: 0.9205368757247925  val: loss: 578.3663940429688 acc: 0.895401656627655\n",
      "step: 2295\n",
      "train: loss: 399.9093933105469 acc: 0.9003777503967285  val: loss: 705.203369140625 acc: 0.8970761895179749\n",
      "step: 2300\n",
      "train: loss: 670.8510131835938 acc: 0.908756673336029  val: loss: 1118.603271484375 acc: 0.8288525938987732\n",
      "step: 2305\n",
      "train: loss: 806.8461303710938 acc: 0.8692582249641418  val: loss: 715.5558471679688 acc: 0.8487147092819214\n",
      "step: 2310\n",
      "train: loss: 1255.989013671875 acc: 0.7486684918403625  val: loss: 1203.960693359375 acc: 0.8079642057418823\n",
      "step: 2315\n",
      "train: loss: 747.2991333007812 acc: 0.874993085861206  val: loss: 506.4494934082031 acc: 0.9233989715576172\n",
      "step: 2320\n",
      "train: loss: 774.3453369140625 acc: 0.8935807347297668  val: loss: 1461.1302490234375 acc: 0.7990347146987915\n",
      "step: 2325\n",
      "train: loss: 675.2798461914062 acc: 0.9128246307373047  val: loss: 1446.176513671875 acc: 0.8222258687019348\n",
      "step: 2330\n",
      "train: loss: 294.7237548828125 acc: 0.9555716514587402  val: loss: 963.263916015625 acc: 0.8910120725631714\n",
      "step: 2335\n",
      "train: loss: 689.3984985351562 acc: 0.8751165866851807  val: loss: 1776.5455322265625 acc: 0.7321358919143677\n",
      "step: 2340\n",
      "train: loss: 600.8673095703125 acc: 0.9036803245544434  val: loss: 807.4144287109375 acc: 0.8743696212768555\n",
      "step: 2345\n",
      "train: loss: 178.6086883544922 acc: 0.959766685962677  val: loss: 1010.460205078125 acc: 0.8479195833206177\n",
      "step: 2350\n",
      "train: loss: 345.7353210449219 acc: 0.9232117533683777  val: loss: 1155.82177734375 acc: 0.7988089919090271\n",
      "step: 2355\n",
      "train: loss: 336.40911865234375 acc: 0.9247359037399292  val: loss: 1599.39404296875 acc: 0.8129368424415588\n",
      "step: 2360\n",
      "train: loss: 410.6270751953125 acc: 0.9362699389457703  val: loss: 452.7225646972656 acc: 0.9318101406097412\n",
      "step: 2365\n",
      "train: loss: 287.662841796875 acc: 0.9179058074951172  val: loss: 628.1319580078125 acc: 0.9143871068954468\n",
      "step: 2370\n",
      "train: loss: 663.2695922851562 acc: 0.8994883894920349  val: loss: 752.965087890625 acc: 0.8894450068473816\n",
      "step: 2375\n",
      "train: loss: 867.24365234375 acc: 0.7938930988311768  val: loss: 1065.939697265625 acc: 0.8571321964263916\n",
      "step: 2380\n",
      "train: loss: 896.7039184570312 acc: 0.8132419586181641  val: loss: 1385.916748046875 acc: 0.7790520787239075\n",
      "step: 2385\n",
      "train: loss: 842.1282348632812 acc: 0.8603986501693726  val: loss: 805.7210693359375 acc: 0.8374133110046387\n",
      "step: 2390\n",
      "train: loss: 579.7296752929688 acc: 0.8833279609680176  val: loss: 473.0418395996094 acc: 0.8952566385269165\n",
      "step: 2395\n",
      "train: loss: 586.8463745117188 acc: 0.8632065653800964  val: loss: 1317.21630859375 acc: 0.8272523880004883\n",
      "step: 2400\n",
      "train: loss: 568.0571899414062 acc: 0.8736075162887573  val: loss: 770.5106811523438 acc: 0.8806618452072144\n",
      "step: 2405\n",
      "train: loss: 479.92706298828125 acc: 0.8899329900741577  val: loss: 518.64453125 acc: 0.9106091260910034\n",
      "step: 2410\n",
      "train: loss: 731.7074584960938 acc: 0.858384907245636  val: loss: 961.0173950195312 acc: 0.8231611847877502\n",
      "step: 2415\n",
      "train: loss: 416.4901123046875 acc: 0.9192416667938232  val: loss: 1344.9893798828125 acc: 0.8359839916229248\n",
      "step: 2420\n",
      "train: loss: 374.8082580566406 acc: 0.9350743293762207  val: loss: 352.5779113769531 acc: 0.9432780742645264\n",
      "step: 2425\n",
      "train: loss: 410.7480163574219 acc: 0.9395841360092163  val: loss: 1346.135498046875 acc: 0.8540631532669067\n",
      "step: 2430\n",
      "train: loss: 837.276123046875 acc: 0.889013409614563  val: loss: 1468.357421875 acc: 0.7941296100616455\n",
      "step: 2435\n",
      "train: loss: 1173.2159423828125 acc: 0.8557437062263489  val: loss: 1358.0816650390625 acc: 0.8104992508888245\n",
      "step: 2440\n",
      "train: loss: 455.1089782714844 acc: 0.9109728932380676  val: loss: 683.6243896484375 acc: 0.8669335246086121\n",
      "step: 2445\n",
      "train: loss: 940.2042846679688 acc: 0.8053287863731384  val: loss: 408.3414306640625 acc: 0.9181452393531799\n",
      "step: 2450\n",
      "train: loss: 881.37890625 acc: 0.8205288052558899  val: loss: 1457.652587890625 acc: 0.8284367322921753\n",
      "step: 2455\n",
      "train: loss: 439.79010009765625 acc: 0.9174031615257263  val: loss: 1279.5389404296875 acc: 0.8230588436126709\n",
      "step: 2460\n",
      "train: loss: 269.5629577636719 acc: 0.9513146281242371  val: loss: 465.1280212402344 acc: 0.9166797995567322\n",
      "step: 2465\n",
      "train: loss: 556.83447265625 acc: 0.8182585835456848  val: loss: 1214.1048583984375 acc: 0.8209024667739868\n",
      "step: 2470\n",
      "train: loss: 494.114990234375 acc: 0.8870070576667786  val: loss: 2052.50927734375 acc: 0.7793488502502441\n",
      "step: 2475\n",
      "train: loss: 671.7279663085938 acc: 0.9082114100456238  val: loss: 1084.607177734375 acc: 0.836018443107605\n",
      "step: 2480\n",
      "train: loss: 483.2509460449219 acc: 0.9246575236320496  val: loss: 1074.663818359375 acc: 0.8088138103485107\n",
      "step: 2485\n",
      "train: loss: 338.80657958984375 acc: 0.9397243857383728  val: loss: 1238.222900390625 acc: 0.8436776399612427\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 2490\n",
      "train: loss: 378.8055419921875 acc: 0.9290532469749451  val: loss: 1326.4976806640625 acc: 0.831153154373169\n",
      "step: 2495\n",
      "train: loss: 797.4287719726562 acc: 0.8271329402923584  val: loss: 634.8094482421875 acc: 0.8837355971336365\n",
      "step: 2500\n",
      "train: loss: 681.3533325195312 acc: 0.8739389181137085  val: loss: 727.9677124023438 acc: 0.8601226806640625\n",
      "step: 2505\n",
      "train: loss: 1124.341552734375 acc: 0.8043100833892822  val: loss: 1677.279052734375 acc: 0.823418140411377\n",
      "step: 2510\n",
      "train: loss: 1188.451904296875 acc: 0.8207347989082336  val: loss: 1493.65576171875 acc: 0.8096605539321899\n",
      "step: 2515\n",
      "train: loss: 521.6614379882812 acc: 0.8924882411956787  val: loss: 1129.564208984375 acc: 0.8408483266830444\n",
      "step: 2520\n",
      "train: loss: 385.67291259765625 acc: 0.913540244102478  val: loss: 888.7410278320312 acc: 0.8618329763412476\n",
      "step: 2525\n",
      "train: loss: 619.0861206054688 acc: 0.8400518298149109  val: loss: 1586.6807861328125 acc: 0.7989897727966309\n",
      "step: 2530\n",
      "train: loss: 610.4461059570312 acc: 0.8780466914176941  val: loss: 1613.6285400390625 acc: 0.761723518371582\n",
      "step: 2535\n",
      "train: loss: 963.2353515625 acc: 0.8766793608665466  val: loss: 972.1727294921875 acc: 0.8363929390907288\n",
      "step: 2540\n",
      "train: loss: 615.8079223632812 acc: 0.8913084268569946  val: loss: 970.8385009765625 acc: 0.8118221163749695\n",
      "step: 2545\n",
      "train: loss: 649.7749633789062 acc: 0.8902016878128052  val: loss: 1363.56787109375 acc: 0.792793869972229\n",
      "step: 2550\n",
      "train: loss: 800.1864013671875 acc: 0.8525291681289673  val: loss: 1725.8173828125 acc: 0.7534987926483154\n",
      "step: 2555\n",
      "train: loss: 976.617431640625 acc: 0.8467087745666504  val: loss: 730.1626586914062 acc: 0.9017710089683533\n",
      "step: 2560\n",
      "train: loss: 1050.2445068359375 acc: 0.7609307169914246  val: loss: 2099.1591796875 acc: 0.7712796330451965\n",
      "step: 2565\n",
      "train: loss: 807.3582153320312 acc: 0.880946159362793  val: loss: 985.3069458007812 acc: 0.8646969795227051\n",
      "step: 2570\n",
      "train: loss: 1342.392578125 acc: 0.7474053502082825  val: loss: 1057.0889892578125 acc: 0.8197920322418213\n",
      "step: 2575\n",
      "train: loss: 632.1282348632812 acc: 0.8494150638580322  val: loss: 1161.467529296875 acc: 0.8061103820800781\n",
      "step: 2580\n",
      "train: loss: 710.2506103515625 acc: 0.8736890554428101  val: loss: 1265.54052734375 acc: 0.8469058275222778\n",
      "step: 2585\n",
      "train: loss: 870.5745849609375 acc: 0.7567756175994873  val: loss: 975.2653198242188 acc: 0.8698276281356812\n",
      "step: 2590\n",
      "train: loss: 461.9007873535156 acc: 0.9217867255210876  val: loss: 1234.7557373046875 acc: 0.8351308107376099\n",
      "step: 2595\n",
      "train: loss: 898.4735107421875 acc: 0.8761563897132874  val: loss: 494.3019714355469 acc: 0.9159110188484192\n",
      "step: 2600\n",
      "train: loss: 257.78515625 acc: 0.9540112018585205  val: loss: 1084.700927734375 acc: 0.8300808668136597\n",
      "step: 2605\n",
      "train: loss: 846.4697875976562 acc: 0.903145432472229  val: loss: 878.60205078125 acc: 0.843226969242096\n",
      "step: 2610\n",
      "train: loss: 922.1815185546875 acc: 0.8426730632781982  val: loss: 424.6273498535156 acc: 0.8876631855964661\n",
      "step: 2615\n",
      "train: loss: 1613.401611328125 acc: 0.7112016081809998  val: loss: 632.773681640625 acc: 0.8868573904037476\n",
      "step: 2620\n",
      "train: loss: 472.82525634765625 acc: 0.9041202068328857  val: loss: 1338.4366455078125 acc: 0.8085974454879761\n",
      "step: 2625\n",
      "train: loss: 187.2727508544922 acc: 0.9664458632469177  val: loss: 1023.2381591796875 acc: 0.8358063101768494\n",
      "step: 2630\n",
      "train: loss: 569.2620239257812 acc: 0.9002283811569214  val: loss: 508.5709533691406 acc: 0.9183440804481506\n",
      "step: 2635\n",
      "train: loss: 593.0659790039062 acc: 0.8799553513526917  val: loss: 1093.67724609375 acc: 0.8250914812088013\n",
      "step: 2640\n",
      "train: loss: 716.9617309570312 acc: 0.8696943521499634  val: loss: 1037.209716796875 acc: 0.8488715887069702\n",
      "step: 2645\n",
      "train: loss: 367.70306396484375 acc: 0.923648476600647  val: loss: 833.6901245117188 acc: 0.8588740229606628\n",
      "step: 2650\n",
      "train: loss: 816.9545288085938 acc: 0.7898414134979248  val: loss: 1657.2401123046875 acc: 0.7562881708145142\n",
      "step: 2655\n",
      "train: loss: 231.26345825195312 acc: 0.9524293541908264  val: loss: 990.9422607421875 acc: 0.8707343339920044\n",
      "step: 2660\n",
      "train: loss: 695.2891845703125 acc: 0.8845640420913696  val: loss: 586.7980346679688 acc: 0.9037439823150635\n",
      "step: 2665\n",
      "train: loss: 781.8980102539062 acc: 0.89995938539505  val: loss: 1692.2835693359375 acc: 0.8062354326248169\n",
      "step: 2670\n",
      "train: loss: 1089.01123046875 acc: 0.8117349147796631  val: loss: 828.2313842773438 acc: 0.8380720615386963\n",
      "step: 2675\n",
      "train: loss: 1313.4696044921875 acc: 0.8125759363174438  val: loss: 877.5385131835938 acc: 0.8520952463150024\n",
      "step: 2680\n",
      "train: loss: 959.8973999023438 acc: 0.8224900960922241  val: loss: 1824.9215087890625 acc: 0.8108772039413452\n",
      "step: 2685\n",
      "train: loss: 665.12646484375 acc: 0.8886565566062927  val: loss: 1212.1751708984375 acc: 0.8540763258934021\n",
      "step: 2690\n",
      "train: loss: 687.0552368164062 acc: 0.8855672478675842  val: loss: 612.945556640625 acc: 0.9108376502990723\n",
      "step: 2695\n",
      "train: loss: 516.9576416015625 acc: 0.9104611873626709  val: loss: 909.5114135742188 acc: 0.840010404586792\n",
      "step: 2700\n",
      "train: loss: 583.9000244140625 acc: 0.8504687547683716  val: loss: 413.77789306640625 acc: 0.9391792416572571\n",
      "step: 2705\n",
      "train: loss: 1077.6783447265625 acc: 0.8199098110198975  val: loss: 464.82623291015625 acc: 0.9180281758308411\n",
      "step: 2710\n",
      "train: loss: 469.43341064453125 acc: 0.9220923185348511  val: loss: 681.2726440429688 acc: 0.8913705348968506\n",
      "step: 2715\n",
      "train: loss: 459.6203308105469 acc: 0.9320278167724609  val: loss: 1281.4588623046875 acc: 0.8276457786560059\n",
      "step: 2720\n",
      "train: loss: 898.1460571289062 acc: 0.8189849853515625  val: loss: 1193.56494140625 acc: 0.8249573707580566\n",
      "step: 2725\n",
      "train: loss: 510.3223571777344 acc: 0.8707901239395142  val: loss: 663.2161254882812 acc: 0.8780993819236755\n",
      "step: 2730\n",
      "train: loss: 799.0201416015625 acc: 0.8150516152381897  val: loss: 1740.52197265625 acc: 0.7820226550102234\n",
      "step: 2735\n",
      "train: loss: 697.2718505859375 acc: 0.8115676045417786  val: loss: 927.0252685546875 acc: 0.8676420450210571\n",
      "step: 2740\n",
      "train: loss: 956.680908203125 acc: 0.7757167816162109  val: loss: 3129.858154296875 acc: 0.7341588735580444\n",
      "step: 2745\n",
      "train: loss: 747.5811157226562 acc: 0.8875636458396912  val: loss: 607.3568725585938 acc: 0.9077009558677673\n",
      "step: 2750\n",
      "train: loss: 1072.919677734375 acc: 0.840462863445282  val: loss: 869.1456298828125 acc: 0.8438267707824707\n",
      "step: 2755\n",
      "train: loss: 732.8125 acc: 0.8662766218185425  val: loss: 975.5112915039062 acc: 0.8409154415130615\n",
      "step: 2760\n",
      "train: loss: 1040.6806640625 acc: 0.8012323379516602  val: loss: 976.2705078125 acc: 0.8431396484375\n",
      "step: 2765\n",
      "train: loss: 417.78961181640625 acc: 0.9138940572738647  val: loss: 1109.010009765625 acc: 0.8595044016838074\n",
      "step: 2770\n",
      "train: loss: 485.9938049316406 acc: 0.9238014817237854  val: loss: 780.197998046875 acc: 0.8734580278396606\n",
      "step: 2775\n",
      "train: loss: 924.7984619140625 acc: 0.8387331962585449  val: loss: 1264.0611572265625 acc: 0.8078622221946716\n",
      "step: 2780\n",
      "train: loss: 1001.2093505859375 acc: 0.8402043581008911  val: loss: 1712.0830078125 acc: 0.7861843705177307\n",
      "step: 2785\n",
      "train: loss: 1046.2191162109375 acc: 0.7647005319595337  val: loss: 728.6012573242188 acc: 0.8686098456382751\n",
      "step: 2790\n",
      "train: loss: 1312.78955078125 acc: 0.7491977214813232  val: loss: 1353.3394775390625 acc: 0.8308509588241577\n",
      "step: 2795\n",
      "train: loss: 933.0173950195312 acc: 0.759221613407135  val: loss: 661.7870483398438 acc: 0.8758170008659363\n",
      "step: 2800\n",
      "train: loss: 995.70361328125 acc: 0.8615881204605103  val: loss: 950.5965576171875 acc: 0.8543753623962402\n",
      "step: 2805\n",
      "train: loss: 602.17041015625 acc: 0.8836181163787842  val: loss: 1087.5560302734375 acc: 0.7950199246406555\n",
      "step: 2810\n",
      "train: loss: 403.6962585449219 acc: 0.9124994874000549  val: loss: 1176.890869140625 acc: 0.855146050453186\n",
      "step: 2815\n",
      "train: loss: 484.22161865234375 acc: 0.9110574126243591  val: loss: 1300.721435546875 acc: 0.8505077362060547\n",
      "step: 2820\n",
      "train: loss: 546.9231567382812 acc: 0.8676757216453552  val: loss: 946.9609985351562 acc: 0.8251076340675354\n",
      "step: 2825\n",
      "train: loss: 660.6237182617188 acc: 0.8441410660743713  val: loss: 1117.004150390625 acc: 0.8338860273361206\n",
      "step: 2830\n",
      "train: loss: 873.7417602539062 acc: 0.9010685682296753  val: loss: 1190.118408203125 acc: 0.8351037502288818\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 2835\n",
      "train: loss: 548.613525390625 acc: 0.9096443057060242  val: loss: 1135.102783203125 acc: 0.8437001705169678\n",
      "step: 2840\n",
      "train: loss: 483.7104187011719 acc: 0.932518482208252  val: loss: 768.9852294921875 acc: 0.8928561806678772\n",
      "step: 2845\n",
      "train: loss: 656.3668823242188 acc: 0.8592430353164673  val: loss: 621.8299560546875 acc: 0.8847514986991882\n",
      "step: 2850\n",
      "train: loss: 989.6171875 acc: 0.8528192639350891  val: loss: 703.3909912109375 acc: 0.9009369015693665\n",
      "step: 2855\n",
      "train: loss: 939.9076538085938 acc: 0.8177328109741211  val: loss: 1101.7264404296875 acc: 0.8682832717895508\n",
      "step: 2860\n",
      "train: loss: 767.820556640625 acc: 0.7949459552764893  val: loss: 1104.7830810546875 acc: 0.8159445524215698\n",
      "step: 2865\n",
      "train: loss: 977.35009765625 acc: 0.8813061118125916  val: loss: 596.3093872070312 acc: 0.8853650689125061\n",
      "step: 2870\n",
      "train: loss: 601.710693359375 acc: 0.9115580320358276  val: loss: 772.3012084960938 acc: 0.8801812529563904\n",
      "step: 2875\n",
      "train: loss: 1451.58740234375 acc: 0.7882262468338013  val: loss: 1102.53759765625 acc: 0.8674668073654175\n",
      "step: 2880\n",
      "train: loss: 1161.1553955078125 acc: 0.7675241231918335  val: loss: 2060.63037109375 acc: 0.7599388360977173\n",
      "step: 2885\n",
      "train: loss: 464.5197448730469 acc: 0.9220127463340759  val: loss: 1292.67578125 acc: 0.8188679218292236\n",
      "step: 2890\n",
      "train: loss: 647.1551513671875 acc: 0.8793595433235168  val: loss: 1557.8592529296875 acc: 0.7991097569465637\n",
      "step: 2895\n",
      "train: loss: 371.05108642578125 acc: 0.9533140063285828  val: loss: 516.4138793945312 acc: 0.8771848678588867\n",
      "step: 2900\n",
      "train: loss: 547.5582275390625 acc: 0.936725914478302  val: loss: 828.57080078125 acc: 0.8851791620254517\n",
      "step: 2905\n",
      "train: loss: 763.4553833007812 acc: 0.8788861036300659  val: loss: 928.7268676757812 acc: 0.8645063638687134\n",
      "step: 2910\n",
      "train: loss: 890.524658203125 acc: 0.8271892666816711  val: loss: 644.0079345703125 acc: 0.8974935412406921\n",
      "step: 2915\n",
      "train: loss: 842.8535766601562 acc: 0.8258642554283142  val: loss: 1600.0987548828125 acc: 0.8142579793930054\n",
      "step: 2920\n",
      "train: loss: 544.9927978515625 acc: 0.9220883846282959  val: loss: 1733.5177001953125 acc: 0.7474163770675659\n",
      "step: 2925\n",
      "train: loss: 842.9122314453125 acc: 0.8730488419532776  val: loss: 1824.4927978515625 acc: 0.7720057368278503\n",
      "step: 2930\n",
      "train: loss: 649.3843383789062 acc: 0.882182776927948  val: loss: 1283.6455078125 acc: 0.7765862941741943\n",
      "step: 2935\n",
      "train: loss: 544.2342529296875 acc: 0.891474187374115  val: loss: 1031.928466796875 acc: 0.8515394926071167\n",
      "step: 2940\n",
      "train: loss: 579.7218017578125 acc: 0.8371349573135376  val: loss: 1293.833251953125 acc: 0.8229807615280151\n",
      "step: 2945\n",
      "train: loss: 948.5410766601562 acc: 0.7348862886428833  val: loss: 947.3088989257812 acc: 0.8493601679801941\n",
      "step: 2950\n",
      "train: loss: 435.9026184082031 acc: 0.9282739758491516  val: loss: 1649.1927490234375 acc: 0.7833271622657776\n",
      "step: 2955\n",
      "train: loss: 927.9967651367188 acc: 0.8533368706703186  val: loss: 1240.2923583984375 acc: 0.8335053324699402\n",
      "step: 2960\n",
      "train: loss: 987.9530639648438 acc: 0.8597412109375  val: loss: 723.4160766601562 acc: 0.8554272055625916\n",
      "step: 2965\n",
      "train: loss: 987.7339477539062 acc: 0.8672680854797363  val: loss: 1013.3958740234375 acc: 0.8509398698806763\n",
      "step: 2970\n",
      "train: loss: 481.7695007324219 acc: 0.915783166885376  val: loss: 1106.6810302734375 acc: 0.8403401374816895\n",
      "step: 2975\n",
      "train: loss: 621.6333618164062 acc: 0.7929447889328003  val: loss: 1302.465087890625 acc: 0.7928301692008972\n",
      "step: 2980\n",
      "train: loss: 852.320068359375 acc: 0.8062974214553833  val: loss: 1037.38037109375 acc: 0.8357670307159424\n",
      "step: 2985\n",
      "train: loss: 820.5517578125 acc: 0.8214557766914368  val: loss: 1433.873046875 acc: 0.8100746273994446\n",
      "step: 2990\n",
      "train: loss: 873.379638671875 acc: 0.8127753734588623  val: loss: 1491.9012451171875 acc: 0.8277477622032166\n",
      "step: 2995\n",
      "train: loss: 734.0443115234375 acc: 0.8577828407287598  val: loss: 1311.329833984375 acc: 0.8479282259941101\n",
      "step: 3000\n",
      "train: loss: 381.1030578613281 acc: 0.896959125995636  val: loss: 1176.362548828125 acc: 0.8125942349433899\n",
      "step: 3005\n",
      "train: loss: 355.37054443359375 acc: 0.945814847946167  val: loss: 882.7907104492188 acc: 0.8740987777709961\n",
      "step: 3010\n",
      "train: loss: 581.3820190429688 acc: 0.9020223021507263  val: loss: 1965.7611083984375 acc: 0.736649215221405\n",
      "step: 3015\n",
      "train: loss: 543.313232421875 acc: 0.9124711155891418  val: loss: 1056.51708984375 acc: 0.8426831960678101\n",
      "step: 3020\n",
      "train: loss: 479.0975341796875 acc: 0.8977657556533813  val: loss: 1347.7880859375 acc: 0.8089480400085449\n",
      "step: 3025\n",
      "train: loss: 610.2330322265625 acc: 0.8490102887153625  val: loss: 1660.1650390625 acc: 0.8015208840370178\n",
      "step: 3030\n",
      "train: loss: 1037.823486328125 acc: 0.7987660765647888  val: loss: 2632.60107421875 acc: 0.6945915818214417\n",
      "step: 3035\n",
      "train: loss: 986.257568359375 acc: 0.7870465517044067  val: loss: 1666.2537841796875 acc: 0.8360617160797119\n",
      "step: 3040\n",
      "train: loss: 717.96142578125 acc: 0.8782067894935608  val: loss: 1602.4248046875 acc: 0.7909701466560364\n",
      "step: 3045\n",
      "train: loss: 954.2306518554688 acc: 0.8398751020431519  val: loss: 852.4725952148438 acc: 0.8576452732086182\n",
      "step: 3050\n",
      "train: loss: 301.08392333984375 acc: 0.9290022253990173  val: loss: 492.09442138671875 acc: 0.8812380433082581\n",
      "step: 3055\n",
      "train: loss: 590.9462280273438 acc: 0.8977675437927246  val: loss: 2182.235595703125 acc: 0.7607002258300781\n",
      "step: 3060\n",
      "train: loss: 308.0328674316406 acc: 0.897537350654602  val: loss: 1610.66943359375 acc: 0.8066787719726562\n",
      "step: 3065\n",
      "train: loss: 742.0538940429688 acc: 0.8704379200935364  val: loss: 1265.9281005859375 acc: 0.8311910629272461\n",
      "step: 3070\n",
      "train: loss: 483.1700134277344 acc: 0.9243138432502747  val: loss: 1272.6431884765625 acc: 0.8161382079124451\n",
      "step: 3075\n",
      "train: loss: 857.5657958984375 acc: 0.820880651473999  val: loss: 951.1678466796875 acc: 0.8366183042526245\n",
      "step: 3080\n",
      "train: loss: 546.8953247070312 acc: 0.9063907861709595  val: loss: 340.68121337890625 acc: 0.9385837912559509\n",
      "step: 3085\n",
      "train: loss: 864.1986694335938 acc: 0.8430643677711487  val: loss: 1393.7412109375 acc: 0.8006587624549866\n",
      "step: 3090\n",
      "train: loss: 1023.42431640625 acc: 0.8051072955131531  val: loss: 732.0940551757812 acc: 0.8995316624641418\n",
      "step: 3095\n",
      "train: loss: 707.03173828125 acc: 0.8283484578132629  val: loss: 863.9619140625 acc: 0.8494035601615906\n",
      "step: 3100\n",
      "train: loss: 1212.650390625 acc: 0.7724189162254333  val: loss: 494.75830078125 acc: 0.8944668173789978\n",
      "step: 3105\n",
      "train: loss: 858.2697143554688 acc: 0.868818998336792  val: loss: 1381.7711181640625 acc: 0.7989906668663025\n",
      "step: 3110\n",
      "train: loss: 670.5693359375 acc: 0.8919631242752075  val: loss: 1476.9735107421875 acc: 0.8471789360046387\n",
      "step: 3115\n",
      "train: loss: 301.2621154785156 acc: 0.9311120510101318  val: loss: 554.4500122070312 acc: 0.9093708395957947\n",
      "step: 3120\n",
      "train: loss: 646.6983642578125 acc: 0.826249361038208  val: loss: 432.4194030761719 acc: 0.9210554361343384\n",
      "step: 3125\n",
      "train: loss: 466.0421447753906 acc: 0.930686891078949  val: loss: 2159.990478515625 acc: 0.7075521349906921\n",
      "step: 3130\n",
      "train: loss: 917.9385375976562 acc: 0.8356969356536865  val: loss: 768.9593505859375 acc: 0.8714717030525208\n",
      "step: 3135\n",
      "train: loss: 595.4812622070312 acc: 0.92349773645401  val: loss: 696.72216796875 acc: 0.8548818230628967\n",
      "step: 3140\n",
      "train: loss: 766.775146484375 acc: 0.8472486734390259  val: loss: 655.8887329101562 acc: 0.9099739193916321\n",
      "step: 3145\n",
      "train: loss: 336.9307861328125 acc: 0.9155972599983215  val: loss: 429.0736083984375 acc: 0.9033738970756531\n",
      "step: 3150\n",
      "train: loss: 755.2603149414062 acc: 0.8371908664703369  val: loss: 1316.8387451171875 acc: 0.8009904623031616\n",
      "step: 3155\n",
      "train: loss: 1329.8251953125 acc: 0.6379314064979553  val: loss: 1248.68603515625 acc: 0.7999477386474609\n",
      "step: 3160\n",
      "train: loss: 709.8565673828125 acc: 0.9009594917297363  val: loss: 367.3923645019531 acc: 0.9317719340324402\n",
      "step: 3165\n",
      "train: loss: 879.5846557617188 acc: 0.8419208526611328  val: loss: 609.5193481445312 acc: 0.8776484131813049\n",
      "step: 3170\n",
      "train: loss: 448.78131103515625 acc: 0.9016057848930359  val: loss: 1629.5548095703125 acc: 0.7623465657234192\n",
      "step: 3175\n",
      "train: loss: 816.805908203125 acc: 0.8708373308181763  val: loss: 1664.96728515625 acc: 0.8081318140029907\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 3180\n",
      "train: loss: 533.3236083984375 acc: 0.8651059865951538  val: loss: 981.1278076171875 acc: 0.8484156131744385\n",
      "step: 3185\n",
      "train: loss: 664.3228759765625 acc: 0.9204838871955872  val: loss: 1294.672119140625 acc: 0.8370020389556885\n",
      "step: 3190\n",
      "train: loss: 455.1617431640625 acc: 0.9273300766944885  val: loss: 1553.772216796875 acc: 0.807522177696228\n",
      "step: 3195\n",
      "train: loss: 951.37109375 acc: 0.866237461566925  val: loss: 1448.7811279296875 acc: 0.8673117756843567\n",
      "step: 3200\n",
      "train: loss: 1307.391845703125 acc: 0.8011156320571899  val: loss: 1565.267578125 acc: 0.8279480934143066\n",
      "step: 3205\n",
      "train: loss: 701.4628295898438 acc: 0.8724766969680786  val: loss: 1521.5531005859375 acc: 0.7990522384643555\n",
      "step: 3210\n",
      "train: loss: 1881.8612060546875 acc: 0.6832211017608643  val: loss: 554.2650756835938 acc: 0.9103112816810608\n",
      "step: 3215\n",
      "train: loss: 1241.9285888671875 acc: 0.8457319736480713  val: loss: 571.2879638671875 acc: 0.9133127927780151\n",
      "step: 3220\n",
      "train: loss: 646.3899536132812 acc: 0.8907477259635925  val: loss: 1118.7684326171875 acc: 0.8331103324890137\n",
      "step: 3225\n",
      "train: loss: 1135.99951171875 acc: 0.6368213295936584  val: loss: 538.8365478515625 acc: 0.914513349533081\n",
      "step: 3230\n",
      "train: loss: 453.03009033203125 acc: 0.8849652409553528  val: loss: 1094.644287109375 acc: 0.8663114905357361\n",
      "step: 3235\n",
      "train: loss: 497.7044677734375 acc: 0.8385959267616272  val: loss: 841.6192626953125 acc: 0.856201708316803\n",
      "step: 3240\n",
      "train: loss: 900.7675170898438 acc: 0.8467521071434021  val: loss: 1127.461669921875 acc: 0.8312689065933228\n",
      "step: 3245\n",
      "train: loss: 413.0880126953125 acc: 0.9407075643539429  val: loss: 944.1301879882812 acc: 0.8316245079040527\n",
      "step: 3250\n",
      "train: loss: 1252.989990234375 acc: 0.7969098091125488  val: loss: 410.31329345703125 acc: 0.9189866781234741\n",
      "step: 3255\n",
      "train: loss: 990.90380859375 acc: 0.8658891320228577  val: loss: 1396.2777099609375 acc: 0.7333924174308777\n",
      "step: 3260\n",
      "train: loss: 1395.2493896484375 acc: 0.7795799374580383  val: loss: 1134.532470703125 acc: 0.8542813658714294\n",
      "step: 3265\n",
      "train: loss: 473.01824951171875 acc: 0.8968035578727722  val: loss: 935.4671630859375 acc: 0.8665744066238403\n",
      "step: 3270\n",
      "train: loss: 679.8577270507812 acc: 0.8634448647499084  val: loss: 955.2479858398438 acc: 0.8367331027984619\n",
      "step: 3275\n",
      "train: loss: 462.1246643066406 acc: 0.937017560005188  val: loss: 1200.9921875 acc: 0.810110867023468\n",
      "step: 3280\n",
      "train: loss: 623.5841674804688 acc: 0.9155421257019043  val: loss: 1473.757080078125 acc: 0.7883545160293579\n",
      "step: 3285\n",
      "train: loss: 759.7421875 acc: 0.8653692007064819  val: loss: 915.696533203125 acc: 0.8600665330886841\n",
      "step: 3290\n",
      "train: loss: 683.6054077148438 acc: 0.8849983215332031  val: loss: 1068.9903564453125 acc: 0.8704100847244263\n",
      "step: 3295\n",
      "train: loss: 546.5833740234375 acc: 0.8702594041824341  val: loss: 1008.5960693359375 acc: 0.8412024974822998\n",
      "step: 3300\n",
      "train: loss: 840.3505249023438 acc: 0.7587248682975769  val: loss: 1159.7447509765625 acc: 0.8089419007301331\n",
      "step: 3305\n",
      "train: loss: 696.9981689453125 acc: 0.9170218706130981  val: loss: 1137.1527099609375 acc: 0.8319200277328491\n",
      "step: 3310\n",
      "train: loss: 386.4381408691406 acc: 0.9057121872901917  val: loss: 2081.460693359375 acc: 0.7684721350669861\n",
      "step: 3315\n",
      "train: loss: 1043.5220947265625 acc: 0.8601688742637634  val: loss: 584.1578979492188 acc: 0.8885593414306641\n",
      "step: 3320\n",
      "train: loss: 727.7678833007812 acc: 0.864528477191925  val: loss: 2265.23779296875 acc: 0.742946207523346\n",
      "step: 3325\n",
      "train: loss: 753.92333984375 acc: 0.8488110899925232  val: loss: 1864.3436279296875 acc: 0.7777895927429199\n",
      "step: 3330\n",
      "train: loss: 911.126708984375 acc: 0.7862961888313293  val: loss: 1837.2379150390625 acc: 0.7389978766441345\n",
      "step: 3335\n",
      "train: loss: 695.177978515625 acc: 0.8695053458213806  val: loss: 946.0262451171875 acc: 0.8698431253433228\n",
      "step: 3340\n",
      "train: loss: 634.0579833984375 acc: 0.9196192026138306  val: loss: 1065.100341796875 acc: 0.8714359402656555\n",
      "step: 3345\n",
      "train: loss: 706.5563354492188 acc: 0.8788931369781494  val: loss: 1892.921875 acc: 0.7798539400100708\n",
      "step: 3350\n",
      "train: loss: 407.2809143066406 acc: 0.9020498394966125  val: loss: 1605.973388671875 acc: 0.7931482195854187\n",
      "step: 3355\n",
      "train: loss: 574.2711791992188 acc: 0.9038705229759216  val: loss: 1709.6944580078125 acc: 0.7543947100639343\n",
      "step: 3360\n",
      "train: loss: 390.5322570800781 acc: 0.9417519569396973  val: loss: 737.613037109375 acc: 0.8669555187225342\n",
      "step: 3365\n",
      "train: loss: 774.7233276367188 acc: 0.9063858389854431  val: loss: 710.2304077148438 acc: 0.9017357230186462\n",
      "step: 3370\n",
      "train: loss: 580.1640014648438 acc: 0.9228708744049072  val: loss: 515.3317260742188 acc: 0.8882030844688416\n",
      "step: 3375\n",
      "train: loss: 642.0492553710938 acc: 0.8499288558959961  val: loss: 1106.225830078125 acc: 0.861030638217926\n",
      "step: 3380\n",
      "train: loss: 622.52587890625 acc: 0.8052911758422852  val: loss: 1326.90771484375 acc: 0.7935202121734619\n",
      "step: 3385\n",
      "train: loss: 1337.3177490234375 acc: 0.7284311056137085  val: loss: 1126.2275390625 acc: 0.854921817779541\n",
      "step: 3390\n",
      "train: loss: 815.6953735351562 acc: 0.8072549104690552  val: loss: 827.4677734375 acc: 0.8706480860710144\n",
      "step: 3395\n",
      "train: loss: 433.3873596191406 acc: 0.9299449324607849  val: loss: 1036.1634521484375 acc: 0.8290984630584717\n",
      "step: 3400\n",
      "train: loss: 979.6119995117188 acc: 0.8089362978935242  val: loss: 819.9177856445312 acc: 0.8793959617614746\n",
      "step: 3405\n",
      "train: loss: 733.3212280273438 acc: 0.8537101149559021  val: loss: 1281.8868408203125 acc: 0.7967437505722046\n",
      "step: 3410\n",
      "train: loss: 635.58544921875 acc: 0.8127810955047607  val: loss: 839.2689819335938 acc: 0.849165678024292\n",
      "step: 3415\n",
      "train: loss: 526.9913940429688 acc: 0.8949355483055115  val: loss: 1099.8409423828125 acc: 0.843833863735199\n",
      "step: 3420\n",
      "train: loss: 462.234619140625 acc: 0.8999711871147156  val: loss: 1854.434326171875 acc: 0.745840311050415\n",
      "step: 3425\n",
      "train: loss: 433.0614013671875 acc: 0.9328612089157104  val: loss: 1212.156494140625 acc: 0.7971441745758057\n",
      "step: 3430\n",
      "train: loss: 710.1775512695312 acc: 0.9186794757843018  val: loss: 454.8248596191406 acc: 0.9139927625656128\n",
      "step: 3435\n",
      "train: loss: 976.4627685546875 acc: 0.8639669418334961  val: loss: 1575.8463134765625 acc: 0.8110470175743103\n",
      "step: 3440\n",
      "train: loss: 848.050048828125 acc: 0.8395177721977234  val: loss: 1330.84326171875 acc: 0.7969437837600708\n",
      "step: 3445\n",
      "train: loss: 602.1263427734375 acc: 0.8627914190292358  val: loss: 1120.228759765625 acc: 0.827861487865448\n",
      "step: 3450\n",
      "train: loss: 906.4239501953125 acc: 0.8551509380340576  val: loss: 874.2014770507812 acc: 0.8294777870178223\n",
      "step: 3455\n",
      "train: loss: 401.1335754394531 acc: 0.9347100257873535  val: loss: 717.270751953125 acc: 0.9073276519775391\n",
      "step: 3460\n",
      "train: loss: 422.1610412597656 acc: 0.9079059958457947  val: loss: 708.3185424804688 acc: 0.9078229665756226\n",
      "step: 3465\n",
      "train: loss: 566.1980590820312 acc: 0.8889800310134888  val: loss: 495.4160461425781 acc: 0.9313756823539734\n",
      "step: 3470\n",
      "train: loss: 207.80592346191406 acc: 0.934158980846405  val: loss: 813.427001953125 acc: 0.8136789202690125\n",
      "step: 3475\n",
      "train: loss: 293.09161376953125 acc: 0.9416507482528687  val: loss: 1552.2021484375 acc: 0.8253375887870789\n",
      "step: 3480\n",
      "train: loss: 574.82275390625 acc: 0.909363865852356  val: loss: 1249.6868896484375 acc: 0.7835790514945984\n",
      "step: 3485\n",
      "train: loss: 811.1558837890625 acc: 0.8242730498313904  val: loss: 1169.374755859375 acc: 0.792913556098938\n",
      "step: 3490\n",
      "train: loss: 789.1936645507812 acc: 0.9095929861068726  val: loss: 1158.56982421875 acc: 0.8350667953491211\n",
      "step: 3495\n",
      "train: loss: 921.5762329101562 acc: 0.8291295170783997  val: loss: 1044.8973388671875 acc: 0.8861933946609497\n",
      "step: 3500\n",
      "train: loss: 839.5072021484375 acc: 0.7901906371116638  val: loss: 1063.07373046875 acc: 0.8121376037597656\n",
      "step: 3505\n",
      "train: loss: 768.7032470703125 acc: 0.8029488325119019  val: loss: 922.0037841796875 acc: 0.8443904519081116\n",
      "step: 3510\n",
      "train: loss: 631.0889892578125 acc: 0.9026457071304321  val: loss: 552.4097900390625 acc: 0.9066216945648193\n",
      "step: 3515\n",
      "train: loss: 921.630615234375 acc: 0.8677296042442322  val: loss: 670.6183471679688 acc: 0.8860424160957336\n",
      "step: 3520\n",
      "train: loss: 771.6880493164062 acc: 0.8589752912521362  val: loss: 628.4056396484375 acc: 0.9051902890205383\n",
      "step: 3525\n",
      "train: loss: 511.94879150390625 acc: 0.8521310091018677  val: loss: 1033.6649169921875 acc: 0.8641185760498047\n",
      "step: 3530\n",
      "train: loss: 519.796142578125 acc: 0.8930833339691162  val: loss: 1023.5852661132812 acc: 0.8365626931190491\n",
      "step: 3535\n",
      "train: loss: 702.9974975585938 acc: 0.8712648153305054  val: loss: 1742.3441162109375 acc: 0.7773857712745667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 3540\n",
      "train: loss: 469.0205993652344 acc: 0.9364498853683472  val: loss: 1315.8326416015625 acc: 0.8319021463394165\n",
      "step: 3545\n",
      "train: loss: 897.295166015625 acc: 0.8273828029632568  val: loss: 401.8372497558594 acc: 0.926477313041687\n",
      "step: 3550\n",
      "train: loss: 942.44580078125 acc: 0.8792576789855957  val: loss: 868.1829833984375 acc: 0.8453832864761353\n",
      "step: 3555\n",
      "train: loss: 1125.8995361328125 acc: 0.7649263143539429  val: loss: 910.3668823242188 acc: 0.8112119436264038\n",
      "step: 3560\n",
      "train: loss: 862.12109375 acc: 0.7958813905715942  val: loss: 776.8460693359375 acc: 0.8722599744796753\n",
      "step: 3565\n",
      "train: loss: 1206.7017822265625 acc: 0.7830913066864014  val: loss: 762.247802734375 acc: 0.8900337219238281\n",
      "step: 3570\n",
      "train: loss: 854.147705078125 acc: 0.8487246036529541  val: loss: 627.9511108398438 acc: 0.8842916488647461\n",
      "step: 3575\n",
      "train: loss: 828.4542846679688 acc: 0.7723895311355591  val: loss: 902.166015625 acc: 0.8856146335601807\n",
      "step: 3580\n",
      "train: loss: 701.185302734375 acc: 0.8593316078186035  val: loss: 1752.87353515625 acc: 0.7732189297676086\n",
      "step: 3585\n",
      "train: loss: 399.51190185546875 acc: 0.8960607051849365  val: loss: 1058.7188720703125 acc: 0.8492850065231323\n",
      "step: 3590\n",
      "train: loss: 623.7247314453125 acc: 0.8940032720565796  val: loss: 927.9900512695312 acc: 0.8544198274612427\n",
      "step: 3595\n",
      "train: loss: 412.1763610839844 acc: 0.9268887639045715  val: loss: 1106.3616943359375 acc: 0.8026751279830933\n",
      "step: 3600\n",
      "train: loss: 220.03970336914062 acc: 0.9686179757118225  val: loss: 1178.4417724609375 acc: 0.8394801020622253\n",
      "step: 3605\n",
      "train: loss: 935.383544921875 acc: 0.8355361819267273  val: loss: 2085.876953125 acc: 0.8036567568778992\n",
      "step: 3610\n",
      "train: loss: 938.6676025390625 acc: 0.8674832582473755  val: loss: 1066.2281494140625 acc: 0.8705319166183472\n",
      "step: 3615\n",
      "train: loss: 933.882080078125 acc: 0.8854274749755859  val: loss: 361.547119140625 acc: 0.9396392107009888\n",
      "step: 3620\n",
      "train: loss: 707.14697265625 acc: 0.8790305852890015  val: loss: 1475.9853515625 acc: 0.8398866653442383\n",
      "step: 3625\n",
      "train: loss: 675.2872924804688 acc: 0.8666777610778809  val: loss: 407.7548828125 acc: 0.9151710867881775\n",
      "step: 3630\n",
      "train: loss: 883.267333984375 acc: 0.8618977069854736  val: loss: 1623.327392578125 acc: 0.8161202669143677\n",
      "step: 3635\n",
      "train: loss: 451.23858642578125 acc: 0.8994891047477722  val: loss: 1832.973388671875 acc: 0.7552497386932373\n",
      "step: 3640\n",
      "train: loss: 645.5245971679688 acc: 0.8144303560256958  val: loss: 827.8598022460938 acc: 0.8764060139656067\n",
      "step: 3645\n",
      "train: loss: 637.540771484375 acc: 0.8137887716293335  val: loss: 1015.8296508789062 acc: 0.8402303457260132\n",
      "step: 3650\n",
      "train: loss: 485.6328430175781 acc: 0.9132474660873413  val: loss: 695.4010620117188 acc: 0.8090337514877319\n",
      "step: 3655\n",
      "train: loss: 503.1386413574219 acc: 0.8712950944900513  val: loss: 917.3493041992188 acc: 0.8703413605690002\n",
      "step: 3660\n",
      "train: loss: 780.9008178710938 acc: 0.9005071520805359  val: loss: 1027.7445068359375 acc: 0.8674899339675903\n",
      "step: 3665\n",
      "train: loss: 454.0197448730469 acc: 0.9220191836357117  val: loss: 2305.237060546875 acc: 0.7394729852676392\n",
      "step: 3670\n",
      "train: loss: 804.3233642578125 acc: 0.877766489982605  val: loss: 1419.98291015625 acc: 0.8047865033149719\n",
      "step: 3675\n",
      "train: loss: 753.1565551757812 acc: 0.8689489364624023  val: loss: 1549.55419921875 acc: 0.812008261680603\n",
      "step: 3680\n",
      "train: loss: 930.85498046875 acc: 0.7907698750495911  val: loss: 1375.5101318359375 acc: 0.8245187997817993\n",
      "step: 3685\n",
      "train: loss: 1200.0186767578125 acc: 0.8020652532577515  val: loss: 1392.9539794921875 acc: 0.821295440196991\n",
      "step: 3690\n",
      "train: loss: 741.3287353515625 acc: 0.820740818977356  val: loss: 827.5018920898438 acc: 0.901151716709137\n",
      "step: 3695\n",
      "train: loss: 403.5334167480469 acc: 0.9382797479629517  val: loss: 365.1581726074219 acc: 0.9461153149604797\n",
      "step: 3700\n",
      "train: loss: 876.9656982421875 acc: 0.7713592052459717  val: loss: 1387.030029296875 acc: 0.7896291613578796\n",
      "step: 3705\n",
      "train: loss: 508.80291748046875 acc: 0.8312310576438904  val: loss: 1562.936767578125 acc: 0.7520856857299805\n",
      "step: 3710\n",
      "train: loss: 773.8236694335938 acc: 0.797683835029602  val: loss: 1980.874755859375 acc: 0.771404504776001\n",
      "step: 3715\n",
      "train: loss: 707.0225830078125 acc: 0.8957587480545044  val: loss: 1373.5657958984375 acc: 0.7967597842216492\n",
      "step: 3720\n",
      "train: loss: 994.017578125 acc: 0.8693487048149109  val: loss: 1708.3480224609375 acc: 0.7678282260894775\n",
      "step: 3725\n",
      "train: loss: 1362.9544677734375 acc: 0.7980462312698364  val: loss: 596.62548828125 acc: 0.8892491459846497\n",
      "step: 3730\n",
      "train: loss: 1043.5794677734375 acc: 0.7350403070449829  val: loss: 1146.0609130859375 acc: 0.841394305229187\n",
      "step: 3735\n",
      "train: loss: 569.8197631835938 acc: 0.871721625328064  val: loss: 1188.61083984375 acc: 0.8398677110671997\n",
      "step: 3740\n",
      "train: loss: 1095.6602783203125 acc: 0.7826324701309204  val: loss: 1245.907958984375 acc: 0.791959285736084\n",
      "step: 3745\n",
      "train: loss: 777.603271484375 acc: 0.8715907335281372  val: loss: 1434.437255859375 acc: 0.7705110907554626\n",
      "step: 3750\n",
      "train: loss: 887.4131469726562 acc: 0.8223116397857666  val: loss: 577.6219482421875 acc: 0.9021177887916565\n",
      "step: 3755\n",
      "train: loss: 429.68572998046875 acc: 0.9314899444580078  val: loss: 796.7156372070312 acc: 0.8755298852920532\n",
      "step: 3760\n",
      "train: loss: 362.6697998046875 acc: 0.9296852946281433  val: loss: 582.8074951171875 acc: 0.909923255443573\n",
      "step: 3765\n",
      "train: loss: 820.6927490234375 acc: 0.8479499220848083  val: loss: 1378.744140625 acc: 0.8304096460342407\n",
      "step: 3770\n",
      "train: loss: 630.5156860351562 acc: 0.897283136844635  val: loss: 1821.944091796875 acc: 0.7642117142677307\n",
      "step: 3775\n",
      "train: loss: 379.59967041015625 acc: 0.9375839233398438  val: loss: 481.1099853515625 acc: 0.9265879988670349\n",
      "step: 3780\n",
      "train: loss: 868.8690795898438 acc: 0.826043426990509  val: loss: 941.48388671875 acc: 0.805058479309082\n",
      "step: 3785\n",
      "train: loss: 794.6673583984375 acc: 0.8964139223098755  val: loss: 926.8501586914062 acc: 0.8618855476379395\n",
      "step: 3790\n",
      "train: loss: 793.4586791992188 acc: 0.8875134587287903  val: loss: 960.9090576171875 acc: 0.8797746896743774\n",
      "step: 3795\n",
      "train: loss: 537.0425415039062 acc: 0.9090376496315002  val: loss: 1082.681396484375 acc: 0.8318175077438354\n",
      "step: 3800\n",
      "train: loss: 1080.8310546875 acc: 0.8294567465782166  val: loss: 2024.6370849609375 acc: 0.7503834962844849\n",
      "step: 3805\n",
      "train: loss: 947.947509765625 acc: 0.8649115562438965  val: loss: 1338.272216796875 acc: 0.8068646788597107\n",
      "step: 3810\n",
      "train: loss: 1039.6451416015625 acc: 0.8363894820213318  val: loss: 895.3170166015625 acc: 0.8627043962478638\n",
      "step: 3815\n",
      "train: loss: 878.6929931640625 acc: 0.8215910792350769  val: loss: 1154.9093017578125 acc: 0.8195335865020752\n",
      "step: 3820\n",
      "train: loss: 712.6134033203125 acc: 0.8612044453620911  val: loss: 1002.0172119140625 acc: 0.8665362000465393\n",
      "step: 3825\n",
      "train: loss: 393.0185546875 acc: 0.9061955213546753  val: loss: 1624.088134765625 acc: 0.8087775111198425\n",
      "step: 3830\n",
      "train: loss: 795.7650146484375 acc: 0.8711925745010376  val: loss: 1135.630615234375 acc: 0.8151400089263916\n",
      "step: 3835\n",
      "train: loss: 709.0823974609375 acc: 0.916347861289978  val: loss: 823.3004150390625 acc: 0.8548617362976074\n",
      "step: 3840\n",
      "train: loss: 355.431884765625 acc: 0.9466038942337036  val: loss: 1051.9539794921875 acc: 0.8464109897613525\n",
      "step: 3845\n",
      "train: loss: 385.24383544921875 acc: 0.9140275716781616  val: loss: 548.09814453125 acc: 0.8972939848899841\n",
      "step: 3850\n",
      "train: loss: 556.0234375 acc: 0.9323152899742126  val: loss: 1169.9700927734375 acc: 0.8616882562637329\n",
      "step: 3855\n",
      "train: loss: 776.5681762695312 acc: 0.836065948009491  val: loss: 508.1847229003906 acc: 0.8824833631515503\n",
      "step: 3860\n",
      "train: loss: 576.260986328125 acc: 0.8752812743186951  val: loss: 1035.303955078125 acc: 0.8410703539848328\n",
      "step: 3865\n",
      "train: loss: 762.412353515625 acc: 0.8931280374526978  val: loss: 548.5625 acc: 0.8994309902191162\n",
      "step: 3870\n",
      "train: loss: 1138.6021728515625 acc: 0.7463474273681641  val: loss: 1804.3780517578125 acc: 0.7643462419509888\n",
      "step: 3875\n",
      "train: loss: 756.1978759765625 acc: 0.8824216723442078  val: loss: 858.5584106445312 acc: 0.8341912031173706\n",
      "step: 3880\n",
      "train: loss: 1253.2767333984375 acc: 0.8502991199493408  val: loss: 727.8977661132812 acc: 0.911772608757019\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 3885\n",
      "train: loss: 390.63525390625 acc: 0.8605190515518188  val: loss: 575.6054077148438 acc: 0.8814271688461304\n",
      "step: 3890\n",
      "train: loss: 321.9048767089844 acc: 0.9410605430603027  val: loss: 916.9677734375 acc: 0.8961482644081116\n",
      "step: 3895\n",
      "train: loss: 766.1049194335938 acc: 0.8687875866889954  val: loss: 3327.542236328125 acc: 0.6337708234786987\n",
      "step: 3900\n",
      "train: loss: 598.931396484375 acc: 0.8938682675361633  val: loss: 557.7779541015625 acc: 0.8505634665489197\n",
      "step: 3905\n",
      "train: loss: 627.6641845703125 acc: 0.9260355830192566  val: loss: 1384.7783203125 acc: 0.8215110898017883\n",
      "step: 3910\n",
      "train: loss: 607.4465942382812 acc: 0.9097512364387512  val: loss: 1558.4793701171875 acc: 0.8366325497627258\n",
      "step: 3915\n",
      "train: loss: 860.8489379882812 acc: 0.8509308695793152  val: loss: 1417.8182373046875 acc: 0.7942970395088196\n",
      "step: 3920\n",
      "train: loss: 853.477783203125 acc: 0.8420838117599487  val: loss: 1225.5186767578125 acc: 0.8500057458877563\n",
      "step: 3925\n",
      "train: loss: 417.69671630859375 acc: 0.9251905679702759  val: loss: 848.141845703125 acc: 0.8994109630584717\n",
      "step: 3930\n",
      "train: loss: 391.57281494140625 acc: 0.9187392592430115  val: loss: 1372.2347412109375 acc: 0.8503801822662354\n",
      "step: 3935\n",
      "train: loss: 491.7690734863281 acc: 0.9111291170120239  val: loss: 1079.485107421875 acc: 0.84233558177948\n",
      "step: 3940\n",
      "train: loss: 534.042236328125 acc: 0.9046933650970459  val: loss: 368.53753662109375 acc: 0.9388185739517212\n",
      "step: 3945\n",
      "train: loss: 600.9683837890625 acc: 0.820595383644104  val: loss: 1063.8746337890625 acc: 0.8524634838104248\n",
      "step: 3950\n",
      "train: loss: 270.59039306640625 acc: 0.9546234607696533  val: loss: 627.22705078125 acc: 0.8820552825927734\n",
      "step: 3955\n",
      "train: loss: 537.7378540039062 acc: 0.9175812005996704  val: loss: 919.4345703125 acc: 0.8858368992805481\n",
      "step: 3960\n",
      "train: loss: 354.6036682128906 acc: 0.9330668449401855  val: loss: 1271.8936767578125 acc: 0.7375105619430542\n",
      "step: 3965\n",
      "train: loss: 715.8796997070312 acc: 0.8882299661636353  val: loss: 815.0670776367188 acc: 0.8538180589675903\n",
      "step: 3970\n",
      "train: loss: 1133.3558349609375 acc: 0.8487274050712585  val: loss: 588.5409545898438 acc: 0.8740251660346985\n",
      "step: 3975\n",
      "train: loss: 759.4031982421875 acc: 0.7538301348686218  val: loss: 1383.7581787109375 acc: 0.7438132166862488\n",
      "step: 3980\n",
      "train: loss: 1426.888671875 acc: 0.7926322221755981  val: loss: 1719.6060791015625 acc: 0.764887809753418\n",
      "step: 3985\n",
      "train: loss: 687.7749633789062 acc: 0.8701541423797607  val: loss: 683.9741821289062 acc: 0.8758595585823059\n",
      "step: 3990\n",
      "train: loss: 567.2994995117188 acc: 0.919907808303833  val: loss: 1187.8739013671875 acc: 0.8438337445259094\n",
      "step: 3995\n",
      "train: loss: 453.44384765625 acc: 0.8943372368812561  val: loss: 724.5579223632812 acc: 0.8947639465332031\n",
      "step: 4000\n",
      "train: loss: 489.6392517089844 acc: 0.8975108861923218  val: loss: 832.7068481445312 acc: 0.8626129031181335\n",
      "step: 4005\n",
      "train: loss: 389.6737976074219 acc: 0.9050785303115845  val: loss: 791.4376220703125 acc: 0.8750838041305542\n",
      "step: 4010\n",
      "train: loss: 603.87255859375 acc: 0.9059569835662842  val: loss: 393.09149169921875 acc: 0.9353042840957642\n",
      "step: 4015\n",
      "train: loss: 456.7979736328125 acc: 0.9392791390419006  val: loss: 1060.091796875 acc: 0.8253294229507446\n",
      "step: 4020\n",
      "train: loss: 960.7781372070312 acc: 0.8884133100509644  val: loss: 1073.8409423828125 acc: 0.8408843874931335\n",
      "step: 4025\n",
      "train: loss: 653.0947265625 acc: 0.8993105888366699  val: loss: 1035.5499267578125 acc: 0.73883056640625\n",
      "step: 4030\n",
      "train: loss: 773.380615234375 acc: 0.8565099239349365  val: loss: 1547.6925048828125 acc: 0.7971934676170349\n",
      "step: 4035\n",
      "train: loss: 604.6373291015625 acc: 0.8782835006713867  val: loss: 1194.2623291015625 acc: 0.7979130744934082\n",
      "step: 4040\n",
      "train: loss: 672.1613159179688 acc: 0.8506188988685608  val: loss: 1105.5078125 acc: 0.8291118144989014\n",
      "step: 4045\n",
      "train: loss: 360.0546875 acc: 0.9390529990196228  val: loss: 550.79150390625 acc: 0.8961466550827026\n",
      "step: 4050\n",
      "train: loss: 378.2984924316406 acc: 0.9439167976379395  val: loss: 1584.0877685546875 acc: 0.8214573264122009\n",
      "step: 4055\n",
      "train: loss: 538.0067138671875 acc: 0.823904275894165  val: loss: 1319.8792724609375 acc: 0.8013096451759338\n",
      "step: 4060\n",
      "train: loss: 831.7409057617188 acc: 0.7308074235916138  val: loss: 1788.7620849609375 acc: 0.796114444732666\n",
      "step: 4065\n",
      "train: loss: 619.8137817382812 acc: 0.8797970414161682  val: loss: 1580.118408203125 acc: 0.8132146596908569\n",
      "step: 4070\n",
      "train: loss: 700.9829711914062 acc: 0.8814815282821655  val: loss: 1316.1640625 acc: 0.8335549831390381\n",
      "step: 4075\n",
      "train: loss: 533.2859497070312 acc: 0.9267674088478088  val: loss: 1345.7568359375 acc: 0.7720257043838501\n",
      "step: 4080\n",
      "train: loss: 400.2432556152344 acc: 0.9206312894821167  val: loss: 1450.0550537109375 acc: 0.8013314604759216\n",
      "step: 4085\n",
      "train: loss: 1109.503662109375 acc: 0.775906503200531  val: loss: 810.1425170898438 acc: 0.8637807965278625\n",
      "step: 4090\n",
      "train: loss: 755.6763305664062 acc: 0.8842208981513977  val: loss: 3583.48974609375 acc: 0.6861494779586792\n",
      "step: 4095\n",
      "train: loss: 726.4296264648438 acc: 0.8928762078285217  val: loss: 857.1276245117188 acc: 0.8972679376602173\n",
      "step: 4100\n",
      "train: loss: 469.6479187011719 acc: 0.920149564743042  val: loss: 335.42242431640625 acc: 0.9316549301147461\n",
      "step: 4105\n",
      "train: loss: 775.4229736328125 acc: 0.9003527760505676  val: loss: 1668.6778564453125 acc: 0.7870765924453735\n",
      "step: 4110\n",
      "train: loss: 402.60479736328125 acc: 0.9165005087852478  val: loss: 805.563232421875 acc: 0.8403142690658569\n",
      "step: 4115\n",
      "train: loss: 553.982177734375 acc: 0.8845523595809937  val: loss: 1736.1590576171875 acc: 0.8119163513183594\n",
      "step: 4120\n",
      "train: loss: 351.0385437011719 acc: 0.8807103037834167  val: loss: 1337.6893310546875 acc: 0.8021286725997925\n",
      "step: 4125\n",
      "train: loss: 455.4307556152344 acc: 0.9208882451057434  val: loss: 1495.9671630859375 acc: 0.8127337694168091\n",
      "step: 4130\n",
      "train: loss: 494.1331787109375 acc: 0.8961111307144165  val: loss: 488.8741760253906 acc: 0.9075121879577637\n",
      "step: 4135\n",
      "train: loss: 379.595703125 acc: 0.9585986137390137  val: loss: 1693.8720703125 acc: 0.7963058352470398\n",
      "step: 4140\n",
      "train: loss: 731.9835815429688 acc: 0.9007683992385864  val: loss: 717.595947265625 acc: 0.881547749042511\n",
      "step: 4145\n",
      "train: loss: 921.0220947265625 acc: 0.8293464183807373  val: loss: 544.7613525390625 acc: 0.9308815598487854\n",
      "step: 4150\n",
      "train: loss: 707.0617065429688 acc: 0.8709867596626282  val: loss: 943.0275268554688 acc: 0.8479849100112915\n",
      "step: 4155\n",
      "train: loss: 713.2374267578125 acc: 0.8629530668258667  val: loss: 1455.4423828125 acc: 0.831357479095459\n",
      "step: 4160\n",
      "train: loss: 792.4048461914062 acc: 0.7997996807098389  val: loss: 1438.2000732421875 acc: 0.8434209227561951\n",
      "step: 4165\n",
      "train: loss: 806.090576171875 acc: 0.8650147914886475  val: loss: 1502.871826171875 acc: 0.7905932664871216\n",
      "step: 4170\n",
      "train: loss: 880.8934326171875 acc: 0.8580589294433594  val: loss: 1624.4056396484375 acc: 0.7664169669151306\n",
      "step: 4175\n",
      "train: loss: 423.88604736328125 acc: 0.9257470965385437  val: loss: 933.0177612304688 acc: 0.8845921754837036\n",
      "step: 4180\n",
      "train: loss: 363.3489074707031 acc: 0.9229158759117126  val: loss: 840.3313598632812 acc: 0.8219122886657715\n",
      "step: 4185\n",
      "train: loss: 488.99053955078125 acc: 0.916877269744873  val: loss: 346.6676940917969 acc: 0.9424722790718079\n",
      "step: 4190\n",
      "train: loss: 666.774169921875 acc: 0.9037508964538574  val: loss: 1912.009033203125 acc: 0.7956527471542358\n",
      "step: 4195\n",
      "train: loss: 376.0379333496094 acc: 0.9216952323913574  val: loss: 1007.9757690429688 acc: 0.8287231922149658\n",
      "step: 4200\n",
      "train: loss: 788.8745727539062 acc: 0.8922391533851624  val: loss: 527.36865234375 acc: 0.8974904417991638\n",
      "step: 4205\n",
      "train: loss: 743.079345703125 acc: 0.8903949856758118  val: loss: 414.5553283691406 acc: 0.9274430274963379\n",
      "step: 4210\n",
      "train: loss: 652.88671875 acc: 0.875147819519043  val: loss: 1598.3060302734375 acc: 0.805505633354187\n",
      "step: 4215\n",
      "train: loss: 1161.816650390625 acc: 0.8041885495185852  val: loss: 834.4331665039062 acc: 0.8515403866767883\n",
      "step: 4220\n",
      "train: loss: 1413.132568359375 acc: 0.7499290108680725  val: loss: 802.6530151367188 acc: 0.8407678604125977\n",
      "step: 4225\n",
      "train: loss: 705.8935546875 acc: 0.8388404250144958  val: loss: 583.4007568359375 acc: 0.8797112107276917\n",
      "step: 4230\n",
      "train: loss: 961.796142578125 acc: 0.7380408048629761  val: loss: 594.9873046875 acc: 0.8979940414428711\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 4235\n",
      "train: loss: 812.8231811523438 acc: 0.8678836822509766  val: loss: 809.9710083007812 acc: 0.8030115962028503\n",
      "step: 4240\n",
      "train: loss: 471.4595031738281 acc: 0.8588269948959351  val: loss: 1043.2496337890625 acc: 0.8643532991409302\n",
      "step: 4245\n",
      "train: loss: 513.9550170898438 acc: 0.9367331266403198  val: loss: 1156.952392578125 acc: 0.8184666633605957\n",
      "step: 4250\n",
      "train: loss: 588.550048828125 acc: 0.8977209329605103  val: loss: 1024.3050537109375 acc: 0.8593333959579468\n",
      "step: 4255\n",
      "train: loss: 714.3788452148438 acc: 0.8624825477600098  val: loss: 823.3517456054688 acc: 0.8548105359077454\n",
      "step: 4260\n",
      "train: loss: 1130.3309326171875 acc: 0.775458037853241  val: loss: 1722.2518310546875 acc: 0.831132173538208\n",
      "step: 4265\n",
      "train: loss: 660.6163330078125 acc: 0.879230797290802  val: loss: 424.1468505859375 acc: 0.9294856190681458\n",
      "step: 4270\n",
      "train: loss: 513.140625 acc: 0.8842940330505371  val: loss: 1044.879638671875 acc: 0.8534858226776123\n",
      "step: 4275\n",
      "train: loss: 699.836669921875 acc: 0.8890239000320435  val: loss: 2018.9383544921875 acc: 0.7524763345718384\n",
      "step: 4280\n",
      "train: loss: 1426.3343505859375 acc: 0.7853844165802002  val: loss: 1164.964111328125 acc: 0.8325117230415344\n",
      "step: 4285\n",
      "train: loss: 687.7904052734375 acc: 0.8380812406539917  val: loss: 1517.71435546875 acc: 0.8124299645423889\n",
      "step: 4290\n",
      "train: loss: 350.9356384277344 acc: 0.9348779916763306  val: loss: 944.7523803710938 acc: 0.852521538734436\n",
      "step: 4295\n",
      "train: loss: 459.34246826171875 acc: 0.8977161049842834  val: loss: 836.2747192382812 acc: 0.8849098086357117\n",
      "step: 4300\n",
      "train: loss: 458.9928283691406 acc: 0.9013198018074036  val: loss: 1472.4949951171875 acc: 0.8064095973968506\n",
      "step: 4305\n",
      "train: loss: 483.2833557128906 acc: 0.9192442297935486  val: loss: 826.5489501953125 acc: 0.86104816198349\n",
      "step: 4310\n",
      "train: loss: 452.98455810546875 acc: 0.9255151152610779  val: loss: 945.4050903320312 acc: 0.8311339020729065\n",
      "step: 4315\n",
      "train: loss: 367.0706787109375 acc: 0.9492079019546509  val: loss: 479.5147399902344 acc: 0.9194154739379883\n",
      "step: 4320\n",
      "train: loss: 837.9536743164062 acc: 0.8391913175582886  val: loss: 442.44482421875 acc: 0.9007389545440674\n",
      "step: 4325\n",
      "train: loss: 670.9818115234375 acc: 0.8341306447982788  val: loss: 1336.8834228515625 acc: 0.8120944499969482\n",
      "step: 4330\n",
      "train: loss: 968.887451171875 acc: 0.7841904163360596  val: loss: 1315.97998046875 acc: 0.8267406225204468\n",
      "step: 4335\n",
      "train: loss: 802.1367797851562 acc: 0.8656504154205322  val: loss: 867.6539916992188 acc: 0.8695904016494751\n",
      "step: 4340\n",
      "train: loss: 650.4220581054688 acc: 0.8715901374816895  val: loss: 690.117431640625 acc: 0.8860282301902771\n",
      "step: 4345\n",
      "train: loss: 588.599365234375 acc: 0.8858345746994019  val: loss: 1146.6671142578125 acc: 0.8066781163215637\n",
      "step: 4350\n",
      "train: loss: 527.8507690429688 acc: 0.8630697727203369  val: loss: 1270.529541015625 acc: 0.8157345652580261\n",
      "step: 4355\n",
      "train: loss: 383.86431884765625 acc: 0.9122222661972046  val: loss: 1174.87939453125 acc: 0.8541441559791565\n",
      "step: 4360\n",
      "train: loss: 381.22967529296875 acc: 0.9295161962509155  val: loss: 920.3160400390625 acc: 0.8634961843490601\n",
      "step: 4365\n",
      "train: loss: 414.7151794433594 acc: 0.9235101342201233  val: loss: 421.666015625 acc: 0.9382530450820923\n",
      "step: 4370\n",
      "train: loss: 434.3663330078125 acc: 0.9347569346427917  val: loss: 778.6806030273438 acc: 0.885307788848877\n",
      "step: 4375\n",
      "train: loss: 781.6651611328125 acc: 0.905728816986084  val: loss: 1093.952880859375 acc: 0.8513073921203613\n",
      "step: 4380\n",
      "train: loss: 883.9484252929688 acc: 0.8163933753967285  val: loss: 1856.5382080078125 acc: 0.7459973692893982\n",
      "step: 4385\n",
      "train: loss: 589.5830078125 acc: 0.871851921081543  val: loss: 445.00457763671875 acc: 0.9310271739959717\n",
      "step: 4390\n",
      "train: loss: 840.4039306640625 acc: 0.8119112849235535  val: loss: 632.971435546875 acc: 0.8968541622161865\n",
      "step: 4395\n",
      "train: loss: 499.9512939453125 acc: 0.9205765724182129  val: loss: 2476.91455078125 acc: 0.6236432790756226\n",
      "step: 4400\n",
      "train: loss: 739.5714721679688 acc: 0.8638004660606384  val: loss: 779.3687133789062 acc: 0.8779298663139343\n",
      "step: 4405\n",
      "train: loss: 604.4866943359375 acc: 0.8759236931800842  val: loss: 1252.269775390625 acc: 0.7856172919273376\n",
      "step: 4410\n",
      "train: loss: 460.0934143066406 acc: 0.8767098784446716  val: loss: 1195.8382568359375 acc: 0.8581578135490417\n",
      "step: 4415\n",
      "train: loss: 655.1207885742188 acc: 0.8861122131347656  val: loss: 1085.58544921875 acc: 0.8484257459640503\n",
      "step: 4420\n",
      "train: loss: 512.4961547851562 acc: 0.9010512828826904  val: loss: 1336.549072265625 acc: 0.8214703798294067\n",
      "step: 4425\n",
      "train: loss: 728.6286010742188 acc: 0.9145543575286865  val: loss: 900.1737060546875 acc: 0.8083404898643494\n",
      "step: 4430\n",
      "train: loss: 555.3969116210938 acc: 0.8784834742546082  val: loss: 1414.290771484375 acc: 0.8136237263679504\n",
      "step: 4435\n",
      "train: loss: 553.724365234375 acc: 0.9207819700241089  val: loss: 1385.056884765625 acc: 0.8042052984237671\n",
      "step: 4440\n",
      "train: loss: 724.003173828125 acc: 0.8230009078979492  val: loss: 1948.0037841796875 acc: 0.7442674040794373\n",
      "step: 4445\n",
      "train: loss: 649.9417114257812 acc: 0.8814039826393127  val: loss: 1585.793212890625 acc: 0.8089137077331543\n",
      "step: 4450\n",
      "train: loss: 787.1809692382812 acc: 0.8545322418212891  val: loss: 578.2608032226562 acc: 0.909015953540802\n",
      "step: 4455\n",
      "train: loss: 616.7676391601562 acc: 0.9259501695632935  val: loss: 733.654052734375 acc: 0.8797751665115356\n",
      "step: 4460\n",
      "train: loss: 675.9343872070312 acc: 0.8648449182510376  val: loss: 835.0377197265625 acc: 0.866053581237793\n",
      "step: 4465\n",
      "train: loss: 703.0415649414062 acc: 0.892315685749054  val: loss: 826.6433715820312 acc: 0.8772727251052856\n",
      "step: 4470\n",
      "train: loss: 432.55645751953125 acc: 0.900395929813385  val: loss: 941.8555297851562 acc: 0.8546936511993408\n",
      "step: 4475\n",
      "train: loss: 602.9420776367188 acc: 0.8911702632904053  val: loss: 862.4994506835938 acc: 0.8485547304153442\n",
      "step: 4480\n",
      "train: loss: 792.6776123046875 acc: 0.7812843918800354  val: loss: 630.7760620117188 acc: 0.9044440984725952\n",
      "step: 4485\n",
      "train: loss: 502.7525634765625 acc: 0.9119299650192261  val: loss: 842.3968505859375 acc: 0.8757704496383667\n",
      "step: 4490\n",
      "train: loss: 646.7290649414062 acc: 0.8793143033981323  val: loss: 605.3971557617188 acc: 0.8937349915504456\n",
      "step: 4495\n",
      "train: loss: 796.2494506835938 acc: 0.8353793621063232  val: loss: 1745.9127197265625 acc: 0.8044793605804443\n",
      "step: 4500\n",
      "train: loss: 503.9152526855469 acc: 0.9163660407066345  val: loss: 855.5402221679688 acc: 0.8905796408653259\n",
      "step: 4505\n",
      "train: loss: 656.2254028320312 acc: 0.8359826803207397  val: loss: 2077.49609375 acc: 0.7173165678977966\n",
      "step: 4510\n",
      "train: loss: 978.9248657226562 acc: 0.7767779231071472  val: loss: 932.718505859375 acc: 0.874957263469696\n",
      "step: 4515\n",
      "train: loss: 1209.3260498046875 acc: 0.8144629597663879  val: loss: 1651.318603515625 acc: 0.7408250570297241\n",
      "step: 4520\n",
      "train: loss: 695.149169921875 acc: 0.8714381456375122  val: loss: 1307.9361572265625 acc: 0.8440524339675903\n",
      "step: 4525\n",
      "train: loss: 1424.8502197265625 acc: 0.7475817799568176  val: loss: 1075.746337890625 acc: 0.8066973686218262\n",
      "step: 4530\n",
      "train: loss: 435.6023254394531 acc: 0.881759524345398  val: loss: 1266.585205078125 acc: 0.8311557769775391\n",
      "step: 4535\n",
      "train: loss: 410.9517517089844 acc: 0.9103209972381592  val: loss: 1236.9906005859375 acc: 0.8425796031951904\n",
      "step: 4540\n",
      "train: loss: 422.6858215332031 acc: 0.9064927101135254  val: loss: 411.2951354980469 acc: 0.9112513661384583\n",
      "step: 4545\n",
      "train: loss: 903.0731811523438 acc: 0.862738847732544  val: loss: 477.0162048339844 acc: 0.9227373003959656\n",
      "step: 4550\n",
      "train: loss: 657.1943359375 acc: 0.8931972980499268  val: loss: 1510.611572265625 acc: 0.7809339165687561\n",
      "step: 4555\n",
      "train: loss: 797.9209594726562 acc: 0.890300452709198  val: loss: 1113.1505126953125 acc: 0.8539451360702515\n",
      "step: 4560\n",
      "train: loss: 728.7361450195312 acc: 0.8364195823669434  val: loss: 999.279296875 acc: 0.8584396243095398\n",
      "step: 4565\n",
      "train: loss: 905.9256591796875 acc: 0.774640679359436  val: loss: 1482.2161865234375 acc: 0.7851417660713196\n",
      "step: 4570\n",
      "train: loss: 1137.4676513671875 acc: 0.7640663385391235  val: loss: 877.443603515625 acc: 0.851508617401123\n",
      "step: 4575\n",
      "train: loss: 625.9840087890625 acc: 0.8574357628822327  val: loss: 383.8834228515625 acc: 0.9287861585617065\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 4580\n",
      "train: loss: 349.5850830078125 acc: 0.938543975353241  val: loss: 1631.599365234375 acc: 0.8174469470977783\n",
      "step: 4585\n",
      "train: loss: 871.2008056640625 acc: 0.7868402600288391  val: loss: 1739.5870361328125 acc: 0.7885246872901917\n",
      "step: 4590\n",
      "train: loss: 549.5291137695312 acc: 0.8493467569351196  val: loss: 730.7448120117188 acc: 0.8402236700057983\n",
      "step: 4595\n",
      "train: loss: 143.77740478515625 acc: 0.9724754095077515  val: loss: 827.9756469726562 acc: 0.87250155210495\n",
      "step: 4600\n",
      "train: loss: 795.7208862304688 acc: 0.8154786825180054  val: loss: 1478.607666015625 acc: 0.7833086848258972\n",
      "step: 4605\n",
      "train: loss: 310.3819580078125 acc: 0.9642764329910278  val: loss: 1611.0242919921875 acc: 0.7844926118850708\n",
      "step: 4610\n",
      "train: loss: 490.0538024902344 acc: 0.9223976135253906  val: loss: 701.1419677734375 acc: 0.8772228956222534\n",
      "step: 4615\n",
      "train: loss: 773.6022338867188 acc: 0.8576617240905762  val: loss: 1128.0550537109375 acc: 0.8549226522445679\n",
      "step: 4620\n",
      "train: loss: 1792.566650390625 acc: 0.6916680932044983  val: loss: 669.1546020507812 acc: 0.8697003126144409\n",
      "step: 4625\n",
      "train: loss: 979.0723876953125 acc: 0.8274787068367004  val: loss: 862.4796142578125 acc: 0.8824683427810669\n",
      "step: 4630\n",
      "train: loss: 1203.157958984375 acc: 0.7731177806854248  val: loss: 1294.528564453125 acc: 0.8646949529647827\n",
      "step: 4635\n",
      "train: loss: 748.9191284179688 acc: 0.8933103084564209  val: loss: 864.3536376953125 acc: 0.8442004919052124\n",
      "step: 4640\n",
      "train: loss: 307.88037109375 acc: 0.9420091509819031  val: loss: 1199.3516845703125 acc: 0.8375636339187622\n",
      "step: 4645\n",
      "train: loss: 567.7868041992188 acc: 0.9141103625297546  val: loss: 1170.8642578125 acc: 0.8213894367218018\n",
      "step: 4650\n",
      "train: loss: 527.3314819335938 acc: 0.8495095372200012  val: loss: 1869.9176025390625 acc: 0.7366946339607239\n",
      "step: 4655\n",
      "train: loss: 321.53192138671875 acc: 0.9432200193405151  val: loss: 1039.95556640625 acc: 0.8003286719322205\n",
      "step: 4660\n",
      "train: loss: 545.0664672851562 acc: 0.9133560061454773  val: loss: 1973.4813232421875 acc: 0.7555793523788452\n",
      "step: 4665\n",
      "train: loss: 381.0251159667969 acc: 0.9554497599601746  val: loss: 1628.7001953125 acc: 0.7802057266235352\n",
      "step: 4670\n",
      "train: loss: 607.9611206054688 acc: 0.8905976414680481  val: loss: 611.4390258789062 acc: 0.8916484117507935\n",
      "step: 4675\n",
      "train: loss: 452.00079345703125 acc: 0.9120510220527649  val: loss: 627.2025756835938 acc: 0.8780880570411682\n",
      "step: 4680\n",
      "train: loss: 604.8955688476562 acc: 0.8537768125534058  val: loss: 1163.8687744140625 acc: 0.8537285327911377\n",
      "step: 4685\n",
      "train: loss: 901.2991333007812 acc: 0.8652877807617188  val: loss: 975.4949951171875 acc: 0.863783597946167\n",
      "step: 4690\n",
      "train: loss: 618.6676635742188 acc: 0.889843225479126  val: loss: 1552.611572265625 acc: 0.8077296018600464\n",
      "step: 4695\n",
      "train: loss: 580.9627075195312 acc: 0.922362208366394  val: loss: 1406.1475830078125 acc: 0.820757269859314\n",
      "step: 4700\n",
      "train: loss: 456.3962097167969 acc: 0.9175549745559692  val: loss: 1394.2542724609375 acc: 0.7719590067863464\n",
      "step: 4705\n",
      "train: loss: 514.1744384765625 acc: 0.8957601189613342  val: loss: 1858.3720703125 acc: 0.766794741153717\n",
      "step: 4710\n",
      "train: loss: 656.829345703125 acc: 0.8343409299850464  val: loss: 1057.94921875 acc: 0.8302161693572998\n",
      "step: 4715\n",
      "train: loss: 379.622802734375 acc: 0.8959145545959473  val: loss: 964.47900390625 acc: 0.8317713737487793\n",
      "step: 4720\n",
      "train: loss: 512.377685546875 acc: 0.9053047299385071  val: loss: 694.5394897460938 acc: 0.879721462726593\n",
      "step: 4725\n",
      "train: loss: 604.5730590820312 acc: 0.925344705581665  val: loss: 1166.92919921875 acc: 0.8589715957641602\n",
      "step: 4730\n",
      "train: loss: 463.5429992675781 acc: 0.9314377307891846  val: loss: 494.0857238769531 acc: 0.9090703129768372\n",
      "step: 4735\n",
      "train: loss: 646.4496459960938 acc: 0.891139030456543  val: loss: 719.9232177734375 acc: 0.8766465187072754\n",
      "step: 4740\n",
      "train: loss: 404.4163513183594 acc: 0.9412739872932434  val: loss: 1155.90234375 acc: 0.7951310873031616\n",
      "step: 4745\n",
      "train: loss: 753.7404174804688 acc: 0.8174521923065186  val: loss: 1220.905029296875 acc: 0.8195638656616211\n",
      "step: 4750\n",
      "train: loss: 668.8776245117188 acc: 0.9057905673980713  val: loss: 454.1581115722656 acc: 0.9306440353393555\n",
      "step: 4755\n",
      "train: loss: 552.026123046875 acc: 0.8931469917297363  val: loss: 1494.255615234375 acc: 0.7821502685546875\n",
      "step: 4760\n",
      "train: loss: 882.4142456054688 acc: 0.832034707069397  val: loss: 950.424072265625 acc: 0.8640394806861877\n",
      "step: 4765\n",
      "train: loss: 373.6529541015625 acc: 0.922269344329834  val: loss: 1975.95263671875 acc: 0.7527676820755005\n",
      "step: 4770\n",
      "train: loss: 607.9967651367188 acc: 0.8357647657394409  val: loss: 2073.386474609375 acc: 0.7254481315612793\n",
      "step: 4775\n",
      "train: loss: 489.962158203125 acc: 0.8951343297958374  val: loss: 1527.416259765625 acc: 0.7844370603561401\n",
      "step: 4780\n",
      "train: loss: 530.0341796875 acc: 0.8936880826950073  val: loss: 543.6275024414062 acc: 0.8842164874076843\n",
      "step: 4785\n",
      "train: loss: 1050.525634765625 acc: 0.8718089461326599  val: loss: 544.5205078125 acc: 0.8988092541694641\n",
      "step: 4790\n",
      "train: loss: 422.42333984375 acc: 0.9288474321365356  val: loss: 1420.7203369140625 acc: 0.8228648900985718\n",
      "step: 4795\n",
      "train: loss: 1083.072021484375 acc: 0.8499820232391357  val: loss: 824.8458862304688 acc: 0.908244788646698\n",
      "step: 4800\n",
      "train: loss: 660.2647094726562 acc: 0.8340749740600586  val: loss: 1102.08544921875 acc: 0.8248838186264038\n",
      "step: 4805\n",
      "train: loss: 469.1135559082031 acc: 0.9304255843162537  val: loss: 407.9495544433594 acc: 0.9273055791854858\n",
      "step: 4810\n",
      "train: loss: 792.0889892578125 acc: 0.8816957473754883  val: loss: 771.595947265625 acc: 0.8743444681167603\n",
      "step: 4815\n",
      "train: loss: 982.7960815429688 acc: 0.8331431150436401  val: loss: 1627.2906494140625 acc: 0.7762696146965027\n",
      "step: 4820\n",
      "train: loss: 611.7132568359375 acc: 0.8869178295135498  val: loss: 1025.138427734375 acc: 0.8016356229782104\n",
      "step: 4825\n",
      "train: loss: 579.3876342773438 acc: 0.9165008068084717  val: loss: 708.5245361328125 acc: 0.8729996681213379\n",
      "step: 4830\n",
      "train: loss: 993.3115234375 acc: 0.8074065446853638  val: loss: 1602.7625732421875 acc: 0.8098336458206177\n",
      "step: 4835\n",
      "train: loss: 655.0573120117188 acc: 0.8769155144691467  val: loss: 1098.9425048828125 acc: 0.8495029807090759\n",
      "step: 4840\n",
      "train: loss: 457.9718017578125 acc: 0.9213031530380249  val: loss: 2152.4423828125 acc: 0.7541914582252502\n",
      "step: 4845\n",
      "train: loss: 356.36663818359375 acc: 0.9418758153915405  val: loss: 1490.9246826171875 acc: 0.8299576044082642\n",
      "step: 4850\n",
      "train: loss: 1239.3267822265625 acc: 0.8249637484550476  val: loss: 2209.879638671875 acc: 0.7953202724456787\n",
      "step: 4855\n",
      "train: loss: 1069.6436767578125 acc: 0.8496096134185791  val: loss: 566.1676025390625 acc: 0.918396532535553\n",
      "step: 4860\n",
      "train: loss: 1043.0753173828125 acc: 0.8374961614608765  val: loss: 1173.0540771484375 acc: 0.8833762407302856\n",
      "step: 4865\n",
      "train: loss: 500.24639892578125 acc: 0.8950861096382141  val: loss: 1188.51025390625 acc: 0.8492385149002075\n",
      "step: 4870\n",
      "train: loss: 505.3743896484375 acc: 0.9201495051383972  val: loss: 987.4985961914062 acc: 0.8409455418586731\n",
      "step: 4875\n",
      "train: loss: 1094.5728759765625 acc: 0.7718255519866943  val: loss: 1093.2940673828125 acc: 0.8081110119819641\n",
      "step: 4880\n",
      "train: loss: 531.5662841796875 acc: 0.8695127964019775  val: loss: 1805.0426025390625 acc: 0.7565640211105347\n",
      "step: 4885\n",
      "train: loss: 833.0889282226562 acc: 0.7278810143470764  val: loss: 760.742431640625 acc: 0.884353518486023\n",
      "step: 4890\n",
      "train: loss: 436.9886474609375 acc: 0.8774462342262268  val: loss: 1211.5928955078125 acc: 0.7921034097671509\n",
      "step: 4895\n",
      "train: loss: 601.9241943359375 acc: 0.8258914947509766  val: loss: 713.4031982421875 acc: 0.8962507247924805\n",
      "step: 4900\n",
      "train: loss: 378.94232177734375 acc: 0.9504810571670532  val: loss: 1635.5601806640625 acc: 0.7731709480285645\n",
      "step: 4905\n",
      "train: loss: 360.6734313964844 acc: 0.923955500125885  val: loss: 1077.2071533203125 acc: 0.8215574026107788\n",
      "step: 4910\n",
      "train: loss: 585.136474609375 acc: 0.907656729221344  val: loss: 1128.48095703125 acc: 0.8583767414093018\n",
      "step: 4915\n",
      "train: loss: 716.2319946289062 acc: 0.8714275360107422  val: loss: 1185.57421875 acc: 0.8638780117034912\n",
      "step: 4920\n",
      "train: loss: 779.5028686523438 acc: 0.8832513093948364  val: loss: 1351.832763671875 acc: 0.810936450958252\n",
      "step: 4925\n",
      "train: loss: 589.6298217773438 acc: 0.8862454295158386  val: loss: 1450.6376953125 acc: 0.7749377489089966\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 4930\n",
      "train: loss: 1087.379150390625 acc: 0.8298168778419495  val: loss: 821.48583984375 acc: 0.8870208263397217\n",
      "step: 4935\n",
      "train: loss: 485.4109191894531 acc: 0.897458016872406  val: loss: 1233.4129638671875 acc: 0.8605823516845703\n",
      "step: 4940\n",
      "train: loss: 445.28350830078125 acc: 0.8673126697540283  val: loss: 1695.4154052734375 acc: 0.7830032706260681\n",
      "step: 4945\n",
      "train: loss: 884.3041381835938 acc: 0.7639042735099792  val: loss: 1430.3013916015625 acc: 0.7655318379402161\n",
      "step: 4950\n",
      "train: loss: 462.5321044921875 acc: 0.8997529745101929  val: loss: 1234.823486328125 acc: 0.8363966941833496\n",
      "step: 4955\n",
      "train: loss: 209.44667053222656 acc: 0.9683245420455933  val: loss: 789.5135498046875 acc: 0.8729864954948425\n",
      "step: 4960\n",
      "train: loss: 523.5542602539062 acc: 0.928714394569397  val: loss: 1704.6365966796875 acc: 0.7874506115913391\n",
      "step: 4965\n",
      "train: loss: 458.6961975097656 acc: 0.9209908246994019  val: loss: 1044.814208984375 acc: 0.850487232208252\n",
      "step: 4970\n",
      "train: loss: 946.6406860351562 acc: 0.8222359418869019  val: loss: 1587.332275390625 acc: 0.8031014204025269\n",
      "step: 4975\n",
      "train: loss: 698.9390258789062 acc: 0.8407580852508545  val: loss: 649.479248046875 acc: 0.8923651576042175\n",
      "step: 4980\n",
      "train: loss: 755.7557373046875 acc: 0.8468451499938965  val: loss: 545.4880981445312 acc: 0.9214226603507996\n",
      "step: 4985\n",
      "train: loss: 851.89404296875 acc: 0.7564859390258789  val: loss: 1674.58251953125 acc: 0.7753962874412537\n",
      "step: 4990\n",
      "train: loss: 1140.3939208984375 acc: 0.8248214721679688  val: loss: 1160.2257080078125 acc: 0.8587630987167358\n",
      "step: 4995\n",
      "train: loss: 816.6962890625 acc: 0.809790849685669  val: loss: 1212.3544921875 acc: 0.7931974530220032\n",
      "step: 5000\n",
      "train: loss: 378.7277526855469 acc: 0.9107156991958618  val: loss: 979.8078002929688 acc: 0.8811618685722351\n",
      "step: 5005\n",
      "train: loss: 480.3953552246094 acc: 0.8488003015518188  val: loss: 1095.03076171875 acc: 0.8404604196548462\n",
      "step: 5010\n",
      "train: loss: 290.67767333984375 acc: 0.919563353061676  val: loss: 623.6047973632812 acc: 0.8913794159889221\n",
      "step: 5015\n",
      "train: loss: 696.3506469726562 acc: 0.8818643689155579  val: loss: 1620.31103515625 acc: 0.7291157245635986\n",
      "step: 5020\n",
      "train: loss: 390.2570495605469 acc: 0.9356522560119629  val: loss: 542.6074829101562 acc: 0.9089730978012085\n",
      "step: 5025\n",
      "train: loss: 451.65948486328125 acc: 0.9300878643989563  val: loss: 697.0624389648438 acc: 0.8990679383277893\n",
      "step: 5030\n",
      "train: loss: 1073.37646484375 acc: 0.807619571685791  val: loss: 1336.684814453125 acc: 0.8331357836723328\n",
      "step: 5035\n",
      "train: loss: 873.7572631835938 acc: 0.8839466571807861  val: loss: 520.15869140625 acc: 0.9047696590423584\n",
      "step: 5040\n",
      "train: loss: 928.5438232421875 acc: 0.7631700038909912  val: loss: 967.447509765625 acc: 0.8605140447616577\n",
      "step: 5045\n",
      "train: loss: 863.2603149414062 acc: 0.8574723601341248  val: loss: 2495.42578125 acc: 0.7080847024917603\n",
      "step: 5050\n",
      "train: loss: 621.4395141601562 acc: 0.9267611503601074  val: loss: 1323.4169921875 acc: 0.8029452562332153\n",
      "step: 5055\n",
      "train: loss: 737.565185546875 acc: 0.8390382528305054  val: loss: 288.5921936035156 acc: 0.9514346122741699\n",
      "step: 5060\n",
      "train: loss: 318.33355712890625 acc: 0.9363935589790344  val: loss: 546.0679321289062 acc: 0.8983527421951294\n",
      "step: 5065\n",
      "train: loss: 546.6715087890625 acc: 0.9054428935050964  val: loss: 1239.7772216796875 acc: 0.8127709031105042\n",
      "step: 5070\n",
      "train: loss: 656.178955078125 acc: 0.8456447720527649  val: loss: 1885.45751953125 acc: 0.7676275968551636\n",
      "step: 5075\n",
      "train: loss: 464.3537292480469 acc: 0.9226983785629272  val: loss: 1711.7374267578125 acc: 0.798992931842804\n",
      "step: 5080\n",
      "train: loss: 476.3821105957031 acc: 0.9390578269958496  val: loss: 2718.92138671875 acc: 0.7078152894973755\n",
      "step: 5085\n",
      "train: loss: 453.6178894042969 acc: 0.9452899098396301  val: loss: 1522.3057861328125 acc: 0.7844097018241882\n",
      "step: 5090\n",
      "train: loss: 482.10748291015625 acc: 0.9236254096031189  val: loss: 715.6367797851562 acc: 0.8922019600868225\n",
      "step: 5095\n",
      "train: loss: 644.53076171875 acc: 0.868775486946106  val: loss: 1135.007080078125 acc: 0.830511748790741\n",
      "step: 5100\n",
      "train: loss: 902.3179321289062 acc: 0.8034204244613647  val: loss: 1791.15380859375 acc: 0.8184784650802612\n",
      "step: 5105\n",
      "train: loss: 1009.1040649414062 acc: 0.7713289260864258  val: loss: 1780.1968994140625 acc: 0.8033105134963989\n",
      "step: 5110\n",
      "train: loss: 894.5176391601562 acc: 0.8232476115226746  val: loss: 527.0882568359375 acc: 0.8964883685112\n",
      "step: 5115\n",
      "train: loss: 545.3209228515625 acc: 0.8660905361175537  val: loss: 1019.2276000976562 acc: 0.8774108290672302\n",
      "step: 5120\n",
      "train: loss: 704.6890258789062 acc: 0.8491096496582031  val: loss: 376.3436584472656 acc: 0.9239524602890015\n",
      "step: 5125\n",
      "train: loss: 563.57470703125 acc: 0.9094193577766418  val: loss: 1976.192626953125 acc: 0.8038554787635803\n",
      "step: 5130\n",
      "train: loss: 813.3887329101562 acc: 0.8816547393798828  val: loss: 1220.278564453125 acc: 0.8670908212661743\n",
      "step: 5135\n",
      "train: loss: 768.0167236328125 acc: 0.8616592884063721  val: loss: 865.7468872070312 acc: 0.8677327632904053\n",
      "step: 5140\n",
      "train: loss: 463.1260986328125 acc: 0.9368990063667297  val: loss: 1367.7021484375 acc: 0.6965125203132629\n",
      "step: 5145\n",
      "train: loss: 1139.2933349609375 acc: 0.8068605661392212  val: loss: 1033.28955078125 acc: 0.8521398305892944\n",
      "step: 5150\n",
      "train: loss: 1097.6361083984375 acc: 0.7694069147109985  val: loss: 916.1746826171875 acc: 0.8319922685623169\n",
      "step: 5155\n",
      "train: loss: 1160.400146484375 acc: 0.8282599449157715  val: loss: 598.6295166015625 acc: 0.9137793779373169\n",
      "step: 5160\n",
      "train: loss: 637.029052734375 acc: 0.8677992820739746  val: loss: 1285.9796142578125 acc: 0.8222379088401794\n",
      "step: 5165\n",
      "train: loss: 658.0260620117188 acc: 0.9147778749465942  val: loss: 908.2393798828125 acc: 0.8996710181236267\n",
      "step: 5170\n",
      "train: loss: 477.6156005859375 acc: 0.8995455503463745  val: loss: 954.9275512695312 acc: 0.8578908443450928\n",
      "step: 5175\n",
      "train: loss: 439.8509826660156 acc: 0.8927164673805237  val: loss: 1057.39697265625 acc: 0.8387922644615173\n",
      "step: 5180\n",
      "train: loss: 855.7255859375 acc: 0.8674890995025635  val: loss: 1199.27685546875 acc: 0.8053073287010193\n",
      "step: 5185\n",
      "train: loss: 760.7877807617188 acc: 0.8389820456504822  val: loss: 779.9395141601562 acc: 0.8596254587173462\n",
      "step: 5190\n",
      "train: loss: 483.421142578125 acc: 0.9168688058853149  val: loss: 1684.3641357421875 acc: 0.7085868120193481\n",
      "step: 5195\n",
      "train: loss: 619.0471801757812 acc: 0.8794288039207458  val: loss: 1260.740234375 acc: 0.8270915746688843\n",
      "step: 5200\n",
      "train: loss: 292.8307800292969 acc: 0.944227397441864  val: loss: 971.8995361328125 acc: 0.8283864855766296\n",
      "step: 5205\n",
      "train: loss: 1067.9720458984375 acc: 0.779140830039978  val: loss: 842.8421630859375 acc: 0.8485829830169678\n",
      "step: 5210\n",
      "train: loss: 1000.4551391601562 acc: 0.8018741011619568  val: loss: 480.2404479980469 acc: 0.9082214832305908\n",
      "step: 5215\n",
      "train: loss: 280.8741149902344 acc: 0.9458842873573303  val: loss: 1264.53515625 acc: 0.819841742515564\n",
      "step: 5220\n",
      "train: loss: 1175.7921142578125 acc: 0.7987432479858398  val: loss: 404.811767578125 acc: 0.9183996319770813\n",
      "step: 5225\n",
      "train: loss: 377.9020690917969 acc: 0.9335929751396179  val: loss: 1535.2005615234375 acc: 0.7959270477294922\n",
      "step: 5230\n",
      "train: loss: 857.0728759765625 acc: 0.8266451358795166  val: loss: 1338.7950439453125 acc: 0.8056687712669373\n",
      "step: 5235\n",
      "train: loss: 1174.9794921875 acc: 0.7859567403793335  val: loss: 1930.5018310546875 acc: 0.7623974084854126\n",
      "step: 5240\n",
      "train: loss: 309.7175598144531 acc: 0.9087916016578674  val: loss: 2106.380859375 acc: 0.7755718231201172\n",
      "step: 5245\n",
      "train: loss: 528.83837890625 acc: 0.8866602182388306  val: loss: 812.432373046875 acc: 0.8517383933067322\n",
      "step: 5250\n",
      "train: loss: 571.81591796875 acc: 0.8842406272888184  val: loss: 826.2775268554688 acc: 0.8470564484596252\n",
      "step: 5255\n",
      "train: loss: 579.76513671875 acc: 0.930600106716156  val: loss: 1152.154052734375 acc: 0.8446288108825684\n",
      "step: 5260\n",
      "train: loss: 400.402587890625 acc: 0.8884618878364563  val: loss: 634.6819458007812 acc: 0.9012228846549988\n",
      "step: 5265\n",
      "train: loss: 500.9556884765625 acc: 0.8850840926170349  val: loss: 666.59423828125 acc: 0.8998558521270752\n",
      "step: 5270\n",
      "train: loss: 686.2994384765625 acc: 0.8758305907249451  val: loss: 1312.9630126953125 acc: 0.8403939604759216\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 5275\n",
      "train: loss: 410.2351989746094 acc: 0.9093482494354248  val: loss: 1212.3468017578125 acc: 0.8210665583610535\n",
      "step: 5280\n",
      "train: loss: 1051.4971923828125 acc: 0.8450115919113159  val: loss: 1501.5838623046875 acc: 0.7886955142021179\n",
      "step: 5285\n",
      "train: loss: 717.9539794921875 acc: 0.8634490370750427  val: loss: 919.4530639648438 acc: 0.8596529364585876\n",
      "step: 5290\n",
      "train: loss: 556.0811767578125 acc: 0.9009903073310852  val: loss: 964.8621826171875 acc: 0.8466749787330627\n",
      "step: 5295\n",
      "train: loss: 980.7013549804688 acc: 0.7564653754234314  val: loss: 1032.11328125 acc: 0.867036759853363\n",
      "step: 5300\n",
      "train: loss: 529.0242919921875 acc: 0.8951306343078613  val: loss: 1986.6317138671875 acc: 0.7351945638656616\n",
      "step: 5305\n",
      "train: loss: 909.6865844726562 acc: 0.8592928051948547  val: loss: 704.5843505859375 acc: 0.8393372893333435\n",
      "step: 5310\n",
      "train: loss: 662.1481323242188 acc: 0.878704845905304  val: loss: 921.4871215820312 acc: 0.8535945415496826\n",
      "step: 5315\n",
      "train: loss: 1003.9097290039062 acc: 0.8520547151565552  val: loss: 493.0734558105469 acc: 0.9162957072257996\n",
      "step: 5320\n",
      "train: loss: 443.99041748046875 acc: 0.9372835755348206  val: loss: 530.6651000976562 acc: 0.9265732765197754\n",
      "step: 5325\n",
      "train: loss: 483.1836242675781 acc: 0.9247099757194519  val: loss: 1385.5692138671875 acc: 0.8139591217041016\n",
      "step: 5330\n",
      "train: loss: 687.5578002929688 acc: 0.8780661225318909  val: loss: 522.0177001953125 acc: 0.9048219919204712\n",
      "step: 5335\n",
      "train: loss: 1308.8359375 acc: 0.650705099105835  val: loss: 628.1466674804688 acc: 0.9127066135406494\n",
      "step: 5340\n",
      "train: loss: 907.8758544921875 acc: 0.8202848434448242  val: loss: 1385.0223388671875 acc: 0.8468273878097534\n",
      "step: 5345\n",
      "train: loss: 806.218017578125 acc: 0.8904299139976501  val: loss: 204.80648803710938 acc: 0.9674832820892334\n",
      "step: 5350\n",
      "train: loss: 665.5405883789062 acc: 0.902153730392456  val: loss: 447.7670593261719 acc: 0.9246789216995239\n",
      "step: 5355\n",
      "train: loss: 417.03753662109375 acc: 0.9189221858978271  val: loss: 1471.1849365234375 acc: 0.8114704489707947\n",
      "step: 5360\n",
      "train: loss: 738.0823974609375 acc: 0.8385477662086487  val: loss: 789.140869140625 acc: 0.8665581941604614\n",
      "step: 5365\n",
      "train: loss: 678.2865600585938 acc: 0.8364555835723877  val: loss: 579.4263916015625 acc: 0.8890774250030518\n",
      "step: 5370\n",
      "train: loss: 911.97705078125 acc: 0.8695660829544067  val: loss: 931.4141845703125 acc: 0.8300284147262573\n",
      "step: 5375\n",
      "train: loss: 450.58795166015625 acc: 0.9356608986854553  val: loss: 957.8984375 acc: 0.8465301990509033\n",
      "step: 5380\n",
      "train: loss: 607.9717407226562 acc: 0.9287707209587097  val: loss: 557.441162109375 acc: 0.9383655786514282\n",
      "step: 5385\n",
      "train: loss: 526.8016357421875 acc: 0.8978409171104431  val: loss: 1270.49951171875 acc: 0.8176002502441406\n",
      "step: 5390\n",
      "train: loss: 862.5547485351562 acc: 0.819267749786377  val: loss: 1305.49365234375 acc: 0.8437057733535767\n",
      "step: 5395\n",
      "train: loss: 520.2486572265625 acc: 0.9149767160415649  val: loss: 1687.580322265625 acc: 0.7813900709152222\n",
      "step: 5400\n",
      "train: loss: 760.46435546875 acc: 0.8363926410675049  val: loss: 1447.4442138671875 acc: 0.7863198518753052\n",
      "step: 5405\n",
      "train: loss: 816.6034545898438 acc: 0.9153591990470886  val: loss: 593.0907592773438 acc: 0.8946395516395569\n",
      "step: 5410\n",
      "train: loss: 615.26806640625 acc: 0.8885869383811951  val: loss: 1129.74853515625 acc: 0.8662975430488586\n",
      "step: 5415\n",
      "train: loss: 410.42401123046875 acc: 0.9167831540107727  val: loss: 599.5841674804688 acc: 0.8831958174705505\n",
      "step: 5420\n",
      "train: loss: 313.0599060058594 acc: 0.9230707883834839  val: loss: 1211.808837890625 acc: 0.8524242639541626\n",
      "step: 5425\n",
      "train: loss: 944.9338989257812 acc: 0.8199445605278015  val: loss: 1180.914306640625 acc: 0.8072280883789062\n",
      "step: 5430\n",
      "train: loss: 280.5967712402344 acc: 0.9482452273368835  val: loss: 2412.248046875 acc: 0.75813889503479\n",
      "step: 5435\n",
      "train: loss: 632.7044067382812 acc: 0.8946384191513062  val: loss: 578.4122924804688 acc: 0.9135109782218933\n",
      "step: 5440\n",
      "train: loss: 1206.633056640625 acc: 0.8331643342971802  val: loss: 1507.47900390625 acc: 0.8440883159637451\n",
      "step: 5445\n",
      "train: loss: 985.5308227539062 acc: 0.8297677636146545  val: loss: 1218.4967041015625 acc: 0.8241385221481323\n",
      "step: 5450\n",
      "train: loss: 1066.8153076171875 acc: 0.8046503067016602  val: loss: 1557.2955322265625 acc: 0.7934021353721619\n",
      "step: 5455\n",
      "train: loss: 869.5701293945312 acc: 0.8462461829185486  val: loss: 713.5759887695312 acc: 0.885814368724823\n",
      "step: 5460\n",
      "train: loss: 1228.431884765625 acc: 0.7886573672294617  val: loss: 842.4546508789062 acc: 0.8754907250404358\n",
      "step: 5465\n",
      "train: loss: 704.230224609375 acc: 0.8421464562416077  val: loss: 464.62396240234375 acc: 0.8725584149360657\n",
      "step: 5470\n",
      "train: loss: 812.0037841796875 acc: 0.8619229197502136  val: loss: 224.42913818359375 acc: 0.9501720070838928\n",
      "step: 5475\n",
      "train: loss: 770.1578369140625 acc: 0.8784666061401367  val: loss: 681.6904296875 acc: 0.866033136844635\n",
      "step: 5480\n",
      "train: loss: 666.400390625 acc: 0.8442785739898682  val: loss: 1074.2025146484375 acc: 0.8613775372505188\n",
      "step: 5485\n",
      "train: loss: 502.595703125 acc: 0.8871843814849854  val: loss: 882.6232299804688 acc: 0.860718846321106\n",
      "step: 5490\n",
      "train: loss: 567.6325073242188 acc: 0.906457245349884  val: loss: 385.7560729980469 acc: 0.936257004737854\n",
      "step: 5495\n",
      "train: loss: 603.5441284179688 acc: 0.8220186233520508  val: loss: 503.94793701171875 acc: 0.9145426750183105\n",
      "step: 5500\n",
      "train: loss: 787.0120849609375 acc: 0.8759292960166931  val: loss: 946.0859375 acc: 0.8392821550369263\n",
      "step: 5505\n",
      "train: loss: 1121.4732666015625 acc: 0.785454511642456  val: loss: 840.4183349609375 acc: 0.8670584559440613\n",
      "step: 5510\n",
      "train: loss: 1669.9591064453125 acc: 0.7426716685295105  val: loss: 880.26708984375 acc: 0.8780624270439148\n",
      "step: 5515\n",
      "train: loss: 662.7648315429688 acc: 0.9023734331130981  val: loss: 1163.2325439453125 acc: 0.8142718076705933\n",
      "step: 5520\n",
      "train: loss: 1336.384765625 acc: 0.8216339349746704  val: loss: 1096.8426513671875 acc: 0.8497110605239868\n",
      "step: 5525\n",
      "train: loss: 520.2303466796875 acc: 0.9101532101631165  val: loss: 747.455322265625 acc: 0.8761565685272217\n",
      "step: 5530\n",
      "train: loss: 493.4879150390625 acc: 0.921301007270813  val: loss: 824.4462890625 acc: 0.8837199211120605\n",
      "step: 5535\n",
      "train: loss: 725.4827880859375 acc: 0.868828296661377  val: loss: 1093.4063720703125 acc: 0.8576604723930359\n",
      "step: 5540\n",
      "train: loss: 582.1500244140625 acc: 0.8641273975372314  val: loss: 1419.586181640625 acc: 0.8192753791809082\n",
      "step: 5545\n",
      "train: loss: 442.51800537109375 acc: 0.9400638341903687  val: loss: 2257.920654296875 acc: 0.7103497385978699\n",
      "step: 5550\n",
      "train: loss: 818.9981689453125 acc: 0.8891388177871704  val: loss: 1038.5660400390625 acc: 0.831855833530426\n",
      "step: 5555\n",
      "train: loss: 654.4172973632812 acc: 0.8656248450279236  val: loss: 2184.47216796875 acc: 0.7609267234802246\n",
      "step: 5560\n",
      "train: loss: 1049.0885009765625 acc: 0.7936009168624878  val: loss: 288.30316162109375 acc: 0.9449187517166138\n",
      "step: 5565\n",
      "train: loss: 747.6107177734375 acc: 0.8696713447570801  val: loss: 1006.01171875 acc: 0.8257097005844116\n",
      "step: 5570\n",
      "train: loss: 937.5719604492188 acc: 0.8348606824874878  val: loss: 839.9666137695312 acc: 0.8706355094909668\n",
      "step: 5575\n",
      "train: loss: 757.5906982421875 acc: 0.8963161110877991  val: loss: 682.1843872070312 acc: 0.8726357221603394\n",
      "step: 5580\n",
      "train: loss: 607.0270385742188 acc: 0.9066454768180847  val: loss: 557.3496704101562 acc: 0.8962684869766235\n",
      "step: 5585\n",
      "train: loss: 948.6542358398438 acc: 0.813915491104126  val: loss: 375.4080810546875 acc: 0.9274535179138184\n",
      "step: 5590\n",
      "train: loss: 701.2532348632812 acc: 0.888685941696167  val: loss: 1071.328857421875 acc: 0.8011049032211304\n",
      "step: 5595\n",
      "train: loss: 488.1707763671875 acc: 0.8989455699920654  val: loss: 1198.4373779296875 acc: 0.8443629145622253\n",
      "step: 5600\n",
      "train: loss: 655.2457885742188 acc: 0.8897022008895874  val: loss: 2578.480712890625 acc: 0.749942421913147\n",
      "step: 5605\n",
      "train: loss: 462.97088623046875 acc: 0.9115829467773438  val: loss: 1638.53759765625 acc: 0.7992793321609497\n",
      "step: 5610\n",
      "train: loss: 603.7019653320312 acc: 0.9216768741607666  val: loss: 2157.635009765625 acc: 0.7124020457267761\n",
      "step: 5615\n",
      "train: loss: 267.56591796875 acc: 0.9542678594589233  val: loss: 1312.7606201171875 acc: 0.8160189986228943\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 5620\n",
      "train: loss: 650.8174438476562 acc: 0.7687251567840576  val: loss: 2337.89599609375 acc: 0.7105486989021301\n",
      "step: 5625\n",
      "train: loss: 455.2154541015625 acc: 0.897426426410675  val: loss: 1424.9930419921875 acc: 0.7990784645080566\n",
      "step: 5630\n",
      "train: loss: 705.84326171875 acc: 0.8634111285209656  val: loss: 549.6314086914062 acc: 0.9123573303222656\n",
      "step: 5635\n",
      "train: loss: 916.17822265625 acc: 0.8569502234458923  val: loss: 1573.7291259765625 acc: 0.7615602016448975\n",
      "step: 5640\n",
      "train: loss: 906.9451904296875 acc: 0.8390035629272461  val: loss: 977.68603515625 acc: 0.8508180379867554\n",
      "step: 5645\n",
      "train: loss: 565.662353515625 acc: 0.895969033241272  val: loss: 953.3886108398438 acc: 0.855927586555481\n",
      "step: 5650\n",
      "train: loss: 980.1199340820312 acc: 0.7696627378463745  val: loss: 549.2005615234375 acc: 0.9056829810142517\n",
      "step: 5655\n",
      "train: loss: 742.1961059570312 acc: 0.8157023191452026  val: loss: 1043.0335693359375 acc: 0.8311843872070312\n",
      "step: 5660\n",
      "train: loss: 992.8312377929688 acc: 0.8315924406051636  val: loss: 379.859130859375 acc: 0.9216206073760986\n",
      "step: 5665\n",
      "train: loss: 393.06610107421875 acc: 0.9410530924797058  val: loss: 912.27392578125 acc: 0.8647903203964233\n",
      "step: 5670\n",
      "train: loss: 663.333251953125 acc: 0.8799152970314026  val: loss: 1901.817626953125 acc: 0.7789252996444702\n",
      "step: 5675\n",
      "train: loss: 465.76025390625 acc: 0.9205593466758728  val: loss: 1679.670654296875 acc: 0.7993771433830261\n",
      "step: 5680\n",
      "train: loss: 671.6597290039062 acc: 0.8738160133361816  val: loss: 1384.0677490234375 acc: 0.7832983732223511\n",
      "step: 5685\n",
      "train: loss: 845.11376953125 acc: 0.8769133687019348  val: loss: 1980.4532470703125 acc: 0.7270275354385376\n",
      "step: 5690\n",
      "train: loss: 721.23046875 acc: 0.8749938607215881  val: loss: 1026.39453125 acc: 0.8467115163803101\n",
      "step: 5695\n",
      "train: loss: 680.3831176757812 acc: 0.9077472686767578  val: loss: 788.4990844726562 acc: 0.8945863842964172\n",
      "step: 5700\n",
      "train: loss: 1386.253662109375 acc: 0.7690507173538208  val: loss: 1832.8741455078125 acc: 0.8118153810501099\n",
      "step: 5705\n",
      "train: loss: 360.5472412109375 acc: 0.9060475826263428  val: loss: 550.745361328125 acc: 0.8991391062736511\n",
      "step: 5710\n",
      "train: loss: 206.56103515625 acc: 0.950823187828064  val: loss: 787.7073364257812 acc: 0.8751233816146851\n",
      "step: 5715\n",
      "train: loss: 1048.1268310546875 acc: 0.817505419254303  val: loss: 2371.957763671875 acc: 0.7469916343688965\n",
      "step: 5720\n",
      "train: loss: 304.88531494140625 acc: 0.952847421169281  val: loss: 1014.58935546875 acc: 0.8691085577011108\n",
      "step: 5725\n",
      "train: loss: 643.52294921875 acc: 0.8934227824211121  val: loss: 476.9950256347656 acc: 0.9195664525032043\n",
      "step: 5730\n",
      "train: loss: 693.5819091796875 acc: 0.9006235003471375  val: loss: 926.5904541015625 acc: 0.8795274496078491\n",
      "step: 5735\n",
      "train: loss: 617.8585205078125 acc: 0.8521531224250793  val: loss: 1743.817138671875 acc: 0.8116166591644287\n",
      "step: 5740\n",
      "train: loss: 865.1410522460938 acc: 0.8593708276748657  val: loss: 2085.866943359375 acc: 0.7438190579414368\n",
      "step: 5745\n",
      "train: loss: 814.160400390625 acc: 0.7742657661437988  val: loss: 1082.0389404296875 acc: 0.8537927269935608\n",
      "step: 5750\n",
      "train: loss: 470.07586669921875 acc: 0.9176002144813538  val: loss: 1003.16015625 acc: 0.8564071655273438\n",
      "step: 5755\n",
      "train: loss: 732.0648803710938 acc: 0.866861879825592  val: loss: 785.08203125 acc: 0.8748975992202759\n",
      "step: 5760\n",
      "train: loss: 626.47265625 acc: 0.8421257138252258  val: loss: 558.9947509765625 acc: 0.8798869252204895\n",
      "step: 5765\n",
      "train: loss: 412.04541015625 acc: 0.9149510264396667  val: loss: 2045.88037109375 acc: 0.7175512909889221\n",
      "step: 5770\n",
      "train: loss: 676.5553588867188 acc: 0.8914456367492676  val: loss: 463.385498046875 acc: 0.8921411633491516\n",
      "step: 5775\n",
      "train: loss: 530.4915161132812 acc: 0.8799600005149841  val: loss: 1469.632568359375 acc: 0.812149703502655\n",
      "step: 5780\n",
      "train: loss: 1030.46337890625 acc: 0.8208009600639343  val: loss: 1839.93896484375 acc: 0.742606520652771\n",
      "step: 5785\n",
      "train: loss: 466.9607849121094 acc: 0.9261490106582642  val: loss: 983.1500244140625 acc: 0.8340471982955933\n",
      "step: 5790\n",
      "train: loss: 453.3912353515625 acc: 0.9415484666824341  val: loss: 950.8169555664062 acc: 0.8545887470245361\n",
      "step: 5795\n",
      "train: loss: 427.7625732421875 acc: 0.9317004680633545  val: loss: 1282.657958984375 acc: 0.8162874579429626\n",
      "step: 5800\n",
      "train: loss: 874.43310546875 acc: 0.8532499670982361  val: loss: 508.1787414550781 acc: 0.8615649938583374\n",
      "step: 5805\n",
      "train: loss: 932.3764038085938 acc: 0.8561347723007202  val: loss: 1441.5653076171875 acc: 0.8230499625205994\n",
      "step: 5810\n",
      "train: loss: 555.1763305664062 acc: 0.9198060035705566  val: loss: 383.4458312988281 acc: 0.9257622957229614\n",
      "step: 5815\n",
      "train: loss: 1037.7740478515625 acc: 0.8298727869987488  val: loss: 1313.3778076171875 acc: 0.8444806933403015\n",
      "step: 5820\n",
      "train: loss: 833.2286987304688 acc: 0.827894926071167  val: loss: 1356.9022216796875 acc: 0.7908189296722412\n",
      "step: 5825\n",
      "train: loss: 336.9884948730469 acc: 0.9367674589157104  val: loss: 727.8078002929688 acc: 0.9033182859420776\n",
      "step: 5830\n",
      "train: loss: 522.93310546875 acc: 0.8591780662536621  val: loss: 2743.526123046875 acc: 0.7030986547470093\n",
      "step: 5835\n",
      "train: loss: 333.0720520019531 acc: 0.9231233596801758  val: loss: 1413.6221923828125 acc: 0.8309265375137329\n",
      "step: 5840\n",
      "train: loss: 872.1797485351562 acc: 0.8668838739395142  val: loss: 603.3873901367188 acc: 0.8652547597885132\n",
      "step: 5845\n",
      "train: loss: 862.9583740234375 acc: 0.8400405049324036  val: loss: 1415.9180908203125 acc: 0.7222611904144287\n",
      "step: 5850\n",
      "train: loss: 578.6173095703125 acc: 0.8462212681770325  val: loss: 1096.530029296875 acc: 0.849166989326477\n",
      "step: 5855\n",
      "train: loss: 997.9961547851562 acc: 0.8384851217269897  val: loss: 1812.40380859375 acc: 0.7923552393913269\n",
      "step: 5860\n",
      "train: loss: 815.7250366210938 acc: 0.8805722594261169  val: loss: 869.1553955078125 acc: 0.8700971603393555\n",
      "step: 5865\n",
      "train: loss: 1045.265380859375 acc: 0.7750188708305359  val: loss: 1137.603271484375 acc: 0.8634007573127747\n",
      "step: 5870\n",
      "train: loss: 921.540771484375 acc: 0.7569305896759033  val: loss: 736.0752563476562 acc: 0.842634916305542\n",
      "step: 5875\n",
      "train: loss: 641.11767578125 acc: 0.9152891635894775  val: loss: 1537.611572265625 acc: 0.7424900531768799\n",
      "step: 5880\n",
      "train: loss: 564.4024047851562 acc: 0.8746198415756226  val: loss: 1032.0787353515625 acc: 0.8539623022079468\n",
      "step: 5885\n",
      "train: loss: 582.0233154296875 acc: 0.892227292060852  val: loss: 947.7715454101562 acc: 0.8266519904136658\n",
      "step: 5890\n",
      "train: loss: 582.2847290039062 acc: 0.887677788734436  val: loss: 814.3702392578125 acc: 0.8617500066757202\n",
      "step: 5895\n",
      "train: loss: 439.9183044433594 acc: 0.8998955488204956  val: loss: 1384.4566650390625 acc: 0.807655930519104\n",
      "step: 5900\n",
      "train: loss: 487.0896301269531 acc: 0.9360570907592773  val: loss: 389.38848876953125 acc: 0.9393668174743652\n",
      "step: 5905\n",
      "train: loss: 952.8936157226562 acc: 0.860682487487793  val: loss: 1139.097412109375 acc: 0.8419497013092041\n",
      "step: 5910\n",
      "train: loss: 244.197265625 acc: 0.9645665287971497  val: loss: 2165.6298828125 acc: 0.7792407274246216\n",
      "step: 5915\n",
      "train: loss: 672.6614990234375 acc: 0.8951631188392639  val: loss: 1362.2353515625 acc: 0.8361053466796875\n",
      "step: 5920\n",
      "train: loss: 628.9122924804688 acc: 0.8076784610748291  val: loss: 2024.8646240234375 acc: 0.7020348906517029\n",
      "step: 5925\n",
      "train: loss: 761.8758544921875 acc: 0.8440393805503845  val: loss: 1085.1536865234375 acc: 0.8257738351821899\n",
      "step: 5930\n",
      "train: loss: 1067.549560546875 acc: 0.7990710139274597  val: loss: 1014.3297729492188 acc: 0.8182380199432373\n",
      "step: 5935\n",
      "train: loss: 913.9183349609375 acc: 0.8899074196815491  val: loss: 1686.6800537109375 acc: 0.8032095432281494\n",
      "step: 5940\n",
      "train: loss: 628.6739501953125 acc: 0.900565505027771  val: loss: 1390.840087890625 acc: 0.7736411094665527\n",
      "step: 5945\n",
      "train: loss: 754.028076171875 acc: 0.797478437423706  val: loss: 568.0193481445312 acc: 0.9222584366798401\n",
      "step: 5950\n",
      "train: loss: 448.3027648925781 acc: 0.9097315669059753  val: loss: 1108.4859619140625 acc: 0.8334723711013794\n",
      "step: 5955\n",
      "train: loss: 351.430419921875 acc: 0.9054363369941711  val: loss: 2003.3616943359375 acc: 0.7729424238204956\n",
      "step: 5960\n",
      "train: loss: 656.2911987304688 acc: 0.8692045211791992  val: loss: 893.9324340820312 acc: 0.8528183102607727\n",
      "step: 5965\n",
      "train: loss: 679.9776000976562 acc: 0.8924518823623657  val: loss: 470.2618713378906 acc: 0.92288738489151\n",
      "step: 5970\n",
      "train: loss: 550.139892578125 acc: 0.8955467939376831  val: loss: 1345.3492431640625 acc: 0.7881605625152588\n",
      "step: 5975\n",
      "train: loss: 844.3572387695312 acc: 0.8774492740631104  val: loss: 755.7275390625 acc: 0.8965995907783508\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 5980\n",
      "train: loss: 506.16986083984375 acc: 0.8767133951187134  val: loss: 364.9095458984375 acc: 0.9350398182868958\n",
      "step: 5985\n",
      "train: loss: 928.3803100585938 acc: 0.8608076572418213  val: loss: 1591.372314453125 acc: 0.809126615524292\n",
      "step: 5990\n",
      "train: loss: 460.3753356933594 acc: 0.921963632106781  val: loss: 1871.7193603515625 acc: 0.7829850316047668\n",
      "step: 5995\n",
      "train: loss: 1034.1607666015625 acc: 0.8267335295677185  val: loss: 603.1701049804688 acc: 0.8852900266647339\n",
      "step: 6000\n",
      "train: loss: 743.046875 acc: 0.8129584789276123  val: loss: 647.9446411132812 acc: 0.8784104585647583\n",
      "step: 6005\n",
      "train: loss: 725.4293823242188 acc: 0.8403466939926147  val: loss: 1755.5740966796875 acc: 0.7650496363639832\n",
      "step: 6010\n",
      "train: loss: 393.36590576171875 acc: 0.9078960418701172  val: loss: 2487.5908203125 acc: 0.7206594347953796\n",
      "step: 6015\n",
      "train: loss: 645.0988159179688 acc: 0.8915255069732666  val: loss: 1468.082275390625 acc: 0.8113375902175903\n",
      "step: 6020\n",
      "train: loss: 931.9266357421875 acc: 0.868524968624115  val: loss: 1270.1954345703125 acc: 0.7760111093521118\n",
      "step: 6025\n",
      "train: loss: 670.6245727539062 acc: 0.9204329252243042  val: loss: 241.06126403808594 acc: 0.9489281177520752\n",
      "step: 6030\n",
      "train: loss: 577.9880981445312 acc: 0.8612244129180908  val: loss: 1906.540283203125 acc: 0.735236644744873\n",
      "step: 6035\n",
      "train: loss: 663.951416015625 acc: 0.8835803866386414  val: loss: 551.609130859375 acc: 0.88330078125\n",
      "step: 6040\n",
      "train: loss: 477.4258728027344 acc: 0.9068718552589417  val: loss: 2491.994384765625 acc: 0.7226839661598206\n",
      "step: 6045\n",
      "train: loss: 926.148193359375 acc: 0.8076751232147217  val: loss: 1966.948486328125 acc: 0.7752830982208252\n",
      "step: 6050\n",
      "train: loss: 625.01708984375 acc: 0.8619024753570557  val: loss: 466.3111267089844 acc: 0.9272340536117554\n",
      "step: 6055\n",
      "train: loss: 803.4884643554688 acc: 0.8873865604400635  val: loss: 808.2421264648438 acc: 0.8565599918365479\n",
      "step: 6060\n",
      "train: loss: 415.3709716796875 acc: 0.9370091557502747  val: loss: 481.35626220703125 acc: 0.9158333539962769\n",
      "step: 6065\n",
      "train: loss: 397.642822265625 acc: 0.8973348736763  val: loss: 1500.5247802734375 acc: 0.7829527258872986\n",
      "step: 6070\n",
      "train: loss: 480.17529296875 acc: 0.893601655960083  val: loss: 1008.8905639648438 acc: 0.848599910736084\n",
      "step: 6075\n",
      "train: loss: 216.72308349609375 acc: 0.9642923474311829  val: loss: 830.4269409179688 acc: 0.8765394687652588\n",
      "step: 6080\n",
      "train: loss: 870.6102294921875 acc: 0.8693183660507202  val: loss: 1204.709716796875 acc: 0.79587721824646\n",
      "step: 6085\n",
      "train: loss: 165.36436462402344 acc: 0.9730148911476135  val: loss: 1148.4810791015625 acc: 0.7988202571868896\n",
      "step: 6090\n",
      "train: loss: 677.49951171875 acc: 0.8621549606323242  val: loss: 639.6653442382812 acc: 0.8916096687316895\n",
      "step: 6095\n",
      "train: loss: 1015.1871948242188 acc: 0.8496981263160706  val: loss: 1593.906494140625 acc: 0.799217164516449\n",
      "step: 6100\n",
      "train: loss: 998.5538940429688 acc: 0.8460668325424194  val: loss: 1718.44580078125 acc: 0.742249071598053\n",
      "step: 6105\n",
      "train: loss: 843.7791748046875 acc: 0.7648395299911499  val: loss: 1548.016357421875 acc: 0.8073372840881348\n",
      "step: 6110\n",
      "train: loss: 584.1715087890625 acc: 0.909044086933136  val: loss: 1122.738525390625 acc: 0.8553141951560974\n",
      "step: 6115\n",
      "train: loss: 712.0353393554688 acc: 0.8889994621276855  val: loss: 246.43267822265625 acc: 0.9598239064216614\n",
      "step: 6120\n",
      "train: loss: 710.8374633789062 acc: 0.8441891074180603  val: loss: 1235.346923828125 acc: 0.7928256988525391\n",
      "step: 6125\n",
      "train: loss: 1042.6004638671875 acc: 0.7755930423736572  val: loss: 608.7831420898438 acc: 0.8789440989494324\n",
      "step: 6130\n",
      "train: loss: 720.3805541992188 acc: 0.8280535340309143  val: loss: 1334.5885009765625 acc: 0.8235890865325928\n",
      "step: 6135\n",
      "train: loss: 472.3268737792969 acc: 0.9171082973480225  val: loss: 1080.3209228515625 acc: 0.808273434638977\n",
      "step: 6140\n",
      "train: loss: 906.6754150390625 acc: 0.8662694692611694  val: loss: 967.4174194335938 acc: 0.8852803707122803\n",
      "step: 6145\n",
      "train: loss: 581.1383056640625 acc: 0.8678773045539856  val: loss: 1214.8924560546875 acc: 0.8099850416183472\n",
      "step: 6150\n",
      "train: loss: 611.1429443359375 acc: 0.9174197316169739  val: loss: 699.1763305664062 acc: 0.8586339950561523\n",
      "step: 6155\n",
      "train: loss: 699.4931030273438 acc: 0.8759006261825562  val: loss: 1035.4786376953125 acc: 0.8674179315567017\n",
      "step: 6160\n",
      "train: loss: 1293.398681640625 acc: 0.8082049489021301  val: loss: 835.4899291992188 acc: 0.8947369456291199\n",
      "step: 6165\n",
      "train: loss: 636.8380737304688 acc: 0.8692584037780762  val: loss: 828.5187377929688 acc: 0.8532862663269043\n",
      "step: 6170\n",
      "train: loss: 756.8695678710938 acc: 0.8913777470588684  val: loss: 598.7980346679688 acc: 0.9087288975715637\n",
      "step: 6175\n",
      "train: loss: 455.22442626953125 acc: 0.9240713715553284  val: loss: 626.580810546875 acc: 0.903619647026062\n",
      "step: 6180\n",
      "train: loss: 700.3187255859375 acc: 0.8635251522064209  val: loss: 992.5734252929688 acc: 0.8236781358718872\n",
      "step: 6185\n",
      "train: loss: 607.7349243164062 acc: 0.836302638053894  val: loss: 690.6192626953125 acc: 0.8895303606987\n",
      "step: 6190\n",
      "train: loss: 759.1912231445312 acc: 0.8464510440826416  val: loss: 1445.0133056640625 acc: 0.8094342947006226\n",
      "step: 6195\n",
      "train: loss: 723.014404296875 acc: 0.7905843257904053  val: loss: 1131.1080322265625 acc: 0.8266928195953369\n",
      "step: 6200\n",
      "train: loss: 511.8212585449219 acc: 0.9264635443687439  val: loss: 1799.048583984375 acc: 0.7600042819976807\n",
      "step: 6205\n",
      "train: loss: 512.3330688476562 acc: 0.9174283146858215  val: loss: 914.5205078125 acc: 0.8799048662185669\n",
      "step: 6210\n",
      "train: loss: 1061.34912109375 acc: 0.7670145630836487  val: loss: 1681.1505126953125 acc: 0.784077525138855\n",
      "step: 6215\n",
      "train: loss: 887.0110473632812 acc: 0.8877555131912231  val: loss: 1518.1741943359375 acc: 0.7987399697303772\n",
      "step: 6220\n",
      "train: loss: 419.58221435546875 acc: 0.9216510057449341  val: loss: 705.0306396484375 acc: 0.9018633365631104\n",
      "step: 6225\n",
      "train: loss: 1101.372802734375 acc: 0.8568069934844971  val: loss: 1059.9427490234375 acc: 0.823204755783081\n",
      "step: 6230\n",
      "train: loss: 809.384765625 acc: 0.8638704419136047  val: loss: 778.4542236328125 acc: 0.8862578868865967\n",
      "step: 6235\n",
      "train: loss: 658.7017822265625 acc: 0.8686436414718628  val: loss: 473.953125 acc: 0.9345333576202393\n",
      "step: 6240\n",
      "train: loss: 389.65667724609375 acc: 0.9004013538360596  val: loss: 2402.0244140625 acc: 0.7037128210067749\n",
      "step: 6245\n",
      "train: loss: 360.8287658691406 acc: 0.9125006794929504  val: loss: 1096.7159423828125 acc: 0.8503913879394531\n",
      "step: 6250\n",
      "train: loss: 380.0279846191406 acc: 0.9098048210144043  val: loss: 1122.9000244140625 acc: 0.7833927273750305\n",
      "step: 6255\n",
      "train: loss: 457.026123046875 acc: 0.9405208826065063  val: loss: 928.3790283203125 acc: 0.8479326963424683\n",
      "step: 6260\n",
      "train: loss: 725.1099853515625 acc: 0.9008318185806274  val: loss: 1209.1806640625 acc: 0.8524458408355713\n",
      "step: 6265\n",
      "train: loss: 661.9343872070312 acc: 0.8997220396995544  val: loss: 918.630615234375 acc: 0.8497494459152222\n",
      "step: 6270\n",
      "train: loss: 569.0186157226562 acc: 0.9036681056022644  val: loss: 2341.33935546875 acc: 0.5524997711181641\n",
      "step: 6275\n",
      "train: loss: 898.281982421875 acc: 0.8448551297187805  val: loss: 2239.80859375 acc: 0.7608970999717712\n",
      "step: 6280\n",
      "train: loss: 806.5677490234375 acc: 0.8459903001785278  val: loss: 675.2625122070312 acc: 0.8915191292762756\n",
      "step: 6285\n",
      "train: loss: 728.5493774414062 acc: 0.8734880685806274  val: loss: 1813.7083740234375 acc: 0.7385958433151245\n",
      "step: 6290\n",
      "train: loss: 953.5045776367188 acc: 0.8338350653648376  val: loss: 491.7115783691406 acc: 0.9064390063285828\n",
      "step: 6295\n",
      "train: loss: 563.7952880859375 acc: 0.9069663286209106  val: loss: 951.5402221679688 acc: 0.83756422996521\n",
      "step: 6300\n",
      "train: loss: 672.166259765625 acc: 0.849358320236206  val: loss: 849.1280517578125 acc: 0.8761041760444641\n",
      "step: 6305\n",
      "train: loss: 478.8709716796875 acc: 0.8813320994377136  val: loss: 1517.863525390625 acc: 0.7790539860725403\n",
      "step: 6310\n",
      "train: loss: 805.0609130859375 acc: 0.8911904096603394  val: loss: 1517.3568115234375 acc: 0.8155230283737183\n",
      "step: 6315\n",
      "train: loss: 628.907958984375 acc: 0.8908687829971313  val: loss: 577.7110595703125 acc: 0.8838413953781128\n",
      "step: 6320\n",
      "train: loss: 485.1206970214844 acc: 0.9406952857971191  val: loss: 1296.4278564453125 acc: 0.8487386107444763\n",
      "step: 6325\n",
      "train: loss: 1243.0054931640625 acc: 0.8456058502197266  val: loss: 1254.219970703125 acc: 0.8318249583244324\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 6330\n",
      "train: loss: 1065.8448486328125 acc: 0.8126986622810364  val: loss: 434.21942138671875 acc: 0.9327982068061829\n",
      "step: 6335\n",
      "train: loss: 1055.2581787109375 acc: 0.7936890125274658  val: loss: 561.4158325195312 acc: 0.8765172362327576\n",
      "step: 6340\n",
      "train: loss: 1009.19970703125 acc: 0.7990996241569519  val: loss: 1974.0537109375 acc: 0.7309154272079468\n",
      "step: 6345\n",
      "train: loss: 329.4988098144531 acc: 0.936436116695404  val: loss: 926.7179565429688 acc: 0.8229455947875977\n",
      "step: 6350\n",
      "train: loss: 1317.6378173828125 acc: 0.7936092615127563  val: loss: 1052.4654541015625 acc: 0.8608655333518982\n",
      "step: 6355\n",
      "train: loss: 1116.2171630859375 acc: 0.8360558152198792  val: loss: 1676.3294677734375 acc: 0.7816236615180969\n",
      "step: 6360\n",
      "train: loss: 234.4469451904297 acc: 0.9198744297027588  val: loss: 614.3982543945312 acc: 0.8744369745254517\n",
      "step: 6365\n",
      "train: loss: 254.2601318359375 acc: 0.9425132274627686  val: loss: 1936.5975341796875 acc: 0.7643323540687561\n",
      "step: 6370\n",
      "train: loss: 492.1578063964844 acc: 0.9197271466255188  val: loss: 1131.549560546875 acc: 0.8415271043777466\n",
      "step: 6375\n",
      "train: loss: 581.426025390625 acc: 0.9232548475265503  val: loss: 2638.3759765625 acc: 0.7202966809272766\n",
      "step: 6380\n",
      "train: loss: 658.3736572265625 acc: 0.8809732794761658  val: loss: 1908.76123046875 acc: 0.752351701259613\n",
      "step: 6385\n",
      "train: loss: 300.6717529296875 acc: 0.9430884718894958  val: loss: 2490.239990234375 acc: 0.7572665214538574\n",
      "step: 6390\n",
      "train: loss: 763.15673828125 acc: 0.9066314697265625  val: loss: 821.4698486328125 acc: 0.8665124177932739\n",
      "step: 6395\n",
      "train: loss: 738.2265014648438 acc: 0.8930307626724243  val: loss: 974.7786865234375 acc: 0.8592451810836792\n",
      "step: 6400\n",
      "train: loss: 931.822509765625 acc: 0.8440065979957581  val: loss: 1176.24560546875 acc: 0.8713152408599854\n",
      "step: 6405\n",
      "train: loss: 430.3230895996094 acc: 0.8933443427085876  val: loss: 1993.9281005859375 acc: 0.7601001858711243\n",
      "step: 6410\n",
      "train: loss: 897.3417358398438 acc: 0.8004828691482544  val: loss: 649.0927734375 acc: 0.894639253616333\n",
      "step: 6415\n",
      "train: loss: 631.850341796875 acc: 0.8568117618560791  val: loss: 514.5230712890625 acc: 0.9042789936065674\n",
      "step: 6420\n",
      "train: loss: 628.1423950195312 acc: 0.9001537561416626  val: loss: 1113.7535400390625 acc: 0.8464624881744385\n",
      "step: 6425\n",
      "train: loss: 454.7884216308594 acc: 0.9029648303985596  val: loss: 1193.808349609375 acc: 0.8479925394058228\n",
      "step: 6430\n",
      "train: loss: 327.10845947265625 acc: 0.9506215453147888  val: loss: 1295.0084228515625 acc: 0.8571711778640747\n",
      "step: 6435\n",
      "train: loss: 468.8188781738281 acc: 0.9192498922348022  val: loss: 1152.7711181640625 acc: 0.8024974465370178\n",
      "step: 6440\n",
      "train: loss: 274.9094543457031 acc: 0.9233330488204956  val: loss: 833.875732421875 acc: 0.8589587211608887\n",
      "step: 6445\n",
      "train: loss: 630.724365234375 acc: 0.908383309841156  val: loss: 1194.0302734375 acc: 0.8315961360931396\n",
      "step: 6450\n",
      "train: loss: 712.7053833007812 acc: 0.8885630369186401  val: loss: 2143.66064453125 acc: 0.7121409177780151\n",
      "step: 6455\n",
      "train: loss: 1183.06640625 acc: 0.8034974932670593  val: loss: 765.3876953125 acc: 0.829429566860199\n",
      "step: 6460\n",
      "train: loss: 1177.7578125 acc: 0.7598754167556763  val: loss: 873.8388061523438 acc: 0.8888399004936218\n",
      "step: 6465\n",
      "train: loss: 895.6188354492188 acc: 0.8417683243751526  val: loss: 969.9720458984375 acc: 0.862212061882019\n",
      "step: 6470\n",
      "train: loss: 559.87841796875 acc: 0.8337503671646118  val: loss: 2277.246826171875 acc: 0.7506121397018433\n",
      "step: 6475\n",
      "train: loss: 426.4298095703125 acc: 0.9016698002815247  val: loss: 1029.0789794921875 acc: 0.8670206069946289\n",
      "step: 6480\n",
      "train: loss: 502.356201171875 acc: 0.8850392699241638  val: loss: 1164.1297607421875 acc: 0.8576611876487732\n",
      "step: 6485\n",
      "train: loss: 368.80853271484375 acc: 0.9208179116249084  val: loss: 1693.8721923828125 acc: 0.7796643376350403\n",
      "step: 6490\n",
      "train: loss: 700.3348999023438 acc: 0.8515675663948059  val: loss: 1268.0762939453125 acc: 0.8146885633468628\n",
      "step: 6495\n",
      "train: loss: 595.4163818359375 acc: 0.8988220691680908  val: loss: 228.55557250976562 acc: 0.9430800676345825\n",
      "step: 6500\n",
      "train: loss: 334.3995666503906 acc: 0.9373828768730164  val: loss: 1770.3399658203125 acc: 0.7979044318199158\n",
      "step: 6505\n",
      "train: loss: 519.844970703125 acc: 0.8937264680862427  val: loss: 614.0330810546875 acc: 0.8893914818763733\n",
      "step: 6510\n",
      "train: loss: 598.5638427734375 acc: 0.8772664666175842  val: loss: 542.8963623046875 acc: 0.8909288644790649\n",
      "step: 6515\n",
      "train: loss: 824.52587890625 acc: 0.7932963967323303  val: loss: 1134.6619873046875 acc: 0.8308931589126587\n",
      "step: 6520\n",
      "train: loss: 786.6337280273438 acc: 0.8680799007415771  val: loss: 643.4192504882812 acc: 0.895374059677124\n",
      "step: 6525\n",
      "train: loss: 507.85894775390625 acc: 0.8983299136161804  val: loss: 1842.3048095703125 acc: 0.7694356441497803\n",
      "step: 6530\n",
      "train: loss: 545.7291259765625 acc: 0.9019091129302979  val: loss: 626.834228515625 acc: 0.9219996929168701\n",
      "step: 6535\n",
      "train: loss: 623.777587890625 acc: 0.8797628879547119  val: loss: 2253.189697265625 acc: 0.744303286075592\n",
      "step: 6540\n",
      "train: loss: 514.4810180664062 acc: 0.8812135457992554  val: loss: 2217.620361328125 acc: 0.7213775515556335\n",
      "step: 6545\n",
      "train: loss: 258.1160583496094 acc: 0.9561247229576111  val: loss: 1114.9984130859375 acc: 0.8192571997642517\n",
      "step: 6550\n",
      "train: loss: 1034.5460205078125 acc: 0.8386165499687195  val: loss: 1635.102294921875 acc: 0.7821208834648132\n",
      "step: 6555\n",
      "train: loss: 449.462646484375 acc: 0.9160019755363464  val: loss: 381.9737854003906 acc: 0.9409468770027161\n",
      "step: 6560\n",
      "train: loss: 822.2386474609375 acc: 0.8502353429794312  val: loss: 502.43408203125 acc: 0.9169489145278931\n",
      "step: 6565\n",
      "train: loss: 485.54718017578125 acc: 0.9167182445526123  val: loss: 2038.0084228515625 acc: 0.7125862240791321\n",
      "step: 6570\n",
      "train: loss: 682.6071166992188 acc: 0.8759902119636536  val: loss: 1717.741455078125 acc: 0.7675266265869141\n",
      "step: 6575\n",
      "train: loss: 731.6864624023438 acc: 0.8810861110687256  val: loss: 1532.6495361328125 acc: 0.8015239238739014\n",
      "step: 6580\n",
      "train: loss: 493.4217224121094 acc: 0.9223746061325073  val: loss: 1112.6827392578125 acc: 0.8580236434936523\n",
      "step: 6585\n",
      "train: loss: 904.0327758789062 acc: 0.8076925873756409  val: loss: 504.3401794433594 acc: 0.915061891078949\n",
      "step: 6590\n",
      "train: loss: 651.4157104492188 acc: 0.8603441715240479  val: loss: 1353.2474365234375 acc: 0.8311272263526917\n",
      "step: 6595\n",
      "train: loss: 464.1607360839844 acc: 0.9161957502365112  val: loss: 446.317138671875 acc: 0.9249948263168335\n",
      "step: 6600\n",
      "train: loss: 623.8204956054688 acc: 0.8375446796417236  val: loss: 1235.0703125 acc: 0.8276209831237793\n",
      "step: 6605\n",
      "train: loss: 311.7105712890625 acc: 0.9341029524803162  val: loss: 2252.533203125 acc: 0.6909059882164001\n",
      "step: 6610\n",
      "train: loss: 701.88720703125 acc: 0.8900660872459412  val: loss: 1385.2186279296875 acc: 0.7893295288085938\n",
      "step: 6615\n",
      "train: loss: 712.453369140625 acc: 0.9032236933708191  val: loss: 1056.37255859375 acc: 0.7957345247268677\n",
      "step: 6620\n",
      "train: loss: 723.0715942382812 acc: 0.8861479163169861  val: loss: 1130.2098388671875 acc: 0.77879399061203\n",
      "step: 6625\n",
      "train: loss: 1103.318115234375 acc: 0.8526498675346375  val: loss: 1287.4017333984375 acc: 0.8619101047515869\n",
      "step: 6630\n",
      "train: loss: 660.2507934570312 acc: 0.8830870389938354  val: loss: 550.767822265625 acc: 0.8789119720458984\n",
      "step: 6635\n",
      "train: loss: 759.5142211914062 acc: 0.7598192691802979  val: loss: 1827.654541015625 acc: 0.7338987588882446\n",
      "step: 6640\n",
      "train: loss: 830.6702270507812 acc: 0.8699489235877991  val: loss: 1677.5697021484375 acc: 0.7872799634933472\n",
      "step: 6645\n",
      "train: loss: 598.3162231445312 acc: 0.8891249299049377  val: loss: 784.4761962890625 acc: 0.8967137336730957\n",
      "step: 6650\n",
      "train: loss: 547.0836791992188 acc: 0.9082497954368591  val: loss: 562.5104370117188 acc: 0.9161993265151978\n",
      "step: 6655\n",
      "train: loss: 494.2755126953125 acc: 0.9085999727249146  val: loss: 851.8734741210938 acc: 0.8467905521392822\n",
      "step: 6660\n",
      "train: loss: 678.8666381835938 acc: 0.8542066216468811  val: loss: 1982.356201171875 acc: 0.8035178780555725\n",
      "step: 6665\n",
      "train: loss: 355.5347900390625 acc: 0.9408247470855713  val: loss: 1245.6917724609375 acc: 0.8269802331924438\n",
      "step: 6670\n",
      "train: loss: 586.8762817382812 acc: 0.8805477023124695  val: loss: 2021.1639404296875 acc: 0.7633179426193237\n",
      "step: 6675\n",
      "train: loss: 725.465576171875 acc: 0.8825123310089111  val: loss: 1607.638427734375 acc: 0.8168526887893677\n",
      "step: 6680\n",
      "train: loss: 572.1023559570312 acc: 0.897869348526001  val: loss: 888.3922119140625 acc: 0.8812932372093201\n",
      "step: 6685\n",
      "train: loss: 819.5869140625 acc: 0.8241971135139465  val: loss: 909.8155517578125 acc: 0.8834508061408997\n",
      "step: 6690\n",
      "train: loss: 351.5071105957031 acc: 0.8984051942825317  val: loss: 595.6449584960938 acc: 0.8598548769950867\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 6695\n",
      "train: loss: 900.3245849609375 acc: 0.7994850277900696  val: loss: 572.7539672851562 acc: 0.9051241874694824\n",
      "step: 6700\n",
      "train: loss: 570.266357421875 acc: 0.9122666120529175  val: loss: 1148.259765625 acc: 0.8390432000160217\n",
      "step: 6705\n",
      "train: loss: 627.345458984375 acc: 0.8973040580749512  val: loss: 1552.322998046875 acc: 0.8059394359588623\n",
      "step: 6710\n",
      "train: loss: 341.76568603515625 acc: 0.9343351125717163  val: loss: 933.3563842773438 acc: 0.8142682313919067\n",
      "step: 6715\n",
      "train: loss: 921.6450805664062 acc: 0.8286010026931763  val: loss: 1009.413818359375 acc: 0.8631542921066284\n",
      "step: 6720\n",
      "train: loss: 652.1687622070312 acc: 0.8912353515625  val: loss: 2650.002197265625 acc: 0.7334656715393066\n",
      "step: 6725\n",
      "train: loss: 490.57501220703125 acc: 0.9271003603935242  val: loss: 882.3526000976562 acc: 0.8863005042076111\n",
      "step: 6730\n",
      "train: loss: 354.2431945800781 acc: 0.9359177350997925  val: loss: 1142.53662109375 acc: 0.8332470655441284\n",
      "step: 6735\n",
      "train: loss: 757.820556640625 acc: 0.8964735865592957  val: loss: 1480.1893310546875 acc: 0.8092108964920044\n",
      "step: 6740\n",
      "train: loss: 472.9330749511719 acc: 0.9218440055847168  val: loss: 499.67535400390625 acc: 0.9199073314666748\n",
      "step: 6745\n",
      "train: loss: 499.4040832519531 acc: 0.9069267511367798  val: loss: 703.7327880859375 acc: 0.8829680681228638\n",
      "step: 6750\n",
      "train: loss: 1262.7816162109375 acc: 0.5046381950378418  val: loss: 1307.6573486328125 acc: 0.8169372081756592\n",
      "step: 6755\n",
      "train: loss: 1095.9378662109375 acc: 0.8368523120880127  val: loss: 597.7869262695312 acc: 0.8638864755630493\n",
      "step: 6760\n",
      "train: loss: 590.5180053710938 acc: 0.9001350402832031  val: loss: 2261.8369140625 acc: 0.7360695004463196\n",
      "step: 6765\n",
      "train: loss: 1011.3651733398438 acc: 0.8073093891143799  val: loss: 1743.7655029296875 acc: 0.7758952379226685\n",
      "step: 6770\n",
      "train: loss: 658.6181030273438 acc: 0.8773081302642822  val: loss: 1823.555908203125 acc: 0.7736667990684509\n",
      "step: 6775\n",
      "train: loss: 848.6063842773438 acc: 0.8529995083808899  val: loss: 609.1283569335938 acc: 0.878878116607666\n",
      "step: 6780\n",
      "train: loss: 599.2850341796875 acc: 0.8743085265159607  val: loss: 445.7229309082031 acc: 0.9167899489402771\n",
      "step: 6785\n",
      "train: loss: 493.4651184082031 acc: 0.9097668528556824  val: loss: 699.70068359375 acc: 0.8984386920928955\n",
      "step: 6790\n",
      "train: loss: 594.2457275390625 acc: 0.9125066995620728  val: loss: 1752.893310546875 acc: 0.7610572576522827\n",
      "step: 6795\n",
      "train: loss: 393.13409423828125 acc: 0.9444491863250732  val: loss: 2048.645751953125 acc: 0.780973494052887\n",
      "step: 6800\n",
      "train: loss: 638.7105712890625 acc: 0.9242175221443176  val: loss: 940.5028076171875 acc: 0.8312704563140869\n",
      "step: 6805\n",
      "train: loss: 642.93896484375 acc: 0.8503594994544983  val: loss: 1498.7847900390625 acc: 0.821500301361084\n",
      "step: 6810\n",
      "train: loss: 574.9412231445312 acc: 0.8698641061782837  val: loss: 527.0304565429688 acc: 0.9105880856513977\n",
      "step: 6815\n",
      "train: loss: 1036.1868896484375 acc: 0.7493841052055359  val: loss: 1559.2674560546875 acc: 0.794005274772644\n",
      "step: 6820\n",
      "train: loss: 1019.4216918945312 acc: 0.8209537267684937  val: loss: 1719.5738525390625 acc: 0.7672890424728394\n",
      "step: 6825\n",
      "train: loss: 407.80780029296875 acc: 0.9200870394706726  val: loss: 1211.7066650390625 acc: 0.8328732848167419\n",
      "step: 6830\n",
      "train: loss: 471.1153564453125 acc: 0.9233360290527344  val: loss: 1457.17919921875 acc: 0.7788795232772827\n",
      "step: 6835\n",
      "train: loss: 527.272705078125 acc: 0.8528447151184082  val: loss: 1345.098876953125 acc: 0.8298316597938538\n",
      "step: 6840\n",
      "train: loss: 532.0345458984375 acc: 0.8897013664245605  val: loss: 858.9087524414062 acc: 0.8217165470123291\n",
      "step: 6845\n",
      "train: loss: 476.1116638183594 acc: 0.9150744080543518  val: loss: 1052.034423828125 acc: 0.8130732774734497\n",
      "step: 6850\n",
      "train: loss: 618.0103759765625 acc: 0.910129964351654  val: loss: 1013.75927734375 acc: 0.836558997631073\n",
      "step: 6855\n",
      "train: loss: 442.4991149902344 acc: 0.9371782541275024  val: loss: 1601.88232421875 acc: 0.7978008389472961\n",
      "step: 6860\n",
      "train: loss: 958.0459594726562 acc: 0.8264136910438538  val: loss: 472.2390441894531 acc: 0.9080483913421631\n",
      "step: 6865\n",
      "train: loss: 667.9307250976562 acc: 0.8780051469802856  val: loss: 858.3506469726562 acc: 0.8282098770141602\n",
      "step: 6870\n",
      "train: loss: 656.2635498046875 acc: 0.8577539920806885  val: loss: 1737.1126708984375 acc: 0.7249248623847961\n",
      "step: 6875\n",
      "train: loss: 620.1612548828125 acc: 0.9161517024040222  val: loss: 1006.2618408203125 acc: 0.8432577252388\n",
      "step: 6880\n",
      "train: loss: 980.9340209960938 acc: 0.8672359585762024  val: loss: 1226.778076171875 acc: 0.7895445227622986\n",
      "step: 6885\n",
      "train: loss: 565.77490234375 acc: 0.9181554317474365  val: loss: 1198.033447265625 acc: 0.8456399440765381\n",
      "step: 6890\n",
      "train: loss: 526.4547729492188 acc: 0.8938263058662415  val: loss: 1432.979248046875 acc: 0.7982332706451416\n",
      "step: 6895\n",
      "train: loss: 352.46942138671875 acc: 0.8995915055274963  val: loss: 1151.0335693359375 acc: 0.8157759308815002\n",
      "step: 6900\n",
      "train: loss: 681.9579467773438 acc: 0.8951820731163025  val: loss: 1562.72998046875 acc: 0.7352166771888733\n",
      "step: 6905\n",
      "train: loss: 358.8118896484375 acc: 0.9492910504341125  val: loss: 539.4666748046875 acc: 0.8923795223236084\n",
      "step: 6910\n",
      "train: loss: 650.3456420898438 acc: 0.8957142233848572  val: loss: 429.7930908203125 acc: 0.9103869199752808\n",
      "step: 6915\n",
      "train: loss: 381.82525634765625 acc: 0.9390951991081238  val: loss: 1077.4556884765625 acc: 0.8105679154396057\n",
      "step: 6920\n",
      "train: loss: 503.7655944824219 acc: 0.9294291734695435  val: loss: 1392.610107421875 acc: 0.8243182301521301\n",
      "step: 6925\n",
      "train: loss: 527.8731079101562 acc: 0.9185600876808167  val: loss: 1510.677978515625 acc: 0.7388840913772583\n",
      "step: 6930\n",
      "train: loss: 746.0590209960938 acc: 0.8356194496154785  val: loss: 1403.267578125 acc: 0.7758299112319946\n",
      "step: 6935\n",
      "train: loss: 610.966796875 acc: 0.9080864191055298  val: loss: 762.6541748046875 acc: 0.8589973449707031\n",
      "step: 6940\n",
      "train: loss: 522.9793701171875 acc: 0.9108630418777466  val: loss: 406.0894470214844 acc: 0.9341896176338196\n",
      "step: 6945\n",
      "train: loss: 442.0469055175781 acc: 0.8637018203735352  val: loss: 976.9153442382812 acc: 0.8388751745223999\n",
      "step: 6950\n",
      "train: loss: 779.2991333007812 acc: 0.8127052783966064  val: loss: 1514.9324951171875 acc: 0.7896162271499634\n",
      "step: 6955\n",
      "train: loss: 260.80828857421875 acc: 0.9399404525756836  val: loss: 1912.4923095703125 acc: 0.7743516564369202\n",
      "step: 6960\n",
      "train: loss: 756.2706298828125 acc: 0.8870400190353394  val: loss: 842.5056762695312 acc: 0.870139479637146\n",
      "step: 6965\n",
      "train: loss: 851.1368408203125 acc: 0.8833520412445068  val: loss: 1485.1348876953125 acc: 0.7916730046272278\n",
      "step: 6970\n",
      "train: loss: 774.2025756835938 acc: 0.8854968547821045  val: loss: 1834.8173828125 acc: 0.7813032865524292\n",
      "step: 6975\n",
      "train: loss: 762.468505859375 acc: 0.9068806767463684  val: loss: 825.4110107421875 acc: 0.8704203367233276\n",
      "step: 6980\n",
      "train: loss: 453.16351318359375 acc: 0.9267767667770386  val: loss: 1401.402099609375 acc: 0.771601676940918\n",
      "step: 6985\n",
      "train: loss: 838.9955444335938 acc: 0.8848912119865417  val: loss: 1050.50634765625 acc: 0.8510535955429077\n",
      "step: 6990\n",
      "train: loss: 962.9998779296875 acc: 0.8319097757339478  val: loss: 1468.7635498046875 acc: 0.8196410536766052\n",
      "step: 6995\n",
      "train: loss: 667.4644165039062 acc: 0.912753164768219  val: loss: 1519.604248046875 acc: 0.8036770820617676\n",
      "step: 7000\n",
      "train: loss: 553.1683349609375 acc: 0.8959720730781555  val: loss: 898.3175659179688 acc: 0.8666391968727112\n",
      "step: 7005\n",
      "train: loss: 1092.8497314453125 acc: 0.8713754415512085  val: loss: 687.0317993164062 acc: 0.9071894288063049\n",
      "step: 7010\n",
      "train: loss: 510.49920654296875 acc: 0.8869132995605469  val: loss: 1156.1734619140625 acc: 0.8440876603126526\n",
      "step: 7015\n",
      "train: loss: 603.8070068359375 acc: 0.8559130430221558  val: loss: 1269.7918701171875 acc: 0.8013968467712402\n",
      "step: 7020\n",
      "train: loss: 621.4701538085938 acc: 0.9034360647201538  val: loss: 1158.589111328125 acc: 0.8390151858329773\n",
      "step: 7025\n",
      "train: loss: 417.4243469238281 acc: 0.9406184554100037  val: loss: 840.8089599609375 acc: 0.9089100956916809\n",
      "step: 7030\n",
      "train: loss: 744.63720703125 acc: 0.9030523300170898  val: loss: 788.6294555664062 acc: 0.8846842050552368\n",
      "step: 7035\n",
      "train: loss: 493.16961669921875 acc: 0.9109430313110352  val: loss: 2004.34619140625 acc: 0.7622770071029663\n",
      "step: 7040\n",
      "train: loss: 696.9412231445312 acc: 0.8298934102058411  val: loss: 1132.288818359375 acc: 0.8470208048820496\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 7045\n",
      "train: loss: 653.137939453125 acc: 0.8893755078315735  val: loss: 958.3519287109375 acc: 0.8758838772773743\n",
      "step: 7050\n",
      "train: loss: 734.8054809570312 acc: 0.8275190591812134  val: loss: 507.52667236328125 acc: 0.8905880451202393\n",
      "step: 7055\n",
      "train: loss: 520.2462158203125 acc: 0.9238885641098022  val: loss: 1527.8338623046875 acc: 0.818968653678894\n",
      "step: 7060\n",
      "train: loss: 884.5839233398438 acc: 0.7951940894126892  val: loss: 1108.246826171875 acc: 0.8280785083770752\n",
      "step: 7065\n",
      "train: loss: 574.4942016601562 acc: 0.852383017539978  val: loss: 1151.839111328125 acc: 0.856216311454773\n",
      "step: 7070\n",
      "train: loss: 285.8434753417969 acc: 0.9389384388923645  val: loss: 1198.601318359375 acc: 0.7874785661697388\n",
      "step: 7075\n",
      "train: loss: 562.5454711914062 acc: 0.8188596963882446  val: loss: 753.8460083007812 acc: 0.8898190259933472\n",
      "step: 7080\n",
      "train: loss: 588.8304443359375 acc: 0.848560631275177  val: loss: 666.5689086914062 acc: 0.8902592658996582\n",
      "step: 7085\n",
      "train: loss: 614.2589111328125 acc: 0.8801882863044739  val: loss: 1366.2601318359375 acc: 0.8277125954627991\n",
      "step: 7090\n",
      "train: loss: 419.3696594238281 acc: 0.9384185671806335  val: loss: 833.8572998046875 acc: 0.8623449802398682\n",
      "step: 7095\n",
      "train: loss: 563.655517578125 acc: 0.8763997554779053  val: loss: 1808.6868896484375 acc: 0.7484084367752075\n",
      "step: 7100\n",
      "train: loss: 681.3993530273438 acc: 0.8973076939582825  val: loss: 1366.90771484375 acc: 0.8432259559631348\n",
      "step: 7105\n",
      "train: loss: 1193.892578125 acc: 0.7831866145133972  val: loss: 1645.7012939453125 acc: 0.7987346649169922\n",
      "step: 7110\n",
      "train: loss: 1053.5701904296875 acc: 0.8151199817657471  val: loss: 1119.5782470703125 acc: 0.7970147132873535\n",
      "step: 7115\n",
      "train: loss: 936.9479370117188 acc: 0.8237638473510742  val: loss: 724.6787719726562 acc: 0.8198012113571167\n",
      "step: 7120\n",
      "train: loss: 479.703125 acc: 0.9156166911125183  val: loss: 861.506103515625 acc: 0.8772807717323303\n",
      "step: 7125\n",
      "train: loss: 758.9876098632812 acc: 0.8476637601852417  val: loss: 1662.5177001953125 acc: 0.8061750531196594\n",
      "step: 7130\n",
      "train: loss: 442.2375793457031 acc: 0.9128091335296631  val: loss: 1672.9630126953125 acc: 0.7852534055709839\n",
      "step: 7135\n",
      "train: loss: 349.7129821777344 acc: 0.9196717739105225  val: loss: 2641.290771484375 acc: 0.7323377728462219\n",
      "step: 7140\n",
      "train: loss: 434.2025451660156 acc: 0.9369279146194458  val: loss: 1378.6846923828125 acc: 0.7903761267662048\n",
      "step: 7145\n",
      "train: loss: 534.4407348632812 acc: 0.9225250482559204  val: loss: 1567.614990234375 acc: 0.7570697665214539\n",
      "step: 7150\n",
      "train: loss: 399.9224548339844 acc: 0.9303066730499268  val: loss: 593.4356079101562 acc: 0.8788976073265076\n",
      "step: 7155\n",
      "train: loss: 618.1609497070312 acc: 0.8895554542541504  val: loss: 423.76141357421875 acc: 0.9158904552459717\n",
      "step: 7160\n",
      "train: loss: 948.7261962890625 acc: 0.83967524766922  val: loss: 1149.57861328125 acc: 0.8232048749923706\n",
      "step: 7165\n",
      "train: loss: 788.482421875 acc: 0.817451000213623  val: loss: 1842.219970703125 acc: 0.7379124760627747\n",
      "step: 7170\n",
      "train: loss: 309.98974609375 acc: 0.9474095106124878  val: loss: 1200.56005859375 acc: 0.8228796720504761\n",
      "step: 7175\n",
      "train: loss: 581.1702270507812 acc: 0.8723175525665283  val: loss: 511.0478820800781 acc: 0.920265257358551\n",
      "step: 7180\n",
      "train: loss: 933.0436401367188 acc: 0.8492158055305481  val: loss: 474.77069091796875 acc: 0.9118797183036804\n",
      "step: 7185\n",
      "train: loss: 917.5733032226562 acc: 0.8825947046279907  val: loss: 1548.360595703125 acc: 0.7553895711898804\n",
      "step: 7190\n",
      "train: loss: 892.7835693359375 acc: 0.6947388052940369  val: loss: 1149.6202392578125 acc: 0.8348290920257568\n",
      "step: 7195\n",
      "train: loss: 682.3327026367188 acc: 0.8701533079147339  val: loss: 2063.012451171875 acc: 0.748839795589447\n",
      "step: 7200\n",
      "train: loss: 509.115234375 acc: 0.9134302139282227  val: loss: 1409.110107421875 acc: 0.8175891637802124\n",
      "step: 7205\n",
      "train: loss: 505.33148193359375 acc: 0.9396138787269592  val: loss: 1186.9993896484375 acc: 0.8571504354476929\n",
      "step: 7210\n",
      "train: loss: 730.133544921875 acc: 0.9096011519432068  val: loss: 620.8995971679688 acc: 0.9009358882904053\n",
      "step: 7215\n",
      "train: loss: 858.7181396484375 acc: 0.8700312376022339  val: loss: 1057.118896484375 acc: 0.8280356526374817\n",
      "step: 7220\n",
      "train: loss: 717.2689819335938 acc: 0.847490668296814  val: loss: 1406.107421875 acc: 0.7828751802444458\n",
      "step: 7225\n",
      "train: loss: 676.8131713867188 acc: 0.8871930837631226  val: loss: 843.565185546875 acc: 0.7961803674697876\n",
      "step: 7230\n",
      "train: loss: 838.5087890625 acc: 0.7836337685585022  val: loss: 1236.644287109375 acc: 0.8299838900566101\n",
      "step: 7235\n",
      "train: loss: 435.59112548828125 acc: 0.9166597723960876  val: loss: 1045.9342041015625 acc: 0.8670430183410645\n",
      "step: 7240\n",
      "train: loss: 755.0989379882812 acc: 0.85563725233078  val: loss: 1487.219970703125 acc: 0.7744574546813965\n",
      "step: 7245\n",
      "train: loss: 584.700927734375 acc: 0.8843576312065125  val: loss: 811.19775390625 acc: 0.883729100227356\n",
      "step: 7250\n",
      "train: loss: 488.61932373046875 acc: 0.9111282825469971  val: loss: 576.7740478515625 acc: 0.8990611433982849\n",
      "step: 7255\n",
      "train: loss: 697.7059326171875 acc: 0.8929457664489746  val: loss: 842.9103393554688 acc: 0.8601682782173157\n",
      "step: 7260\n",
      "train: loss: 324.8926086425781 acc: 0.9356467127799988  val: loss: 887.7053833007812 acc: 0.876704752445221\n",
      "step: 7265\n",
      "train: loss: 623.6870727539062 acc: 0.9187406897544861  val: loss: 442.18426513671875 acc: 0.9161258339881897\n",
      "step: 7270\n",
      "train: loss: 528.3947143554688 acc: 0.9250996112823486  val: loss: 1120.068359375 acc: 0.861082136631012\n",
      "step: 7275\n",
      "train: loss: 479.74285888671875 acc: 0.9320871829986572  val: loss: 2023.431640625 acc: 0.7489975094795227\n",
      "step: 7280\n",
      "train: loss: 825.4982299804688 acc: 0.8320773839950562  val: loss: 405.2655029296875 acc: 0.9291074275970459\n",
      "step: 7285\n",
      "train: loss: 583.1351928710938 acc: 0.8570141792297363  val: loss: 550.6246948242188 acc: 0.8945974111557007\n",
      "step: 7290\n",
      "train: loss: 839.0307006835938 acc: 0.8335850238800049  val: loss: 1485.0399169921875 acc: 0.8225736618041992\n",
      "step: 7295\n",
      "train: loss: 755.667236328125 acc: 0.8427709341049194  val: loss: 2017.6484375 acc: 0.7526712417602539\n",
      "step: 7300\n",
      "train: loss: 334.2915344238281 acc: 0.9248513579368591  val: loss: 979.5899047851562 acc: 0.850719153881073\n",
      "step: 7305\n",
      "train: loss: 668.3394775390625 acc: 0.8566542863845825  val: loss: 1211.913818359375 acc: 0.8334213495254517\n",
      "step: 7310\n",
      "train: loss: 722.1580200195312 acc: 0.8668222427368164  val: loss: 717.0130004882812 acc: 0.8341369032859802\n",
      "step: 7315\n",
      "train: loss: 593.8776245117188 acc: 0.8749358057975769  val: loss: 1396.1688232421875 acc: 0.814356803894043\n",
      "step: 7320\n",
      "train: loss: 855.0372314453125 acc: 0.8842858076095581  val: loss: 2147.303466796875 acc: 0.7395541667938232\n",
      "step: 7325\n",
      "train: loss: 599.9512939453125 acc: 0.9181666374206543  val: loss: 1221.3211669921875 acc: 0.8019014596939087\n",
      "step: 7330\n",
      "train: loss: 772.9200439453125 acc: 0.8886425495147705  val: loss: 1007.5169677734375 acc: 0.8514381647109985\n",
      "step: 7335\n",
      "train: loss: 403.41497802734375 acc: 0.9282092452049255  val: loss: 889.0845947265625 acc: 0.8480638861656189\n",
      "step: 7340\n",
      "train: loss: 552.2098999023438 acc: 0.7999722361564636  val: loss: 1055.9473876953125 acc: 0.8586606979370117\n",
      "step: 7345\n",
      "train: loss: 965.9327392578125 acc: 0.8569592237472534  val: loss: 1300.5943603515625 acc: 0.8448359966278076\n",
      "step: 7350\n",
      "train: loss: 491.0050048828125 acc: 0.9126233458518982  val: loss: 1686.59716796875 acc: 0.7813242673873901\n",
      "step: 7355\n",
      "train: loss: 557.2909545898438 acc: 0.9044160842895508  val: loss: 957.8634643554688 acc: 0.8698777556419373\n",
      "step: 7360\n",
      "train: loss: 482.51654052734375 acc: 0.9089369773864746  val: loss: 748.2776489257812 acc: 0.9046573042869568\n",
      "step: 7365\n",
      "train: loss: 711.440673828125 acc: 0.8476544618606567  val: loss: 1083.0523681640625 acc: 0.8191390037536621\n",
      "step: 7370\n",
      "train: loss: 773.0325317382812 acc: 0.8664451837539673  val: loss: 781.6134643554688 acc: 0.8734614849090576\n",
      "step: 7375\n",
      "train: loss: 962.0249633789062 acc: 0.811251163482666  val: loss: 1630.4908447265625 acc: 0.7387721538543701\n",
      "step: 7380\n",
      "train: loss: 1293.48876953125 acc: 0.8624334335327148  val: loss: 1073.5775146484375 acc: 0.8330192565917969\n",
      "step: 7385\n",
      "train: loss: 737.050537109375 acc: 0.870917797088623  val: loss: 2757.736572265625 acc: 0.7147926092147827\n",
      "step: 7390\n",
      "train: loss: 742.6917114257812 acc: 0.9173955917358398  val: loss: 1288.013427734375 acc: 0.8140900135040283\n",
      "step: 7395\n",
      "train: loss: 731.7096557617188 acc: 0.8607305288314819  val: loss: 1183.69970703125 acc: 0.8239037990570068\n",
      "step: 7400\n",
      "train: loss: 1005.4011840820312 acc: 0.8101829290390015  val: loss: 1064.85498046875 acc: 0.7813074588775635\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 7405\n",
      "train: loss: 730.4088134765625 acc: 0.8734869360923767  val: loss: 495.9116516113281 acc: 0.894177258014679\n",
      "step: 7410\n",
      "train: loss: 1005.1385498046875 acc: 0.7471839189529419  val: loss: 688.1776123046875 acc: 0.9146102070808411\n",
      "step: 7415\n",
      "train: loss: 644.8844604492188 acc: 0.8284263014793396  val: loss: 1045.4237060546875 acc: 0.8199201822280884\n",
      "step: 7420\n",
      "train: loss: 353.4095153808594 acc: 0.9235576391220093  val: loss: 1056.370849609375 acc: 0.8323513269424438\n",
      "step: 7425\n",
      "train: loss: 944.2337036132812 acc: 0.8160408735275269  val: loss: 892.564697265625 acc: 0.8482896089553833\n",
      "step: 7430\n",
      "train: loss: 1054.2098388671875 acc: 0.7174471616744995  val: loss: 2230.611328125 acc: 0.7650317549705505\n",
      "step: 7435\n",
      "train: loss: 692.9686279296875 acc: 0.9044331908226013  val: loss: 1036.302978515625 acc: 0.8172434568405151\n",
      "step: 7440\n",
      "train: loss: 521.6828002929688 acc: 0.9108853340148926  val: loss: 1577.7840576171875 acc: 0.7860735058784485\n",
      "step: 7445\n",
      "train: loss: 426.1725769042969 acc: 0.9214732646942139  val: loss: 1502.62451171875 acc: 0.7976799607276917\n",
      "step: 7450\n",
      "train: loss: 757.1864624023438 acc: 0.860917866230011  val: loss: 679.453369140625 acc: 0.9048242568969727\n",
      "step: 7455\n",
      "train: loss: 1113.387939453125 acc: 0.8222085237503052  val: loss: 1909.906494140625 acc: 0.7173369526863098\n",
      "step: 7460\n",
      "train: loss: 824.1961059570312 acc: 0.8588877320289612  val: loss: 1431.0987548828125 acc: 0.7970545291900635\n",
      "step: 7465\n",
      "train: loss: 548.556640625 acc: 0.9045484662055969  val: loss: 1487.2296142578125 acc: 0.7963407039642334\n",
      "step: 7470\n",
      "train: loss: 471.3197021484375 acc: 0.9318121671676636  val: loss: 1936.3001708984375 acc: 0.7313360571861267\n",
      "step: 7475\n",
      "train: loss: 1077.057373046875 acc: 0.7829242944717407  val: loss: 1440.0291748046875 acc: 0.8254565000534058\n",
      "step: 7480\n",
      "train: loss: 509.29766845703125 acc: 0.8754492402076721  val: loss: 1639.09033203125 acc: 0.7945103049278259\n",
      "step: 7485\n",
      "train: loss: 703.3862915039062 acc: 0.7974262237548828  val: loss: 2048.803466796875 acc: 0.7538711428642273\n",
      "step: 7490\n",
      "train: loss: 490.4232177734375 acc: 0.8991684913635254  val: loss: 861.9335327148438 acc: 0.8670127987861633\n",
      "step: 7495\n",
      "train: loss: 522.75634765625 acc: 0.9224950671195984  val: loss: 1162.969482421875 acc: 0.8605210781097412\n",
      "step: 7500\n",
      "train: loss: 420.87701416015625 acc: 0.9257371425628662  val: loss: 2181.742431640625 acc: 0.7799338698387146\n",
      "step: 7505\n",
      "train: loss: 490.47259521484375 acc: 0.918813943862915  val: loss: 1978.3111572265625 acc: 0.7801298499107361\n",
      "step: 7510\n",
      "train: loss: 746.5047607421875 acc: 0.8817119598388672  val: loss: 384.8023376464844 acc: 0.9236192107200623\n",
      "step: 7515\n",
      "train: loss: 804.285888671875 acc: 0.8789929151535034  val: loss: 1042.30029296875 acc: 0.8690956830978394\n",
      "step: 7520\n",
      "train: loss: 700.7052001953125 acc: 0.9197201728820801  val: loss: 1401.0546875 acc: 0.7949413061141968\n",
      "step: 7525\n",
      "train: loss: 978.794189453125 acc: 0.8817132115364075  val: loss: 1002.5714721679688 acc: 0.8266757726669312\n",
      "step: 7530\n",
      "train: loss: 962.8741455078125 acc: 0.869415283203125  val: loss: 1636.817138671875 acc: 0.796812891960144\n",
      "step: 7535\n",
      "train: loss: 859.3526611328125 acc: 0.8001135587692261  val: loss: 470.8924255371094 acc: 0.9016376733779907\n",
      "step: 7540\n",
      "train: loss: 197.38320922851562 acc: 0.9575082063674927  val: loss: 681.21337890625 acc: 0.8456090688705444\n",
      "step: 7545\n",
      "train: loss: 587.0836181640625 acc: 0.8532752990722656  val: loss: 1471.9234619140625 acc: 0.775693953037262\n",
      "step: 7550\n",
      "train: loss: 701.8688354492188 acc: 0.8803524374961853  val: loss: 1110.5667724609375 acc: 0.8219655156135559\n",
      "step: 7555\n",
      "train: loss: 448.9325256347656 acc: 0.9317681193351746  val: loss: 885.9680786132812 acc: 0.8727971911430359\n",
      "step: 7560\n",
      "train: loss: 790.6505126953125 acc: 0.9081754684448242  val: loss: 1708.9942626953125 acc: 0.7776276469230652\n",
      "step: 7565\n",
      "train: loss: 533.5526123046875 acc: 0.8752434849739075  val: loss: 349.2498474121094 acc: 0.9301623106002808\n",
      "step: 7570\n",
      "train: loss: 644.4058227539062 acc: 0.8324161767959595  val: loss: 1138.91796875 acc: 0.8322855234146118\n",
      "step: 7575\n",
      "train: loss: 639.8441772460938 acc: 0.8108518123626709  val: loss: 1951.744384765625 acc: 0.7694772481918335\n",
      "step: 7580\n",
      "train: loss: 1029.6612548828125 acc: 0.854854166507721  val: loss: 1589.7037353515625 acc: 0.7800204753875732\n",
      "step: 7585\n",
      "train: loss: 428.6815185546875 acc: 0.9169924855232239  val: loss: 1805.64599609375 acc: 0.7308744192123413\n",
      "step: 7590\n",
      "train: loss: 679.5648803710938 acc: 0.8931359648704529  val: loss: 1578.2852783203125 acc: 0.7674544453620911\n",
      "step: 7595\n",
      "train: loss: 419.7862548828125 acc: 0.9011138677597046  val: loss: 2222.889892578125 acc: 0.663750410079956\n",
      "step: 7600\n",
      "train: loss: 629.8934936523438 acc: 0.8493186235427856  val: loss: 880.9315185546875 acc: 0.8736705780029297\n",
      "step: 7605\n",
      "train: loss: 355.7016296386719 acc: 0.9089148044586182  val: loss: 920.1921997070312 acc: 0.8858482837677002\n",
      "step: 7610\n",
      "train: loss: 725.4490966796875 acc: 0.8683052062988281  val: loss: 402.03375244140625 acc: 0.9212518334388733\n",
      "step: 7615\n",
      "train: loss: 676.0521240234375 acc: 0.8762710094451904  val: loss: 1068.130126953125 acc: 0.8044025897979736\n",
      "step: 7620\n",
      "train: loss: 472.11163330078125 acc: 0.9184224605560303  val: loss: 502.0826721191406 acc: 0.9167464971542358\n",
      "step: 7625\n",
      "train: loss: 892.2151489257812 acc: 0.8563061952590942  val: loss: 917.3870239257812 acc: 0.8514165282249451\n",
      "step: 7630\n",
      "train: loss: 1042.1357421875 acc: 0.7735636234283447  val: loss: 1363.6954345703125 acc: 0.7940565943717957\n",
      "step: 7635\n",
      "train: loss: 804.706787109375 acc: 0.8304294347763062  val: loss: 439.46856689453125 acc: 0.9326892495155334\n",
      "step: 7640\n",
      "train: loss: 529.691162109375 acc: 0.9203703999519348  val: loss: 1254.2723388671875 acc: 0.8317903280258179\n",
      "step: 7645\n",
      "train: loss: 834.8155517578125 acc: 0.8676506876945496  val: loss: 939.1432495117188 acc: 0.829913854598999\n",
      "step: 7650\n",
      "train: loss: 1012.5836791992188 acc: 0.8480967879295349  val: loss: 1221.604248046875 acc: 0.8350486755371094\n",
      "step: 7655\n",
      "train: loss: 281.22320556640625 acc: 0.9557253122329712  val: loss: 230.12188720703125 acc: 0.9648258090019226\n",
      "step: 7660\n",
      "train: loss: 288.00823974609375 acc: 0.9269849061965942  val: loss: 1717.353515625 acc: 0.7934513688087463\n",
      "step: 7665\n",
      "train: loss: 409.0265197753906 acc: 0.9139730334281921  val: loss: 1126.53271484375 acc: 0.8094135522842407\n",
      "step: 7670\n",
      "train: loss: 425.1772766113281 acc: 0.9393507838249207  val: loss: 416.13037109375 acc: 0.9129763841629028\n",
      "step: 7675\n",
      "train: loss: 731.4943237304688 acc: 0.8979837894439697  val: loss: 1650.3033447265625 acc: 0.7764328122138977\n",
      "step: 7680\n",
      "train: loss: 396.8098449707031 acc: 0.9376424551010132  val: loss: 1087.703125 acc: 0.841729998588562\n",
      "step: 7685\n",
      "train: loss: 719.529296875 acc: 0.9189728498458862  val: loss: 1131.406005859375 acc: 0.8251442313194275\n",
      "step: 7690\n",
      "train: loss: 437.71234130859375 acc: 0.892092227935791  val: loss: 542.5810546875 acc: 0.928656816482544\n",
      "step: 7695\n",
      "train: loss: 588.4916381835938 acc: 0.9034568667411804  val: loss: 267.2059631347656 acc: 0.9432754516601562\n",
      "step: 7700\n",
      "train: loss: 1007.8167114257812 acc: 0.7132420539855957  val: loss: 568.0819702148438 acc: 0.9269086718559265\n",
      "step: 7705\n",
      "train: loss: 821.1541748046875 acc: 0.8624957799911499  val: loss: 1841.8319091796875 acc: 0.7878944873809814\n",
      "step: 7710\n",
      "train: loss: 336.9622497558594 acc: 0.947758674621582  val: loss: 1475.77587890625 acc: 0.8106940984725952\n",
      "step: 7715\n",
      "train: loss: 714.7020874023438 acc: 0.8660804033279419  val: loss: 1224.1436767578125 acc: 0.8284206390380859\n",
      "step: 7720\n",
      "train: loss: 903.6162109375 acc: 0.784343421459198  val: loss: 1225.1021728515625 acc: 0.8093223571777344\n",
      "step: 7725\n",
      "train: loss: 566.8226928710938 acc: 0.8816835284233093  val: loss: 831.9212036132812 acc: 0.8498967885971069\n",
      "step: 7730\n",
      "train: loss: 436.75201416015625 acc: 0.9128022193908691  val: loss: 1389.088134765625 acc: 0.8220229744911194\n",
      "step: 7735\n",
      "train: loss: 673.8355712890625 acc: 0.8583619594573975  val: loss: 608.1532592773438 acc: 0.9081619381904602\n",
      "step: 7740\n",
      "train: loss: 672.1173706054688 acc: 0.8943419456481934  val: loss: 425.5792236328125 acc: 0.9401350021362305\n",
      "step: 7745\n",
      "train: loss: 860.0130615234375 acc: 0.8635976314544678  val: loss: 465.95989990234375 acc: 0.9066814184188843\n",
      "step: 7750\n",
      "train: loss: 788.6956787109375 acc: 0.8681316375732422  val: loss: 1142.4290771484375 acc: 0.8567754626274109\n",
      "step: 7755\n",
      "train: loss: 482.42108154296875 acc: 0.9019205570220947  val: loss: 1423.0946044921875 acc: 0.7967997789382935\n",
      "step: 7760\n",
      "train: loss: 594.9380493164062 acc: 0.9015359878540039  val: loss: 1290.30322265625 acc: 0.8353674411773682\n",
      "step: 7765\n",
      "train: loss: 362.43780517578125 acc: 0.9339753985404968  val: loss: 1343.5811767578125 acc: 0.7504652142524719\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 7770\n",
      "train: loss: 687.9291381835938 acc: 0.9107044339179993  val: loss: 1112.78369140625 acc: 0.8639066219329834\n",
      "step: 7775\n",
      "train: loss: 412.5104675292969 acc: 0.9198000431060791  val: loss: 1466.488525390625 acc: 0.8160996437072754\n",
      "step: 7780\n",
      "train: loss: 467.47216796875 acc: 0.8718615770339966  val: loss: 1553.4937744140625 acc: 0.7746232748031616\n",
      "step: 7785\n",
      "train: loss: 566.151123046875 acc: 0.8715785145759583  val: loss: 1423.9560546875 acc: 0.7561089396476746\n",
      "step: 7790\n",
      "train: loss: 607.5575561523438 acc: 0.8974739909172058  val: loss: 946.250244140625 acc: 0.8558775782585144\n",
      "step: 7795\n",
      "train: loss: 807.7391967773438 acc: 0.8820348381996155  val: loss: 443.510986328125 acc: 0.9059780240058899\n",
      "step: 7800\n",
      "train: loss: 1058.6357421875 acc: 0.8030009865760803  val: loss: 1436.294677734375 acc: 0.811765193939209\n",
      "step: 7805\n",
      "train: loss: 833.6422119140625 acc: 0.8781307935714722  val: loss: 718.4822387695312 acc: 0.8643038272857666\n",
      "step: 7810\n",
      "train: loss: 534.2755737304688 acc: 0.9015200138092041  val: loss: 1735.5003662109375 acc: 0.7575948238372803\n",
      "step: 7815\n",
      "train: loss: 495.41131591796875 acc: 0.8785858154296875  val: loss: 2195.21142578125 acc: 0.7866959571838379\n",
      "step: 7820\n",
      "train: loss: 1036.242919921875 acc: 0.8122425079345703  val: loss: 1800.3564453125 acc: 0.7698854207992554\n",
      "step: 7825\n",
      "train: loss: 976.9645385742188 acc: 0.6971814632415771  val: loss: 609.2957763671875 acc: 0.8936821818351746\n",
      "step: 7830\n",
      "train: loss: 594.2003784179688 acc: 0.9157771468162537  val: loss: 902.264404296875 acc: 0.8091418743133545\n",
      "step: 7835\n",
      "train: loss: 635.474609375 acc: 0.7967497706413269  val: loss: 917.6097412109375 acc: 0.8708033561706543\n",
      "step: 7840\n",
      "train: loss: 662.2125854492188 acc: 0.8498247861862183  val: loss: 1310.39892578125 acc: 0.8321614265441895\n",
      "step: 7845\n",
      "train: loss: 571.8263549804688 acc: 0.873170793056488  val: loss: 2771.728759765625 acc: 0.7448394894599915\n",
      "step: 7850\n",
      "train: loss: 491.419921875 acc: 0.9320106506347656  val: loss: 860.3491821289062 acc: 0.8324226140975952\n",
      "step: 7855\n",
      "train: loss: 440.50885009765625 acc: 0.9260464310646057  val: loss: 723.2234497070312 acc: 0.8868221640586853\n",
      "step: 7860\n",
      "train: loss: 570.310791015625 acc: 0.9048849940299988  val: loss: 555.2139282226562 acc: 0.9027418494224548\n",
      "step: 7865\n",
      "train: loss: 1267.447998046875 acc: 0.8446005582809448  val: loss: 984.0720825195312 acc: 0.8484578132629395\n",
      "step: 7870\n",
      "train: loss: 983.0323486328125 acc: 0.8186376690864563  val: loss: 825.8047485351562 acc: 0.8781318068504333\n",
      "step: 7875\n",
      "train: loss: 736.9674682617188 acc: 0.8718597888946533  val: loss: 958.96826171875 acc: 0.868750274181366\n",
      "step: 7880\n",
      "train: loss: 704.0270385742188 acc: 0.8661671280860901  val: loss: 858.9599609375 acc: 0.8804155588150024\n",
      "step: 7885\n",
      "train: loss: 702.3460083007812 acc: 0.8869088888168335  val: loss: 828.8063354492188 acc: 0.8853024840354919\n",
      "step: 7890\n",
      "train: loss: 796.3158569335938 acc: 0.8728560209274292  val: loss: 659.7866821289062 acc: 0.8832919001579285\n",
      "step: 7895\n",
      "train: loss: 801.7137451171875 acc: 0.8460695147514343  val: loss: 1538.911865234375 acc: 0.8410589098930359\n",
      "step: 7900\n",
      "train: loss: 609.6787719726562 acc: 0.9089754819869995  val: loss: 1559.8341064453125 acc: 0.7756815552711487\n",
      "step: 7905\n",
      "train: loss: 612.6776123046875 acc: 0.8757149577140808  val: loss: 1193.9217529296875 acc: 0.8227804899215698\n",
      "step: 7910\n",
      "train: loss: 512.9412841796875 acc: 0.9389504194259644  val: loss: 975.5131225585938 acc: 0.8343695402145386\n",
      "step: 7915\n",
      "train: loss: 425.9517517089844 acc: 0.9258139729499817  val: loss: 1981.4840087890625 acc: 0.7444958686828613\n",
      "step: 7920\n",
      "train: loss: 448.6993408203125 acc: 0.8739122152328491  val: loss: 952.0961303710938 acc: 0.8523003458976746\n",
      "step: 7925\n",
      "train: loss: 883.1580810546875 acc: 0.7934014797210693  val: loss: 1796.18994140625 acc: 0.7491705417633057\n",
      "step: 7930\n",
      "train: loss: 913.3897705078125 acc: 0.7974900603294373  val: loss: 1177.1181640625 acc: 0.8358574509620667\n",
      "step: 7935\n",
      "train: loss: 682.053955078125 acc: 0.8822593092918396  val: loss: 1332.30419921875 acc: 0.8319610357284546\n",
      "step: 7940\n",
      "train: loss: 561.7412719726562 acc: 0.9041500687599182  val: loss: 1841.7437744140625 acc: 0.7585376501083374\n",
      "step: 7945\n",
      "train: loss: 554.4089965820312 acc: 0.9054853916168213  val: loss: 2151.78466796875 acc: 0.7508097887039185\n",
      "step: 7950\n",
      "train: loss: 375.84576416015625 acc: 0.9131977558135986  val: loss: 795.398193359375 acc: 0.8076363801956177\n",
      "step: 7955\n",
      "train: loss: 500.17724609375 acc: 0.9022910594940186  val: loss: 1153.3721923828125 acc: 0.7702282667160034\n",
      "step: 7960\n",
      "train: loss: 685.48779296875 acc: 0.8276798725128174  val: loss: 944.0198974609375 acc: 0.8708174824714661\n",
      "step: 7965\n",
      "train: loss: 726.6318969726562 acc: 0.8310590982437134  val: loss: 2117.39306640625 acc: 0.7382194399833679\n",
      "step: 7970\n",
      "train: loss: 816.7849731445312 acc: 0.8983196020126343  val: loss: 2188.345458984375 acc: 0.6759167909622192\n",
      "step: 7975\n",
      "train: loss: 354.5507507324219 acc: 0.9336557388305664  val: loss: 1030.9932861328125 acc: 0.8546916842460632\n",
      "step: 7980\n",
      "train: loss: 655.4774169921875 acc: 0.9079472422599792  val: loss: 1672.829833984375 acc: 0.8007020950317383\n",
      "step: 7985\n",
      "train: loss: 1013.242919921875 acc: 0.8160961270332336  val: loss: 1967.1356201171875 acc: 0.751435399055481\n",
      "step: 7990\n",
      "train: loss: 476.7799987792969 acc: 0.8823709487915039  val: loss: 998.8369750976562 acc: 0.820801317691803\n",
      "step: 7995\n",
      "train: loss: 730.8517456054688 acc: 0.8505175113677979  val: loss: 698.8055419921875 acc: 0.8694343566894531\n",
      "step: 8000\n",
      "train: loss: 925.5516967773438 acc: 0.8157581090927124  val: loss: 2125.239990234375 acc: 0.7383544445037842\n",
      "step: 8005\n",
      "train: loss: 629.8148803710938 acc: 0.8801776766777039  val: loss: 593.3453979492188 acc: 0.9085187315940857\n",
      "step: 8010\n",
      "train: loss: 657.35791015625 acc: 0.8275365829467773  val: loss: 1538.0289306640625 acc: 0.7921169996261597\n",
      "step: 8015\n",
      "train: loss: 573.1151733398438 acc: 0.8295614719390869  val: loss: 421.759521484375 acc: 0.918947696685791\n",
      "step: 8020\n",
      "train: loss: 530.28857421875 acc: 0.8953356146812439  val: loss: 463.8955078125 acc: 0.934417188167572\n",
      "step: 8025\n",
      "train: loss: 355.1861877441406 acc: 0.9018296003341675  val: loss: 3754.075927734375 acc: 0.60581374168396\n",
      "step: 8030\n",
      "train: loss: 539.3204345703125 acc: 0.9302334189414978  val: loss: 1434.4117431640625 acc: 0.8093709945678711\n",
      "step: 8035\n",
      "train: loss: 317.7421569824219 acc: 0.9100520014762878  val: loss: 1791.3499755859375 acc: 0.7955394387245178\n",
      "step: 8040\n",
      "train: loss: 720.8427124023438 acc: 0.8486190438270569  val: loss: 1044.447021484375 acc: 0.8301059603691101\n",
      "step: 8045\n",
      "train: loss: 774.0384521484375 acc: 0.9016852974891663  val: loss: 963.3571166992188 acc: 0.8673603534698486\n",
      "step: 8050\n",
      "train: loss: 1102.737060546875 acc: 0.8101303577423096  val: loss: 1096.3182373046875 acc: 0.8472066521644592\n",
      "step: 8055\n",
      "train: loss: 1008.7244262695312 acc: 0.8576309084892273  val: loss: 1873.2823486328125 acc: 0.8095795512199402\n",
      "step: 8060\n",
      "train: loss: 957.9315185546875 acc: 0.8602074384689331  val: loss: 1395.3145751953125 acc: 0.763545036315918\n",
      "step: 8065\n",
      "train: loss: 1068.6007080078125 acc: 0.7499801516532898  val: loss: 1194.17333984375 acc: 0.8092695474624634\n",
      "step: 8070\n",
      "train: loss: 325.1029052734375 acc: 0.9201124310493469  val: loss: 912.7801513671875 acc: 0.8190498948097229\n",
      "step: 8075\n",
      "train: loss: 628.328369140625 acc: 0.8811233639717102  val: loss: 964.8386840820312 acc: 0.8042162656784058\n",
      "step: 8080\n",
      "train: loss: 551.9650268554688 acc: 0.87727952003479  val: loss: 737.9916381835938 acc: 0.8459400534629822\n",
      "step: 8085\n",
      "train: loss: 669.9678344726562 acc: 0.8876714706420898  val: loss: 335.3515930175781 acc: 0.9283906817436218\n",
      "step: 8090\n",
      "train: loss: 408.4994812011719 acc: 0.950928807258606  val: loss: 1450.1011962890625 acc: 0.7610335350036621\n",
      "step: 8095\n",
      "train: loss: 363.20452880859375 acc: 0.9322364330291748  val: loss: 2220.47216796875 acc: 0.7375263571739197\n",
      "step: 8100\n",
      "train: loss: 807.7994384765625 acc: 0.8872796297073364  val: loss: 1336.41748046875 acc: 0.8556158542633057\n",
      "step: 8105\n",
      "train: loss: 1217.739013671875 acc: 0.844282865524292  val: loss: 1162.736572265625 acc: 0.821031928062439\n",
      "step: 8110\n",
      "train: loss: 697.2918701171875 acc: 0.8452383875846863  val: loss: 1051.8182373046875 acc: 0.8348910212516785\n",
      "step: 8115\n",
      "train: loss: 653.61181640625 acc: 0.9156861901283264  val: loss: 778.3120727539062 acc: 0.8879217505455017\n",
      "step: 8120\n",
      "train: loss: 800.9124145507812 acc: 0.8337689638137817  val: loss: 2150.30078125 acc: 0.7999122142791748\n",
      "step: 8125\n",
      "train: loss: 1047.9842529296875 acc: 0.6699028015136719  val: loss: 1826.07763671875 acc: 0.765954852104187\n",
      "step: 8130\n",
      "train: loss: 518.8602905273438 acc: 0.880303144454956  val: loss: 1635.1207275390625 acc: 0.7787133455276489\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 8135\n",
      "train: loss: 293.1299743652344 acc: 0.9029143452644348  val: loss: 1027.396484375 acc: 0.8023059964179993\n",
      "step: 8140\n",
      "train: loss: 532.9220581054688 acc: 0.8647530674934387  val: loss: 1897.421142578125 acc: 0.7601195573806763\n",
      "step: 8145\n",
      "train: loss: 433.2577209472656 acc: 0.9306731224060059  val: loss: 995.0195922851562 acc: 0.8385707139968872\n",
      "step: 8150\n",
      "train: loss: 465.1715087890625 acc: 0.9344269633293152  val: loss: 2358.334716796875 acc: 0.7170814275741577\n",
      "step: 8155\n",
      "train: loss: 568.4126586914062 acc: 0.9169361591339111  val: loss: 547.5128784179688 acc: 0.9304404258728027\n",
      "step: 8160\n",
      "train: loss: 1020.5686645507812 acc: 0.852405309677124  val: loss: 1674.0772705078125 acc: 0.7534398436546326\n",
      "step: 8165\n",
      "train: loss: 524.3760986328125 acc: 0.897233247756958  val: loss: 1226.981201171875 acc: 0.8489983081817627\n",
      "step: 8170\n",
      "train: loss: 660.046875 acc: 0.8648101091384888  val: loss: 1274.7274169921875 acc: 0.8101916313171387\n",
      "step: 8175\n",
      "train: loss: 649.9783325195312 acc: 0.8863733410835266  val: loss: 420.21429443359375 acc: 0.9176638722419739\n",
      "step: 8180\n",
      "train: loss: 569.3923950195312 acc: 0.8849664926528931  val: loss: 623.8893432617188 acc: 0.9077714681625366\n",
      "step: 8185\n",
      "train: loss: 418.91339111328125 acc: 0.9300212860107422  val: loss: 968.6316528320312 acc: 0.8870178461074829\n",
      "step: 8190\n",
      "train: loss: 548.8937377929688 acc: 0.9131022691726685  val: loss: 669.9214477539062 acc: 0.8965458869934082\n",
      "step: 8195\n",
      "train: loss: 481.092529296875 acc: 0.8755912184715271  val: loss: 1465.2855224609375 acc: 0.7525051832199097\n",
      "step: 8200\n",
      "train: loss: 478.9295654296875 acc: 0.9011560082435608  val: loss: 536.0263061523438 acc: 0.9012612104415894\n",
      "step: 8205\n",
      "train: loss: 754.2021484375 acc: 0.8887359499931335  val: loss: 1034.244140625 acc: 0.796673595905304\n",
      "step: 8210\n",
      "train: loss: 489.966064453125 acc: 0.9117456078529358  val: loss: 1643.6512451171875 acc: 0.7983565926551819\n",
      "step: 8215\n",
      "train: loss: 462.8603820800781 acc: 0.9000535011291504  val: loss: 1048.5626220703125 acc: 0.8317626714706421\n",
      "step: 8220\n",
      "train: loss: 827.121826171875 acc: 0.8897532224655151  val: loss: 1879.375732421875 acc: 0.7829269766807556\n",
      "step: 8225\n",
      "train: loss: 909.2449340820312 acc: 0.8572362065315247  val: loss: 1401.0728759765625 acc: 0.8127678632736206\n",
      "step: 8230\n",
      "train: loss: 919.3231201171875 acc: 0.8055704832077026  val: loss: 677.2455444335938 acc: 0.9001529812812805\n",
      "step: 8235\n",
      "train: loss: 391.98480224609375 acc: 0.9063352942466736  val: loss: 554.232421875 acc: 0.910007655620575\n",
      "step: 8240\n",
      "train: loss: 381.95928955078125 acc: 0.894637942314148  val: loss: 987.8011474609375 acc: 0.8545609712600708\n",
      "step: 8245\n",
      "train: loss: 873.5385131835938 acc: 0.8441472053527832  val: loss: 762.23876953125 acc: 0.8807708621025085\n",
      "step: 8250\n",
      "train: loss: 532.1749877929688 acc: 0.9022467136383057  val: loss: 1429.3779296875 acc: 0.8199300765991211\n",
      "step: 8255\n",
      "train: loss: 594.7703247070312 acc: 0.8951985836029053  val: loss: 914.2704467773438 acc: 0.8742169737815857\n",
      "step: 8260\n",
      "train: loss: 530.1427612304688 acc: 0.8826252818107605  val: loss: 1090.617919921875 acc: 0.8505643606185913\n",
      "step: 8265\n",
      "train: loss: 420.5279541015625 acc: 0.9253519177436829  val: loss: 881.7611083984375 acc: 0.8734055757522583\n",
      "step: 8270\n",
      "train: loss: 960.4268798828125 acc: 0.837700605392456  val: loss: 504.5717468261719 acc: 0.8943350315093994\n",
      "step: 8275\n",
      "train: loss: 1045.9903564453125 acc: 0.8106725811958313  val: loss: 417.9888000488281 acc: 0.9119868874549866\n",
      "step: 8280\n",
      "train: loss: 845.4668579101562 acc: 0.9046157002449036  val: loss: 1360.76171875 acc: 0.8118295669555664\n",
      "step: 8285\n",
      "train: loss: 898.3029174804688 acc: 0.8463857173919678  val: loss: 1252.30126953125 acc: 0.8290547132492065\n",
      "step: 8290\n",
      "train: loss: 1016.576904296875 acc: 0.8386967182159424  val: loss: 1039.6112060546875 acc: 0.8383305668830872\n",
      "step: 8295\n",
      "train: loss: 325.4874572753906 acc: 0.941217303276062  val: loss: 1673.779541015625 acc: 0.777228832244873\n",
      "step: 8300\n",
      "train: loss: 887.7723388671875 acc: 0.7994020581245422  val: loss: 322.50201416015625 acc: 0.9399653077125549\n",
      "step: 8305\n",
      "train: loss: 372.8836975097656 acc: 0.9323492646217346  val: loss: 949.126220703125 acc: 0.8797679543495178\n",
      "step: 8310\n",
      "train: loss: 807.5613403320312 acc: 0.8212947845458984  val: loss: 1142.2528076171875 acc: 0.836164116859436\n",
      "step: 8315\n",
      "train: loss: 989.5474243164062 acc: 0.8142633438110352  val: loss: 1245.056396484375 acc: 0.8397837281227112\n",
      "step: 8320\n",
      "train: loss: 313.14971923828125 acc: 0.912003219127655  val: loss: 930.4837036132812 acc: 0.8567517399787903\n",
      "step: 8325\n",
      "train: loss: 711.4567260742188 acc: 0.9172506332397461  val: loss: 711.8841552734375 acc: 0.893237292766571\n",
      "step: 8330\n",
      "train: loss: 809.1870727539062 acc: 0.8895364999771118  val: loss: 658.6603393554688 acc: 0.897830605506897\n",
      "step: 8335\n",
      "train: loss: 739.3553466796875 acc: 0.8164085149765015  val: loss: 2012.0635986328125 acc: 0.7694238424301147\n",
      "step: 8340\n",
      "train: loss: 937.3521728515625 acc: 0.8583223819732666  val: loss: 999.9073486328125 acc: 0.8229577541351318\n",
      "step: 8345\n",
      "train: loss: 633.6223754882812 acc: 0.8926702737808228  val: loss: 957.9752197265625 acc: 0.8593140840530396\n",
      "step: 8350\n",
      "train: loss: 1005.6432495117188 acc: 0.7935434579849243  val: loss: 870.9296264648438 acc: 0.8006333112716675\n",
      "step: 8355\n",
      "train: loss: 582.7058715820312 acc: 0.88130122423172  val: loss: 1077.3690185546875 acc: 0.8383669853210449\n",
      "step: 8360\n",
      "train: loss: 969.82568359375 acc: 0.8281065225601196  val: loss: 956.0631713867188 acc: 0.8746809959411621\n",
      "step: 8365\n",
      "train: loss: 486.0271911621094 acc: 0.8842803835868835  val: loss: 1240.6143798828125 acc: 0.803230345249176\n",
      "step: 8370\n",
      "train: loss: 136.40219116210938 acc: 0.9537498354911804  val: loss: 1858.3060302734375 acc: 0.7234224081039429\n",
      "step: 8375\n",
      "train: loss: 240.4684600830078 acc: 0.9389393925666809  val: loss: 1331.995849609375 acc: 0.8068055510520935\n",
      "step: 8380\n",
      "train: loss: 607.256103515625 acc: 0.8675814270973206  val: loss: 2422.2998046875 acc: 0.7022057175636292\n",
      "step: 8385\n",
      "train: loss: 462.2244873046875 acc: 0.9100807309150696  val: loss: 1269.957763671875 acc: 0.7958610653877258\n",
      "step: 8390\n",
      "train: loss: 816.1105346679688 acc: 0.8729723691940308  val: loss: 1032.364013671875 acc: 0.8607840538024902\n",
      "step: 8395\n",
      "train: loss: 649.1190795898438 acc: 0.8367364406585693  val: loss: 273.81793212890625 acc: 0.9466592669487\n",
      "step: 8400\n",
      "train: loss: 783.8485107421875 acc: 0.8252666592597961  val: loss: 952.7379760742188 acc: 0.8070275783538818\n",
      "step: 8405\n",
      "train: loss: 612.9712524414062 acc: 0.8455683588981628  val: loss: 1821.1363525390625 acc: 0.7865678071975708\n",
      "step: 8410\n",
      "train: loss: 1141.8719482421875 acc: 0.8202362060546875  val: loss: 639.5913696289062 acc: 0.8894055485725403\n",
      "step: 8415\n",
      "train: loss: 698.5172729492188 acc: 0.895226240158081  val: loss: 654.477294921875 acc: 0.9007782340049744\n",
      "step: 8420\n",
      "train: loss: 654.2192993164062 acc: 0.8875754475593567  val: loss: 406.13812255859375 acc: 0.932083010673523\n",
      "step: 8425\n",
      "train: loss: 541.094970703125 acc: 0.8986387252807617  val: loss: 559.0968627929688 acc: 0.9042941927909851\n",
      "step: 8430\n",
      "train: loss: 441.29791259765625 acc: 0.9319323897361755  val: loss: 767.9774169921875 acc: 0.8926991820335388\n",
      "step: 8435\n",
      "train: loss: 464.4987487792969 acc: 0.9055370688438416  val: loss: 993.8375854492188 acc: 0.8442473411560059\n",
      "step: 8440\n",
      "train: loss: 538.6527709960938 acc: 0.920059323310852  val: loss: 1749.556640625 acc: 0.7206198573112488\n",
      "step: 8445\n",
      "train: loss: 874.803955078125 acc: 0.8587230443954468  val: loss: 1466.6070556640625 acc: 0.8313713669776917\n",
      "step: 8450\n",
      "train: loss: 1076.119384765625 acc: 0.8059155344963074  val: loss: 1506.343994140625 acc: 0.7981402277946472\n",
      "step: 8455\n",
      "train: loss: 313.80657958984375 acc: 0.9415265321731567  val: loss: 1052.334716796875 acc: 0.8140082359313965\n",
      "step: 8460\n",
      "train: loss: 694.6885986328125 acc: 0.8296449780464172  val: loss: 1013.8250732421875 acc: 0.8595054149627686\n",
      "step: 8465\n",
      "train: loss: 1066.2674560546875 acc: 0.7729983329772949  val: loss: 640.5248413085938 acc: 0.8859851360321045\n",
      "step: 8470\n",
      "train: loss: 817.690185546875 acc: 0.8787779211997986  val: loss: 1732.2919921875 acc: 0.7676399946212769\n",
      "step: 8475\n",
      "train: loss: 398.0868835449219 acc: 0.9393420219421387  val: loss: 928.509521484375 acc: 0.8648775219917297\n",
      "step: 8480\n",
      "train: loss: 695.7515869140625 acc: 0.868624210357666  val: loss: 1518.99169921875 acc: 0.7907729744911194\n",
      "step: 8485\n",
      "train: loss: 817.580322265625 acc: 0.8505626320838928  val: loss: 1041.055419921875 acc: 0.8457930088043213\n",
      "step: 8490\n",
      "train: loss: 336.0209045410156 acc: 0.9008182883262634  val: loss: 596.1229858398438 acc: 0.8839216828346252\n",
      "step: 8495\n",
      "train: loss: 252.79922485351562 acc: 0.9557591080665588  val: loss: 1294.145751953125 acc: 0.7649150490760803\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 8500\n",
      "train: loss: 687.704833984375 acc: 0.900650143623352  val: loss: 1409.8101806640625 acc: 0.7983688116073608\n",
      "step: 8505\n",
      "train: loss: 408.391845703125 acc: 0.920957088470459  val: loss: 836.69482421875 acc: 0.8565846681594849\n",
      "step: 8510\n",
      "train: loss: 776.7756958007812 acc: 0.884085476398468  val: loss: 962.3833618164062 acc: 0.8789849281311035\n",
      "step: 8515\n",
      "train: loss: 648.1157836914062 acc: 0.9117481112480164  val: loss: 2023.659423828125 acc: 0.7843455076217651\n",
      "step: 8520\n",
      "train: loss: 615.33642578125 acc: 0.7887818217277527  val: loss: 1269.32373046875 acc: 0.802931547164917\n",
      "step: 8525\n",
      "train: loss: 988.6300048828125 acc: 0.8734413385391235  val: loss: 1350.7132568359375 acc: 0.775631844997406\n",
      "step: 8530\n",
      "train: loss: 895.8613891601562 acc: 0.8690325617790222  val: loss: 823.646728515625 acc: 0.8963270783424377\n",
      "step: 8535\n",
      "train: loss: 533.6116333007812 acc: 0.9120465517044067  val: loss: 1319.608642578125 acc: 0.8578731417655945\n",
      "step: 8540\n",
      "train: loss: 700.94140625 acc: 0.8860021233558655  val: loss: 1185.281494140625 acc: 0.786689043045044\n",
      "step: 8545\n",
      "train: loss: 731.451904296875 acc: 0.8179870247840881  val: loss: 1625.9447021484375 acc: 0.800505518913269\n",
      "step: 8550\n",
      "train: loss: 507.3475646972656 acc: 0.8702843189239502  val: loss: 1231.7232666015625 acc: 0.7958064675331116\n",
      "step: 8555\n",
      "train: loss: 431.6876220703125 acc: 0.9164440631866455  val: loss: 510.03680419921875 acc: 0.9057090282440186\n",
      "step: 8560\n",
      "train: loss: 742.12060546875 acc: 0.8972234129905701  val: loss: 380.8836364746094 acc: 0.9265090227127075\n",
      "step: 8565\n",
      "train: loss: 288.5545349121094 acc: 0.9543054699897766  val: loss: 1197.61279296875 acc: 0.8284485340118408\n",
      "step: 8570\n",
      "train: loss: 594.9580078125 acc: 0.9247622489929199  val: loss: 556.0799560546875 acc: 0.9035639762878418\n",
      "step: 8575\n",
      "train: loss: 573.40771484375 acc: 0.8899849057197571  val: loss: 1026.025146484375 acc: 0.8625237941741943\n",
      "step: 8580\n",
      "train: loss: 1266.988037109375 acc: 0.7768925428390503  val: loss: 511.321533203125 acc: 0.927631676197052\n",
      "step: 8585\n",
      "train: loss: 713.0886840820312 acc: 0.8599599599838257  val: loss: 1208.22998046875 acc: 0.8046684265136719\n",
      "step: 8590\n",
      "train: loss: 513.1421508789062 acc: 0.9185375571250916  val: loss: 1128.4169921875 acc: 0.8302910327911377\n",
      "step: 8595\n",
      "train: loss: 690.8743286132812 acc: 0.7921592593193054  val: loss: 1272.43359375 acc: 0.8077248930931091\n",
      "step: 8600\n",
      "train: loss: 464.5120849609375 acc: 0.917273759841919  val: loss: 475.1517333984375 acc: 0.8767927885055542\n",
      "step: 8605\n",
      "train: loss: 620.6168823242188 acc: 0.8819502592086792  val: loss: 483.1902160644531 acc: 0.9191896319389343\n",
      "step: 8610\n",
      "train: loss: 413.1988830566406 acc: 0.892691433429718  val: loss: 1281.229736328125 acc: 0.7554488182067871\n",
      "step: 8615\n",
      "train: loss: 445.10906982421875 acc: 0.9264575242996216  val: loss: 1963.7813720703125 acc: 0.7962597012519836\n",
      "step: 8620\n",
      "train: loss: 1221.201171875 acc: 0.8522647619247437  val: loss: 1216.2408447265625 acc: 0.7703837156295776\n",
      "step: 8625\n",
      "train: loss: 608.408203125 acc: 0.9017772078514099  val: loss: 1157.19677734375 acc: 0.7892624735832214\n",
      "step: 8630\n",
      "train: loss: 652.7901000976562 acc: 0.8630164861679077  val: loss: 1385.1163330078125 acc: 0.8045135736465454\n",
      "step: 8635\n",
      "train: loss: 813.39599609375 acc: 0.8601686954498291  val: loss: 418.79266357421875 acc: 0.9280946850776672\n",
      "step: 8640\n",
      "train: loss: 483.17987060546875 acc: 0.8853697180747986  val: loss: 1939.4820556640625 acc: 0.7508464455604553\n",
      "step: 8645\n",
      "train: loss: 805.5679321289062 acc: 0.8043612241744995  val: loss: 1966.6014404296875 acc: 0.7810665965080261\n",
      "step: 8650\n",
      "train: loss: 383.1371154785156 acc: 0.9535688757896423  val: loss: 1957.216552734375 acc: 0.8048573732376099\n",
      "step: 8655\n",
      "train: loss: 418.0421447753906 acc: 0.9192276000976562  val: loss: 410.1529235839844 acc: 0.9266330003738403\n",
      "step: 8660\n",
      "train: loss: 251.59828186035156 acc: 0.9494569897651672  val: loss: 605.3330078125 acc: 0.9095009565353394\n",
      "step: 8665\n",
      "train: loss: 270.01300048828125 acc: 0.9407247304916382  val: loss: 1844.913330078125 acc: 0.7508262395858765\n",
      "step: 8670\n",
      "train: loss: 559.311279296875 acc: 0.8836484551429749  val: loss: 474.0819396972656 acc: 0.9279237389564514\n",
      "step: 8675\n",
      "train: loss: 409.66717529296875 acc: 0.9226037263870239  val: loss: 1426.3157958984375 acc: 0.791578471660614\n",
      "step: 8680\n",
      "train: loss: 650.6212768554688 acc: 0.9003651738166809  val: loss: 1673.60107421875 acc: 0.8262238502502441\n",
      "step: 8685\n",
      "train: loss: 746.8916015625 acc: 0.9167194962501526  val: loss: 730.2578735351562 acc: 0.8847759962081909\n",
      "step: 8690\n",
      "train: loss: 1101.448974609375 acc: 0.8126239776611328  val: loss: 1027.0345458984375 acc: 0.8223655819892883\n",
      "step: 8695\n",
      "train: loss: 465.9350891113281 acc: 0.8657535314559937  val: loss: 742.263427734375 acc: 0.8870364427566528\n",
      "step: 8700\n",
      "train: loss: 556.6997680664062 acc: 0.9048771262168884  val: loss: 2122.716064453125 acc: 0.7498495578765869\n",
      "step: 8705\n",
      "train: loss: 497.734619140625 acc: 0.8297693729400635  val: loss: 1687.6639404296875 acc: 0.766424834728241\n",
      "step: 8710\n",
      "train: loss: 444.203369140625 acc: 0.9379123449325562  val: loss: 506.4285888671875 acc: 0.9060935974121094\n",
      "step: 8715\n",
      "train: loss: 371.8291931152344 acc: 0.9250847697257996  val: loss: 1767.2554931640625 acc: 0.7788254022598267\n",
      "step: 8720\n",
      "train: loss: 486.8250732421875 acc: 0.9252027869224548  val: loss: 982.7940063476562 acc: 0.8601976633071899\n",
      "step: 8725\n",
      "train: loss: 779.9370727539062 acc: 0.8252110481262207  val: loss: 1639.1318359375 acc: 0.7175024151802063\n",
      "step: 8730\n",
      "train: loss: 513.2432250976562 acc: 0.9071821570396423  val: loss: 965.1467895507812 acc: 0.8354493379592896\n",
      "step: 8735\n",
      "train: loss: 724.0537109375 acc: 0.8766946196556091  val: loss: 770.1880493164062 acc: 0.8991069793701172\n",
      "step: 8740\n",
      "train: loss: 629.1427001953125 acc: 0.8939487934112549  val: loss: 1794.5146484375 acc: 0.7534857392311096\n",
      "step: 8745\n",
      "train: loss: 1243.8128662109375 acc: 0.8589531779289246  val: loss: 1777.490234375 acc: 0.7412875294685364\n",
      "step: 8750\n",
      "train: loss: 804.6375122070312 acc: 0.781563937664032  val: loss: 1220.53564453125 acc: 0.866599977016449\n",
      "step: 8755\n",
      "train: loss: 715.3824462890625 acc: 0.8801805377006531  val: loss: 1944.2989501953125 acc: 0.7690677046775818\n",
      "step: 8760\n",
      "train: loss: 802.9400024414062 acc: 0.8642424941062927  val: loss: 1608.752197265625 acc: 0.8182138204574585\n",
      "step: 8765\n",
      "train: loss: 727.9752197265625 acc: 0.8992975950241089  val: loss: 1420.069091796875 acc: 0.8194556832313538\n",
      "step: 8770\n",
      "train: loss: 676.2804565429688 acc: 0.9068439602851868  val: loss: 1423.688232421875 acc: 0.7665807008743286\n",
      "step: 8775\n",
      "train: loss: 377.88134765625 acc: 0.898321807384491  val: loss: 2805.513427734375 acc: 0.7384300231933594\n",
      "step: 8780\n",
      "train: loss: 777.8441772460938 acc: 0.8585543632507324  val: loss: 1566.3831787109375 acc: 0.8048325181007385\n",
      "step: 8785\n",
      "train: loss: 923.1956176757812 acc: 0.8165040016174316  val: loss: 889.8980102539062 acc: 0.8502238392829895\n",
      "step: 8790\n",
      "train: loss: 458.77313232421875 acc: 0.9204527139663696  val: loss: 636.2779541015625 acc: 0.8785669207572937\n",
      "step: 8795\n",
      "train: loss: 566.8395385742188 acc: 0.877572774887085  val: loss: 966.6724853515625 acc: 0.7737303972244263\n",
      "step: 8800\n",
      "train: loss: 361.2201232910156 acc: 0.9347036480903625  val: loss: 1589.35986328125 acc: 0.7556455731391907\n",
      "step: 8805\n",
      "train: loss: 530.99072265625 acc: 0.90134596824646  val: loss: 959.3573608398438 acc: 0.8706362247467041\n",
      "step: 8810\n",
      "train: loss: 615.41943359375 acc: 0.9040400981903076  val: loss: 1659.84326171875 acc: 0.8061039447784424\n",
      "step: 8815\n",
      "train: loss: 586.9970703125 acc: 0.8920432925224304  val: loss: 2232.08447265625 acc: 0.7518947124481201\n",
      "step: 8820\n",
      "train: loss: 981.6043090820312 acc: 0.788567304611206  val: loss: 1838.70263671875 acc: 0.7367954254150391\n",
      "step: 8825\n",
      "train: loss: 605.357177734375 acc: 0.8996927738189697  val: loss: 1865.2891845703125 acc: 0.7590335011482239\n",
      "step: 8830\n",
      "train: loss: 629.1226806640625 acc: 0.8336250185966492  val: loss: 880.9515380859375 acc: 0.8475693464279175\n",
      "step: 8835\n",
      "train: loss: 794.11279296875 acc: 0.8546947240829468  val: loss: 511.6064758300781 acc: 0.9265058040618896\n",
      "step: 8840\n",
      "train: loss: 684.10693359375 acc: 0.855566143989563  val: loss: 731.7783813476562 acc: 0.8853352069854736\n",
      "step: 8845\n",
      "train: loss: 438.9524841308594 acc: 0.9057738184928894  val: loss: 1148.882568359375 acc: 0.7951353192329407\n",
      "step: 8850\n",
      "train: loss: 508.2793273925781 acc: 0.9006291627883911  val: loss: 1169.999267578125 acc: 0.8571053743362427\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 8855\n",
      "train: loss: 429.2545166015625 acc: 0.90177321434021  val: loss: 1856.115478515625 acc: 0.7406032085418701\n",
      "step: 8860\n",
      "train: loss: 803.7786254882812 acc: 0.8673467636108398  val: loss: 898.0857543945312 acc: 0.8932377696037292\n",
      "step: 8865\n",
      "train: loss: 766.498779296875 acc: 0.8710805177688599  val: loss: 2163.07080078125 acc: 0.7461322546005249\n",
      "step: 8870\n",
      "train: loss: 723.1231689453125 acc: 0.8639578819274902  val: loss: 577.04931640625 acc: 0.8906832933425903\n",
      "step: 8875\n",
      "train: loss: 566.288330078125 acc: 0.8984544277191162  val: loss: 701.6502075195312 acc: 0.8537136316299438\n",
      "step: 8880\n",
      "train: loss: 685.7084350585938 acc: 0.8112854361534119  val: loss: 2677.42626953125 acc: 0.7110626697540283\n",
      "step: 8885\n",
      "train: loss: 728.3138427734375 acc: 0.8881998658180237  val: loss: 2170.997802734375 acc: 0.7253420948982239\n",
      "step: 8890\n",
      "train: loss: 501.41632080078125 acc: 0.9126753211021423  val: loss: 946.3541870117188 acc: 0.8454703092575073\n",
      "step: 8895\n",
      "train: loss: 579.8384399414062 acc: 0.890937864780426  val: loss: 714.3377075195312 acc: 0.8583621382713318\n",
      "step: 8900\n",
      "train: loss: 586.2113037109375 acc: 0.8910218477249146  val: loss: 1144.154541015625 acc: 0.7705228328704834\n",
      "step: 8905\n",
      "train: loss: 520.76611328125 acc: 0.8918060660362244  val: loss: 513.5264892578125 acc: 0.9059838652610779\n",
      "step: 8910\n",
      "train: loss: 377.020263671875 acc: 0.9326712489128113  val: loss: 748.5316772460938 acc: 0.9059575796127319\n",
      "step: 8915\n",
      "train: loss: 693.9939575195312 acc: 0.8693416118621826  val: loss: 819.5668334960938 acc: 0.8855089545249939\n",
      "step: 8920\n",
      "train: loss: 540.5938720703125 acc: 0.9254456758499146  val: loss: 1785.0604248046875 acc: 0.7865067720413208\n",
      "step: 8925\n",
      "train: loss: 692.1881103515625 acc: 0.8841902613639832  val: loss: 458.35467529296875 acc: 0.9066270589828491\n",
      "step: 8930\n",
      "train: loss: 1058.0252685546875 acc: 0.7900530099868774  val: loss: 719.92822265625 acc: 0.8729411959648132\n",
      "step: 8935\n",
      "train: loss: 757.2093505859375 acc: 0.8611902594566345  val: loss: 2134.259521484375 acc: 0.7460522055625916\n",
      "step: 8940\n",
      "train: loss: 800.7862548828125 acc: 0.8837112784385681  val: loss: 363.8910217285156 acc: 0.9399251937866211\n",
      "step: 8945\n",
      "train: loss: 1181.911376953125 acc: 0.8499151468276978  val: loss: 806.0289916992188 acc: 0.8537312746047974\n",
      "step: 8950\n",
      "train: loss: 399.46856689453125 acc: 0.9126253128051758  val: loss: 891.4320678710938 acc: 0.8620771169662476\n",
      "step: 8955\n",
      "train: loss: 393.28948974609375 acc: 0.9252667427062988  val: loss: 835.1443481445312 acc: 0.8917810320854187\n",
      "step: 8960\n",
      "train: loss: 633.3256225585938 acc: 0.7912044525146484  val: loss: 1285.5169677734375 acc: 0.760744571685791\n",
      "step: 8965\n",
      "train: loss: 567.4424438476562 acc: 0.8959435224533081  val: loss: 566.6414184570312 acc: 0.8923829197883606\n",
      "step: 8970\n",
      "train: loss: 632.4170532226562 acc: 0.9078124761581421  val: loss: 1177.0064697265625 acc: 0.8376283645629883\n",
      "step: 8975\n",
      "train: loss: 286.14447021484375 acc: 0.9649105072021484  val: loss: 1424.47998046875 acc: 0.7779309153556824\n",
      "step: 8980\n",
      "train: loss: 432.6308898925781 acc: 0.8974648118019104  val: loss: 2672.433837890625 acc: 0.667260468006134\n",
      "step: 8985\n",
      "train: loss: 441.1935119628906 acc: 0.9284907579421997  val: loss: 872.3865966796875 acc: 0.8949998021125793\n",
      "step: 8990\n",
      "train: loss: 833.0377807617188 acc: 0.8644611835479736  val: loss: 1317.0814208984375 acc: 0.8069816827774048\n",
      "step: 8995\n",
      "train: loss: 725.8292846679688 acc: 0.8698457479476929  val: loss: 807.6895751953125 acc: 0.8975377082824707\n",
      "step: 9000\n",
      "train: loss: 869.9274291992188 acc: 0.8442517518997192  val: loss: 434.39251708984375 acc: 0.9067217111587524\n",
      "step: 9005\n",
      "train: loss: 490.5273742675781 acc: 0.8775624632835388  val: loss: 655.9157104492188 acc: 0.8982030153274536\n",
      "step: 9010\n",
      "train: loss: 340.65509033203125 acc: 0.9169567227363586  val: loss: 935.8051147460938 acc: 0.8192795515060425\n",
      "step: 9015\n",
      "train: loss: 788.2191772460938 acc: 0.8573958277702332  val: loss: 1398.4207763671875 acc: 0.8154818415641785\n",
      "step: 9020\n",
      "train: loss: 555.4722900390625 acc: 0.8533815145492554  val: loss: 639.3843994140625 acc: 0.7962173223495483\n",
      "step: 9025\n",
      "train: loss: 669.8941650390625 acc: 0.8607932925224304  val: loss: 1694.8212890625 acc: 0.8013805150985718\n",
      "step: 9030\n",
      "train: loss: 333.5589294433594 acc: 0.9370101094245911  val: loss: 1436.404052734375 acc: 0.7783045172691345\n",
      "step: 9035\n",
      "train: loss: 621.114013671875 acc: 0.9032155871391296  val: loss: 1323.1671142578125 acc: 0.82857346534729\n",
      "step: 9040\n",
      "train: loss: 560.02880859375 acc: 0.9319724440574646  val: loss: 671.281982421875 acc: 0.9302718043327332\n",
      "step: 9045\n",
      "train: loss: 748.6197509765625 acc: 0.9027538895606995  val: loss: 1548.3463134765625 acc: 0.7882974147796631\n",
      "step: 9050\n",
      "train: loss: 1027.8553466796875 acc: 0.8319793343544006  val: loss: 1182.9012451171875 acc: 0.8123626708984375\n",
      "step: 9055\n",
      "train: loss: 1134.8055419921875 acc: 0.7364130616188049  val: loss: 1198.11181640625 acc: 0.799949049949646\n",
      "step: 9060\n",
      "train: loss: 598.659912109375 acc: 0.8698586225509644  val: loss: 681.469970703125 acc: 0.902635931968689\n",
      "step: 9065\n",
      "train: loss: 610.9854125976562 acc: 0.8915405869483948  val: loss: 1523.5096435546875 acc: 0.7817277908325195\n",
      "step: 9070\n",
      "train: loss: 1884.509033203125 acc: 0.7365087270736694  val: loss: 1020.2744750976562 acc: 0.8344608545303345\n",
      "step: 9075\n",
      "train: loss: 313.0691833496094 acc: 0.9375027418136597  val: loss: 1909.59521484375 acc: 0.7933596968650818\n",
      "step: 9080\n",
      "train: loss: 610.9839477539062 acc: 0.8995769023895264  val: loss: 1505.3076171875 acc: 0.8114812970161438\n",
      "step: 9085\n",
      "train: loss: 490.3714294433594 acc: 0.90840083360672  val: loss: 1442.2490234375 acc: 0.8381378650665283\n",
      "step: 9090\n",
      "train: loss: 668.445556640625 acc: 0.8409551382064819  val: loss: 970.6773071289062 acc: 0.8746675848960876\n",
      "step: 9095\n",
      "train: loss: 470.1711120605469 acc: 0.9472918510437012  val: loss: 1079.826416015625 acc: 0.82351154088974\n",
      "step: 9100\n",
      "train: loss: 742.3001098632812 acc: 0.848793625831604  val: loss: 984.5234985351562 acc: 0.856724739074707\n",
      "step: 9105\n",
      "train: loss: 682.858154296875 acc: 0.8365190029144287  val: loss: 1538.986083984375 acc: 0.8183920383453369\n",
      "step: 9110\n",
      "train: loss: 1202.0126953125 acc: 0.8049448728561401  val: loss: 1950.185546875 acc: 0.7665148973464966\n",
      "step: 9115\n",
      "train: loss: 988.3257446289062 acc: 0.7666345834732056  val: loss: 831.7799072265625 acc: 0.8737808465957642\n",
      "step: 9120\n",
      "train: loss: 429.88226318359375 acc: 0.9314233064651489  val: loss: 1183.5931396484375 acc: 0.8209773302078247\n",
      "step: 9125\n",
      "train: loss: 1106.9285888671875 acc: 0.7617289423942566  val: loss: 485.2120361328125 acc: 0.8763344287872314\n",
      "step: 9130\n",
      "train: loss: 550.039794921875 acc: 0.8797231316566467  val: loss: 1074.7581787109375 acc: 0.8614902496337891\n",
      "step: 9135\n",
      "train: loss: 768.0245971679688 acc: 0.8549909591674805  val: loss: 946.6234130859375 acc: 0.8715355396270752\n",
      "step: 9140\n",
      "train: loss: 581.1856079101562 acc: 0.8698597550392151  val: loss: 1092.891357421875 acc: 0.8002299070358276\n",
      "step: 9145\n",
      "train: loss: 418.1409606933594 acc: 0.9103167057037354  val: loss: 518.9136352539062 acc: 0.8947734236717224\n",
      "step: 9150\n",
      "train: loss: 561.3128051757812 acc: 0.9182270169258118  val: loss: 511.7929382324219 acc: 0.9317503571510315\n",
      "step: 9155\n",
      "train: loss: 644.3037109375 acc: 0.9042940735816956  val: loss: 1386.879150390625 acc: 0.806689977645874\n",
      "step: 9160\n",
      "train: loss: 871.2514038085938 acc: 0.8382983803749084  val: loss: 1342.15087890625 acc: 0.8514409065246582\n",
      "step: 9165\n",
      "train: loss: 659.6572265625 acc: 0.8983957767486572  val: loss: 1449.77587890625 acc: 0.8346167206764221\n",
      "step: 9170\n",
      "train: loss: 685.1602783203125 acc: 0.8743802309036255  val: loss: 509.90570068359375 acc: 0.9154770374298096\n",
      "step: 9175\n",
      "train: loss: 493.4997863769531 acc: 0.8314647078514099  val: loss: 2363.406494140625 acc: 0.7386001944541931\n",
      "step: 9180\n",
      "train: loss: 325.8489990234375 acc: 0.9428868889808655  val: loss: 725.8023071289062 acc: 0.8740320205688477\n",
      "step: 9185\n",
      "train: loss: 620.7911987304688 acc: 0.8787755966186523  val: loss: 1057.55419921875 acc: 0.8201199769973755\n",
      "step: 9190\n",
      "train: loss: 610.558837890625 acc: 0.897619903087616  val: loss: 1882.361572265625 acc: 0.8196529150009155\n",
      "step: 9195\n",
      "train: loss: 534.8492431640625 acc: 0.8783926963806152  val: loss: 1603.8018798828125 acc: 0.7886080741882324\n",
      "step: 9200\n",
      "train: loss: 515.1624145507812 acc: 0.9092801809310913  val: loss: 1403.7874755859375 acc: 0.8104734420776367\n",
      "step: 9205\n",
      "train: loss: 533.4288940429688 acc: 0.9255266785621643  val: loss: 884.671630859375 acc: 0.839442789554596\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 9210\n",
      "train: loss: 469.4143981933594 acc: 0.918169379234314  val: loss: 1157.838623046875 acc: 0.8320972919464111\n",
      "step: 9215\n",
      "train: loss: 495.0947570800781 acc: 0.9221024513244629  val: loss: 907.3358764648438 acc: 0.882439911365509\n",
      "step: 9220\n",
      "train: loss: 537.4715576171875 acc: 0.9134214520454407  val: loss: 481.8546142578125 acc: 0.920678973197937\n",
      "step: 9225\n",
      "train: loss: 797.101318359375 acc: 0.8655365705490112  val: loss: 1590.71630859375 acc: 0.7835911512374878\n",
      "step: 9230\n",
      "train: loss: 757.6064453125 acc: 0.8645948767662048  val: loss: 825.3831787109375 acc: 0.8734874129295349\n",
      "step: 9235\n",
      "train: loss: 814.8681030273438 acc: 0.8357874155044556  val: loss: 435.66864013671875 acc: 0.902341902256012\n",
      "step: 9240\n",
      "train: loss: 697.2196044921875 acc: 0.8836809992790222  val: loss: 950.3919677734375 acc: 0.8678218126296997\n",
      "step: 9245\n",
      "train: loss: 358.7632141113281 acc: 0.9432554841041565  val: loss: 704.6840209960938 acc: 0.8843609094619751\n",
      "step: 9250\n",
      "train: loss: 778.4578247070312 acc: 0.8515865802764893  val: loss: 1208.266845703125 acc: 0.821574330329895\n",
      "step: 9255\n",
      "train: loss: 407.9781188964844 acc: 0.8950273990631104  val: loss: 1167.660400390625 acc: 0.8425705432891846\n",
      "step: 9260\n",
      "train: loss: 817.0111694335938 acc: 0.8665478229522705  val: loss: 1217.9798583984375 acc: 0.7997970581054688\n",
      "step: 9265\n",
      "train: loss: 266.6224060058594 acc: 0.932260274887085  val: loss: 957.3555908203125 acc: 0.8669283390045166\n",
      "step: 9270\n",
      "train: loss: 637.5005493164062 acc: 0.9019955992698669  val: loss: 1034.6597900390625 acc: 0.8785800933837891\n",
      "step: 9275\n",
      "train: loss: 530.443603515625 acc: 0.9279149770736694  val: loss: 882.170654296875 acc: 0.8717848062515259\n",
      "step: 9280\n",
      "train: loss: 563.3576049804688 acc: 0.9280089139938354  val: loss: 1505.4725341796875 acc: 0.83062744140625\n",
      "step: 9285\n",
      "train: loss: 1200.6002197265625 acc: 0.7522350549697876  val: loss: 1106.9534912109375 acc: 0.8623139262199402\n",
      "step: 9290\n",
      "train: loss: 697.4873046875 acc: 0.8748577833175659  val: loss: 608.1022338867188 acc: 0.8798067569732666\n",
      "step: 9295\n",
      "train: loss: 909.90283203125 acc: 0.8400484919548035  val: loss: 1398.5460205078125 acc: 0.8175621628761292\n",
      "step: 9300\n",
      "train: loss: 965.984375 acc: 0.8561883568763733  val: loss: 917.706298828125 acc: 0.8508991599082947\n",
      "step: 9305\n",
      "train: loss: 552.3782348632812 acc: 0.8839139938354492  val: loss: 1650.60546875 acc: 0.8013535737991333\n",
      "step: 9310\n",
      "train: loss: 639.18994140625 acc: 0.8597999215126038  val: loss: 640.983642578125 acc: 0.8692874312400818\n",
      "step: 9315\n",
      "train: loss: 757.9258422851562 acc: 0.8614301681518555  val: loss: 1307.0364990234375 acc: 0.8272351026535034\n",
      "step: 9320\n",
      "train: loss: 683.7748413085938 acc: 0.7921563386917114  val: loss: 520.89013671875 acc: 0.8963543176651001\n",
      "step: 9325\n",
      "train: loss: 322.7782287597656 acc: 0.9392020106315613  val: loss: 633.1602783203125 acc: 0.8601354956626892\n",
      "step: 9330\n",
      "train: loss: 745.7102661132812 acc: 0.9195199012756348  val: loss: 1120.1776123046875 acc: 0.8375028967857361\n",
      "step: 9335\n",
      "train: loss: 461.26336669921875 acc: 0.9185876846313477  val: loss: 2998.771484375 acc: 0.708480715751648\n",
      "step: 9340\n",
      "train: loss: 499.5887145996094 acc: 0.9147893786430359  val: loss: 877.0155029296875 acc: 0.8592047691345215\n",
      "step: 9345\n",
      "train: loss: 765.3947143554688 acc: 0.8159607648849487  val: loss: 1793.7347412109375 acc: 0.7604133486747742\n",
      "step: 9350\n",
      "train: loss: 977.6762084960938 acc: 0.8365371823310852  val: loss: 2457.552001953125 acc: 0.7596616148948669\n",
      "step: 9355\n",
      "train: loss: 977.6595458984375 acc: 0.8419798612594604  val: loss: 646.87451171875 acc: 0.8690627217292786\n",
      "step: 9360\n",
      "train: loss: 602.6553955078125 acc: 0.9057495594024658  val: loss: 834.2666015625 acc: 0.8742867708206177\n",
      "step: 9365\n",
      "train: loss: 1035.5977783203125 acc: 0.7300146818161011  val: loss: 1376.798583984375 acc: 0.8044377565383911\n",
      "step: 9370\n",
      "train: loss: 582.2709350585938 acc: 0.8806167840957642  val: loss: 1586.3975830078125 acc: 0.8076156377792358\n",
      "step: 9375\n",
      "train: loss: 332.5660095214844 acc: 0.9035884141921997  val: loss: 1102.2581787109375 acc: 0.8609573841094971\n",
      "step: 9380\n",
      "train: loss: 611.8993530273438 acc: 0.8471757173538208  val: loss: 614.1161499023438 acc: 0.8732783198356628\n",
      "step: 9385\n",
      "train: loss: 446.18951416015625 acc: 0.9163727760314941  val: loss: 1796.03125 acc: 0.7797930240631104\n",
      "step: 9390\n",
      "train: loss: 409.5814514160156 acc: 0.9432157278060913  val: loss: 1432.5535888671875 acc: 0.8471799492835999\n",
      "step: 9395\n",
      "train: loss: 409.391357421875 acc: 0.9401163458824158  val: loss: 883.9822998046875 acc: 0.9102826714515686\n",
      "step: 9400\n",
      "train: loss: 688.0718994140625 acc: 0.8620328307151794  val: loss: 1515.7373046875 acc: 0.8132953643798828\n",
      "step: 9405\n",
      "train: loss: 465.845947265625 acc: 0.9182527661323547  val: loss: 1690.1878662109375 acc: 0.7684550881385803\n",
      "step: 9410\n",
      "train: loss: 855.9304809570312 acc: 0.8875944018363953  val: loss: 940.9476318359375 acc: 0.8763138055801392\n",
      "step: 9415\n",
      "train: loss: 591.3460693359375 acc: 0.8517482876777649  val: loss: 655.4334716796875 acc: 0.8827279210090637\n",
      "step: 9420\n",
      "train: loss: 706.13671875 acc: 0.8737210631370544  val: loss: 1552.3714599609375 acc: 0.7365970611572266\n",
      "step: 9425\n",
      "train: loss: 515.8379516601562 acc: 0.8995274305343628  val: loss: 589.063720703125 acc: 0.9191474914550781\n",
      "step: 9430\n",
      "train: loss: 845.5630493164062 acc: 0.7953183054924011  val: loss: 1464.6243896484375 acc: 0.8263243436813354\n",
      "step: 9435\n",
      "train: loss: 285.01751708984375 acc: 0.9442682266235352  val: loss: 665.3345336914062 acc: 0.8635084629058838\n",
      "step: 9440\n",
      "train: loss: 436.04547119140625 acc: 0.9180956482887268  val: loss: 785.70068359375 acc: 0.8896645307540894\n",
      "step: 9445\n",
      "train: loss: 602.8389892578125 acc: 0.8831153512001038  val: loss: 523.9625854492188 acc: 0.8892171382904053\n",
      "step: 9450\n",
      "train: loss: 957.2589721679688 acc: 0.8611763715744019  val: loss: 932.184326171875 acc: 0.8601768016815186\n",
      "step: 9455\n",
      "train: loss: 745.3491821289062 acc: 0.8925608396530151  val: loss: 1431.2254638671875 acc: 0.8195159435272217\n",
      "step: 9460\n",
      "train: loss: 1306.1986083984375 acc: 0.7842666506767273  val: loss: 981.9375 acc: 0.851963222026825\n",
      "step: 9465\n",
      "train: loss: 724.1224975585938 acc: 0.879080593585968  val: loss: 1771.86474609375 acc: 0.8171462416648865\n",
      "step: 9470\n",
      "train: loss: 1262.146728515625 acc: 0.7542033791542053  val: loss: 1344.63330078125 acc: 0.793524444103241\n",
      "step: 9475\n",
      "train: loss: 1044.7685546875 acc: 0.8164529800415039  val: loss: 1782.51318359375 acc: 0.7554253935813904\n",
      "step: 9480\n",
      "train: loss: 1121.38916015625 acc: 0.7984731197357178  val: loss: 1483.418701171875 acc: 0.7587116956710815\n",
      "step: 9485\n",
      "train: loss: 775.1341552734375 acc: 0.8414864540100098  val: loss: 1531.066162109375 acc: 0.8076974153518677\n",
      "step: 9490\n",
      "train: loss: 365.17889404296875 acc: 0.9401852488517761  val: loss: 1461.9166259765625 acc: 0.7812658548355103\n",
      "step: 9495\n",
      "train: loss: 598.4530029296875 acc: 0.8750643134117126  val: loss: 1862.42333984375 acc: 0.699657678604126\n",
      "step: 9500\n",
      "train: loss: 287.395751953125 acc: 0.9372729063034058  val: loss: 514.8278198242188 acc: 0.8978614807128906\n",
      "step: 9505\n",
      "train: loss: 338.3421936035156 acc: 0.9618491530418396  val: loss: 1015.7754516601562 acc: 0.8246497511863708\n",
      "step: 9510\n",
      "train: loss: 977.297607421875 acc: 0.8532103896141052  val: loss: 1995.967041015625 acc: 0.7600128054618835\n",
      "step: 9515\n",
      "train: loss: 1063.1802978515625 acc: 0.8468273282051086  val: loss: 824.3988647460938 acc: 0.8051785230636597\n",
      "step: 9520\n",
      "train: loss: 719.1553344726562 acc: 0.893135130405426  val: loss: 1299.307373046875 acc: 0.7805647850036621\n",
      "step: 9525\n",
      "train: loss: 1337.1986083984375 acc: 0.7755786776542664  val: loss: 759.3942260742188 acc: 0.8610951900482178\n",
      "step: 9530\n",
      "train: loss: 825.6133422851562 acc: 0.835010826587677  val: loss: 2378.16064453125 acc: 0.758335292339325\n",
      "step: 9535\n",
      "train: loss: 420.2216491699219 acc: 0.93857741355896  val: loss: 474.0531921386719 acc: 0.9162261486053467\n",
      "step: 9540\n",
      "train: loss: 351.0553894042969 acc: 0.9404459595680237  val: loss: 1027.8798828125 acc: 0.8016024231910706\n",
      "step: 9545\n",
      "train: loss: 510.1053161621094 acc: 0.8997783660888672  val: loss: 990.1451416015625 acc: 0.8955769538879395\n",
      "step: 9550\n",
      "train: loss: 391.2879638671875 acc: 0.929863691329956  val: loss: 992.7225341796875 acc: 0.8438026905059814\n",
      "step: 9555\n",
      "train: loss: 572.9826049804688 acc: 0.8464608192443848  val: loss: 2119.09423828125 acc: 0.7802349328994751\n",
      "step: 9560\n",
      "train: loss: 473.3760681152344 acc: 0.882053554058075  val: loss: 1327.347412109375 acc: 0.7978115677833557\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 9565\n",
      "train: loss: 312.3390808105469 acc: 0.9452136754989624  val: loss: 1562.0858154296875 acc: 0.7845171689987183\n",
      "step: 9570\n",
      "train: loss: 693.4005737304688 acc: 0.8647830486297607  val: loss: 889.2564697265625 acc: 0.8392196893692017\n",
      "step: 9575\n",
      "train: loss: 971.1431884765625 acc: 0.8573188781738281  val: loss: 1607.0361328125 acc: 0.8423718810081482\n",
      "step: 9580\n",
      "train: loss: 613.978759765625 acc: 0.882280707359314  val: loss: 1692.768798828125 acc: 0.7969653606414795\n",
      "step: 9585\n",
      "train: loss: 598.9357299804688 acc: 0.8861812949180603  val: loss: 657.6634521484375 acc: 0.8965150117874146\n",
      "step: 9590\n",
      "train: loss: 742.8358764648438 acc: 0.8379669785499573  val: loss: 1792.3336181640625 acc: 0.7690132260322571\n",
      "step: 9595\n",
      "train: loss: 947.204833984375 acc: 0.8588352203369141  val: loss: 1264.194580078125 acc: 0.7876793146133423\n",
      "step: 9600\n",
      "train: loss: 585.4906005859375 acc: 0.8914563059806824  val: loss: 1519.0164794921875 acc: 0.7756802439689636\n",
      "step: 9605\n",
      "train: loss: 429.3856201171875 acc: 0.8931345343589783  val: loss: 1066.666259765625 acc: 0.8624272346496582\n",
      "step: 9610\n",
      "train: loss: 440.88134765625 acc: 0.884801983833313  val: loss: 676.7905883789062 acc: 0.894930899143219\n",
      "step: 9615\n",
      "train: loss: 383.4928283691406 acc: 0.8981928825378418  val: loss: 1022.843994140625 acc: 0.8169415593147278\n",
      "step: 9620\n",
      "train: loss: 387.15142822265625 acc: 0.9364449977874756  val: loss: 1669.5361328125 acc: 0.775819718837738\n",
      "step: 9625\n",
      "train: loss: 489.15911865234375 acc: 0.9227769374847412  val: loss: 532.4492797851562 acc: 0.9027759432792664\n",
      "step: 9630\n",
      "train: loss: 781.79541015625 acc: 0.8777865767478943  val: loss: 976.91796875 acc: 0.8493369817733765\n",
      "step: 9635\n",
      "train: loss: 459.2890930175781 acc: 0.9227284789085388  val: loss: 1121.4649658203125 acc: 0.8474836945533752\n",
      "step: 9640\n",
      "train: loss: 598.9801635742188 acc: 0.9053575396537781  val: loss: 1491.3997802734375 acc: 0.7961321473121643\n",
      "step: 9645\n",
      "train: loss: 437.0025329589844 acc: 0.9203386902809143  val: loss: 671.0382690429688 acc: 0.8766526579856873\n",
      "step: 9650\n",
      "train: loss: 617.5758666992188 acc: 0.9172120690345764  val: loss: 700.2416381835938 acc: 0.885230541229248\n",
      "step: 9655\n",
      "train: loss: 678.6631469726562 acc: 0.8619245886802673  val: loss: 449.3013916015625 acc: 0.8823363184928894\n",
      "step: 9660\n",
      "train: loss: 800.3157348632812 acc: 0.8391646146774292  val: loss: 655.3465576171875 acc: 0.8941009640693665\n",
      "step: 9665\n",
      "train: loss: 947.4359741210938 acc: 0.83160001039505  val: loss: 510.1455383300781 acc: 0.8931750059127808\n",
      "step: 9670\n",
      "train: loss: 675.0109252929688 acc: 0.8276951313018799  val: loss: 1089.6927490234375 acc: 0.8280416131019592\n",
      "step: 9675\n",
      "train: loss: 379.573486328125 acc: 0.9072030186653137  val: loss: 1356.7752685546875 acc: 0.8169720768928528\n",
      "step: 9680\n",
      "train: loss: 826.10009765625 acc: 0.8268321752548218  val: loss: 1410.7808837890625 acc: 0.7883456945419312\n",
      "step: 9685\n",
      "train: loss: 665.4680786132812 acc: 0.8993386030197144  val: loss: 422.8243713378906 acc: 0.9090975522994995\n",
      "step: 9690\n",
      "train: loss: 856.2088012695312 acc: 0.8496595621109009  val: loss: 2418.570556640625 acc: 0.7139668464660645\n",
      "step: 9695\n",
      "train: loss: 892.3312377929688 acc: 0.8407855033874512  val: loss: 1165.7939453125 acc: 0.8591775894165039\n",
      "step: 9700\n",
      "train: loss: 843.9140014648438 acc: 0.8350280523300171  val: loss: 1325.0303955078125 acc: 0.7754155397415161\n",
      "step: 9705\n",
      "train: loss: 905.4324340820312 acc: 0.7725992202758789  val: loss: 1810.0570068359375 acc: 0.8088544607162476\n",
      "step: 9710\n",
      "train: loss: 412.2936706542969 acc: 0.9294449090957642  val: loss: 795.6829833984375 acc: 0.8686329126358032\n",
      "step: 9715\n",
      "train: loss: 774.9627075195312 acc: 0.8107552528381348  val: loss: 1264.4014892578125 acc: 0.8341449499130249\n",
      "step: 9720\n",
      "train: loss: 415.2479553222656 acc: 0.9327957630157471  val: loss: 2638.618408203125 acc: 0.6705514788627625\n",
      "step: 9725\n",
      "train: loss: 336.9046325683594 acc: 0.9216232895851135  val: loss: 411.2896728515625 acc: 0.9142980575561523\n",
      "step: 9730\n",
      "train: loss: 742.4933471679688 acc: 0.836584210395813  val: loss: 1155.03369140625 acc: 0.8042640686035156\n",
      "step: 9735\n",
      "train: loss: 888.3087158203125 acc: 0.7824313640594482  val: loss: 694.3378295898438 acc: 0.8630317449569702\n",
      "step: 9740\n",
      "train: loss: 606.311767578125 acc: 0.913144052028656  val: loss: 1650.9278564453125 acc: 0.7758715152740479\n",
      "step: 9745\n",
      "train: loss: 556.0578002929688 acc: 0.9213154315948486  val: loss: 614.968994140625 acc: 0.8769148588180542\n",
      "step: 9750\n",
      "train: loss: 648.2136840820312 acc: 0.8911889791488647  val: loss: 711.6094970703125 acc: 0.8867586255073547\n",
      "step: 9755\n",
      "train: loss: 742.4509887695312 acc: 0.8857854604721069  val: loss: 1331.196044921875 acc: 0.7840287685394287\n",
      "step: 9760\n",
      "train: loss: 592.0611572265625 acc: 0.9133744239807129  val: loss: 1533.03662109375 acc: 0.7804641723632812\n",
      "step: 9765\n",
      "train: loss: 966.2222900390625 acc: 0.8132820129394531  val: loss: 784.0302734375 acc: 0.890464186668396\n",
      "step: 9770\n",
      "train: loss: 503.9693603515625 acc: 0.9354854226112366  val: loss: 950.9915161132812 acc: 0.8578088879585266\n",
      "step: 9775\n",
      "train: loss: 606.1057739257812 acc: 0.899634838104248  val: loss: 497.8841552734375 acc: 0.9135311245918274\n",
      "step: 9780\n",
      "train: loss: 624.557861328125 acc: 0.8955737352371216  val: loss: 1706.841552734375 acc: 0.7393356561660767\n",
      "step: 9785\n",
      "train: loss: 402.10919189453125 acc: 0.8954102396965027  val: loss: 897.116455078125 acc: 0.8687204122543335\n",
      "step: 9790\n",
      "train: loss: 344.2296447753906 acc: 0.9405202865600586  val: loss: 544.6697387695312 acc: 0.9130544066429138\n",
      "step: 9795\n",
      "train: loss: 667.0868530273438 acc: 0.8646974563598633  val: loss: 1513.843994140625 acc: 0.8500185012817383\n",
      "step: 9800\n",
      "train: loss: 560.8803100585938 acc: 0.9004263281822205  val: loss: 1393.513427734375 acc: 0.8140789270401001\n",
      "step: 9805\n",
      "train: loss: 1064.2711181640625 acc: 0.8696669340133667  val: loss: 492.13214111328125 acc: 0.9208766222000122\n",
      "step: 9810\n",
      "train: loss: 1023.481689453125 acc: 0.7651265263557434  val: loss: 682.8192138671875 acc: 0.8846285939216614\n",
      "step: 9815\n",
      "train: loss: 1003.2027587890625 acc: 0.7997331619262695  val: loss: 720.7620239257812 acc: 0.8680016398429871\n",
      "step: 9820\n",
      "train: loss: 778.6162719726562 acc: 0.8623647093772888  val: loss: 711.9519653320312 acc: 0.8278043866157532\n",
      "step: 9825\n",
      "train: loss: 825.6692504882812 acc: 0.8493850231170654  val: loss: 959.2781982421875 acc: 0.8699889183044434\n",
      "step: 9830\n",
      "train: loss: 1180.7249755859375 acc: 0.8423574566841125  val: loss: 416.55535888671875 acc: 0.9322739243507385\n",
      "step: 9835\n",
      "train: loss: 541.7208862304688 acc: 0.9244557619094849  val: loss: 931.8711547851562 acc: 0.8260208368301392\n",
      "step: 9840\n",
      "train: loss: 355.4564208984375 acc: 0.8933367729187012  val: loss: 1734.0240478515625 acc: 0.7830148935317993\n",
      "step: 9845\n",
      "train: loss: 775.0426025390625 acc: 0.8529567718505859  val: loss: 1094.123291015625 acc: 0.846108078956604\n",
      "step: 9850\n",
      "train: loss: 328.5180358886719 acc: 0.9367403388023376  val: loss: 2472.442626953125 acc: 0.7634962201118469\n",
      "step: 9855\n",
      "train: loss: 578.6814575195312 acc: 0.8726532459259033  val: loss: 1224.4261474609375 acc: 0.8283663988113403\n",
      "step: 9860\n",
      "train: loss: 560.5427856445312 acc: 0.9213062524795532  val: loss: 891.1448974609375 acc: 0.8882429599761963\n",
      "step: 9865\n",
      "train: loss: 649.1090698242188 acc: 0.9005147814750671  val: loss: 2509.0927734375 acc: 0.711371123790741\n",
      "step: 9870\n",
      "train: loss: 927.761474609375 acc: 0.8669381737709045  val: loss: 1536.84228515625 acc: 0.7771420478820801\n",
      "step: 9875\n",
      "train: loss: 616.7586059570312 acc: 0.8900139927864075  val: loss: 776.6619873046875 acc: 0.882666826248169\n",
      "step: 9880\n",
      "train: loss: 962.5232543945312 acc: 0.830426812171936  val: loss: 1269.2965087890625 acc: 0.8124601244926453\n",
      "step: 9885\n",
      "train: loss: 876.8442993164062 acc: 0.7891944050788879  val: loss: 2681.764404296875 acc: 0.6860831379890442\n",
      "step: 9890\n",
      "train: loss: 837.833740234375 acc: 0.8814826011657715  val: loss: 756.642333984375 acc: 0.8552476167678833\n",
      "step: 9895\n",
      "train: loss: 831.837890625 acc: 0.8828617334365845  val: loss: 1933.6182861328125 acc: 0.7781762480735779\n",
      "step: 9900\n",
      "train: loss: 601.2313842773438 acc: 0.8885751962661743  val: loss: 1274.7794189453125 acc: 0.8126029968261719\n",
      "step: 9905\n",
      "train: loss: 472.9572448730469 acc: 0.8919991850852966  val: loss: 1530.727294921875 acc: 0.8254972696304321\n",
      "step: 9910\n",
      "train: loss: 537.3851928710938 acc: 0.9103884100914001  val: loss: 1172.6221923828125 acc: 0.8187252879142761\n",
      "step: 9915\n",
      "train: loss: 1132.072265625 acc: 0.8532808423042297  val: loss: 1872.450927734375 acc: 0.7482401728630066\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 9920\n",
      "train: loss: 1076.327392578125 acc: 0.8336434364318848  val: loss: 2047.62841796875 acc: 0.7329540252685547\n",
      "step: 9925\n",
      "train: loss: 481.42535400390625 acc: 0.9055980443954468  val: loss: 1691.3907470703125 acc: 0.7253077626228333\n",
      "step: 9930\n",
      "train: loss: 798.6771850585938 acc: 0.8409454822540283  val: loss: 1526.6142578125 acc: 0.7664470076560974\n",
      "step: 9935\n",
      "train: loss: 519.7805786132812 acc: 0.9067558646202087  val: loss: 884.3716430664062 acc: 0.8576140403747559\n",
      "step: 9940\n",
      "train: loss: 839.7199096679688 acc: 0.8386643528938293  val: loss: 1549.58349609375 acc: 0.7832158803939819\n",
      "step: 9945\n",
      "train: loss: 856.1129150390625 acc: 0.8671003580093384  val: loss: 1101.0050048828125 acc: 0.8374244570732117\n",
      "step: 9950\n",
      "train: loss: 455.7612609863281 acc: 0.9223410487174988  val: loss: 1920.9176025390625 acc: 0.6916379332542419\n",
      "step: 9955\n",
      "train: loss: 865.54052734375 acc: 0.8470934629440308  val: loss: 1859.4483642578125 acc: 0.7788092494010925\n",
      "step: 9960\n",
      "train: loss: 350.4779968261719 acc: 0.9427632689476013  val: loss: 1194.252685546875 acc: 0.8759918808937073\n",
      "step: 9965\n",
      "train: loss: 288.1669921875 acc: 0.9344608187675476  val: loss: 1165.57080078125 acc: 0.8372807502746582\n",
      "step: 9970\n",
      "train: loss: 404.9139099121094 acc: 0.8872061967849731  val: loss: 755.9281616210938 acc: 0.8605936169624329\n",
      "step: 9975\n",
      "train: loss: 361.0109558105469 acc: 0.9262765645980835  val: loss: 741.96875 acc: 0.897695779800415\n",
      "step: 9980\n",
      "train: loss: 1022.3493041992188 acc: 0.8435266613960266  val: loss: 1724.20556640625 acc: 0.7900444269180298\n",
      "step: 9985\n",
      "train: loss: 849.2064819335938 acc: 0.7876552939414978  val: loss: 742.2481689453125 acc: 0.8779951333999634\n",
      "step: 9990\n",
      "train: loss: 718.1389770507812 acc: 0.9035921692848206  val: loss: 1107.328857421875 acc: 0.8542742729187012\n",
      "step: 9995\n",
      "train: loss: 497.1622619628906 acc: 0.9218079447746277  val: loss: 961.4710083007812 acc: 0.8531575798988342\n"
     ]
    }
   ],
   "source": [
    "min_loss=10000\n",
    "sess=tf.Session()\n",
    "train_writer=tf.summary.FileWriter('D:/graph/wind-4-1/train/',sess.graph)\n",
    "test_writer = tf.summary.FileWriter('D:/graph/wind-4-1/test/', sess.graph)\n",
    "saver=tf.train.Saver(max_to_keep=3)\n",
    "\n",
    "init_op = tf.global_variables_initializer()\n",
    "sess.run(init_op)\n",
    "coord = tf.train.Coordinator()\n",
    "threads = tf.train.start_queue_runners(sess, coord)\n",
    "\n",
    "for training_itr in range(10000):\n",
    "    x1, y1 = sess.run([batch_xt,batch_yt])\n",
    "    feed_dict1 = {enc_inp: x1}\n",
    "    feed_dict1.update({expected_sparse_output: y1})\n",
    "    feed_dict1.update({pharse:True})\n",
    "    _, loss1,acc1,summaries1 = sess.run([train_op, loss,acc,merged_summary], feed_dict1)\n",
    "\n",
    "    train_writer.add_summary(summaries1, training_itr)\n",
    "    if training_itr %5==0:\n",
    "#             saver.save(sess=sess, save_path='model/hand_landmark_v6.1_model/model.ckpt',global_step=(global_step + 1))\n",
    "        mean_val_loss = 0\n",
    "\n",
    "        x2,y2=sess.run([batch_xv,batch_yv])\n",
    "        feed_dict2 = {enc_inp: x2}\n",
    "        feed_dict2.update({expected_sparse_output: y2})\n",
    "        feed_dict2.update({pharse:False})\n",
    "        loss2,acc2,summaries2 = sess.run([loss,acc,merged_summary], feed_dict2)\n",
    "\n",
    "        print('step: {}'.format(training_itr))\n",
    "        print('train: loss: {} acc: {}  val: loss: {} acc: {}'.format(loss1,acc1,loss2,acc2))\n",
    "        test_writer.add_summary(summaries2, training_itr)\n",
    "        if loss1 < min_loss:\n",
    "            min_loss=loss1\n",
    "            saver.save(sess=sess, save_path='D:/model/wind-4-1/model.ckpt',global_step=(training_itr + 1))\n",
    "sess.close()\n",
    "coord.request_stop()\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
